{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import pdb\n",
    "from six.moves import zip_longest\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.python import debug as tf_debug\n",
    "import _pickle as cPickle\n",
    "import random\n",
    "\n",
    "from data_structure import load_data\n",
    "from components import tf_log, encode_latents, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn, DiagonalGaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '2', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/apnews/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'sports', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 10, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 8, 'batch size')\n",
    "flags.DEFINE_integer('log_period', 100, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.01, 'lr')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 1.0, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_integer('warmup', 10, 'warmup period for KL')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 10, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 10, 'number of topic')\n",
    "flags.DEFINE_integer('dim_topic', 256, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_latent')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_output')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "config = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train examples: 39553\n"
     ]
    }
   ],
   "source": [
    "num_train_batches, train_batches, dev_batches, test_batches, word_to_idx, idx_to_word, bow_idxs = load_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('PAD_IDX', word_to_idx[PAD], 'PAD_IDX')\n",
    "flags.DEFINE_integer('UNK_IDX', word_to_idx[UNK], 'UNK_IDX')\n",
    "flags.DEFINE_integer('BOS_IDX', word_to_idx[BOS], 'BOS_IDX')\n",
    "flags.DEFINE_integer('EOS_IDX', word_to_idx[EOS], 'EOS_IDX')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_iterations = max([max([instance.max_sent_l for instance in batch]) for ct, batch in dev_batches])\n",
    "flags.DEFINE_integer('maximum_iterations', maximum_iterations, 'maximum_iterations')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build language model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train'):\n",
    "    bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "    \n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    doc_l_matrix = np.array([instance.doc_l for instance in batch]).astype(np.int32)\n",
    "\n",
    "    max_doc_l = np.max(doc_l_matrix)\n",
    "    max_sent_l = max([instance.max_sent_l for instance in batch])\n",
    "\n",
    "    token_idxs_matrix = np.zeros([batch_size, max_doc_l, max_sent_l], np.int32)\n",
    "    dec_input_idxs_matrix = np.zeros([batch_size, max_doc_l, max_sent_l+1], np.int32)\n",
    "    dec_target_idxs_matrix = np.zeros([batch_size, max_doc_l, max_sent_l+1], np.int32)\n",
    "    sent_l_matrix = np.zeros([batch_size, max_doc_l], np.int32)\n",
    "    dec_sent_l_matrix = np.zeros([batch_size, max_doc_l], np.int32)\n",
    "\n",
    "    for i, instance in enumerate(batch):\n",
    "        for j, sent_idxs in enumerate(instance.token_idxs):\n",
    "            token_idxs_matrix[i, j, :len(sent_idxs)] = np.asarray(sent_idxs)\n",
    "            \n",
    "            sent_idxs_dropout = np.asarray(sent_idxs)\n",
    "            sent_idxs_dropout[np.random.rand(len(sent_idxs)) > config.word_keep_prob] = config.UNK_IDX\n",
    "            dec_input_idxs_matrix[i, j, :len(sent_idxs)+1] = np.concatenate([[config.BOS_IDX], sent_idxs_dropout])\n",
    "            \n",
    "            dec_target_idxs_matrix[i, j, :len(sent_idxs)+1] = np.asarray(sent_idxs + [config.EOS_IDX])\n",
    "            sent_l_matrix[i, j] = len(sent_idxs)\n",
    "            dec_sent_l_matrix[i, j] = len(sent_idxs)+1\n",
    "\n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "\n",
    "    feed_dict = {\n",
    "                t_variables['bow']: bow, t_variables['token_idxs']: token_idxs_matrix,\n",
    "                t_variables['dec_input_idxs']: dec_input_idxs_matrix, t_variables['dec_target_idxs']: dec_target_idxs_matrix, \n",
    "                t_variables['batch_l']: batch_size, t_variables['doc_l']: doc_l_matrix, t_variables['sent_l']: sent_l_matrix, t_variables['dec_sent_l']: dec_sent_l_matrix,\n",
    "                t_variables['max_doc_l']: max_doc_l, t_variables['max_sent_l']: max_sent_l, \n",
    "                t_variables['keep_prob']: keep_prob}\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_shape(variables):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        sample_batch = test_batches[0][1]\n",
    "        feed_dict = get_feed_dict(sample_batch)\n",
    "        _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "            \n",
    "        sess.close()\n",
    "        \n",
    "def debug_value(variables):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        sample_batch = test_batches[0][1]\n",
    "        feed_dict = get_feed_dict(sample_batch)\n",
    "        _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            print(variable.name, ':', _variable)\n",
    "            \n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fed variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['bow'] = tf.placeholder(tf.float32, [None, config.dim_bow])\n",
    "t_variables['token_idxs'] = tf.placeholder(tf.int32, [None, None, None])\n",
    "t_variables['dec_input_idxs'] = tf.placeholder(tf.int32, [None, None, None])\n",
    "t_variables['dec_target_idxs'] = tf.placeholder(tf.int32, [None, None, None])\n",
    "t_variables['batch_l'] = tf.placeholder(tf.int32, [])\n",
    "t_variables['doc_l'] = tf.placeholder(tf.int32, [None])\n",
    "t_variables['sent_l'] = tf.placeholder(tf.int32, [None, None])\n",
    "t_variables['dec_sent_l'] = tf.placeholder(tf.int32, [None, None])\n",
    "t_variables['max_doc_l'] = tf.placeholder(tf.int32, [])\n",
    "t_variables['max_sent_l'] = tf.placeholder(tf.int32, [])\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trained variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = t_variables['bow']\n",
    "with tf.variable_scope('topic/enc', reuse=False):\n",
    "    means_bow, logvars_bow = encode_latents(bow, dim=config.dim_topic, name='latent') # encode to parameters of gaussian distribution\n",
    "    latents_bow = sample_latents(means_bow, logvars_bow) # sample latent vectors\n",
    "    hidden_bow = tf.layers.Dense(units=config.dim_topic, activation=tf.nn.relu, name='hidden')(latents_bow)\n",
    "    prob_topic = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob')(hidden_bow) # inference of topic probabilities\n",
    "\n",
    "with tf.variable_scope('shared', reuse=False):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "bow_embeddings = tf.nn.embedding_lookup(embeddings, bow_idxs) # embeddings of each bow features\n",
    "    \n",
    "with tf.variable_scope('topic/dec', reuse=False):\n",
    "    topic_embeddings = tf.get_variable('topic_emb', [config.n_topic, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of topics\n",
    "    \n",
    "    topic_bow = tf.nn.softmax(tf.matmul(topic_embeddings, bow_embeddings, transpose_b=True), 1) # bow vectors for each topic\n",
    "    prob_bow = tf_log(tf.matmul(prob_topic, topic_bow)) # predicted bow distribution\n",
    "    \n",
    "topic_loss_recon = -tf.reduce_mean(tf.reduce_sum(tf.multiply(bow, prob_bow), 1)) # negative log likelihood of each words\n",
    "topic_loss_kl = compute_kl_loss(means_bow, logvars_bow) # KL divergence b/w latent dist & gaussian std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder:0 : (8, 4022)\n",
      "topic/enc/add:0 : (8, 256)\n",
      "topic/enc/hidden/Relu:0 : (8, 256)\n",
      "topic/enc/prob/Softmax:0 : (8, 10)\n",
      "shared/emb:0 : (29743, 256)\n",
      "embedding_lookup:0 : (4022, 256)\n",
      "topic/dec/topic_emb:0 : (10, 256)\n",
      "topic/dec/Softmax:0 : (10, 4022)\n",
      "topic/dec/Log:0 : (8, 4022)\n",
      "Neg:0 : ()\n"
     ]
    }
   ],
   "source": [
    "debug_shape([bow, latents_bow, hidden_bow, prob_topic, embeddings, bow_embeddings, topic_embeddings, topic_bow, prob_bow, topic_loss_recon])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum_2:0 : [1.0000001  1.         1.         0.9999999  0.99999994 1.\n",
      " 1.         1.        ]\n",
      "Sum_3:0 : [0.9999999  0.99999994 1.         0.9999999  1.         1.\n",
      " 0.99999994 1.         1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "debug_value([tf.reduce_sum(prob_topic, -1), tf.reduce_sum(topic_bow, -1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_bow = tf.exp(0.5 * logvars_bow)\n",
    "dist_bow = tfd.Normal(means_bow, sigma_bow)\n",
    "dist_std = tfd.Normal(0., 1.)\n",
    "topic_loss_kl_tmp = tf.reduce_mean(tf.reduce_sum(tfd.kl_divergence(dist_bow, dist_std), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg:0 : 433.66687\n",
      "Mean_1:0 : 3.8835375\n",
      "Mean_2:0 : 3.883537\n"
     ]
    }
   ],
   "source": [
    "debug_value([topic_loss_recon, topic_loss_kl, topic_loss_kl_tmp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "batch_l = t_variables['batch_l']\n",
    "max_doc_l = t_variables['max_doc_l']\n",
    "max_sent_l = t_variables['max_sent_l']\n",
    "token_idxs = t_variables['token_idxs'][:, :max_doc_l, :max_sent_l]\n",
    "\n",
    "# get word embedding\n",
    "with tf.variable_scope('sent/enc', reuse=False):\n",
    "    enc_input = tf.nn.embedding_lookup(embeddings, token_idxs)\n",
    "    enc_input_do = tf.reshape(enc_input, [batch_l * max_doc_l, max_sent_l, config.dim_emb])\n",
    "\n",
    "    # get sentence embedding\n",
    "    sent_l = t_variables['sent_l']\n",
    "    sent_l_do = tf.reshape(sent_l, [batch_l * max_doc_l])\n",
    "\n",
    "    _, enc_state_do = dynamic_rnn(enc_input_do, sent_l_do, config.dim_hidden, t_variables['keep_prob'])\n",
    "    enc_state = tf.reshape(enc_state_do, [batch_l, max_doc_l, config.dim_hidden])\n",
    "\n",
    "    # TODO House Holder flow\n",
    "    prob_topic_sent = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax)(enc_state)\n",
    "    \n",
    "    # inference of each gaussian dist. parameter\n",
    "    enc_state_topic = tf.tile(tf.expand_dims(enc_state, 2), [1, 1, config.n_topic, 1]) # tile over topics\n",
    "    means_sent_topic, logvars_sent_topic = encode_latents(enc_state_topic, dim=config.dim_latent, name='latent') \n",
    "    \n",
    "    # latent vectors from each gaussian dist.\n",
    "    latents_sent_topic = sample_latents(means_sent_topic, logvars_sent_topic) \n",
    "    # latent vector from gaussian mixture    \n",
    "    latents_sent = tf.squeeze(tf.matmul(tf.expand_dims(prob_topic_sent, -1), latents_sent_topic, transpose_a=True), 2) \n",
    "    latents_sent_do = tf.reshape(latents_sent, [batch_l * max_doc_l, config.dim_latent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strided_slice:0 : (8, 6, 48)\n",
      "sent/enc/embedding_lookup:0 : (8, 6, 48, 256)\n",
      "sent/enc/Reshape_2:0 : (8, 6, 512)\n"
     ]
    }
   ],
   "source": [
    "debug_shape([token_idxs, enc_input, enc_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent/enc/dense/Reshape_1:0 : (8, 6, 10)\n",
      "sent/enc/Tile:0 : (8, 6, 10, 512)\n",
      "sent/enc/add:0 : (8, 6, 10, 32)\n",
      "sent/enc/Squeeze:0 : (8, 6, 32)\n",
      "sent/enc/Reshape_3:0 : (48, 32)\n"
     ]
    }
   ],
   "source": [
    "debug_shape([prob_topic_sent, enc_state_topic, latents_sent_topic, latents_sent, latents_sent_do])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum_5:0 : [[0.99999994 0.99999994 1.         0.99999994 1.         1.        ]\n",
      " [1.0000001  1.         0.99999994 0.9999999  0.99999994 1.        ]\n",
      " [0.99999994 0.99999994 1.         1.         1.         1.        ]\n",
      " [0.99999994 1.         1.         1.         1.         0.9999999 ]\n",
      " [1.0000001  0.99999994 0.99999994 1.         1.         1.        ]\n",
      " [1.         0.99999994 0.99999994 1.         0.99999994 1.        ]\n",
      " [1.0000001  1.         0.99999994 1.         1.         0.99999994]\n",
      " [1.         1.         1.         1.         1.0000001  1.0000001 ]]\n"
     ]
    }
   ],
   "source": [
    "debug_value([tf.reduce_sum(prob_topic_sent, -1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for decoding\n",
    "dec_input_idxs = t_variables['dec_input_idxs']\n",
    "dec_input_idxs_do = tf.reshape(dec_input_idxs, [batch_l * max_doc_l, max_sent_l+1])\n",
    "dec_input_do = tf.nn.embedding_lookup(embeddings, dec_input_idxs_do)\n",
    "\n",
    "dec_latent_input_do = tf.tile(tf.expand_dims(latents_sent_do, 1), [1, tf.shape(dec_input_do)[1], 1])\n",
    "dec_concat_input_do = tf.concat([dec_input_do, dec_latent_input_do], 2)\n",
    "\n",
    "# decode for training\n",
    "dec_sent_l = t_variables['dec_sent_l']\n",
    "dec_sent_l_do = tf.reshape(dec_sent_l, [batch_l * max_doc_l])\n",
    "\n",
    "with tf.variable_scope('sent/dec/rnn', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32, reuse=tf.AUTO_REUSE):\n",
    "    dec_cell = tf.contrib.rnn.GRUCell(config.dim_hidden)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob = t_variables['keep_prob'])\n",
    "\n",
    "    dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu)(latents_sent_do)\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_concat_input_do, sequence_length=dec_sent_l_do)\n",
    "\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=dec_cell,\n",
    "        helper=helper,\n",
    "        initial_state=dec_initial_state)\n",
    "\n",
    "    dec_outputs, _, output_sent_l = tf.contrib.seq2seq.dynamic_decode(train_decoder)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(config.n_vocab, use_bias=False, name='out')\n",
    "    output_logits = output_layer(dec_outputs.rnn_output)\n",
    "    \n",
    "    output_token_idxs_do = tf.argmax(output_logits, 2)\n",
    "    output_token_idxs = tf.reshape(output_token_idxs_do, [batch_l, max_doc_l, tf.shape(output_token_idxs_do)[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder_2:0 : (8, 6, 49)\n",
      "concat:0 : (48, 49, 288)\n",
      "sent/dec/rnn/dense/Relu:0 : (48, 512)\n",
      "sent/dec/rnn/out/Tensordot:0 : (48, 49, 29743)\n",
      "sent/dec/rnn/Reshape:0 : (8, 6, 49)\n"
     ]
    }
   ],
   "source": [
    "debug_shape([dec_input_idxs, dec_concat_input_do, dec_initial_state, output_logits, output_token_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language modeling cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target and mask\n",
    "dec_target_idxs = t_variables['dec_target_idxs']\n",
    "dec_target_idxs_do = tf.reshape(dec_target_idxs, [batch_l * max_doc_l, max_sent_l+1])                \n",
    "dec_mask_tokens_do = tf.sequence_mask(dec_sent_l_do, maxlen=max_sent_l+1, dtype=tf.float32)\n",
    "\n",
    "# nll for each token (averaged over batch & sentence)\n",
    "sent_loss_recon = tf.contrib.seq2seq.sequence_loss(output_logits, dec_target_idxs_do, dec_mask_tokens_do) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferred mixture probabilities (computed for each sentence)\n",
    "categ_topic_sent = tfd.Categorical(probs=prob_topic_sent)\n",
    "\n",
    "# prior of mixture probabilities (computed for each document)\n",
    "categ_topic = tfd.Categorical(probs=tf.expand_dims(prob_topic, 1))\n",
    "\n",
    "sent_loss_kl_categ = tf.reduce_mean(tfd.kl_divergence(categ_topic_sent, categ_topic))\n",
    "\n",
    "# prior of each gaussian gaussribution (computed for each topic)\n",
    "with tf.variable_scope('topic/enc', reuse=False):\n",
    "    means_topic_bow, logvars_topic_bow = encode_latents(topic_bow, dim=config.dim_latent, name='prior')\n",
    "sigma_topic_bow = tf.exp(0.5 * logvars_topic_bow)\n",
    "gauss_topic_bow = tfd.Normal(loc=means_topic_bow, scale=sigma_topic_bow)\n",
    "\n",
    "# inference of each gaussian gaussribution (computed for each sentence)\n",
    "sigma_sent_topic = tf.exp(0.5 * logvars_sent_topic)\n",
    "gauss_sent_topic = tfd.Normal(loc=means_sent_topic, scale=sigma_sent_topic)\n",
    "\n",
    "sent_loss_kl_gauss = tf.reduce_sum(tfd.kl_divergence(gauss_sent_topic, gauss_topic_bow), -1)\n",
    "sent_loss_kl_gmm = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_sent, sent_loss_kl_gauss), -1))\n",
    "\n",
    "sent_loss_kl = sent_loss_kl_categ + sent_loss_kl_gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_3:0 : 0.6479989\n",
      "Mean_5:0 : 0.647999\n"
     ]
    }
   ],
   "source": [
    "# kl loss for categorical distribution\n",
    "sent_loss_kl_categ_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_sent, tf_log(prob_topic_sent/tf.expand_dims(prob_topic, 1))), -1)) \n",
    "debug_value([sent_loss_kl_categ, sent_loss_kl_categ_tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent/enc/dense/Reshape_1:0 : (8, 6, 10)\n",
      "Sum_6:0 : (8, 6, 10)\n",
      "Mean_4:0 : ()\n"
     ]
    }
   ],
   "source": [
    "debug_shape([prob_topic_sent, sent_loss_kl_gauss, sent_loss_kl_gmm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_loss/truediv:0 : 10.3004675\n",
      "add_4:0 : 0.26921743\n"
     ]
    }
   ],
   "source": [
    "debug_value([sent_loss_recon, sent_loss_kl])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.warmup > 0:\n",
    "    beta = tf.Variable(0.1, name='beta', trainable=False)\n",
    "\n",
    "topic_loss = topic_loss_recon + beta * topic_loss_kl\n",
    "sent_loss = sent_loss_recon + beta * sent_loss_kl\n",
    "loss = topic_loss + sent_loss\n",
    "\n",
    "# define optimizer\n",
    "if (config.opt == 'Adam'):\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif (config.opt == 'Adagrad'):\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "    \n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_sents(token_idxs, config, idx_to_word):\n",
    "    sents = []\n",
    "    for line_idxs in token_idxs:\n",
    "        tokens = []\n",
    "        for idx in line_idxs:\n",
    "            if idx == config.EOS_IDX: break\n",
    "            tokens.append(idx_to_word[idx])\n",
    "        sent = ' '.join(tokens)\n",
    "        sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch = sess.run(loss, feed_dict = feed_dict)\n",
    "        losses += [loss_batch]        \n",
    "    loss_mean = np.mean(losses)\n",
    "    return loss_mean\n",
    "\n",
    "def get_all_losses(sess, batches):\n",
    "    losses = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch = \\\n",
    "        sess.run([loss, topic_loss_recon, topic_loss_kl, sent_loss_recon, sent_loss_kl], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "    print('LOSS %.2f | TM NLL: %.2f, KL: %.4f | LM NLL: %.2f, KL: %.4f' %  np.mean(losses, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_batch):\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    pred_token_idxs_batch = sess.run(output_token_idxs, feed_dict = feed_dict)\n",
    "    true_token_idxs_batch = [instance.token_idxs for instance in sample_batch]\n",
    "    \n",
    "    assert len(pred_token_idxs_batch) == len(true_token_idxs_batch)\n",
    "    \n",
    "    for true_token_idxs, pred_token_idxs in zip(true_token_idxs_batch, pred_token_idxs_batch):\n",
    "        true_sents = idxs_to_sents(true_token_idxs, config, idx_to_word)\n",
    "        pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "        assert len(true_sents) == len(pred_sents)\n",
    "        \n",
    "        for true_sent, pred_sent in zip(true_sents, pred_sents):\n",
    "            print('True: %s' % true_sent)\n",
    "            print('Pred: %s' % pred_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "logs = []\n",
    "losses_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 0.\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.tools.api.generator.api.train' has no attribute 'SummaryWriter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-7b8f6ebd3439>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../log'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.tools.api.generator.api.train' has no attribute 'SummaryWriter'"
     ]
    }
   ],
   "source": [
    "tf.train.SummaryWriter('../log', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0 | LOSS TRAIN: 449.13, DEV: 699.12 | TM NLL: 438.83, KL: 3.3189 | LM NLL: 10.30, KL: 0.3819 | BETA: 0.000000\n",
      "Epoch: 0, Step: 100 | LOSS TRAIN: 464.78, DEV: 676.30 | TM NLL: 454.48, KL: 4.1655 | LM NLL: 10.30, KL: 2.7532 | BETA: 0.000253\n",
      "Epoch: 0, Step: 200 | LOSS TRAIN: 451.42, DEV: 662.42 | TM NLL: 441.12, KL: 4.2104 | LM NLL: 10.30, KL: 4.5798 | BETA: 0.000506\n",
      "Epoch: 0, Step: 300 | LOSS TRAIN: 453.84, DEV: 660.69 | TM NLL: 443.55, KL: 4.3519 | LM NLL: 10.29, KL: 5.3766 | BETA: 0.000758\n",
      "Epoch: 0, Step: 400 | LOSS TRAIN: 456.45, DEV: 657.62 | TM NLL: 446.30, KL: 4.4431 | LM NLL: 10.14, KL: 6.3616 | BETA: 0.001011\n",
      "Epoch: 0, Step: 500 | LOSS TRAIN: 456.73, DEV: 656.70 | TM NLL: 447.00, KL: 4.5060 | LM NLL: 9.72, KL: 14.9999 | BETA: 0.001264\n",
      "Epoch: 0, Step: 600 | LOSS TRAIN: 457.90, DEV: 656.25 | TM NLL: 448.51, KL: 4.5352 | LM NLL: 9.36, KL: 22.2632 | BETA: 0.001517\n",
      "Epoch: 0, Step: 700 | LOSS TRAIN: 455.48, DEV: 656.38 | TM NLL: 446.37, KL: 4.5211 | LM NLL: 9.07, KL: 26.2474 | BETA: 0.001770\n",
      "Epoch: 0, Step: 800 | LOSS TRAIN: 457.93, DEV: 655.70 | TM NLL: 449.05, KL: 4.6336 | LM NLL: 8.83, KL: 28.1796 | BETA: 0.002023\n",
      "Epoch: 0, Step: 900 | LOSS TRAIN: 457.13, DEV: 655.58 | TM NLL: 448.44, KL: 4.6037 | LM NLL: 8.64, KL: 29.0281 | BETA: 0.002275\n",
      "Epoch: 0, Step: 1000 | LOSS TRAIN: 455.11, DEV: 655.59 | TM NLL: 446.57, KL: 4.5567 | LM NLL: 8.48, KL: 29.4190 | BETA: 0.002528\n",
      "Epoch: 0, Step: 1100 | LOSS TRAIN: 454.84, DEV: 655.40 | TM NLL: 446.44, KL: 4.5134 | LM NLL: 8.35, KL: 29.4780 | BETA: 0.002781\n",
      "Epoch: 0, Step: 1200 | LOSS TRAIN: 452.90, DEV: 655.41 | TM NLL: 444.61, KL: 4.4560 | LM NLL: 8.23, KL: 29.3005 | BETA: 0.003034\n",
      "Epoch: 0, Step: 1300 | LOSS TRAIN: 456.21, DEV: 655.24 | TM NLL: 448.02, KL: 4.4655 | LM NLL: 8.13, KL: 28.9798 | BETA: 0.003287\n",
      "Epoch: 0, Step: 1400 | LOSS TRAIN: 456.60, DEV: 655.08 | TM NLL: 448.50, KL: 4.4359 | LM NLL: 8.04, KL: 28.5608 | BETA: 0.003540\n",
      "Epoch: 0, Step: 1500 | LOSS TRAIN: 455.72, DEV: 655.13 | TM NLL: 447.70, KL: 4.3886 | LM NLL: 7.96, KL: 28.0833 | BETA: 0.003792\n",
      "Epoch: 0, Step: 1600 | LOSS TRAIN: 455.70, DEV: 654.85 | TM NLL: 447.74, KL: 4.3538 | LM NLL: 7.89, KL: 27.5544 | BETA: 0.004045\n",
      "Epoch: 0, Step: 1700 | LOSS TRAIN: 454.05, DEV: 654.83 | TM NLL: 446.16, KL: 4.2911 | LM NLL: 7.83, KL: 27.0109 | BETA: 0.004298\n",
      "Epoch: 0, Step: 1800 | LOSS TRAIN: 454.02, DEV: 654.85 | TM NLL: 446.18, KL: 4.2436 | LM NLL: 7.77, KL: 26.4494 | BETA: 0.004551\n",
      "Epoch: 0, Step: 1900 | LOSS TRAIN: 455.14, DEV: 654.63 | TM NLL: 447.35, KL: 4.2199 | LM NLL: 7.72, KL: 25.9109 | BETA: 0.004804\n",
      "Epoch: 0, Step: 2000 | LOSS TRAIN: 456.61, DEV: 654.66 | TM NLL: 448.86, KL: 4.1913 | LM NLL: 7.67, KL: 25.3736 | BETA: 0.005057\n",
      "Epoch: 0, Step: 2100 | LOSS TRAIN: 457.59, DEV: 654.62 | TM NLL: 449.89, KL: 4.1606 | LM NLL: 7.63, KL: 24.8424 | BETA: 0.005309\n",
      "Epoch: 0, Step: 2200 | LOSS TRAIN: 460.03, DEV: 654.47 | TM NLL: 452.37, KL: 4.1432 | LM NLL: 7.59, KL: 24.3324 | BETA: 0.005562\n",
      "Epoch: 0, Step: 2300 | LOSS TRAIN: 460.51, DEV: 654.39 | TM NLL: 452.87, KL: 4.1032 | LM NLL: 7.56, KL: 23.8438 | BETA: 0.005815\n",
      "Epoch: 0, Step: 2400 | LOSS TRAIN: 461.65, DEV: 654.32 | TM NLL: 454.05, KL: 4.0665 | LM NLL: 7.52, KL: 23.3672 | BETA: 0.006068\n",
      "Epoch: 0, Step: 2500 | LOSS TRAIN: 462.39, DEV: 654.20 | TM NLL: 454.82, KL: 4.0274 | LM NLL: 7.49, KL: 22.9050 | BETA: 0.006321\n",
      "Epoch: 0, Step: 2600 | LOSS TRAIN: 462.07, DEV: 654.44 | TM NLL: 454.52, KL: 3.9823 | LM NLL: 7.46, KL: 22.4722 | BETA: 0.006573\n",
      "Epoch: 0, Step: 2700 | LOSS TRAIN: 463.01, DEV: 654.39 | TM NLL: 455.49, KL: 3.9457 | LM NLL: 7.44, KL: 22.0444 | BETA: 0.006826\n",
      "Epoch: 0, Step: 2800 | LOSS TRAIN: 462.91, DEV: 654.43 | TM NLL: 455.41, KL: 3.9015 | LM NLL: 7.41, KL: 21.6327 | BETA: 0.007079\n",
      "Epoch: 0, Step: 2900 | LOSS TRAIN: 463.22, DEV: 654.46 | TM NLL: 455.76, KL: 3.8627 | LM NLL: 7.39, KL: 21.2308 | BETA: 0.007332\n",
      "Epoch: 0, Step: 3000 | LOSS TRAIN: 463.08, DEV: 654.42 | TM NLL: 455.63, KL: 3.8212 | LM NLL: 7.37, KL: 20.8406 | BETA: 0.007585\n",
      "Epoch: 0, Step: 3100 | LOSS TRAIN: 462.39, DEV: 654.45 | TM NLL: 454.97, KL: 3.7756 | LM NLL: 7.34, KL: 20.4722 | BETA: 0.007838\n",
      "Epoch: 0, Step: 3200 | LOSS TRAIN: 462.86, DEV: 654.32 | TM NLL: 455.45, KL: 3.7421 | LM NLL: 7.33, KL: 20.1126 | BETA: 0.008090\n",
      "Epoch: 0, Step: 3300 | LOSS TRAIN: 463.29, DEV: 654.28 | TM NLL: 455.90, KL: 3.7064 | LM NLL: 7.31, KL: 19.7638 | BETA: 0.008343\n",
      "Epoch: 0, Step: 3400 | LOSS TRAIN: 462.78, DEV: 654.41 | TM NLL: 455.41, KL: 3.6654 | LM NLL: 7.29, KL: 19.4301 | BETA: 0.008596\n",
      "Epoch: 0, Step: 3500 | LOSS TRAIN: 461.56, DEV: 654.75 | TM NLL: 454.20, KL: 3.6229 | LM NLL: 7.27, KL: 19.1075 | BETA: 0.008849\n",
      "Epoch: 0, Step: 3600 | LOSS TRAIN: 461.05, DEV: 654.63 | TM NLL: 453.71, KL: 3.5855 | LM NLL: 7.26, KL: 18.7968 | BETA: 0.009102\n",
      "Epoch: 0, Step: 3700 | LOSS TRAIN: 460.85, DEV: 654.45 | TM NLL: 453.53, KL: 3.5526 | LM NLL: 7.24, KL: 18.4942 | BETA: 0.009355\n",
      "Epoch: 0, Step: 3800 | LOSS TRAIN: 460.42, DEV: 654.44 | TM NLL: 453.11, KL: 3.5191 | LM NLL: 7.23, KL: 18.2004 | BETA: 0.009607\n",
      "Epoch: 0, Step: 3900 | LOSS TRAIN: 460.42, DEV: 654.47 | TM NLL: 453.12, KL: 3.4903 | LM NLL: 7.21, KL: 17.9155 | BETA: 0.009860\n",
      "Epoch: 0, Step: 4000 | LOSS TRAIN: 459.71, DEV: 654.57 | TM NLL: 452.43, KL: 3.4599 | LM NLL: 7.20, KL: 17.6471 | BETA: 0.010113\n",
      "Epoch: 0, Step: 4100 | LOSS TRAIN: 459.42, DEV: 654.59 | TM NLL: 452.15, KL: 3.4345 | LM NLL: 7.19, KL: 17.3867 | BETA: 0.010366\n",
      "Epoch: 0, Step: 4200 | LOSS TRAIN: 459.48, DEV: 654.41 | TM NLL: 452.22, KL: 3.4103 | LM NLL: 7.18, KL: 17.1272 | BETA: 0.010619\n",
      "Epoch: 0, Step: 4300 | LOSS TRAIN: 459.70, DEV: 654.33 | TM NLL: 452.45, KL: 3.3874 | LM NLL: 7.16, KL: 16.8781 | BETA: 0.010871\n",
      "Epoch: 0, Step: 4400 | LOSS TRAIN: 459.16, DEV: 654.36 | TM NLL: 451.92, KL: 3.3629 | LM NLL: 7.15, KL: 16.6369 | BETA: 0.011124\n",
      "Epoch: 0, Step: 4500 | LOSS TRAIN: 458.43, DEV: 654.54 | TM NLL: 451.20, KL: 3.3399 | LM NLL: 7.14, KL: 16.4009 | BETA: 0.011377\n",
      "Epoch: 0, Step: 4600 | LOSS TRAIN: 459.50, DEV: 654.22 | TM NLL: 452.28, KL: 3.3315 | LM NLL: 7.13, KL: 16.1690 | BETA: 0.011630\n",
      "Epoch: 0, Step: 4700 | LOSS TRAIN: 460.68, DEV: 654.02 | TM NLL: 453.47, KL: 3.3257 | LM NLL: 7.12, KL: 15.9453 | BETA: 0.011883\n",
      "Epoch: 0, Step: 4800 | LOSS TRAIN: 460.60, DEV: 653.97 | TM NLL: 453.40, KL: 3.3111 | LM NLL: 7.11, KL: 15.7322 | BETA: 0.012136\n",
      "Epoch: 0, Step: 4900 | LOSS TRAIN: 460.23, DEV: 654.23 | TM NLL: 453.04, KL: 3.2959 | LM NLL: 7.11, KL: 15.5225 | BETA: 0.012388\n",
      "Epoch: 0, Step: 5000 | LOSS TRAIN: 460.53, DEV: 654.05 | TM NLL: 453.35, KL: 3.2860 | LM NLL: 7.10, KL: 15.3205 | BETA: 0.012641\n",
      "Epoch: 0, Step: 5100 | LOSS TRAIN: 459.70, DEV: 654.24 | TM NLL: 452.52, KL: 3.2695 | LM NLL: 7.09, KL: 15.1271 | BETA: 0.012894\n",
      "Epoch: 0, Step: 5200 | LOSS TRAIN: 459.65, DEV: 654.19 | TM NLL: 452.48, KL: 3.2582 | LM NLL: 7.08, KL: 14.9346 | BETA: 0.013147\n",
      "Epoch: 0, Step: 5300 | LOSS TRAIN: 459.12, DEV: 654.36 | TM NLL: 451.96, KL: 3.2455 | LM NLL: 7.07, KL: 14.7487 | BETA: 0.013400\n",
      "Epoch: 0, Step: 5400 | LOSS TRAIN: 459.20, DEV: 654.23 | TM NLL: 452.04, KL: 3.2347 | LM NLL: 7.07, KL: 14.5649 | BETA: 0.013653\n",
      "Epoch: 0, Step: 5500 | LOSS TRAIN: 459.26, DEV: 654.12 | TM NLL: 452.12, KL: 3.2288 | LM NLL: 7.06, KL: 14.3861 | BETA: 0.013905\n",
      "Epoch: 0, Step: 5600 | LOSS TRAIN: 458.79, DEV: 654.27 | TM NLL: 451.65, KL: 3.2197 | LM NLL: 7.05, KL: 14.2138 | BETA: 0.014158\n",
      "Epoch: 0, Step: 5700 | LOSS TRAIN: 459.20, DEV: 654.37 | TM NLL: 452.07, KL: 3.2196 | LM NLL: 7.04, KL: 14.0441 | BETA: 0.014411\n",
      "Epoch: 0, Step: 5800 | LOSS TRAIN: 459.16, DEV: 654.32 | TM NLL: 452.04, KL: 3.2159 | LM NLL: 7.04, KL: 13.8768 | BETA: 0.014664\n",
      "Epoch: 0, Step: 6000 | LOSS TRAIN: 458.61, DEV: 654.22 | TM NLL: 451.50, KL: 3.2043 | LM NLL: 7.03, KL: 13.5562 | BETA: 0.015170\n",
      "Epoch: 0, Step: 6100 | LOSS TRAIN: 458.36, DEV: 654.29 | TM NLL: 451.25, KL: 3.1995 | LM NLL: 7.02, KL: 13.4033 | BETA: 0.015422\n",
      "Epoch: 0, Step: 6200 | LOSS TRAIN: 458.61, DEV: 654.17 | TM NLL: 451.51, KL: 3.2005 | LM NLL: 7.01, KL: 13.2522 | BETA: 0.015675\n",
      "Epoch: 0, Step: 6300 | LOSS TRAIN: 458.72, DEV: 654.20 | TM NLL: 451.62, KL: 3.1997 | LM NLL: 7.01, KL: 13.1048 | BETA: 0.015928\n",
      "Epoch: 0, Step: 6400 | LOSS TRAIN: 458.38, DEV: 654.29 | TM NLL: 451.29, KL: 3.1974 | LM NLL: 7.00, KL: 12.9599 | BETA: 0.016181\n",
      "Epoch: 0, Step: 6500 | LOSS TRAIN: 458.26, DEV: 654.21 | TM NLL: 451.18, KL: 3.1990 | LM NLL: 7.00, KL: 12.8180 | BETA: 0.016434\n",
      "Epoch: 0, Step: 6600 | LOSS TRAIN: 457.84, DEV: 654.24 | TM NLL: 450.76, KL: 3.1974 | LM NLL: 6.99, KL: 12.6803 | BETA: 0.016686\n",
      "Epoch: 0, Step: 6700 | LOSS TRAIN: 457.75, DEV: 654.15 | TM NLL: 450.67, KL: 3.1977 | LM NLL: 6.99, KL: 12.5450 | BETA: 0.016939\n",
      "Epoch: 0, Step: 6800 | LOSS TRAIN: 457.53, DEV: 654.07 | TM NLL: 450.46, KL: 3.1989 | LM NLL: 6.98, KL: 12.4129 | BETA: 0.017192\n",
      "Epoch: 0, Step: 6900 | LOSS TRAIN: 457.99, DEV: 654.07 | TM NLL: 450.92, KL: 3.2041 | LM NLL: 6.98, KL: 12.2828 | BETA: 0.017445\n",
      "Epoch: 0, Step: 7000 | LOSS TRAIN: 458.50, DEV: 654.09 | TM NLL: 451.44, KL: 3.2157 | LM NLL: 6.97, KL: 12.1562 | BETA: 0.017698\n",
      "Epoch: 0, Step: 7100 | LOSS TRAIN: 459.20, DEV: 653.95 | TM NLL: 452.14, KL: 3.2258 | LM NLL: 6.97, KL: 12.0316 | BETA: 0.017951\n",
      "Epoch: 0, Step: 7200 | LOSS TRAIN: 459.19, DEV: 653.94 | TM NLL: 452.13, KL: 3.2286 | LM NLL: 6.96, KL: 11.9103 | BETA: 0.018203\n",
      "Epoch: 0, Step: 7300 | LOSS TRAIN: 459.80, DEV: 653.83 | TM NLL: 452.75, KL: 3.2365 | LM NLL: 6.96, KL: 11.7908 | BETA: 0.018456\n",
      "Epoch: 0, Step: 7400 | LOSS TRAIN: 459.89, DEV: 653.81 | TM NLL: 452.84, KL: 3.2390 | LM NLL: 6.96, KL: 11.6721 | BETA: 0.018709\n",
      "Epoch: 0, Step: 7500 | LOSS TRAIN: 460.23, DEV: 653.77 | TM NLL: 453.18, KL: 3.2469 | LM NLL: 6.95, KL: 11.5582 | BETA: 0.018962\n",
      "Epoch: 0, Step: 7600 | LOSS TRAIN: 460.50, DEV: 653.77 | TM NLL: 453.45, KL: 3.2527 | LM NLL: 6.95, KL: 11.4463 | BETA: 0.019215\n",
      "Epoch: 0, Step: 7700 | LOSS TRAIN: 460.26, DEV: 653.87 | TM NLL: 453.22, KL: 3.2534 | LM NLL: 6.94, KL: 11.3361 | BETA: 0.019468\n",
      "Epoch: 0, Step: 7800 | LOSS TRAIN: 460.37, DEV: 653.84 | TM NLL: 453.34, KL: 3.2593 | LM NLL: 6.94, KL: 11.2285 | BETA: 0.019720\n",
      "Epoch: 0, Step: 7900 | LOSS TRAIN: 460.55, DEV: 653.85 | TM NLL: 453.51, KL: 3.2656 | LM NLL: 6.93, KL: 11.1230 | BETA: 0.019973\n",
      "Epoch: 0, Step: 8000 | LOSS TRAIN: 460.44, DEV: 653.90 | TM NLL: 453.41, KL: 3.2680 | LM NLL: 6.93, KL: 11.0194 | BETA: 0.020226\n",
      "Epoch: 0, Step: 8100 | LOSS TRAIN: 460.37, DEV: 653.85 | TM NLL: 453.35, KL: 3.2710 | LM NLL: 6.93, KL: 10.9179 | BETA: 0.020479\n",
      "Epoch: 0, Step: 8200 | LOSS TRAIN: 460.37, DEV: 653.81 | TM NLL: 453.35, KL: 3.2763 | LM NLL: 6.92, KL: 10.8176 | BETA: 0.020732\n",
      "Epoch: 0, Step: 8300 | LOSS TRAIN: 460.83, DEV: 653.74 | TM NLL: 453.81, KL: 3.2842 | LM NLL: 6.92, KL: 10.7189 | BETA: 0.020985\n",
      "Epoch: 0, Step: 8400 | LOSS TRAIN: 460.34, DEV: 653.95 | TM NLL: 453.32, KL: 3.2852 | LM NLL: 6.92, KL: 10.6224 | BETA: 0.021237\n",
      "Epoch: 0, Step: 8500 | LOSS TRAIN: 459.93, DEV: 654.01 | TM NLL: 452.92, KL: 3.2868 | LM NLL: 6.91, KL: 10.5279 | BETA: 0.021490\n",
      "Epoch: 0, Step: 8600 | LOSS TRAIN: 459.60, DEV: 653.99 | TM NLL: 452.59, KL: 3.2905 | LM NLL: 6.91, KL: 10.4349 | BETA: 0.021743\n",
      "Epoch: 0, Step: 8700 | LOSS TRAIN: 459.75, DEV: 653.81 | TM NLL: 452.74, KL: 3.2981 | LM NLL: 6.91, KL: 10.3433 | BETA: 0.021996\n",
      "Epoch: 0, Step: 8800 | LOSS TRAIN: 459.65, DEV: 653.84 | TM NLL: 452.65, KL: 3.3011 | LM NLL: 6.90, KL: 10.2528 | BETA: 0.022249\n",
      "Epoch: 0, Step: 8900 | LOSS TRAIN: 459.44, DEV: 653.90 | TM NLL: 452.43, KL: 3.3047 | LM NLL: 6.90, KL: 10.1641 | BETA: 0.022501\n",
      "Epoch: 0, Step: 9000 | LOSS TRAIN: 459.36, DEV: 653.79 | TM NLL: 452.36, KL: 3.3095 | LM NLL: 6.90, KL: 10.0771 | BETA: 0.022754\n",
      "Epoch: 0, Step: 9100 | LOSS TRAIN: 459.20, DEV: 653.83 | TM NLL: 452.21, KL: 3.3146 | LM NLL: 6.90, KL: 9.9917 | BETA: 0.023007\n",
      "Epoch: 0, Step: 9200 | LOSS TRAIN: 459.23, DEV: 653.64 | TM NLL: 452.24, KL: 3.3210 | LM NLL: 6.89, KL: 9.9076 | BETA: 0.023260\n",
      "Epoch: 0, Step: 9300 | LOSS TRAIN: 459.12, DEV: 653.56 | TM NLL: 452.12, KL: 3.3275 | LM NLL: 6.89, KL: 9.8242 | BETA: 0.023513\n",
      "Epoch: 0, Step: 9400 | LOSS TRAIN: 458.87, DEV: 653.59 | TM NLL: 451.88, KL: 3.3356 | LM NLL: 6.89, KL: 9.7426 | BETA: 0.023766\n",
      "Epoch: 0, Step: 9500 | LOSS TRAIN: 458.77, DEV: 653.35 | TM NLL: 451.78, KL: 3.3468 | LM NLL: 6.88, KL: 9.6625 | BETA: 0.024018\n",
      "Epoch: 0, Step: 9600 | LOSS TRAIN: 459.28, DEV: 652.89 | TM NLL: 452.29, KL: 3.3682 | LM NLL: 6.88, KL: 9.5845 | BETA: 0.024271\n",
      "Epoch: 0, Step: 9700 | LOSS TRAIN: 459.59, DEV: 652.19 | TM NLL: 452.60, KL: 3.3877 | LM NLL: 6.88, KL: 9.5082 | BETA: 0.024524\n",
      "Epoch: 0, Step: 9800 | LOSS TRAIN: 459.61, DEV: 651.77 | TM NLL: 452.63, KL: 3.4062 | LM NLL: 6.88, KL: 9.4345 | BETA: 0.024777\n",
      "Epoch: 0, Step: 9900 | LOSS TRAIN: 459.57, DEV: 651.22 | TM NLL: 452.59, KL: 3.4290 | LM NLL: 6.87, KL: 9.3630 | BETA: 0.025030\n",
      "Epoch: 0, Step: 10000 | LOSS TRAIN: 459.31, DEV: 650.74 | TM NLL: 452.33, KL: 3.4523 | LM NLL: 6.87, KL: 9.2944 | BETA: 0.025283\n",
      "Epoch: 0, Step: 10100 | LOSS TRAIN: 458.97, DEV: 650.20 | TM NLL: 451.99, KL: 3.4837 | LM NLL: 6.87, KL: 9.2297 | BETA: 0.025535\n",
      "Epoch: 0, Step: 10200 | LOSS TRAIN: 458.88, DEV: 649.60 | TM NLL: 451.90, KL: 3.5287 | LM NLL: 6.87, KL: 9.1686 | BETA: 0.025788\n",
      "Epoch: 0, Step: 10300 | LOSS TRAIN: 458.85, DEV: 648.93 | TM NLL: 451.87, KL: 3.5825 | LM NLL: 6.86, KL: 9.1111 | BETA: 0.026041\n",
      "Epoch: 0, Step: 10400 | LOSS TRAIN: 458.77, DEV: 648.22 | TM NLL: 451.79, KL: 3.6450 | LM NLL: 6.86, KL: 9.0561 | BETA: 0.026294\n",
      "Epoch: 0, Step: 10500 | LOSS TRAIN: 458.75, DEV: 647.77 | TM NLL: 451.78, KL: 3.7103 | LM NLL: 6.86, KL: 9.0029 | BETA: 0.026547\n",
      "True: members of a legislative task force charged with finding ways to stem new hampshire 's opioid abuse crisis will meet thursday to discuss the rules and membership of the board as well as policies governing the state 's prescription drug monitoring program\n",
      "Pred: the the the the the the the the the the the the the the the the the the the the the the the\n",
      "True: the task force will also hear from a texas based expert on the intersection of behavioral health and criminal justice\n",
      "Pred: the the the the the the the the the the the the the the the the the the\n",
      "True: attempts by gov . maggie hassan earlier this year to tighten prescribing rules drew strong pushback from the board\n",
      "Pred: the the the the the the the the the the the the\n",
      "True: the task force is working through december to write legislation that can come up quickly for votes when the full legislature returns in january\n",
      "Pred: the the the the the the the the the the the the the the the\n",
      "True: more than < num > people have already died from heroin or opioid overdoses this year\n",
      "Pred: the the the the the the the the the the the\n",
      "True: an investigation into a fatal apartment fire that killed two oregon boys last february has failed to pinpoint the exact cause\n",
      "Pred: the the the the the the the the the the the the the the the the\n",
      "True: the oregonian reports a <unk> investigation led by the clackamas county sheriff 's office found the feb. < num > fire started in the living room when an open flame ignited items in the second story unit at the apartment in suburban milwaukie\n",
      "Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "True: but the cause of the fire officially will go into the books as `` undetermined .\n",
      "Pred: the the the the the the the the the the\n",
      "True: based on the findings , the clackamas county district attorney 's office has declined to file charges against < num > year old kimberly hasty , the mother of the boys\n",
      "Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "True: hasty escaped the blaze with her infant son but left behind his < num > and < num > year old brothers , who died of smoke inhalation\n",
      "Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "True: a crash involving as many as < num > vehicles on an orange county freeway has left one person dead and at least seven injured\n",
      "Pred: the the the the the the the the the the the the the the the the the the the the\n",
      "True: the california highway patrol says a big rig blew a tire and went out of control shortly after < num > p.m. wednesday on southbound state route < num > in fullerton\n",
      "Pred: the the the the the the the the the the the the the the the the the the the the the the the\n",
      "True: the rig hit the center divider and rolled over into northbound lanes , causing accidents involved at least < num > other vehicles\n",
      "Pred: the the the the the the the the the the the the the the the the the the the\n",
      "True: the dead man was the driver was in a white honda\n",
      "Pred: the the the the the the the the\n",
      "True: several lanes are closed in both directions and traffic is backed up for several miles\n",
      "Pred: the the the the the the the the the the\n",
      "True: one of the san francisco bay area rapid transit agency 's largest unions is scheduled to vote on a revised labor deal to resolve months of heated negotiations\n",
      "Pred: the the the the the the the the the the the the the the the the the\n",
      "True: members of the <unk> transit union , local < num > is expected to vote friday on a four year contract to settle a dispute over paid medical leave for employees that arose in november after the <unk> , seiu local < num > and bart management previously reached a deal ending a second strike in october\n",
      "Pred: the the the the the < < < < > > > > > > > > > > > < < < < the the the the the the the the the the the the\n",
      "True: the bart board on thursday approved the new deal that dropped a controversial clause granting union members up to six weeks of paid family medical leave that bart said was mistakenly included and would cost $ < num > million if one third of the agency 's workers took advantage of it\n",
      "Pred: the the the the the the the the num > > > > > > the the the the the the the the the the the the the\n",
      "True: instead , workers will get an extended <unk> leave and potential bonuses\n",
      "Pred: the the the the the the the the the\n",
      "True: the seiu will vote on the contract on jan. < num >\n",
      "Pred: the the the the the the the the the\n",
      "True: state officials say sex offenders on parole in georgia face several restrictions aimed at keeping them from using halloween to commit crimes\n",
      "Pred: the the the the the the the the the the the the the the the\n",
      "True: georgia board of pardons and paroles officials say that sex offenders under parole supervision will not be allowed to decorate the outside of their homes or have outside lights turned on\n",
      "Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "True: jay <unk> , the agency 's field operations director , says the state 's sex offenders have been `` strictly directed `` not to do anything to entice children to their homes\n",
      "Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "True: state officials say they plan to check up on sex offenders on halloween and also conduct random searches of residences\n",
      "Pred: the the the the the the the the the the the the\n",
      "True: officials said that in some cases , sex offenders on parole are being directed to attend programs at specified locations during the evening hours wednesday when children are trick or treating\n",
      "Pred: the the the the the the the the the the the the the the the the the the the\n",
      "True: owners of several stores in connecticut communities near a deadly christmas morning fire are reporting a noticeable jump in purchases of smoke detectors , fire extinguishers and escape ladders\n",
      "Pred: the the the the the the the the the the the the the the the the the the the the the\n",
      "True: the time of greenwich reports ( http : <unk> ) some hardware store owners say people also have been bringing in their existing alarms to check and replace batteries after the deadly stamford fire\n",
      "Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "True: officials say the improper disposal of fireplace ashes sparked the blaze , which killed < num > year old twins grace and sarah badger ; their < num > year old sister , lily ; and their grandparents , pauline and <unk> johnson\n",
      "Pred: the the the the the the the the the num > > > > > > > > > the the the the the the the the the the the the the the the the the the the\n",
      "True: their mother and a friend were able to escape the burning mansion , which later had to be torn down\n",
      "Pred: the the the the the the the the the the the the the the the the\n",
      "True: fire officials say the deaths underscore the importance of having working smoke detectors and an escape plan , and taking precautions with fireplace ashes and other flammable materials\n",
      "Pred: the the the the the the the the the the the the the the the the\n",
      "True: authorities are trying to identify a woman found dead near an intersection in cedar rapids\n",
      "Pred: the the the the the the the the the the the the the\n",
      "True: police say the woman was found early friday\n",
      "Pred: the the the the the the the the the the\n",
      "True: her body is being sent to a medical examiner to determine the cause of death\n",
      "Pred: the the the the the the the the the the the the the\n",
      "True: additional information about the case has not been released\n",
      "Pred: the the the the\n",
      "True: police say they do not believe there is any risk to the public , but they did not elaborate\n",
      "Pred: the the the the the the the the the the\n",
      "True: the price of gasoline in new hampshire has dropped a couple of pennies per gallon in the last week\n",
      "Pred: the the the the the the the the the the the the the\n",
      "True: the average price was $ < num > on sunday , slightly higher than the national average of $ < num >\n",
      "Pred: the the the the the the the the the the the the the the the the the the the the the the the\n",
      "True: the gasoline price website <unk> says new hampshire 's price was < num > cents per gallon lower than the same day a year ago\n",
      "Pred: the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "True: it was < num > cents per gallon higher than a month ago\n",
      "Pred: the the the the the the the the the the the\n",
      "True: nationally , the average price has increased < num > cents per gallon during the last month and stands < num > cents per gallon lower than a year ago\n",
      "Pred: the the the the the the the the the the the the the the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "for ct, batch in train_batches:\n",
    "    feed_dict = get_feed_dict(batch)\n",
    "    if config.warmup > 0: sess.run(beta.assign(np.minimum(1., ct/(config.warmup*num_train_batches))))\n",
    "\n",
    "    _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch = \\\n",
    "    sess.run([opt, loss, topic_loss_recon, topic_loss_kl, sent_loss_recon, sent_loss_kl], feed_dict = feed_dict)\n",
    "    \n",
    "    losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "\n",
    "    if ct%config.log_period==0:\n",
    "        loss_train, topic_loss_recon_train, topic_loss_kl_train, sent_loss_recon_train, sent_loss_kl_train = np.mean(losses_train, 0)\n",
    "        if config.warmup > 0: beta_eval = beta.eval(session=sess)\n",
    "        loss_dev = get_loss(sess, dev_batches)\n",
    "\n",
    "#             if loss_dev <= loss_min:\n",
    "#                 loss_min = loss_dev\n",
    "#                 loss_test = get_loss(sess, test_batches)\n",
    "\n",
    "        clear_output()\n",
    "\n",
    "        logs += [(epoch, ct, loss_train, loss_dev, topic_loss_recon_train, topic_loss_kl_train, sent_loss_recon_train, sent_loss_kl_train, beta_eval)]\n",
    "        for log in logs:\n",
    "            print('Epoch: %i, Step: %i | LOSS TRAIN: %.2f, DEV: %.2f | TM NLL: %.2f, KL: %.4f | LM NLL: %.2f, KL: %.4f | BETA: %.6f' %  log)\n",
    "\n",
    "        print_sample(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-908c989dff5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_all_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-fe1b0e65b557>\u001b[0m in \u001b[0;36mget_all_losses\u001b[0;34m(sess, batches)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_recon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_kl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_recon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_kl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_recon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_kl_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_recon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_kl_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LOSS %.2f | TM NLL: %.2f, KL: %.4f | LM NLL: %.2f, KL: %.4f'\u001b[0m \u001b[0;34m%\u001b[0m  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "get_all_losses(sess, dev_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "_logvars, _means, _kl_losses, _latents, _output_logits = sess.run([logvars, means, kl_losses, latents, output_logits], feed_dict=feed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 32), (32, 32), (32,), (32, 32))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_logvars.shape, _means.shape, _kl_losses.shape, _latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_output_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dec_target_idxs_do' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-7de59bc2cc54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_output_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dec_target_idxs_do\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dec_mask_tokens_do\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_recon_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_kl_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_target_idxs_do\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_mask_tokens_do\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dec_target_idxs_do' is not defined"
     ]
    }
   ],
   "source": [
    "_output_logits, _dec_target_idxs_do, _dec_mask_tokens_do, _recon_loss, _kl_losses, _ = sess.run([output_logits, dec_target_idxs_do, dec_mask_tokens_do, recon_loss, kl_losses, opt], feed_dict=feed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 46)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_max(output_logits, 2).eval(session=sess, feed_dict=feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 46, 20000), (120, 46), (120, 46))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_output_logits.shape, _dec_target_idxs_do.shape, _dec_mask_tokens_do.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "_logits = np.exp(_output_logits) / np.sum(np.exp(_output_logits), 2)[:, :, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "_idxs = _dec_target_idxs_do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "_losses = np.array([[-np.log(_logits[i, j, _idxs[i, j]]) for j in range(_idxs.shape[1])] for i in range(_idxs.shape[0])]) * _dec_mask_tokens_do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.903732"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(_losses)/np.sum(_dec_mask_tokens_do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.903732"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_recon_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_kl_losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
