{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.python import debug as tf_debug\n",
    "\n",
    "from data_structure import get_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "from topic_model import TopicModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '2', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/apnews/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'sports', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 1000, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 128, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 500, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.1, 'lr')\n",
    "flags.DEFINE_float('reg', 0.1, 'regularization term')\n",
    "flags.DEFINE_float('beta', 0.0, 'initial value of beta')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.6, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_integer('warmup', 10, 'warmup period for KL')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 10, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_topic', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "config = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('PAD_IDX', word_to_idx[PAD], 'PAD_IDX')\n",
    "flags.DEFINE_integer('UNK_IDX', word_to_idx[UNK], 'UNK_IDX')\n",
    "flags.DEFINE_integer('BOS_IDX', word_to_idx[BOS], 'BOS_IDX')\n",
    "flags.DEFINE_integer('EOS_IDX', word_to_idx[EOS], 'EOS_IDX')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "maximum_iterations = max([max([instance.max_sent_l for instance in batch]) for ct, batch in dev_batches])\n",
    "flags.DEFINE_integer('maximum_iterations', maximum_iterations, 'maximum_iterations')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build language model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train'):\n",
    "    bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "    \n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    doc_l_matrix = np.array([instance.doc_l for instance in batch]).astype(np.int32)\n",
    "\n",
    "    max_doc_l = np.max(doc_l_matrix)\n",
    "    max_sent_l = max([instance.max_sent_l for instance in batch])\n",
    "\n",
    "    token_idxs_matrix = np.zeros([batch_size, max_doc_l, max_sent_l], np.int32)\n",
    "    dec_input_idxs_matrix = np.zeros([batch_size, max_doc_l, max_sent_l+1], np.int32)\n",
    "    dec_target_idxs_matrix = np.zeros([batch_size, max_doc_l, max_sent_l+1], np.int32)\n",
    "    sent_l_matrix = np.zeros([batch_size, max_doc_l], np.int32)\n",
    "    dec_sent_l_matrix = np.zeros([batch_size, max_doc_l], np.int32)\n",
    "\n",
    "    for i, instance in enumerate(batch):\n",
    "        for j, sent_idxs in enumerate(instance.token_idxs):\n",
    "            token_idxs_matrix[i, j, :len(sent_idxs)] = np.asarray(sent_idxs)\n",
    "            \n",
    "            sent_idxs_dropout = np.asarray(sent_idxs)\n",
    "            sent_idxs_dropout[np.random.rand(len(sent_idxs)) > config.word_keep_prob] = config.UNK_IDX\n",
    "            dec_input_idxs_matrix[i, j, :len(sent_idxs)+1] = np.concatenate([[config.BOS_IDX], sent_idxs_dropout])\n",
    "            \n",
    "            dec_target_idxs_matrix[i, j, :len(sent_idxs)+1] = np.asarray(sent_idxs + [config.EOS_IDX])\n",
    "            sent_l_matrix[i, j] = len(sent_idxs)\n",
    "            dec_sent_l_matrix[i, j] = len(sent_idxs)+1\n",
    "\n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "\n",
    "    feed_dict = {\n",
    "                t_variables['bow']: bow, t_variables['token_idxs']: token_idxs_matrix,\n",
    "                t_variables['dec_input_idxs']: dec_input_idxs_matrix, t_variables['dec_target_idxs']: dec_target_idxs_matrix, \n",
    "                t_variables['batch_l']: batch_size, t_variables['doc_l']: doc_l_matrix, t_variables['sent_l']: sent_l_matrix, t_variables['dec_sent_l']: dec_sent_l_matrix,\n",
    "                t_variables['max_doc_l']: max_doc_l, t_variables['max_sent_l']: max_sent_l, \n",
    "                t_variables['keep_prob']: keep_prob}\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_shape(variables, sess_init=None):\n",
    "    if sess_init is None:\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    else:\n",
    "        sess = sess_init\n",
    "        \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    if sess_init is None: sess.close()\n",
    "\n",
    "def debug_value(variables, return_value=False, sess_init=None):\n",
    "    if sess_init is None:\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    else:\n",
    "        sess = sess_init\n",
    "\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "    if sess_init is None: sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fed variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['bow'] = tf.placeholder(tf.float32, [None, config.dim_bow])\n",
    "t_variables['token_idxs'] = tf.placeholder(tf.int32, [None, None, None])\n",
    "t_variables['dec_input_idxs'] = tf.placeholder(tf.int32, [None, None, None])\n",
    "t_variables['dec_target_idxs'] = tf.placeholder(tf.int32, [None, None, None])\n",
    "t_variables['batch_l'] = tf.placeholder(tf.int32, [])\n",
    "t_variables['doc_l'] = tf.placeholder(tf.int32, [None])\n",
    "t_variables['sent_l'] = tf.placeholder(tf.int32, [None, None])\n",
    "t_variables['dec_sent_l'] = tf.placeholder(tf.int32, [None, None])\n",
    "t_variables['max_doc_l'] = tf.placeholder(tf.int32, [])\n",
    "t_variables['max_sent_l'] = tf.placeholder(tf.int32, [])\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode bow\n",
    "with tf.variable_scope('topic/enc', reuse=False):\n",
    "    hidden_bow_ = tf.keras.layers.Dense(units=config.dim_hidden_bow, activation=tf.nn.relu, name='hidden')(t_variables['bow'])\n",
    "    hidden_bow = tf.keras.layers.Dropout(t_variables['keep_prob'])(hidden_bow_)\n",
    "    means_bow = tf.keras.layers.Dense(units=config.dim_latent_topic)(hidden_bow)\n",
    "    logvars_bow = tf.keras.layers.Dense(units=config.dim_latent_topic, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0))(hidden_bow)\n",
    "    latents_bow = sample_latents(means_bow, logvars_bow) # sample latent vectors\n",
    "\n",
    "    prob_topic = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob')(latents_bow) # inference of topic probabilities\n",
    "\n",
    "# decode bow\n",
    "with tf.variable_scope('shared', reuse=False):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "bow_embeddings = tf.nn.embedding_lookup(embeddings, bow_idxs) # embeddings of each bow features\n",
    "\n",
    "with tf.variable_scope('topic/dec', reuse=False):\n",
    "    topic_embeddings = tf.get_variable('topic_emb', [config.n_topic, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of topics\n",
    "\n",
    "    topic_bow = tf.nn.softmax(tf.matmul(topic_embeddings, bow_embeddings, transpose_b=True), 1) # bow vectors for each topic\n",
    "    prob_bow = tf_log(tf.matmul(prob_topic, topic_bow)) # predicted bow distribution\n",
    "\n",
    "# define losses\n",
    "topic_losses_recon = -tf.reduce_sum(tf.multiply(t_variables['bow'], prob_bow), 1)\n",
    "topic_loss_recon = tf.reduce_mean(topic_losses_recon) # negative log likelihood of each words\n",
    "\n",
    "topic_loss_kl = compute_kl_loss(means_bow, logvars_bow) # KL divergence b/w latent dist & gaussian std\n",
    "\n",
    "topic_embeddings_norm = topic_embeddings / tf.norm(topic_embeddings, axis=1, keepdims=True)\n",
    "topic_angles = tf.matmul(topic_embeddings_norm, tf.transpose(topic_embeddings_norm))\n",
    "topic_angles_mean = tf.reduce_mean(topic_angles, keepdims=True)\n",
    "topic_angles_vars = tf.reduce_mean(tf.square(topic_angles - topic_angles_mean))\n",
    "topic_loss_reg = topic_angles_vars - tf.squeeze(topic_angles_mean)\n",
    "\n",
    "# monitor\n",
    "n_bow = tf.reduce_sum(t_variables['bow'], 1)\n",
    "topic_ppls = tf.divide(topic_losses_recon, n_bow)\n",
    "topics_freq_bow_indices = tf.nn.top_k(topic_bow, 10, name='topic_freq_bow').indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "batch_l = t_variables['batch_l']\n",
    "max_doc_l = t_variables['max_doc_l']\n",
    "max_sent_l = t_variables['max_sent_l']\n",
    "token_idxs = t_variables['token_idxs'][:, :max_doc_l, :max_sent_l]\n",
    "\n",
    "# get word embedding\n",
    "with tf.variable_scope('sent/enc', reuse=False):\n",
    "    enc_input = tf.nn.embedding_lookup(embeddings, token_idxs)\n",
    "    enc_input_do = tf.reshape(enc_input, [batch_l * max_doc_l, max_sent_l, config.dim_emb])\n",
    "\n",
    "    # get sentence embedding\n",
    "    sent_l = t_variables['sent_l']\n",
    "    sent_l_do = tf.reshape(sent_l, [batch_l * max_doc_l])\n",
    "\n",
    "    _, enc_state_do = dynamic_rnn(enc_input_do, sent_l_do, config.dim_hidden, t_variables['keep_prob'])\n",
    "    enc_state = tf.reshape(enc_state_do, [batch_l, max_doc_l, config.dim_hidden])\n",
    "\n",
    "    # TODO House Holder flow\n",
    "    prob_topic_sent = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax)(enc_state)\n",
    "    \n",
    "    # inference of each gaussian dist. parameter\n",
    "    enc_state_topic = tf.tile(tf.expand_dims(enc_state, 2), [1, 1, config.n_topic, 1]) # tile over topics\n",
    "    means_sent_topic = tf.keras.layers.Dense(units=config.dim_latent)(enc_state_topic)\n",
    "    logvars_sent_topic = tf.keras.layers.Dense(units=config.dim_latent, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0))(enc_state_topic)\n",
    "    \n",
    "    # latent vectors from each gaussian dist.\n",
    "    latents_sent_topic = sample_latents(means_sent_topic, logvars_sent_topic) \n",
    "    # latent vector from gaussian mixture    \n",
    "    latents_sent = tf.squeeze(tf.matmul(tf.expand_dims(prob_topic_sent, -1), latents_sent_topic, transpose_a=True), 2) \n",
    "    latents_sent_do = tf.reshape(latents_sent, [batch_l * max_doc_l, config.dim_latent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strided_slice:0 : (20, 9, 38)\n",
      "sent/enc/Reshape_2:0 : (20, 9, 512)\n",
      "sent/enc/dense/Reshape_1:0 : (20, 9, 10)\n",
      "sent/enc/add:0 : (20, 9, 10, 32)\n",
      "sent/enc/dense/Reshape_1:0 : (20, 9, 10)\n"
     ]
    }
   ],
   "source": [
    "debug_shape([token_idxs, enc_state, prob_topic_sent, latents_sent_topic, prob_topic_sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for decoding\n",
    "dec_input_idxs = t_variables['dec_input_idxs']\n",
    "dec_input_idxs_do = tf.reshape(dec_input_idxs, [batch_l * max_doc_l, max_sent_l+1])\n",
    "dec_input_do = tf.nn.embedding_lookup(embeddings, dec_input_idxs_do)\n",
    "\n",
    "dec_latent_input_do = tf.tile(tf.expand_dims(latents_sent_do, 1), [1, tf.shape(dec_input_do)[1], 1])\n",
    "dec_concat_input_do = tf.concat([dec_input_do, dec_latent_input_do], 2)\n",
    "\n",
    "# decode for training\n",
    "dec_sent_l = t_variables['dec_sent_l']\n",
    "dec_sent_l_do = tf.reshape(dec_sent_l, [batch_l * max_doc_l])\n",
    "\n",
    "with tf.variable_scope('sent/dec/rnn', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32, reuse=tf.AUTO_REUSE):\n",
    "    dec_cell = tf.contrib.rnn.GRUCell(config.dim_hidden)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob = t_variables['keep_prob'])\n",
    "\n",
    "    dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu)(latents_sent_do)\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_concat_input_do, sequence_length=dec_sent_l_do)\n",
    "\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=dec_cell,\n",
    "        helper=helper,\n",
    "        initial_state=dec_initial_state)\n",
    "\n",
    "    dec_outputs, _, output_sent_l = tf.contrib.seq2seq.dynamic_decode(train_decoder)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(config.n_vocab, use_bias=False, name='out')\n",
    "    output_logits = output_layer(dec_outputs.rnn_output)\n",
    "    \n",
    "    output_token_idxs_do = tf.argmax(output_logits, 2)\n",
    "    output_token_idxs = tf.reshape(output_token_idxs_do, [batch_l, max_doc_l, tf.shape(output_token_idxs_do)[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language modeling cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target and mask\n",
    "dec_target_idxs = t_variables['dec_target_idxs']\n",
    "dec_target_idxs_do = tf.reshape(dec_target_idxs, [batch_l * max_doc_l, max_sent_l+1])                \n",
    "dec_mask_tokens_do = tf.sequence_mask(dec_sent_l_do, maxlen=max_sent_l+1, dtype=tf.float32)\n",
    "\n",
    "# nll for each token (averaged over batch & sentence)\n",
    "sent_loss_recon = tf.contrib.seq2seq.sequence_loss(output_logits, dec_target_idxs_do, dec_mask_tokens_do) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferred mixture probabilities (computed for each sentence)\n",
    "categ_topic_sent = tfd.Categorical(probs=prob_topic_sent)\n",
    "\n",
    "# prior of mixture probabilities (computed for each document)\n",
    "categ_topic = tfd.Categorical(probs=tf.expand_dims(prob_topic, 1))\n",
    "\n",
    "sent_loss_kl_categ = tf.reduce_mean(tfd.kl_divergence(categ_topic_sent, categ_topic))\n",
    "\n",
    "# prior of each gaussian gaussribution (computed for each topic)\n",
    "with tf.variable_scope('topic/enc/infer', reuse=False):\n",
    "    means_topic_bow = tf.keras.layers.Dense(units=config.dim_latent)(topic_bow)\n",
    "    logvars_topic_bow = tf.keras.layers.Dense(units=config.dim_latent, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0))(topic_bow)\n",
    "sigma_topic_bow = tf.exp(0.5 * logvars_topic_bow)\n",
    "gauss_topic_bow = tfd.Normal(loc=means_topic_bow, scale=sigma_topic_bow)\n",
    "\n",
    "# inference of each gaussian gaussribution (computed for each sentence)\n",
    "sigma_sent_topic = tf.exp(0.5 * logvars_sent_topic)\n",
    "gauss_sent_topic = tfd.Normal(loc=means_sent_topic, scale=sigma_sent_topic)\n",
    "\n",
    "sent_loss_kl_gauss = tf.reduce_sum(tfd.kl_divergence(gauss_sent_topic, gauss_topic_bow), -1)\n",
    "sent_loss_kl_gmm = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_sent, sent_loss_kl_gauss), -1))\n",
    "\n",
    "sent_loss_kl = sent_loss_kl_categ + sent_loss_kl_gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent/enc/dense/Reshape_1:0 : (20, 9, 10)\n",
      "topic/enc/prob/Softmax:0 : (20, 10)\n"
     ]
    }
   ],
   "source": [
    "debug_shape([prob_topic_sent, prob_topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic/enc/infer/dense/BiasAdd:0 : (10, 32)\n",
      "sent/enc/dense_1/BiasAdd:0 : (20, 9, 10, 32)\n"
     ]
    }
   ],
   "source": [
    "debug_shape([means_topic_bow, means_sent_topic])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = tf.Variable(config.beta, name='beta', trainable=False) if config.warmup > 0 else tf.constant(1., name='beta')\n",
    "update_beta = tf.assign_add(beta, 1./(config.warmup*len(train_batches)))\n",
    "sent_loss = sent_loss_recon + beta * sent_loss_kl\n",
    "\n",
    "topic_loss = topic_loss_recon + topic_loss_kl + config.reg * topic_loss_reg\n",
    "loss = topic_loss + sent_loss\n",
    "\n",
    "# define optimizer\n",
    "if config.opt == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif config.opt == 'Adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "    \n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_sents(token_idxs, config, idx_to_word):\n",
    "    sents = []\n",
    "    for line_idxs in token_idxs:\n",
    "        tokens = []\n",
    "        for idx in line_idxs:\n",
    "            if idx == config.EOS_IDX: break\n",
    "            tokens.append(idx_to_word[idx])\n",
    "        sent = ' '.join(tokens)\n",
    "        sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_batch, sent_loss_batch, ppls_batch = sess.run([loss, topic_loss, sent_loss, topic_ppls], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_batch, sent_loss_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "    loss_mean, topic_loss_mean, sent_loss_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    return loss_mean, topic_loss_mean, sent_loss_mean, ppl_mean\n",
    "\n",
    "def get_all_losses(sess, batches):\n",
    "    losses = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch = \\\n",
    "        sess.run([loss, topic_loss_recon, topic_loss_kl, sent_loss_recon, sent_loss_kl], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "    print('LOSS %.2f | TM NLL: %.2f, KL: %.4f | LM NLL: %.2f, KL: %.4f' %  np.mean(losses, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_batch):\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    pred_token_idxs_batch = sess.run(output_token_idxs, feed_dict = feed_dict)\n",
    "    true_token_idxs_batch = [instance.token_idxs for instance in sample_batch]\n",
    "    \n",
    "    assert len(pred_token_idxs_batch) == len(true_token_idxs_batch)\n",
    "    \n",
    "    for true_token_idxs, pred_token_idxs in zip(true_token_idxs_batch, pred_token_idxs_batch):\n",
    "        true_sents = idxs_to_sents(true_token_idxs, config, idx_to_word)\n",
    "        pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "        assert len(true_sents) == len(pred_sents)\n",
    "        \n",
    "        for true_sent, pred_sent in zip(true_sents, pred_sents):\n",
    "            print('True: %s' % true_sent)\n",
    "            print('Pred: %s' % pred_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "logs = []\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "008[s], 06[s], Ep: 00, Ct: 00000|TR LOSS: 363, PPL: 2498|TM NLL: 352, KL: 0.60 | LM NLL: 9.95, KL: 0.81|DE LOSS: 340, PPL: 2494, TM: 330, LM: 9.95|BETA: 0.000000\n",
      "True: the alaska board of fisheries and game\n",
      "Pred: backbone main crossed decide caylee cheney manley canoe pekin manley supermarket textbook cheney denise barrels maher tools cheney major larger since pavement swords lap fulghum canoe undermine shattered pointe cheney address veterinary foundry furnace lockwood cheney cheney marek pointe canoe\n",
      "True: a joint board\n",
      "Pred: mia sponsorships mnsure listeriosis drought nordic bus ruby shelters submitting rowan cells garbage taylor cage productions corroon telling productions farina treats productions ihs z epilepsy steen productions dependent firefighters fact embrace pumpkins productions z jurisdiction sucked potential jacket ongoing dependent\n",
      "True: has decided to forward two names to the governor for the position of department of fish and game commissioner\n",
      "Pred: abbott tighten played naylor dousing n.y takeover hovered bremerton infiltrated curbs calif bathrooms oswego adp champagne edmonds assurance scout growing swamped colo. navigation sums ham aldrich ham grenades rockaway packet bremerton colo. choppy infiltrated blaze featured meakens grenades guilt colo.\n",
      "True: the board held a <unk> tuesday in which it approved two candidates\n",
      "Pred: mobilization mclaren gresham cathy concord class exist borden tait cockfighting wills larger atmospheric exist administrators st ceo presumably expanding wins throughout young expedia edward cheney coahoma stalled photograph assignments runner grosse bled relative founder concert monmouth dismiss structure susceptible coahoma\n",
      "True: the alaska journal of commerce reports the first is gov . sean parnell 's pick to head the agency\n",
      "Pred: rosol spartanburg healy crying icelandic communicate string remote opposition spartanburg imperative unaffiliated ruin refinery kentuckiana haute dash string sweepstakes haute ease haute string spartanburg turmoil string lifespan rodent she majors haute samoa peacekeepers demand rodent turmoil compromised feasibility haute upon\n",
      "True: thirty one year old cora campbell is a fisheries policy aide\n",
      "Pred: disabled arraigned luba elizabeth factset patterson middleman conveyed misdemeanors reached joni importance coughing factset happen invitation announce comey columns setting kodiak coughing four custody inserted infringement attending defect roberts bearden grip stun undergoing middleman groom coughing contesting trailers civility monthlong\n",
      "True: the other candidate is ron somerville , a former chairman of the board of game and former executive director of the alaska outdoor council\n",
      "Pred: battles hungry oh vaughn incredible jackie pepper shaking dolan quincy f. nutter childcare mcclure archdiocese manning taking tentatively leeway nutter mcclure garvin rd charged why steelmaker laredo bankers bankers believing manning pompano tattoos nutter tornado solvent products tornado tracking outfitted\n",
      "True: the person who is chosen must be approved by the full legislature\n",
      "Pred: maynard may regency inspirato rioting forum avondale knot shellfish subordinate morley morley urging prescribed therapists surpassing terminals gopher breaks livestock foreseeable away poorly surpassing spoke subordinate dry mushrooms trek comey poorly drama comey comey raton size stabilized jazz subordinate spy\n",
      "True: former fish and game commissioner <unk> lloyd retired after he was charged with driving under the influence\n",
      "Pred: canadian referrals caylee footsteps escaped appropriations paving recovering escaped d'amato postal orderly indians indians revamping looked transported paiva abiding scooter disagree indians footsteps delhi divorcing disagree symbols wtva safety indians revamping scooter indians orthodox mclean obstacle disagree indians barfield resolve\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-800e1447b1c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%03d[s], %02d[s], Ep: %02d, Ct: %05d|TR LOSS: %.0f, PPL: %.0f|TM NLL: %.0f, KL: %.2f | LM NLL: %.2f, KL: %.2f|DE LOSS: %.0f, PPL: %.0f, TM: %.0f, LM: %.2f|BETA: %.6f'\u001b[0m \u001b[0;34m%\u001b[0m  \u001b[0mlog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mprint_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtime_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-6fa3dcb112b6>\u001b[0m in \u001b[0;36mprint_sample\u001b[0;34m(sample_batch)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtrue_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midxs_to_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_token_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_to_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mpred_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midxs_to_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_token_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_to_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_sents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtrue_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_sent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_sents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "for ct, batch in train_batches:\n",
    "    feed_dict = get_feed_dict(batch)\n",
    "    if config.warmup > 0 and beta_eval < 1.0: sess.run(update_beta)\n",
    "\n",
    "    _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch, ppls_batch = \\\n",
    "    sess.run([opt, loss, topic_loss_recon, topic_loss_kl, sent_loss_recon, sent_loss_kl, topic_ppls], feed_dict = feed_dict)\n",
    "    \n",
    "    losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "    ppls_train += list(ppls_batch)\n",
    "\n",
    "    if ct%config.log_period==0:\n",
    "        time_dev = time.time()\n",
    "        loss_train, topic_loss_recon_train, topic_loss_kl_train, sent_loss_recon_train, sent_loss_kl_train = np.mean(losses_train, 0)\n",
    "        ppl_train = np.exp(np.mean(ppls_train))\n",
    "        loss_dev, topic_loss_dev, sent_loss_dev, ppl_dev = get_loss(sess, dev_batches)\n",
    "        \n",
    "        if config.warmup > 0: beta_eval = beta.eval(session=sess)\n",
    "        \n",
    "        clear_output()\n",
    "        time_finish = time.time()\n",
    "        time_log = int(time_finish - time_start)\n",
    "        time_log_dev = int(time_finish - time_dev)\n",
    "        logs += [(time_log, time_log_dev, epoch, ct, loss_train, ppl_train, topic_loss_recon_train, topic_loss_kl_train, sent_loss_recon_train, sent_loss_kl_train, loss_dev, ppl_dev, topic_loss_dev, sent_loss_dev, beta_eval)]\n",
    "        for log in logs:\n",
    "            print('%03d[s], %02d[s], Ep: %02d, Ct: %05d|TR LOSS: %.0f, PPL: %.0f|TM NLL: %.0f, KL: %.2f | LM NLL: %.2f, KL: %.2f|DE LOSS: %.0f, PPL: %.0f, TM: %.0f, LM: %.2f|BETA: %.6f' %  log)\n",
    "\n",
    "        print_sample(batch)\n",
    "        \n",
    "        time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-29-6fa3dcb112b6>\u001b[0m(11)\u001b[0;36mprint_sample\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      9 \u001b[0;31m        \u001b[0mtrue_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midxs_to_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_token_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_to_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     10 \u001b[0;31m        \u001b[0mpred_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midxs_to_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_token_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_to_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 11 \u001b[0;31m        \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_sents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     13 \u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0mtrue_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_sent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_sents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> l 1\n",
      "\u001b[1;32m      1 \u001b[0m\u001b[0;32mdef\u001b[0m \u001b[0mprint_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      2 \u001b[0m    \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      3 \u001b[0m    \u001b[0mpred_token_idxs_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_token_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      4 \u001b[0m    \u001b[0mtrue_token_idxs_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_idxs\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      5 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      6 \u001b[0m    \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_token_idxs_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_token_idxs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      7 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      8 \u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0mtrue_token_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_token_idxs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_token_idxs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_token_idxs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      9 \u001b[0m        \u001b[0mtrue_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midxs_to_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_token_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_to_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     10 \u001b[0m        \u001b[0mpred_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midxs_to_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_token_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_to_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 11 \u001b[0;31m        \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_sents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> len(pred_token_idxs_batch\n",
      "*** SyntaxError: unexpected EOF while parsing\n",
      "ipdb> len(pred_token_idxs_batch)\n",
      "20\n",
      "ipdb> len(true_token_idxs_batch)\n",
      "20\n",
      "ipdb> true_sents\n",
      "['( ap )', 'ohio county prosecutors have asked a judge to schedule a retrial for a wheeling man accused of neglect in the death of his < num > year old daughter', \"circuit judge james <unk> declared a mistrial in edward cole 's case april < num > after learning that attorneys did n't <unk> certain information from medical records before providing them to the jury\", 'the jury also was deadlocked', 'cole is charged with child neglect resulting in death', \"the intelligencer and wheeling news register ( http : //bit.ly/ < num > <unk> < num > x ) reports that assistant prosecutor jenna perkins told <unk> in a memorandum that the prosecutor 's office plans to retry cole\", \"she asked the judge to schedule the new trial after cole 's wife , cindy cole , is tried on similar charges\", \"the couple 's daughter , patience cole , died on march < num > , < num > , after ingesting her father 's anti <unk> pills\"]\n",
      "ipdb> len(true_sents)\n",
      "8\n",
      "ipdb> len(pred_sents)\n",
      "9\n",
      "ipdb> pred_sents\n",
      "[\"laporte blaine employers notification aging malcolm landowners tornado veered olin tornado culmination slim tornado kickbacks calif preserving forgotten bridenstine tahoe tornado juveniles dense genesee overly slim laden genesee actor tornado biography overturning tornado tornado tornado unclaimed tornado 're tornado expansions\", 'enterprises scott festivals lobstermen scotch experiments double observe bb cockfighting double double bb cockfighting catawba cafes briefs goehring cockfighting paladino procedures sirens richard parma centerline shelly maintained bb cockfighting double cockfighting double sperry procedures lots what tube fixtures rick greene', 'manoa goodson abbott mennonite paved sheer mennonite riley connor impassable sony downer pinon deaths impassable connor seizure aids abduction quarantined impassable compost thieves stump retiring connor continents barriers dividends barriers impassable connor bunker been impassable earthen barriers perhaps impassable compost', 'certification distinguished allred write ottawa distinguished distinguished dragging reiterated immunization dragging runoff appearances scathing reiterated exxon detroit poster collegiate hailing redd rewritten deaf rosemary knotts reiterated trailers polygamous sox trailers electrical china rhodes knotts marys bragged detroit china applauded bragged', 'collegiate trapper museums tinley whiteman ao pierce nesting lawful redd paglia approach sublette mayo sharks pronghorn blistering excluding paglia owls owls traced owls kline constitution traced moorhead heart traced traced owls raber barriers olivia owls owls pretended samara raber there', 'evidence contentious germantown colaites work mikhail intruders sergey does attitude ave. hospitalization necropsy fellow mushrooms library pedestrians tecumseh pressurized legalized attitude pressurized destin pressurized generation stalemate allows strict legalized misstatements ejected albany lobbied why pines boulders pines pressurized griggs geneva', 'bighorn clothed erin steps zeigler scheme pillar passer zeigler boating taxation euros morsi study sonny robbery trey future humphreys crushing dubbed discipline revising light informal discipline dutton standards bergman irvington consumed britt fourth cardin daoud britt brochin manipulate le equipment', \"virginian cory slashed unsustainable attempted oren amount n.y officers schaefer officers catlett obamacare officers officers oren officers peanut arraignments mechanics stubborn unitedhealthcare 'll arraignments arraignments enforced smallwood predecessors conn. peanut officers oren officers stanwood page learn memorials collided reggie smallwood\", 'oval combination albuquerque tolling albuquerque deed records offline albuquerque outcry outcry ensign cooperation czech smack deed matosantos conn. strained berry inducing rankings hilts jungle casting zajac skipping incarcerated commemorative hopping suffocate incarcerated prevail sincere selectman kennesaw ticketing prevail selectman suffocate']\n",
      "ipdb> len(true_token_idxs\n",
      "*** SyntaxError: unexpected EOF while parsing\n",
      "ipdb> len(true_token_idxs)\n",
      "8\n",
      "ipdb> sample_batch\n",
      "[<data_structure.Instance object at 0x7fe7c7eb9e10>, <data_structure.Instance object at 0x7fe7d6b22d30>, <data_structure.Instance object at 0x7fe7d1319c18>, <data_structure.Instance object at 0x7fe7b81af400>, <data_structure.Instance object at 0x7fe7d68c5898>, <data_structure.Instance object at 0x7fe7d1dd5ef0>, <data_structure.Instance object at 0x7fe7c7d02d30>, <data_structure.Instance object at 0x7fe7ba844080>, <data_structure.Instance object at 0x7fe7a0df4ba8>, <data_structure.Instance object at 0x7fe7d6e37518>, <data_structure.Instance object at 0x7fe7d1152550>, <data_structure.Instance object at 0x7fe7c29a9240>, <data_structure.Instance object at 0x7fe7bd934400>, <data_structure.Instance object at 0x7fe7a03592e8>, <data_structure.Instance object at 0x7fe7dd78d128>, <data_structure.Instance object at 0x7fe7d1284fd0>, <data_structure.Instance object at 0x7fe7bd8919e8>, <data_structure.Instance object at 0x7fe7bd0ff748>, <data_structure.Instance object at 0x7fe7d6957898>, <data_structure.Instance object at 0x7fe79fd60c50>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--KeyboardInterrupt--\n",
      "ipdb> exit\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "_logvars, _means, _kl_losses, _latents, _output_logits = sess.run([logvars, means, kl_losses, latents, output_logits], feed_dict=feed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 32), (32, 32), (32,), (32, 32))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_logvars.shape, _means.shape, _kl_losses.shape, _latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_output_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dec_target_idxs_do' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-7de59bc2cc54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_output_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dec_target_idxs_do\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dec_mask_tokens_do\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_recon_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_kl_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_target_idxs_do\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_mask_tokens_do\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dec_target_idxs_do' is not defined"
     ]
    }
   ],
   "source": [
    "_output_logits, _dec_target_idxs_do, _dec_mask_tokens_do, _recon_loss, _kl_losses, _ = sess.run([output_logits, dec_target_idxs_do, dec_mask_tokens_do, recon_loss, kl_losses, opt], feed_dict=feed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 46)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_max(output_logits, 2).eval(session=sess, feed_dict=feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 46, 20000), (120, 46), (120, 46))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_output_logits.shape, _dec_target_idxs_do.shape, _dec_mask_tokens_do.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "_logits = np.exp(_output_logits) / np.sum(np.exp(_output_logits), 2)[:, :, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "_idxs = _dec_target_idxs_do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "_losses = np.array([[-np.log(_logits[i, j, _idxs[i, j]]) for j in range(_idxs.shape[1])] for i in range(_idxs.shape[0])]) * _dec_mask_tokens_do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.903732"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(_losses)/np.sum(_dec_mask_tokens_do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.903732"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_recon_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_kl_losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
