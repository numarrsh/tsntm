{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from data_structure import get_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "from topic_model import TopicModel\n",
    "\n",
    "from topic_beam_search_decoder import BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '1', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/apnews/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/rnn_vae', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'apnews', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 1000, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 500, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.1, 'lr')\n",
    "flags.DEFINE_float('reg', 0.1, 'regularization term')\n",
    "flags.DEFINE_float('beta', 0.001, 'initial value of beta')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_integer('warmup', 5000, 'warmup period for KL')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 10, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_topic', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('PAD_IDX', word_to_idx[PAD], 'PAD_IDX')\n",
    "flags.DEFINE_integer('UNK_IDX', word_to_idx[UNK], 'UNK_IDX')\n",
    "flags.DEFINE_integer('BOS_IDX', word_to_idx[BOS], 'BOS_IDX')\n",
    "flags.DEFINE_integer('EOS_IDX', word_to_idx[EOS], 'EOS_IDX')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "maximum_iterations = max([max([instance.max_sent_l for instance in batch]) for ct, batch in dev_batches])\n",
    "flags.DEFINE_integer('maximum_iterations', maximum_iterations, 'maximum_iterations')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build language model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['bow'] = tf.placeholder(tf.float32, [None, config.dim_bow], name='bow')\n",
    "t_variables['input_token_idxs'] = tf.placeholder(tf.int32, [None, None], name='input_token_idxs')\n",
    "t_variables['dec_input_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_input_idxs')\n",
    "t_variables['dec_target_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_target_idxs')\n",
    "t_variables['batch_l'] = tf.placeholder(tf.int32, name='batch_l')\n",
    "t_variables['doc_l'] = tf.placeholder(tf.int32, [None], name='doc_l')\n",
    "t_variables['sent_l'] = tf.placeholder(tf.int32, [None], name='sent_l')\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train', assertion=False):\n",
    "    def token_dropout(sent_idxs):\n",
    "        sent_idxs_dropout = np.asarray(sent_idxs)\n",
    "        sent_idxs_dropout[np.random.rand(len(sent_idxs)) > config.word_keep_prob] = config.UNK_IDX\n",
    "        return list(sent_idxs_dropout)\n",
    "\n",
    "    bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "    \n",
    "    doc_l = np.array([len(instance.token_idxs) for instance in batch])\n",
    "    \n",
    "    feed_input_token_idxs_list = [sent_idxs for instance in batch for sent_idxs in instance.token_idxs]\n",
    "    feed_dec_input_idxs_list = [[config.BOS_IDX] + token_dropout(sent_idxs) for sent_idxs in feed_input_token_idxs_list]\n",
    "    feed_dec_target_idxs_list = [sent_idxs + [config.EOS_IDX]  for sent_idxs in feed_input_token_idxs_list]\n",
    "        \n",
    "    sent_l = np.array([len(sent_idxs) for sent_idxs in feed_input_token_idxs_list], np.int32)\n",
    "    batch_l = len(sent_l)\n",
    "    \n",
    "    feed_input_token_idxs = pad_sequences(feed_input_token_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_input_idxs = pad_sequences(feed_dec_input_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_target_idxs = pad_sequences(feed_dec_target_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    \n",
    "    if assertion:\n",
    "        index = 0\n",
    "        for instance in batch:\n",
    "            for line_idxs in instance.token_idxs:\n",
    "                assert feed_input_token_idxs_list[index] == line_idxs\n",
    "                index += 1\n",
    "        assert feed_input_token_idxs.shape[1] == np.max(sent_l)\n",
    "        assert feed_dec_input_idxs.shape[1] == np.max(sent_l) + 1\n",
    "        assert feed_dec_target_idxs.shape[1] == np.max(sent_l) + 1\n",
    "    \n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "\n",
    "    feed_dict = {\n",
    "                t_variables['bow']: bow, \n",
    "                t_variables['batch_l']: batch_l, t_variables['doc_l']: doc_l, t_variables['sent_l']: sent_l, \n",
    "                t_variables['input_token_idxs']: feed_input_token_idxs, t_variables['dec_input_idxs']: feed_dec_input_idxs, t_variables['dec_target_idxs']: feed_dec_target_idxs, \n",
    "                t_variables['keep_prob']: keep_prob\n",
    "    }\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables, sess_init=False):\n",
    "#     if sess_init:\n",
    "#         sess = tf.Session()\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    if sess_init: sess.close()\n",
    "\n",
    "def debug_value(variables, return_value=False, sess_init=None):\n",
    "#     if sess_init:\n",
    "#         sess = tf.Session()\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "    if sess_init: sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "input_token_idxs = t_variables['input_token_idxs']\n",
    "batch_l = t_variables['batch_l']\n",
    "sent_l = t_variables['sent_l']\n",
    "max_sent_l = tf.reduce_max(sent_l)\n",
    "\n",
    "with tf.variable_scope('shared', reuse=False):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "with tf.variable_scope('sent/enc', reuse=False):\n",
    "    # get word embedding\n",
    "    enc_input = tf.nn.embedding_lookup(embeddings, input_token_idxs)\n",
    "\n",
    "    # get sentence embedding\n",
    "    _, enc_state = dynamic_rnn(enc_input, sent_l, config.dim_hidden, t_variables['keep_prob'])\n",
    "\n",
    "    means_infer = tf.keras.layers.Dense(units=config.dim_latent, name='mean')(enc_state)\n",
    "    logvars_infer = tf.keras.layers.Dense(units=config.dim_latent, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar')(enc_state)\n",
    "\n",
    "    # latent vector from gaussian mixture    \n",
    "    latents_input = sample_latents(means_infer, logvars_infer) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for decoding\n",
    "dec_sent_l = tf.add(sent_l, 1)\n",
    "dec_input_idxs = t_variables['dec_input_idxs']\n",
    "dec_input = tf.nn.embedding_lookup(embeddings, dec_input_idxs)\n",
    "\n",
    "dec_latents_input = tf.tile(tf.expand_dims(latents_input, 1), [1, tf.shape(dec_input)[1], 1])\n",
    "dec_concat_input = tf.concat([dec_input, dec_latents_input], -1)\n",
    "\n",
    "# decode for training\n",
    "with tf.variable_scope('sent/dec/rnn', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32, reuse=False):\n",
    "    dec_cell = tf.contrib.rnn.GRUCell(config.dim_hidden)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob = t_variables['keep_prob'])\n",
    "\n",
    "    dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(latents_input)\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_concat_input, sequence_length=dec_sent_l)\n",
    "\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=dec_cell,\n",
    "        helper=helper,\n",
    "        initial_state=dec_initial_state)\n",
    "\n",
    "    dec_outputs, _, output_sent_l = tf.contrib.seq2seq.dynamic_decode(train_decoder)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(config.n_vocab, use_bias=False, name='out')\n",
    "    output_logits = output_layer(dec_outputs.rnn_output)\n",
    "    \n",
    "    output_token_idxs = tf.argmax(output_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tokens = tf.fill([batch_l], config.BOS_IDX)\n",
    "end_token = config.EOS_IDX\n",
    "\n",
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    infer_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_infer)\n",
    "    beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(infer_dec_initial_state, multiplier=config.beam_width)\n",
    "    beam_latents_input = tf.contrib.seq2seq.tile_batch(means_infer, multiplier=config.beam_width) # added\n",
    "    \n",
    "    beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=beam_latents_input)\n",
    "\n",
    "    beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    beam_output_token_idxs = beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    inter_means_infer = tf.placeholder(tf.float32, [None, config.dim_latent])\n",
    "    \n",
    "    inter_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(inter_means_infer)\n",
    "    inter_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(inter_dec_initial_state, multiplier=config.beam_width)\n",
    "    inter_beam_latents_input = tf.contrib.seq2seq.tile_batch(inter_means_infer, multiplier=config.beam_width) # added\n",
    "    \n",
    "    inter_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=inter_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=inter_beam_latents_input)\n",
    "\n",
    "    inter_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inter_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    inter_beam_output_token_idxs = inter_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language modeling cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target and mask\n",
    "dec_target_idxs = t_variables['dec_target_idxs']\n",
    "dec_mask_tokens = tf.sequence_mask(dec_sent_l, maxlen=max_sent_l+1, dtype=tf.float32)\n",
    "\n",
    "# nll for each token (averaged over batch & sentence)\n",
    "sent_loss_recon = tf.contrib.seq2seq.sequence_loss(output_logits, dec_target_idxs, dec_mask_tokens)\n",
    "\n",
    "sent_loss_kl = compute_kl_loss(means_infer, logvars_infer) # KL divergence b/w latent dist & gaussian std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = tf.Variable(config.beta, name='beta', trainable=False) if config.warmup > 0 else tf.constant(1., name='beta')\n",
    "update_beta = tf.assign_add(beta, 1./(config.warmup*len(train_batches)))\n",
    "loss = sent_loss_recon + beta * sent_loss_kl\n",
    "\n",
    "# define optimizer\n",
    "if config.opt == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif config.opt == 'Adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "    \n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_sents(token_idxs, config, idx_to_word):\n",
    "    sents = []\n",
    "    for sent_idxs in token_idxs:\n",
    "        tokens = []\n",
    "        for idx in sent_idxs:\n",
    "            if idx == config.EOS_IDX: break\n",
    "            tokens.append(idx_to_word[idx])\n",
    "        sent = ' '.join(tokens)\n",
    "        sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch = sess.run(loss, feed_dict = feed_dict)\n",
    "        losses.append(loss_batch)\n",
    "    loss_mean = np.mean(losses)\n",
    "    return loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_batch):\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    pred_token_idxs = sess.run(output_token_idxs, feed_dict = feed_dict)\n",
    "    true_token_idxs = [sent_idxs for instance in sample_batch for sent_idxs in instance.token_idxs]\n",
    "    \n",
    "    assert len(pred_token_idxs) == len(true_token_idxs)\n",
    "    \n",
    "    pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "    true_sents = idxs_to_sents(true_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    for true_sent, pred_sent in zip(true_sents, pred_sents):        \n",
    "        print('True: %s' % true_sent)\n",
    "        print('Pred: %s' % pred_sent)\n",
    "        \n",
    "def print_beam_sample(sample_batch):\n",
    "    feed_dict = get_feed_dict(sample_batch, mode='test')\n",
    "    pred_token_idxs = sess.run(beam_output_token_idxs, feed_dict = feed_dict)\n",
    "    true_token_idxs = [sent_idxs for instance in sample_batch for sent_idxs in instance.token_idxs]\n",
    "    \n",
    "    assert len(pred_token_idxs) == len(true_token_idxs)\n",
    "    \n",
    "    pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "    true_sents = idxs_to_sents(true_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    for true_sent, pred_sent in zip(true_sents, pred_sents):        \n",
    "        print('True: %s' % true_sent)\n",
    "        print('Pred: %s' % pred_sent)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "logs = []\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "saver = tf.train.Saver(max_to_keep=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "028[s], 26[s], Ep: 00, Ct: 00000|TR LOSS: 10.34 LM NLL: 10.34, KL: 0.00|DE LOSS: 10.34|BETA: 0.001000\n",
      "093[s], 26[s], Ep: 00, Ct: 00500|TR LOSS: 7.39 LM NLL: 7.37, KL: 22.99|DE LOSS: 6.88|BETA: 0.001035\n",
      "093[s], 26[s], Ep: 00, Ct: 01000|TR LOSS: 7.13 LM NLL: 7.11, KL: 21.87|DE LOSS: 6.73|BETA: 0.001069\n",
      "093[s], 26[s], Ep: 00, Ct: 01500|TR LOSS: 7.02 LM NLL: 7.00, KL: 21.06|DE LOSS: 6.69|BETA: 0.001104\n",
      "094[s], 26[s], Ep: 00, Ct: 02000|TR LOSS: 6.96 LM NLL: 6.94, KL: 20.39|DE LOSS: 6.67|BETA: 0.001139\n",
      "094[s], 26[s], Ep: 00, Ct: 02500|TR LOSS: 6.91 LM NLL: 6.89, KL: 19.81|DE LOSS: 6.65|BETA: 0.001173\n",
      "060[s], 07[s], Ep: 01, Ct: 00000|TR LOSS: 6.89 LM NLL: 6.87, KL: 19.38|DE LOSS: 6.73|BETA: 0.001200\n",
      "093[s], 26[s], Ep: 01, Ct: 00500|TR LOSS: 6.86 LM NLL: 6.84, KL: 18.80|DE LOSS: 6.59|BETA: 0.001235\n",
      "094[s], 26[s], Ep: 01, Ct: 01000|TR LOSS: 6.83 LM NLL: 6.81, KL: 18.29|DE LOSS: 6.51|BETA: 0.001269\n",
      "094[s], 26[s], Ep: 01, Ct: 01500|TR LOSS: 6.80 LM NLL: 6.78, KL: 17.88|DE LOSS: 6.41|BETA: 0.001304\n",
      "094[s], 26[s], Ep: 01, Ct: 02000|TR LOSS: 6.76 LM NLL: 6.74, KL: 17.73|DE LOSS: 6.37|BETA: 0.001339\n",
      "094[s], 26[s], Ep: 01, Ct: 02500|TR LOSS: 6.73 LM NLL: 6.71, KL: 17.79|DE LOSS: 6.25|BETA: 0.001374\n",
      "078[s], 26[s], Ep: 02, Ct: 00000|TR LOSS: 6.70 LM NLL: 6.68, KL: 18.00|DE LOSS: 6.20|BETA: 0.001400\n",
      "091[s], 25[s], Ep: 02, Ct: 00500|TR LOSS: 6.66 LM NLL: 6.64, KL: 18.45|DE LOSS: 6.09|BETA: 0.001435\n",
      "093[s], 26[s], Ep: 02, Ct: 01000|TR LOSS: 6.62 LM NLL: 6.60, KL: 19.08|DE LOSS: 6.01|BETA: 0.001470\n",
      "093[s], 26[s], Ep: 02, Ct: 01500|TR LOSS: 6.58 LM NLL: 6.56, KL: 19.80|DE LOSS: 5.91|BETA: 0.001504\n",
      "093[s], 26[s], Ep: 02, Ct: 02000|TR LOSS: 6.54 LM NLL: 6.52, KL: 20.59|DE LOSS: 5.87|BETA: 0.001539\n",
      "094[s], 27[s], Ep: 02, Ct: 02500|TR LOSS: 6.50 LM NLL: 6.47, KL: 21.42|DE LOSS: 5.78|BETA: 0.001574\n",
      "078[s], 26[s], Ep: 03, Ct: 00000|TR LOSS: 6.47 LM NLL: 6.44, KL: 22.07|DE LOSS: 5.77|BETA: 0.001600\n",
      "093[s], 26[s], Ep: 03, Ct: 00500|TR LOSS: 6.43 LM NLL: 6.40, KL: 22.95|DE LOSS: 5.60|BETA: 0.001635\n",
      "094[s], 26[s], Ep: 03, Ct: 01000|TR LOSS: 6.39 LM NLL: 6.36, KL: 23.85|DE LOSS: 5.49|BETA: 0.001670\n",
      "075[s], 07[s], Ep: 03, Ct: 01500|TR LOSS: 6.35 LM NLL: 6.32, KL: 24.77|DE LOSS: 5.50|BETA: 0.001704\n",
      "094[s], 26[s], Ep: 03, Ct: 02000|TR LOSS: 6.31 LM NLL: 6.27, KL: 25.69|DE LOSS: 5.34|BETA: 0.001739\n",
      "094[s], 26[s], Ep: 03, Ct: 02500|TR LOSS: 6.27 LM NLL: 6.23, KL: 26.59|DE LOSS: 5.29|BETA: 0.001774\n",
      "078[s], 26[s], Ep: 04, Ct: 00000|TR LOSS: 6.24 LM NLL: 6.20, KL: 27.30|DE LOSS: 5.20|BETA: 0.001800\n",
      "075[s], 07[s], Ep: 04, Ct: 00500|TR LOSS: 6.20 LM NLL: 6.16, KL: 28.22|DE LOSS: 5.21|BETA: 0.001835\n",
      "094[s], 26[s], Ep: 04, Ct: 01000|TR LOSS: 6.16 LM NLL: 6.12, KL: 29.12|DE LOSS: 5.16|BETA: 0.001870\n",
      "094[s], 26[s], Ep: 04, Ct: 01500|TR LOSS: 6.12 LM NLL: 6.08, KL: 30.04|DE LOSS: 4.99|BETA: 0.001904\n",
      "075[s], 08[s], Ep: 04, Ct: 02000|TR LOSS: 6.09 LM NLL: 6.04, KL: 30.96|DE LOSS: 5.00|BETA: 0.001939\n",
      "090[s], 26[s], Ep: 04, Ct: 02500|TR LOSS: 6.05 LM NLL: 6.00, KL: 31.87|DE LOSS: 4.85|BETA: 0.001974\n",
      "056[s], 07[s], Ep: 05, Ct: 00000|TR LOSS: 6.02 LM NLL: 5.97, KL: 32.56|DE LOSS: 4.91|BETA: 0.002001\n",
      "088[s], 25[s], Ep: 05, Ct: 00500|TR LOSS: 5.98 LM NLL: 5.93, KL: 33.45|DE LOSS: 4.77|BETA: 0.002035\n",
      "087[s], 25[s], Ep: 05, Ct: 01000|TR LOSS: 5.95 LM NLL: 5.89, KL: 34.33|DE LOSS: 4.76|BETA: 0.002070\n",
      "090[s], 25[s], Ep: 05, Ct: 01500|TR LOSS: 5.91 LM NLL: 5.85, KL: 35.19|DE LOSS: 4.64|BETA: 0.002105\n",
      "087[s], 25[s], Ep: 05, Ct: 02000|TR LOSS: 5.87 LM NLL: 5.81, KL: 36.03|DE LOSS: 4.60|BETA: 0.002139\n",
      "087[s], 25[s], Ep: 05, Ct: 02500|TR LOSS: 5.84 LM NLL: 5.78, KL: 36.86|DE LOSS: 4.55|BETA: 0.002174\n",
      "073[s], 25[s], Ep: 06, Ct: 00000|TR LOSS: 5.81 LM NLL: 5.75, KL: 37.48|DE LOSS: 4.51|BETA: 0.002201\n",
      "087[s], 25[s], Ep: 06, Ct: 00500|TR LOSS: 5.78 LM NLL: 5.71, KL: 38.28|DE LOSS: 4.48|BETA: 0.002235\n",
      "068[s], 07[s], Ep: 06, Ct: 01000|TR LOSS: 5.75 LM NLL: 5.68, KL: 39.05|DE LOSS: 4.56|BETA: 0.002270\n",
      "087[s], 25[s], Ep: 06, Ct: 01500|TR LOSS: 5.71 LM NLL: 5.64, KL: 39.82|DE LOSS: 4.39|BETA: 0.002305\n",
      "088[s], 26[s], Ep: 06, Ct: 02000|TR LOSS: 5.68 LM NLL: 5.61, KL: 40.58|DE LOSS: 4.39|BETA: 0.002339\n",
      "087[s], 25[s], Ep: 06, Ct: 02500|TR LOSS: 5.65 LM NLL: 5.57, KL: 41.32|DE LOSS: 4.30|BETA: 0.002374\n",
      "074[s], 27[s], Ep: 07, Ct: 00000|TR LOSS: 5.63 LM NLL: 5.55, KL: 41.87|DE LOSS: 4.27|BETA: 0.002401\n",
      "088[s], 25[s], Ep: 07, Ct: 00500|TR LOSS: 5.60 LM NLL: 5.52, KL: 42.58|DE LOSS: 4.22|BETA: 0.002435\n",
      "090[s], 26[s], Ep: 07, Ct: 01000|TR LOSS: 5.57 LM NLL: 5.48, KL: 43.25|DE LOSS: 4.19|BETA: 0.002470\n",
      "091[s], 26[s], Ep: 07, Ct: 01500|TR LOSS: 5.54 LM NLL: 5.45, KL: 43.91|DE LOSS: 4.15|BETA: 0.002505\n",
      "072[s], 07[s], Ep: 07, Ct: 02000|TR LOSS: 5.51 LM NLL: 5.42, KL: 44.55|DE LOSS: 4.30|BETA: 0.002539\n",
      "072[s], 07[s], Ep: 07, Ct: 02500|TR LOSS: 5.48 LM NLL: 5.39, KL: 45.17|DE LOSS: 4.20|BETA: 0.002574\n",
      "076[s], 26[s], Ep: 08, Ct: 00000|TR LOSS: 5.46 LM NLL: 5.37, KL: 45.62|DE LOSS: 4.11|BETA: 0.002601\n",
      "091[s], 26[s], Ep: 08, Ct: 00500|TR LOSS: 5.43 LM NLL: 5.34, KL: 46.20|DE LOSS: 4.08|BETA: 0.002636\n",
      "090[s], 25[s], Ep: 08, Ct: 01000|TR LOSS: 5.41 LM NLL: 5.31, KL: 46.76|DE LOSS: 4.04|BETA: 0.002670\n",
      "072[s], 07[s], Ep: 08, Ct: 01500|TR LOSS: 5.38 LM NLL: 5.28, KL: 47.29|DE LOSS: 4.11|BETA: 0.002705\n",
      "072[s], 07[s], Ep: 08, Ct: 02000|TR LOSS: 5.36 LM NLL: 5.26, KL: 47.81|DE LOSS: 4.13|BETA: 0.002740\n",
      "090[s], 25[s], Ep: 08, Ct: 02500|TR LOSS: 5.33 LM NLL: 5.23, KL: 48.31|DE LOSS: 3.98|BETA: 0.002774\n",
      "074[s], 25[s], Ep: 09, Ct: 00000|TR LOSS: 5.31 LM NLL: 5.21, KL: 48.69|DE LOSS: 3.97|BETA: 0.002801\n",
      "087[s], 25[s], Ep: 09, Ct: 00500|TR LOSS: 5.29 LM NLL: 5.19, KL: 49.16|DE LOSS: 3.97|BETA: 0.002836\n",
      "087[s], 25[s], Ep: 09, Ct: 01000|TR LOSS: 5.27 LM NLL: 5.16, KL: 49.61|DE LOSS: 3.95|BETA: 0.002870\n",
      "087[s], 25[s], Ep: 09, Ct: 01500|TR LOSS: 5.25 LM NLL: 5.14, KL: 50.06|DE LOSS: 3.92|BETA: 0.002905\n",
      "068[s], 07[s], Ep: 09, Ct: 02000|TR LOSS: 5.22 LM NLL: 5.11, KL: 50.49|DE LOSS: 3.94|BETA: 0.002940\n",
      "069[s], 06[s], Ep: 09, Ct: 02500|TR LOSS: 5.20 LM NLL: 5.09, KL: 50.90|DE LOSS: 3.95|BETA: 0.002974\n",
      "054[s], 07[s], Ep: 10, Ct: 00000|TR LOSS: 5.19 LM NLL: 5.07, KL: 51.21|DE LOSS: 4.44|BETA: 0.003001\n",
      "087[s], 25[s], Ep: 10, Ct: 00500|TR LOSS: 5.17 LM NLL: 5.05, KL: 51.60|DE LOSS: 3.87|BETA: 0.003036\n",
      "069[s], 07[s], Ep: 10, Ct: 01000|TR LOSS: 5.15 LM NLL: 5.03, KL: 51.98|DE LOSS: 3.91|BETA: 0.003070\n",
      "068[s], 07[s], Ep: 10, Ct: 01500|TR LOSS: 5.13 LM NLL: 5.01, KL: 52.35|DE LOSS: 3.88|BETA: 0.003105\n",
      "068[s], 07[s], Ep: 10, Ct: 02000|TR LOSS: 5.11 LM NLL: 4.99, KL: 52.71|DE LOSS: 3.93|BETA: 0.003140\n",
      "088[s], 25[s], Ep: 10, Ct: 02500|TR LOSS: 5.09 LM NLL: 4.96, KL: 53.05|DE LOSS: 3.82|BETA: 0.003174\n",
      "084[s], 36[s], Ep: 11, Ct: 00000|TR LOSS: 5.07 LM NLL: 4.95, KL: 53.31|DE LOSS: 3.82|BETA: 0.003201\n",
      "068[s], 07[s], Ep: 11, Ct: 00500|TR LOSS: 5.06 LM NLL: 4.93, KL: 53.64|DE LOSS: 3.91|BETA: 0.003236\n",
      "103[s], 41[s], Ep: 11, Ct: 01000|TR LOSS: 5.04 LM NLL: 4.91, KL: 53.96|DE LOSS: 3.79|BETA: 0.003271\n",
      "068[s], 06[s], Ep: 11, Ct: 01500|TR LOSS: 5.02 LM NLL: 4.89, KL: 54.27|DE LOSS: 3.83|BETA: 0.003305\n",
      "087[s], 25[s], Ep: 11, Ct: 02000|TR LOSS: 5.00 LM NLL: 4.87, KL: 54.57|DE LOSS: 3.78|BETA: 0.003340\n",
      "087[s], 25[s], Ep: 11, Ct: 02500|TR LOSS: 4.99 LM NLL: 4.85, KL: 54.86|DE LOSS: 3.78|BETA: 0.003375\n",
      "073[s], 25[s], Ep: 12, Ct: 00000|TR LOSS: 4.97 LM NLL: 4.84, KL: 55.08|DE LOSS: 3.76|BETA: 0.003401\n",
      "071[s], 07[s], Ep: 12, Ct: 00500|TR LOSS: 4.96 LM NLL: 4.82, KL: 55.35|DE LOSS: 3.81|BETA: 0.003436\n",
      "087[s], 25[s], Ep: 12, Ct: 01000|TR LOSS: 4.94 LM NLL: 4.80, KL: 55.62|DE LOSS: 3.75|BETA: 0.003471\n",
      "070[s], 07[s], Ep: 12, Ct: 01500|TR LOSS: 4.93 LM NLL: 4.79, KL: 55.88|DE LOSS: 3.79|BETA: 0.003505\n",
      "089[s], 26[s], Ep: 12, Ct: 02000|TR LOSS: 4.91 LM NLL: 4.77, KL: 56.14|DE LOSS: 3.74|BETA: 0.003540\n",
      "091[s], 27[s], Ep: 12, Ct: 02500|TR LOSS: 4.90 LM NLL: 4.75, KL: 56.39|DE LOSS: 3.73|BETA: 0.003575\n",
      "060[s], 07[s], Ep: 13, Ct: 00000|TR LOSS: 4.89 LM NLL: 4.74, KL: 56.57|DE LOSS: 3.74|BETA: 0.003601\n",
      "093[s], 26[s], Ep: 13, Ct: 00500|TR LOSS: 4.87 LM NLL: 4.72, KL: 56.81|DE LOSS: 3.73|BETA: 0.003636\n",
      "075[s], 07[s], Ep: 13, Ct: 01000|TR LOSS: 4.86 LM NLL: 4.71, KL: 57.04|DE LOSS: 3.85|BETA: 0.003671\n",
      "093[s], 26[s], Ep: 13, Ct: 01500|TR LOSS: 4.84 LM NLL: 4.69, KL: 57.27|DE LOSS: 3.71|BETA: 0.003705\n",
      "075[s], 07[s], Ep: 13, Ct: 02000|TR LOSS: 4.83 LM NLL: 4.68, KL: 57.48|DE LOSS: 3.72|BETA: 0.003740\n",
      "093[s], 26[s], Ep: 13, Ct: 02500|TR LOSS: 4.82 LM NLL: 4.66, KL: 57.70|DE LOSS: 3.69|BETA: 0.003775\n",
      "079[s], 26[s], Ep: 14, Ct: 00000|TR LOSS: 4.81 LM NLL: 4.65, KL: 57.86|DE LOSS: 3.67|BETA: 0.003801\n",
      "075[s], 07[s], Ep: 14, Ct: 00500|TR LOSS: 4.79 LM NLL: 4.64, KL: 58.06|DE LOSS: 3.68|BETA: 0.003836\n",
      "075[s], 08[s], Ep: 14, Ct: 01000|TR LOSS: 4.78 LM NLL: 4.62, KL: 58.26|DE LOSS: 3.71|BETA: 0.003871\n",
      "094[s], 26[s], Ep: 14, Ct: 01500|TR LOSS: 4.77 LM NLL: 4.61, KL: 58.45|DE LOSS: 3.66|BETA: 0.003906\n",
      "093[s], 26[s], Ep: 14, Ct: 02000|TR LOSS: 4.75 LM NLL: 4.59, KL: 58.64|DE LOSS: 3.66|BETA: 0.003940\n",
      "093[s], 26[s], Ep: 14, Ct: 02500|TR LOSS: 4.74 LM NLL: 4.58, KL: 58.83|DE LOSS: 3.65|BETA: 0.003975\n",
      "059[s], 07[s], Ep: 15, Ct: 00000|TR LOSS: 4.73 LM NLL: 4.57, KL: 58.97|DE LOSS: 3.70|BETA: 0.004002\n",
      "075[s], 07[s], Ep: 15, Ct: 00500|TR LOSS: 4.72 LM NLL: 4.56, KL: 59.14|DE LOSS: 3.69|BETA: 0.004036\n",
      "075[s], 07[s], Ep: 15, Ct: 01000|TR LOSS: 4.71 LM NLL: 4.54, KL: 59.31|DE LOSS: 3.68|BETA: 0.004071\n",
      "094[s], 26[s], Ep: 15, Ct: 01500|TR LOSS: 4.70 LM NLL: 4.53, KL: 59.48|DE LOSS: 3.64|BETA: 0.004106\n",
      "074[s], 07[s], Ep: 15, Ct: 02000|TR LOSS: 4.69 LM NLL: 4.52, KL: 59.65|DE LOSS: 3.65|BETA: 0.004140\n",
      "093[s], 26[s], Ep: 15, Ct: 02500|TR LOSS: 4.68 LM NLL: 4.51, KL: 59.81|DE LOSS: 3.62|BETA: 0.004175\n",
      "059[s], 08[s], Ep: 16, Ct: 00000|TR LOSS: 4.67 LM NLL: 4.50, KL: 59.93|DE LOSS: 3.63|BETA: 0.004202\n",
      "075[s], 07[s], Ep: 16, Ct: 00500|TR LOSS: 4.66 LM NLL: 4.48, KL: 60.08|DE LOSS: 3.63|BETA: 0.004236\n",
      "072[s], 07[s], Ep: 16, Ct: 01000|TR LOSS: 4.65 LM NLL: 4.47, KL: 60.23|DE LOSS: 3.79|BETA: 0.004271\n",
      "076[s], 07[s], Ep: 16, Ct: 01500|TR LOSS: 4.64 LM NLL: 4.46, KL: 60.38|DE LOSS: 3.63|BETA: 0.004306\n",
      "075[s], 07[s], Ep: 16, Ct: 02000|TR LOSS: 4.63 LM NLL: 4.45, KL: 60.52|DE LOSS: 3.69|BETA: 0.004340\n",
      "094[s], 26[s], Ep: 16, Ct: 02500|TR LOSS: 4.62 LM NLL: 4.44, KL: 60.66|DE LOSS: 3.61|BETA: 0.004375\n",
      "059[s], 07[s], Ep: 17, Ct: 00000|TR LOSS: 4.61 LM NLL: 4.43, KL: 60.77|DE LOSS: 3.64|BETA: 0.004402\n",
      "092[s], 26[s], Ep: 17, Ct: 00500|TR LOSS: 4.60 LM NLL: 4.42, KL: 60.90|DE LOSS: 3.60|BETA: 0.004436\n",
      "073[s], 07[s], Ep: 17, Ct: 01000|TR LOSS: 4.59 LM NLL: 4.40, KL: 61.03|DE LOSS: 3.62|BETA: 0.004471\n",
      "073[s], 07[s], Ep: 17, Ct: 01500|TR LOSS: 4.58 LM NLL: 4.39, KL: 61.16|DE LOSS: 3.67|BETA: 0.004506\n",
      "073[s], 07[s], Ep: 17, Ct: 02000|TR LOSS: 4.57 LM NLL: 4.38, KL: 61.29|DE LOSS: 3.62|BETA: 0.004541\n",
      "075[s], 07[s], Ep: 17, Ct: 02500|TR LOSS: 4.56 LM NLL: 4.37, KL: 61.41|DE LOSS: 3.61|BETA: 0.004575\n",
      "078[s], 26[s], Ep: 18, Ct: 00000|TR LOSS: 4.55 LM NLL: 4.36, KL: 61.50|DE LOSS: 3.58|BETA: 0.004602\n",
      "075[s], 07[s], Ep: 18, Ct: 00500|TR LOSS: 4.54 LM NLL: 4.35, KL: 61.62|DE LOSS: 3.58|BETA: 0.004637\n",
      "074[s], 07[s], Ep: 18, Ct: 01000|TR LOSS: 4.54 LM NLL: 4.34, KL: 61.73|DE LOSS: 3.59|BETA: 0.004671\n",
      "092[s], 26[s], Ep: 18, Ct: 01500|TR LOSS: 4.53 LM NLL: 4.33, KL: 61.85|DE LOSS: 3.57|BETA: 0.004706\n",
      "092[s], 26[s], Ep: 18, Ct: 02000|TR LOSS: 4.52 LM NLL: 4.32, KL: 61.96|DE LOSS: 3.56|BETA: 0.004741\n",
      "091[s], 25[s], Ep: 18, Ct: 02500|TR LOSS: 4.51 LM NLL: 4.31, KL: 62.07|DE LOSS: 3.55|BETA: 0.004775\n",
      "059[s], 07[s], Ep: 19, Ct: 00000|TR LOSS: 4.50 LM NLL: 4.31, KL: 62.15|DE LOSS: 3.57|BETA: 0.004802\n",
      "094[s], 26[s], Ep: 19, Ct: 00500|TR LOSS: 4.49 LM NLL: 4.30, KL: 62.25|DE LOSS: 3.55|BETA: 0.004837\n",
      "075[s], 07[s], Ep: 19, Ct: 01000|TR LOSS: 4.49 LM NLL: 4.29, KL: 62.35|DE LOSS: 3.58|BETA: 0.004871\n",
      "075[s], 07[s], Ep: 19, Ct: 01500|TR LOSS: 4.48 LM NLL: 4.28, KL: 62.45|DE LOSS: 3.62|BETA: 0.004906\n",
      "074[s], 07[s], Ep: 19, Ct: 02000|TR LOSS: 4.47 LM NLL: 4.27, KL: 62.55|DE LOSS: 3.61|BETA: 0.004941\n",
      "074[s], 07[s], Ep: 19, Ct: 02500|TR LOSS: 4.46 LM NLL: 4.26, KL: 62.64|DE LOSS: 3.74|BETA: 0.004975\n",
      "080[s], 29[s], Ep: 20, Ct: 00000|TR LOSS: 4.46 LM NLL: 4.25, KL: 62.72|DE LOSS: 3.54|BETA: 0.005002\n",
      "072[s], 07[s], Ep: 20, Ct: 00500|TR LOSS: 4.45 LM NLL: 4.24, KL: 62.81|DE LOSS: 3.56|BETA: 0.005037\n",
      "092[s], 26[s], Ep: 20, Ct: 01000|TR LOSS: 4.44 LM NLL: 4.23, KL: 62.90|DE LOSS: 3.54|BETA: 0.005071\n",
      "094[s], 26[s], Ep: 20, Ct: 01500|TR LOSS: 4.43 LM NLL: 4.22, KL: 62.99|DE LOSS: 3.53|BETA: 0.005106\n",
      "075[s], 07[s], Ep: 20, Ct: 02000|TR LOSS: 4.43 LM NLL: 4.21, KL: 63.07|DE LOSS: 3.63|BETA: 0.005141\n",
      "075[s], 07[s], Ep: 20, Ct: 02500|TR LOSS: 4.42 LM NLL: 4.21, KL: 63.16|DE LOSS: 3.54|BETA: 0.005176\n",
      "077[s], 26[s], Ep: 21, Ct: 00000|TR LOSS: 4.41 LM NLL: 4.20, KL: 63.22|DE LOSS: 3.53|BETA: 0.005202\n",
      "073[s], 07[s], Ep: 21, Ct: 00500|TR LOSS: 4.41 LM NLL: 4.19, KL: 63.30|DE LOSS: 3.54|BETA: 0.005237\n",
      "094[s], 26[s], Ep: 21, Ct: 01000|TR LOSS: 4.40 LM NLL: 4.18, KL: 63.38|DE LOSS: 3.53|BETA: 0.005272\n",
      "076[s], 08[s], Ep: 21, Ct: 01500|TR LOSS: 4.39 LM NLL: 4.17, KL: 63.46|DE LOSS: 3.55|BETA: 0.005306\n",
      "076[s], 08[s], Ep: 21, Ct: 02000|TR LOSS: 4.39 LM NLL: 4.17, KL: 63.53|DE LOSS: 3.56|BETA: 0.005341\n",
      "094[s], 26[s], Ep: 21, Ct: 02500|TR LOSS: 4.38 LM NLL: 4.16, KL: 63.61|DE LOSS: 3.53|BETA: 0.005376\n",
      "060[s], 07[s], Ep: 22, Ct: 00000|TR LOSS: 4.37 LM NLL: 4.15, KL: 63.66|DE LOSS: 3.74|BETA: 0.005402\n",
      "094[s], 26[s], Ep: 22, Ct: 00500|TR LOSS: 4.37 LM NLL: 4.14, KL: 63.74|DE LOSS: 3.52|BETA: 0.005437\n",
      "076[s], 08[s], Ep: 22, Ct: 01000|TR LOSS: 4.36 LM NLL: 4.14, KL: 63.81|DE LOSS: 3.52|BETA: 0.005472\n",
      "076[s], 07[s], Ep: 22, Ct: 01500|TR LOSS: 4.35 LM NLL: 4.13, KL: 63.88|DE LOSS: 3.52|BETA: 0.005506\n",
      "074[s], 08[s], Ep: 22, Ct: 02000|TR LOSS: 4.35 LM NLL: 4.12, KL: 63.94|DE LOSS: 3.55|BETA: 0.005541\n",
      "075[s], 07[s], Ep: 22, Ct: 02500|TR LOSS: 4.34 LM NLL: 4.11, KL: 64.01|DE LOSS: 3.54|BETA: 0.005576\n",
      "058[s], 08[s], Ep: 23, Ct: 00000|TR LOSS: 4.34 LM NLL: 4.11, KL: 64.06|DE LOSS: 3.52|BETA: 0.005602\n",
      "076[s], 08[s], Ep: 23, Ct: 00500|TR LOSS: 4.33 LM NLL: 4.10, KL: 64.12|DE LOSS: 3.65|BETA: 0.005637\n",
      "074[s], 07[s], Ep: 23, Ct: 01000|TR LOSS: 4.32 LM NLL: 4.09, KL: 64.19|DE LOSS: 3.53|BETA: 0.005672\n",
      "073[s], 07[s], Ep: 23, Ct: 01500|TR LOSS: 4.32 LM NLL: 4.08, KL: 64.25|DE LOSS: 3.58|BETA: 0.005706\n",
      "075[s], 07[s], Ep: 23, Ct: 02000|TR LOSS: 4.31 LM NLL: 4.08, KL: 64.31|DE LOSS: 3.53|BETA: 0.005741\n",
      "075[s], 07[s], Ep: 23, Ct: 02500|TR LOSS: 4.31 LM NLL: 4.07, KL: 64.37|DE LOSS: 3.53|BETA: 0.005776\n",
      "079[s], 26[s], Ep: 24, Ct: 00000|TR LOSS: 4.30 LM NLL: 4.06, KL: 64.41|DE LOSS: 3.51|BETA: 0.005802\n",
      "091[s], 26[s], Ep: 24, Ct: 00500|TR LOSS: 4.30 LM NLL: 4.06, KL: 64.47|DE LOSS: 3.50|BETA: 0.005837\n",
      "076[s], 08[s], Ep: 24, Ct: 01000|TR LOSS: 4.29 LM NLL: 4.05, KL: 64.52|DE LOSS: 3.70|BETA: 0.005872\n",
      "074[s], 07[s], Ep: 24, Ct: 01500|TR LOSS: 4.28 LM NLL: 4.04, KL: 64.58|DE LOSS: 3.52|BETA: 0.005907\n",
      "094[s], 26[s], Ep: 24, Ct: 02000|TR LOSS: 4.28 LM NLL: 4.04, KL: 64.63|DE LOSS: 3.50|BETA: 0.005941\n",
      "075[s], 08[s], Ep: 24, Ct: 02500|TR LOSS: 4.27 LM NLL: 4.03, KL: 64.68|DE LOSS: 3.51|BETA: 0.005976\n",
      "077[s], 25[s], Ep: 25, Ct: 00000|TR LOSS: 4.27 LM NLL: 4.02, KL: 64.72|DE LOSS: 3.49|BETA: 0.006003\n",
      "075[s], 07[s], Ep: 25, Ct: 00500|TR LOSS: 4.26 LM NLL: 4.02, KL: 64.77|DE LOSS: 3.49|BETA: 0.006037\n",
      "072[s], 07[s], Ep: 25, Ct: 01000|TR LOSS: 4.26 LM NLL: 4.01, KL: 64.82|DE LOSS: 3.54|BETA: 0.006072\n",
      "075[s], 08[s], Ep: 25, Ct: 01500|TR LOSS: 4.25 LM NLL: 4.00, KL: 64.87|DE LOSS: 3.49|BETA: 0.006107\n",
      "074[s], 07[s], Ep: 25, Ct: 02000|TR LOSS: 4.25 LM NLL: 4.00, KL: 64.91|DE LOSS: 3.49|BETA: 0.006141\n",
      "075[s], 07[s], Ep: 25, Ct: 02500|TR LOSS: 4.24 LM NLL: 3.99, KL: 64.96|DE LOSS: 3.49|BETA: 0.006176\n",
      "059[s], 07[s], Ep: 26, Ct: 00000|TR LOSS: 4.24 LM NLL: 3.99, KL: 65.00|DE LOSS: 3.50|BETA: 0.006203\n",
      "074[s], 07[s], Ep: 26, Ct: 00500|TR LOSS: 4.23 LM NLL: 3.98, KL: 65.04|DE LOSS: 3.52|BETA: 0.006237\n",
      "092[s], 26[s], Ep: 26, Ct: 01000|TR LOSS: 4.23 LM NLL: 3.97, KL: 65.08|DE LOSS: 3.49|BETA: 0.006272\n",
      "075[s], 07[s], Ep: 26, Ct: 01500|TR LOSS: 4.22 LM NLL: 3.97, KL: 65.13|DE LOSS: 3.51|BETA: 0.006307\n",
      "075[s], 07[s], Ep: 26, Ct: 02000|TR LOSS: 4.22 LM NLL: 3.96, KL: 65.17|DE LOSS: 3.50|BETA: 0.006341\n",
      "076[s], 08[s], Ep: 26, Ct: 02500|TR LOSS: 4.21 LM NLL: 3.95, KL: 65.21|DE LOSS: 3.51|BETA: 0.006376\n",
      "078[s], 26[s], Ep: 27, Ct: 00000|TR LOSS: 4.21 LM NLL: 3.95, KL: 65.24|DE LOSS: 3.48|BETA: 0.006403\n",
      "075[s], 08[s], Ep: 27, Ct: 00500|TR LOSS: 4.20 LM NLL: 3.94, KL: 65.28|DE LOSS: 3.51|BETA: 0.006437\n",
      "075[s], 08[s], Ep: 27, Ct: 01000|TR LOSS: 4.20 LM NLL: 3.94, KL: 65.32|DE LOSS: 3.52|BETA: 0.006472\n",
      "075[s], 08[s], Ep: 27, Ct: 01500|TR LOSS: 4.20 LM NLL: 3.93, KL: 65.36|DE LOSS: 3.52|BETA: 0.006507\n",
      "076[s], 08[s], Ep: 27, Ct: 02000|TR LOSS: 4.19 LM NLL: 3.93, KL: 65.39|DE LOSS: 3.49|BETA: 0.006542\n",
      "075[s], 08[s], Ep: 27, Ct: 02500|TR LOSS: 4.19 LM NLL: 3.92, KL: 65.43|DE LOSS: 3.50|BETA: 0.006576\n",
      "060[s], 08[s], Ep: 28, Ct: 00000|TR LOSS: 4.18 LM NLL: 3.92, KL: 65.45|DE LOSS: 3.49|BETA: 0.006603\n",
      "075[s], 07[s], Ep: 28, Ct: 00500|TR LOSS: 4.18 LM NLL: 3.91, KL: 65.49|DE LOSS: 3.49|BETA: 0.006638\n",
      "076[s], 08[s], Ep: 28, Ct: 01000|TR LOSS: 4.17 LM NLL: 3.90, KL: 65.52|DE LOSS: 3.52|BETA: 0.006672\n",
      "076[s], 08[s], Ep: 28, Ct: 01500|TR LOSS: 4.17 LM NLL: 3.90, KL: 65.56|DE LOSS: 4.16|BETA: 0.006707\n",
      "075[s], 08[s], Ep: 28, Ct: 02000|TR LOSS: 4.16 LM NLL: 3.89, KL: 65.59|DE LOSS: 3.51|BETA: 0.006742\n",
      "075[s], 08[s], Ep: 28, Ct: 02500|TR LOSS: 4.16 LM NLL: 3.89, KL: 65.62|DE LOSS: 3.50|BETA: 0.006776\n",
      "060[s], 08[s], Ep: 29, Ct: 00000|TR LOSS: 4.16 LM NLL: 3.88, KL: 65.65|DE LOSS: 3.48|BETA: 0.006803\n",
      "076[s], 07[s], Ep: 29, Ct: 00500|TR LOSS: 4.15 LM NLL: 3.88, KL: 65.68|DE LOSS: 3.49|BETA: 0.006838\n",
      "094[s], 26[s], Ep: 29, Ct: 01000|TR LOSS: 4.15 LM NLL: 3.87, KL: 65.71|DE LOSS: 3.48|BETA: 0.006872\n",
      "076[s], 08[s], Ep: 29, Ct: 01500|TR LOSS: 4.14 LM NLL: 3.87, KL: 65.74|DE LOSS: 3.50|BETA: 0.006907\n",
      "075[s], 08[s], Ep: 29, Ct: 02000|TR LOSS: 4.14 LM NLL: 3.86, KL: 65.76|DE LOSS: 3.53|BETA: 0.006942\n",
      "075[s], 07[s], Ep: 29, Ct: 02500|TR LOSS: 4.14 LM NLL: 3.86, KL: 65.79|DE LOSS: 3.58|BETA: 0.006976\n",
      "060[s], 07[s], Ep: 30, Ct: 00000|TR LOSS: 4.13 LM NLL: 3.85, KL: 65.81|DE LOSS: 3.49|BETA: 0.007003\n",
      "076[s], 08[s], Ep: 30, Ct: 00500|TR LOSS: 4.13 LM NLL: 3.85, KL: 65.84|DE LOSS: 3.52|BETA: 0.007038\n",
      "076[s], 08[s], Ep: 30, Ct: 01000|TR LOSS: 4.13 LM NLL: 3.84, KL: 65.87|DE LOSS: 3.49|BETA: 0.007072\n",
      "075[s], 07[s], Ep: 30, Ct: 01500|TR LOSS: 4.12 LM NLL: 3.84, KL: 65.89|DE LOSS: 3.50|BETA: 0.007107\n",
      "075[s], 07[s], Ep: 30, Ct: 02000|TR LOSS: 4.12 LM NLL: 3.83, KL: 65.92|DE LOSS: 3.48|BETA: 0.007142\n",
      "075[s], 07[s], Ep: 30, Ct: 02500|TR LOSS: 4.11 LM NLL: 3.83, KL: 65.94|DE LOSS: 3.48|BETA: 0.007177\n",
      "060[s], 08[s], Ep: 31, Ct: 00000|TR LOSS: 4.11 LM NLL: 3.82, KL: 65.96|DE LOSS: 3.48|BETA: 0.007203\n",
      "076[s], 07[s], Ep: 31, Ct: 00500|TR LOSS: 4.11 LM NLL: 3.82, KL: 65.98|DE LOSS: 3.54|BETA: 0.007238\n",
      "075[s], 07[s], Ep: 31, Ct: 01000|TR LOSS: 4.10 LM NLL: 3.81, KL: 66.01|DE LOSS: 3.55|BETA: 0.007273\n",
      "075[s], 08[s], Ep: 31, Ct: 01500|TR LOSS: 4.10 LM NLL: 3.81, KL: 66.03|DE LOSS: 3.48|BETA: 0.007307\n",
      "076[s], 08[s], Ep: 31, Ct: 02000|TR LOSS: 4.10 LM NLL: 3.80, KL: 66.05|DE LOSS: 3.50|BETA: 0.007342\n",
      "075[s], 08[s], Ep: 31, Ct: 02500|TR LOSS: 4.09 LM NLL: 3.80, KL: 66.07|DE LOSS: 3.49|BETA: 0.007377\n",
      "058[s], 07[s], Ep: 32, Ct: 00000|TR LOSS: 4.09 LM NLL: 3.79, KL: 66.09|DE LOSS: 3.48|BETA: 0.007403\n",
      "076[s], 07[s], Ep: 32, Ct: 00500|TR LOSS: 4.09 LM NLL: 3.79, KL: 66.11|DE LOSS: 3.54|BETA: 0.007438\n",
      "076[s], 07[s], Ep: 32, Ct: 01000|TR LOSS: 4.08 LM NLL: 3.79, KL: 66.13|DE LOSS: 3.58|BETA: 0.007473\n",
      "075[s], 07[s], Ep: 32, Ct: 01500|TR LOSS: 4.08 LM NLL: 3.78, KL: 66.15|DE LOSS: 3.50|BETA: 0.007507\n",
      "074[s], 07[s], Ep: 32, Ct: 02000|TR LOSS: 4.08 LM NLL: 3.78, KL: 66.17|DE LOSS: 3.55|BETA: 0.007542\n",
      "074[s], 07[s], Ep: 32, Ct: 02500|TR LOSS: 4.07 LM NLL: 3.77, KL: 66.19|DE LOSS: 3.50|BETA: 0.007577\n",
      "059[s], 08[s], Ep: 33, Ct: 00000|TR LOSS: 4.07 LM NLL: 3.77, KL: 66.20|DE LOSS: 3.50|BETA: 0.007603\n",
      "075[s], 08[s], Ep: 33, Ct: 00500|TR LOSS: 4.07 LM NLL: 3.76, KL: 66.22|DE LOSS: 3.48|BETA: 0.007638\n",
      "075[s], 07[s], Ep: 33, Ct: 01000|TR LOSS: 4.06 LM NLL: 3.76, KL: 66.24|DE LOSS: 3.48|BETA: 0.007673\n",
      "075[s], 08[s], Ep: 33, Ct: 01500|TR LOSS: 4.06 LM NLL: 3.75, KL: 66.25|DE LOSS: 3.48|BETA: 0.007707\n",
      "075[s], 08[s], Ep: 33, Ct: 02000|TR LOSS: 4.06 LM NLL: 3.75, KL: 66.27|DE LOSS: 3.49|BETA: 0.007742\n",
      "075[s], 08[s], Ep: 33, Ct: 02500|TR LOSS: 4.05 LM NLL: 3.75, KL: 66.29|DE LOSS: 3.48|BETA: 0.007777\n",
      "060[s], 08[s], Ep: 34, Ct: 00000|TR LOSS: 4.05 LM NLL: 3.74, KL: 66.30|DE LOSS: 3.49|BETA: 0.007803\n",
      "075[s], 08[s], Ep: 34, Ct: 00500|TR LOSS: 4.05 LM NLL: 3.74, KL: 66.31|DE LOSS: 3.50|BETA: 0.007838\n",
      "075[s], 08[s], Ep: 34, Ct: 01000|TR LOSS: 4.04 LM NLL: 3.73, KL: 66.33|DE LOSS: 3.48|BETA: 0.007872\n",
      "075[s], 08[s], Ep: 34, Ct: 01500|TR LOSS: 4.04 LM NLL: 3.73, KL: 66.34|DE LOSS: 3.48|BETA: 0.007907\n",
      "076[s], 08[s], Ep: 34, Ct: 02000|TR LOSS: 4.04 LM NLL: 3.72, KL: 66.36|DE LOSS: 3.49|BETA: 0.007941\n",
      "075[s], 08[s], Ep: 34, Ct: 02500|TR LOSS: 4.03 LM NLL: 3.72, KL: 66.37|DE LOSS: 3.49|BETA: 0.007976\n",
      "059[s], 08[s], Ep: 35, Ct: 00000|TR LOSS: 4.03 LM NLL: 3.72, KL: 66.38|DE LOSS: 3.48|BETA: 0.008002\n",
      "075[s], 08[s], Ep: 35, Ct: 00500|TR LOSS: 4.03 LM NLL: 3.71, KL: 66.39|DE LOSS: 3.49|BETA: 0.008037\n",
      "075[s], 08[s], Ep: 35, Ct: 01000|TR LOSS: 4.03 LM NLL: 3.71, KL: 66.40|DE LOSS: 3.48|BETA: 0.008071\n",
      "075[s], 08[s], Ep: 35, Ct: 01500|TR LOSS: 4.02 LM NLL: 3.70, KL: 66.42|DE LOSS: 3.50|BETA: 0.008106\n",
      "075[s], 08[s], Ep: 35, Ct: 02000|TR LOSS: 4.02 LM NLL: 3.70, KL: 66.43|DE LOSS: 3.52|BETA: 0.008140\n",
      "075[s], 08[s], Ep: 35, Ct: 02500|TR LOSS: 4.02 LM NLL: 3.70, KL: 66.44|DE LOSS: 3.53|BETA: 0.008175\n",
      "060[s], 08[s], Ep: 36, Ct: 00000|TR LOSS: 4.02 LM NLL: 3.69, KL: 66.45|DE LOSS: 3.49|BETA: 0.008201\n",
      "075[s], 07[s], Ep: 36, Ct: 00500|TR LOSS: 4.01 LM NLL: 3.69, KL: 66.46|DE LOSS: 3.49|BETA: 0.008236\n",
      "076[s], 08[s], Ep: 36, Ct: 01000|TR LOSS: 4.01 LM NLL: 3.69, KL: 66.47|DE LOSS: 3.49|BETA: 0.008270\n",
      "076[s], 08[s], Ep: 36, Ct: 01500|TR LOSS: 4.01 LM NLL: 3.68, KL: 66.48|DE LOSS: 3.50|BETA: 0.008304\n",
      "075[s], 08[s], Ep: 36, Ct: 02000|TR LOSS: 4.00 LM NLL: 3.68, KL: 66.49|DE LOSS: 3.50|BETA: 0.008339\n",
      "076[s], 08[s], Ep: 36, Ct: 02500|TR LOSS: 4.00 LM NLL: 3.67, KL: 66.50|DE LOSS: 3.50|BETA: 0.008373\n",
      "060[s], 08[s], Ep: 37, Ct: 00000|TR LOSS: 4.00 LM NLL: 3.67, KL: 66.50|DE LOSS: 3.50|BETA: 0.008400\n",
      "076[s], 08[s], Ep: 37, Ct: 00500|TR LOSS: 4.00 LM NLL: 3.67, KL: 66.51|DE LOSS: 3.50|BETA: 0.008434\n",
      "075[s], 08[s], Ep: 37, Ct: 01000|TR LOSS: 3.99 LM NLL: 3.66, KL: 66.52|DE LOSS: 3.49|BETA: 0.008469\n",
      "076[s], 07[s], Ep: 37, Ct: 01500|TR LOSS: 3.99 LM NLL: 3.66, KL: 66.53|DE LOSS: 3.49|BETA: 0.008503\n",
      "071[s], 07[s], Ep: 37, Ct: 02000|TR LOSS: 3.99 LM NLL: 3.65, KL: 66.54|DE LOSS: 3.52|BETA: 0.008538\n",
      "070[s], 07[s], Ep: 37, Ct: 02500|TR LOSS: 3.99 LM NLL: 3.65, KL: 66.55|DE LOSS: 3.51|BETA: 0.008572\n",
      "055[s], 07[s], Ep: 38, Ct: 00000|TR LOSS: 3.98 LM NLL: 3.65, KL: 66.55|DE LOSS: 3.50|BETA: 0.008599\n",
      "070[s], 07[s], Ep: 38, Ct: 00500|TR LOSS: 3.98 LM NLL: 3.64, KL: 66.56|DE LOSS: 3.50|BETA: 0.008633\n",
      "070[s], 07[s], Ep: 38, Ct: 01000|TR LOSS: 3.98 LM NLL: 3.64, KL: 66.56|DE LOSS: 3.55|BETA: 0.008667\n",
      "070[s], 07[s], Ep: 38, Ct: 01500|TR LOSS: 3.98 LM NLL: 3.64, KL: 66.57|DE LOSS: 3.50|BETA: 0.008702\n",
      "069[s], 07[s], Ep: 38, Ct: 02000|TR LOSS: 3.97 LM NLL: 3.63, KL: 66.58|DE LOSS: 3.50|BETA: 0.008736\n",
      "071[s], 07[s], Ep: 38, Ct: 02500|TR LOSS: 3.97 LM NLL: 3.63, KL: 66.58|DE LOSS: 3.50|BETA: 0.008771\n",
      "056[s], 07[s], Ep: 39, Ct: 00000|TR LOSS: 3.97 LM NLL: 3.63, KL: 66.59|DE LOSS: 3.50|BETA: 0.008797\n",
      "070[s], 07[s], Ep: 39, Ct: 00500|TR LOSS: 3.97 LM NLL: 3.62, KL: 66.59|DE LOSS: 3.56|BETA: 0.008832\n",
      "070[s], 07[s], Ep: 39, Ct: 01000|TR LOSS: 3.96 LM NLL: 3.62, KL: 66.60|DE LOSS: 3.64|BETA: 0.008866\n",
      "070[s], 07[s], Ep: 39, Ct: 01500|TR LOSS: 3.96 LM NLL: 3.62, KL: 66.60|DE LOSS: 3.50|BETA: 0.008901\n",
      "070[s], 07[s], Ep: 39, Ct: 02000|TR LOSS: 3.96 LM NLL: 3.61, KL: 66.60|DE LOSS: 3.50|BETA: 0.008935\n",
      "070[s], 07[s], Ep: 39, Ct: 02500|TR LOSS: 3.96 LM NLL: 3.61, KL: 66.61|DE LOSS: 3.51|BETA: 0.008970\n",
      "055[s], 07[s], Ep: 40, Ct: 00000|TR LOSS: 3.95 LM NLL: 3.61, KL: 66.61|DE LOSS: 3.51|BETA: 0.008996\n",
      "070[s], 07[s], Ep: 40, Ct: 00500|TR LOSS: 3.95 LM NLL: 3.60, KL: 66.61|DE LOSS: 3.53|BETA: 0.009031\n",
      "070[s], 07[s], Ep: 40, Ct: 01000|TR LOSS: 3.95 LM NLL: 3.60, KL: 66.62|DE LOSS: 3.51|BETA: 0.009065\n",
      "070[s], 07[s], Ep: 40, Ct: 01500|TR LOSS: 3.95 LM NLL: 3.60, KL: 66.62|DE LOSS: 3.52|BETA: 0.009099\n",
      "070[s], 07[s], Ep: 40, Ct: 02000|TR LOSS: 3.95 LM NLL: 3.59, KL: 66.62|DE LOSS: 3.50|BETA: 0.009134\n",
      "070[s], 07[s], Ep: 40, Ct: 02500|TR LOSS: 3.94 LM NLL: 3.59, KL: 66.63|DE LOSS: 3.52|BETA: 0.009168\n",
      "056[s], 07[s], Ep: 41, Ct: 00000|TR LOSS: 3.94 LM NLL: 3.59, KL: 66.63|DE LOSS: 3.51|BETA: 0.009195\n",
      "070[s], 07[s], Ep: 41, Ct: 00500|TR LOSS: 3.94 LM NLL: 3.58, KL: 66.63|DE LOSS: 3.51|BETA: 0.009229\n",
      "070[s], 07[s], Ep: 41, Ct: 01000|TR LOSS: 3.94 LM NLL: 3.58, KL: 66.63|DE LOSS: 3.53|BETA: 0.009264\n",
      "070[s], 07[s], Ep: 41, Ct: 01500|TR LOSS: 3.93 LM NLL: 3.58, KL: 66.63|DE LOSS: 3.52|BETA: 0.009298\n",
      "070[s], 07[s], Ep: 41, Ct: 02000|TR LOSS: 3.93 LM NLL: 3.57, KL: 66.63|DE LOSS: 3.52|BETA: 0.009333\n",
      "070[s], 07[s], Ep: 41, Ct: 02500|TR LOSS: 3.93 LM NLL: 3.57, KL: 66.64|DE LOSS: 3.53|BETA: 0.009367\n",
      "056[s], 07[s], Ep: 42, Ct: 00000|TR LOSS: 3.93 LM NLL: 3.57, KL: 66.64|DE LOSS: 3.52|BETA: 0.009394\n",
      "069[s], 07[s], Ep: 42, Ct: 00500|TR LOSS: 3.93 LM NLL: 3.56, KL: 66.64|DE LOSS: 3.54|BETA: 0.009428\n",
      "068[s], 06[s], Ep: 42, Ct: 01000|TR LOSS: 3.92 LM NLL: 3.56, KL: 66.64|DE LOSS: 3.52|BETA: 0.009463\n",
      "070[s], 07[s], Ep: 42, Ct: 01500|TR LOSS: 3.92 LM NLL: 3.56, KL: 66.64|DE LOSS: 3.52|BETA: 0.009497\n",
      "070[s], 07[s], Ep: 42, Ct: 02000|TR LOSS: 3.92 LM NLL: 3.55, KL: 66.64|DE LOSS: 3.54|BETA: 0.009531\n",
      "070[s], 07[s], Ep: 42, Ct: 02500|TR LOSS: 3.92 LM NLL: 3.55, KL: 66.64|DE LOSS: 3.55|BETA: 0.009566\n",
      "056[s], 07[s], Ep: 43, Ct: 00000|TR LOSS: 3.92 LM NLL: 3.55, KL: 66.64|DE LOSS: 3.52|BETA: 0.009592\n",
      "099[s], 15[s], Ep: 43, Ct: 00500|TR LOSS: 3.91 LM NLL: 3.55, KL: 66.64|DE LOSS: 3.52|BETA: 0.009627\n",
      "179[s], 19[s], Ep: 43, Ct: 01000|TR LOSS: 3.91 LM NLL: 3.54, KL: 66.64|DE LOSS: 3.53|BETA: 0.009661\n",
      "213[s], 22[s], Ep: 43, Ct: 01500|TR LOSS: 3.91 LM NLL: 3.54, KL: 66.64|DE LOSS: 3.53|BETA: 0.009696\n",
      "164[s], 17[s], Ep: 43, Ct: 02000|TR LOSS: 3.91 LM NLL: 3.54, KL: 66.63|DE LOSS: 3.54|BETA: 0.009730\n",
      "191[s], 21[s], Ep: 43, Ct: 02500|TR LOSS: 3.91 LM NLL: 3.53, KL: 66.63|DE LOSS: 3.53|BETA: 0.009765\n",
      "170[s], 21[s], Ep: 44, Ct: 00000|TR LOSS: 3.90 LM NLL: 3.53, KL: 66.63|DE LOSS: 3.53|BETA: 0.009791\n",
      "167[s], 17[s], Ep: 44, Ct: 00500|TR LOSS: 3.90 LM NLL: 3.53, KL: 66.63|DE LOSS: 3.53|BETA: 0.009826\n",
      "186[s], 19[s], Ep: 44, Ct: 01000|TR LOSS: 3.90 LM NLL: 3.52, KL: 66.63|DE LOSS: 3.53|BETA: 0.009860\n",
      "217[s], 21[s], Ep: 44, Ct: 01500|TR LOSS: 3.90 LM NLL: 3.52, KL: 66.63|DE LOSS: 3.53|BETA: 0.009895\n",
      "090[s], 07[s], Ep: 44, Ct: 02000|TR LOSS: 3.90 LM NLL: 3.52, KL: 66.62|DE LOSS: 3.55|BETA: 0.009929\n",
      "070[s], 07[s], Ep: 44, Ct: 02500|TR LOSS: 3.90 LM NLL: 3.52, KL: 66.62|DE LOSS: 3.53|BETA: 0.009963\n",
      "055[s], 07[s], Ep: 45, Ct: 00000|TR LOSS: 3.89 LM NLL: 3.51, KL: 66.62|DE LOSS: 3.53|BETA: 0.009990\n",
      "070[s], 07[s], Ep: 45, Ct: 00500|TR LOSS: 3.89 LM NLL: 3.51, KL: 66.62|DE LOSS: 3.53|BETA: 0.010024\n",
      "070[s], 07[s], Ep: 45, Ct: 01000|TR LOSS: 3.89 LM NLL: 3.51, KL: 66.61|DE LOSS: 3.53|BETA: 0.010059\n",
      "070[s], 07[s], Ep: 45, Ct: 01500|TR LOSS: 3.89 LM NLL: 3.51, KL: 66.61|DE LOSS: 3.53|BETA: 0.010093\n",
      "070[s], 07[s], Ep: 45, Ct: 02000|TR LOSS: 3.89 LM NLL: 3.50, KL: 66.61|DE LOSS: 3.54|BETA: 0.010128\n",
      "070[s], 07[s], Ep: 45, Ct: 02500|TR LOSS: 3.88 LM NLL: 3.50, KL: 66.61|DE LOSS: 3.54|BETA: 0.010162\n",
      "055[s], 07[s], Ep: 46, Ct: 00000|TR LOSS: 3.88 LM NLL: 3.50, KL: 66.60|DE LOSS: 3.58|BETA: 0.010189\n",
      "070[s], 07[s], Ep: 46, Ct: 00500|TR LOSS: 3.88 LM NLL: 3.49, KL: 66.60|DE LOSS: 3.57|BETA: 0.010223\n",
      "070[s], 07[s], Ep: 46, Ct: 01000|TR LOSS: 3.88 LM NLL: 3.49, KL: 66.60|DE LOSS: 3.54|BETA: 0.010258\n",
      "070[s], 07[s], Ep: 46, Ct: 01500|TR LOSS: 3.88 LM NLL: 3.49, KL: 66.59|DE LOSS: 3.55|BETA: 0.010292\n",
      "070[s], 07[s], Ep: 46, Ct: 02000|TR LOSS: 3.88 LM NLL: 3.49, KL: 66.59|DE LOSS: 3.59|BETA: 0.010326\n",
      "070[s], 07[s], Ep: 46, Ct: 02500|TR LOSS: 3.87 LM NLL: 3.48, KL: 66.58|DE LOSS: 3.58|BETA: 0.010361\n",
      "056[s], 07[s], Ep: 47, Ct: 00000|TR LOSS: 3.87 LM NLL: 3.48, KL: 66.58|DE LOSS: 3.55|BETA: 0.010387\n",
      "070[s], 07[s], Ep: 47, Ct: 00500|TR LOSS: 3.87 LM NLL: 3.48, KL: 66.57|DE LOSS: 3.58|BETA: 0.010422\n",
      "070[s], 07[s], Ep: 47, Ct: 01000|TR LOSS: 3.87 LM NLL: 3.48, KL: 66.57|DE LOSS: 3.55|BETA: 0.010456\n",
      "070[s], 07[s], Ep: 47, Ct: 01500|TR LOSS: 3.87 LM NLL: 3.47, KL: 66.57|DE LOSS: 3.55|BETA: 0.010491\n",
      "070[s], 07[s], Ep: 47, Ct: 02000|TR LOSS: 3.87 LM NLL: 3.47, KL: 66.56|DE LOSS: 3.55|BETA: 0.010525\n",
      "070[s], 07[s], Ep: 47, Ct: 02500|TR LOSS: 3.86 LM NLL: 3.47, KL: 66.56|DE LOSS: 3.55|BETA: 0.010560\n",
      "056[s], 07[s], Ep: 48, Ct: 00000|TR LOSS: 3.86 LM NLL: 3.47, KL: 66.55|DE LOSS: 3.54|BETA: 0.010586\n",
      "070[s], 07[s], Ep: 48, Ct: 00500|TR LOSS: 3.86 LM NLL: 3.46, KL: 66.55|DE LOSS: 3.55|BETA: 0.010621\n",
      "070[s], 07[s], Ep: 48, Ct: 01000|TR LOSS: 3.86 LM NLL: 3.46, KL: 66.54|DE LOSS: 3.55|BETA: 0.010655\n",
      "070[s], 07[s], Ep: 48, Ct: 01500|TR LOSS: 3.86 LM NLL: 3.46, KL: 66.53|DE LOSS: 3.57|BETA: 0.010690\n",
      "070[s], 07[s], Ep: 48, Ct: 02000|TR LOSS: 3.86 LM NLL: 3.45, KL: 66.53|DE LOSS: 3.57|BETA: 0.010724\n",
      "070[s], 07[s], Ep: 48, Ct: 02500|TR LOSS: 3.86 LM NLL: 3.45, KL: 66.52|DE LOSS: 3.58|BETA: 0.010758\n",
      "056[s], 07[s], Ep: 49, Ct: 00000|TR LOSS: 3.85 LM NLL: 3.45, KL: 66.52|DE LOSS: 3.56|BETA: 0.010785\n",
      "070[s], 07[s], Ep: 49, Ct: 00500|TR LOSS: 3.85 LM NLL: 3.45, KL: 66.51|DE LOSS: 3.56|BETA: 0.010819\n",
      "070[s], 07[s], Ep: 49, Ct: 01000|TR LOSS: 3.85 LM NLL: 3.44, KL: 66.50|DE LOSS: 3.55|BETA: 0.010854\n",
      "070[s], 07[s], Ep: 49, Ct: 01500|TR LOSS: 3.85 LM NLL: 3.44, KL: 66.50|DE LOSS: 3.55|BETA: 0.010888\n",
      "070[s], 07[s], Ep: 49, Ct: 02000|TR LOSS: 3.85 LM NLL: 3.44, KL: 66.49|DE LOSS: 3.56|BETA: 0.010923\n",
      "070[s], 07[s], Ep: 49, Ct: 02500|TR LOSS: 3.85 LM NLL: 3.44, KL: 66.48|DE LOSS: 3.57|BETA: 0.010957\n",
      "055[s], 07[s], Ep: 50, Ct: 00000|TR LOSS: 3.85 LM NLL: 3.44, KL: 66.48|DE LOSS: 3.56|BETA: 0.010984\n",
      "070[s], 07[s], Ep: 50, Ct: 00500|TR LOSS: 3.84 LM NLL: 3.43, KL: 66.47|DE LOSS: 3.61|BETA: 0.011018\n",
      "070[s], 07[s], Ep: 50, Ct: 01000|TR LOSS: 3.84 LM NLL: 3.43, KL: 66.47|DE LOSS: 3.56|BETA: 0.011053\n",
      "070[s], 07[s], Ep: 50, Ct: 01500|TR LOSS: 3.84 LM NLL: 3.43, KL: 66.46|DE LOSS: 3.56|BETA: 0.011087\n",
      "070[s], 07[s], Ep: 50, Ct: 02000|TR LOSS: 3.84 LM NLL: 3.43, KL: 66.45|DE LOSS: 3.57|BETA: 0.011122\n",
      "070[s], 07[s], Ep: 50, Ct: 02500|TR LOSS: 3.84 LM NLL: 3.42, KL: 66.44|DE LOSS: 3.58|BETA: 0.011156\n",
      "056[s], 07[s], Ep: 51, Ct: 00000|TR LOSS: 3.84 LM NLL: 3.42, KL: 66.44|DE LOSS: 3.58|BETA: 0.011182\n",
      "070[s], 07[s], Ep: 51, Ct: 00500|TR LOSS: 3.84 LM NLL: 3.42, KL: 66.43|DE LOSS: 3.57|BETA: 0.011217\n",
      "070[s], 07[s], Ep: 51, Ct: 01000|TR LOSS: 3.83 LM NLL: 3.42, KL: 66.42|DE LOSS: 3.57|BETA: 0.011251\n",
      "070[s], 07[s], Ep: 51, Ct: 01500|TR LOSS: 3.83 LM NLL: 3.41, KL: 66.41|DE LOSS: 3.58|BETA: 0.011286\n",
      "070[s], 07[s], Ep: 51, Ct: 02000|TR LOSS: 3.83 LM NLL: 3.41, KL: 66.41|DE LOSS: 3.59|BETA: 0.011320\n",
      "073[s], 07[s], Ep: 51, Ct: 02500|TR LOSS: 3.83 LM NLL: 3.41, KL: 66.40|DE LOSS: 3.58|BETA: 0.011355\n",
      "058[s], 07[s], Ep: 52, Ct: 00000|TR LOSS: 3.83 LM NLL: 3.41, KL: 66.39|DE LOSS: 3.58|BETA: 0.011381\n",
      "073[s], 07[s], Ep: 52, Ct: 00500|TR LOSS: 3.83 LM NLL: 3.40, KL: 66.38|DE LOSS: 3.58|BETA: 0.011416\n",
      "072[s], 07[s], Ep: 52, Ct: 01000|TR LOSS: 3.83 LM NLL: 3.40, KL: 66.37|DE LOSS: 3.59|BETA: 0.011450\n",
      "074[s], 07[s], Ep: 52, Ct: 01500|TR LOSS: 3.83 LM NLL: 3.40, KL: 66.37|DE LOSS: 3.57|BETA: 0.011485\n",
      "073[s], 07[s], Ep: 52, Ct: 02000|TR LOSS: 3.82 LM NLL: 3.40, KL: 66.36|DE LOSS: 3.58|BETA: 0.011519\n",
      "073[s], 08[s], Ep: 52, Ct: 02500|TR LOSS: 3.82 LM NLL: 3.39, KL: 66.35|DE LOSS: 3.58|BETA: 0.011553\n",
      "060[s], 08[s], Ep: 53, Ct: 00000|TR LOSS: 3.82 LM NLL: 3.39, KL: 66.34|DE LOSS: 3.58|BETA: 0.011580\n",
      "075[s], 07[s], Ep: 53, Ct: 00500|TR LOSS: 3.82 LM NLL: 3.39, KL: 66.33|DE LOSS: 3.58|BETA: 0.011614\n",
      "074[s], 08[s], Ep: 53, Ct: 01000|TR LOSS: 3.82 LM NLL: 3.39, KL: 66.32|DE LOSS: 3.60|BETA: 0.011649\n",
      "075[s], 08[s], Ep: 53, Ct: 01500|TR LOSS: 3.82 LM NLL: 3.39, KL: 66.32|DE LOSS: 3.58|BETA: 0.011683\n",
      "075[s], 08[s], Ep: 53, Ct: 02000|TR LOSS: 3.82 LM NLL: 3.38, KL: 66.31|DE LOSS: 3.59|BETA: 0.011718\n",
      "075[s], 08[s], Ep: 53, Ct: 02500|TR LOSS: 3.82 LM NLL: 3.38, KL: 66.30|DE LOSS: 3.59|BETA: 0.011752\n",
      "060[s], 08[s], Ep: 54, Ct: 00000|TR LOSS: 3.81 LM NLL: 3.38, KL: 66.29|DE LOSS: 3.60|BETA: 0.011779\n",
      "075[s], 08[s], Ep: 54, Ct: 00500|TR LOSS: 3.81 LM NLL: 3.38, KL: 66.28|DE LOSS: 3.59|BETA: 0.011813\n",
      "075[s], 08[s], Ep: 54, Ct: 01000|TR LOSS: 3.81 LM NLL: 3.38, KL: 66.27|DE LOSS: 3.59|BETA: 0.011848\n",
      "075[s], 08[s], Ep: 54, Ct: 01500|TR LOSS: 3.81 LM NLL: 3.37, KL: 66.26|DE LOSS: 3.60|BETA: 0.011882\n",
      "075[s], 08[s], Ep: 54, Ct: 02000|TR LOSS: 3.81 LM NLL: 3.37, KL: 66.25|DE LOSS: 3.66|BETA: 0.011917\n",
      "075[s], 08[s], Ep: 54, Ct: 02500|TR LOSS: 3.81 LM NLL: 3.37, KL: 66.24|DE LOSS: 3.59|BETA: 0.011951\n",
      "060[s], 08[s], Ep: 55, Ct: 00000|TR LOSS: 3.81 LM NLL: 3.37, KL: 66.23|DE LOSS: 3.60|BETA: 0.011977\n",
      "075[s], 08[s], Ep: 55, Ct: 00500|TR LOSS: 3.81 LM NLL: 3.37, KL: 66.22|DE LOSS: 3.60|BETA: 0.012012\n",
      "075[s], 07[s], Ep: 55, Ct: 01000|TR LOSS: 3.80 LM NLL: 3.36, KL: 66.21|DE LOSS: 3.60|BETA: 0.012046\n",
      "075[s], 08[s], Ep: 55, Ct: 01500|TR LOSS: 3.80 LM NLL: 3.36, KL: 66.20|DE LOSS: 3.60|BETA: 0.012081\n",
      "075[s], 08[s], Ep: 55, Ct: 02000|TR LOSS: 3.80 LM NLL: 3.36, KL: 66.19|DE LOSS: 3.69|BETA: 0.012115\n",
      "075[s], 07[s], Ep: 55, Ct: 02500|TR LOSS: 3.80 LM NLL: 3.36, KL: 66.18|DE LOSS: 3.60|BETA: 0.012150\n",
      "059[s], 08[s], Ep: 56, Ct: 00000|TR LOSS: 3.80 LM NLL: 3.35, KL: 66.17|DE LOSS: 3.62|BETA: 0.012176\n",
      "075[s], 08[s], Ep: 56, Ct: 00500|TR LOSS: 3.80 LM NLL: 3.35, KL: 66.16|DE LOSS: 3.60|BETA: 0.012211\n",
      "075[s], 08[s], Ep: 56, Ct: 01000|TR LOSS: 3.80 LM NLL: 3.35, KL: 66.15|DE LOSS: 3.60|BETA: 0.012245\n",
      "075[s], 08[s], Ep: 56, Ct: 01500|TR LOSS: 3.80 LM NLL: 3.35, KL: 66.14|DE LOSS: 3.60|BETA: 0.012280\n",
      "075[s], 08[s], Ep: 56, Ct: 02000|TR LOSS: 3.80 LM NLL: 3.35, KL: 66.13|DE LOSS: 3.63|BETA: 0.012314\n",
      "075[s], 08[s], Ep: 56, Ct: 02500|TR LOSS: 3.79 LM NLL: 3.34, KL: 66.12|DE LOSS: 3.60|BETA: 0.012349\n",
      "060[s], 08[s], Ep: 57, Ct: 00000|TR LOSS: 3.79 LM NLL: 3.34, KL: 66.11|DE LOSS: 3.61|BETA: 0.012375\n",
      "076[s], 08[s], Ep: 57, Ct: 00500|TR LOSS: 3.79 LM NLL: 3.34, KL: 66.10|DE LOSS: 3.61|BETA: 0.012409\n",
      "075[s], 08[s], Ep: 57, Ct: 01000|TR LOSS: 3.79 LM NLL: 3.34, KL: 66.09|DE LOSS: 3.68|BETA: 0.012444\n",
      "076[s], 08[s], Ep: 57, Ct: 01500|TR LOSS: 3.79 LM NLL: 3.34, KL: 66.08|DE LOSS: 3.61|BETA: 0.012478\n",
      "075[s], 08[s], Ep: 57, Ct: 02000|TR LOSS: 3.79 LM NLL: 3.33, KL: 66.07|DE LOSS: 3.62|BETA: 0.012513\n",
      "076[s], 08[s], Ep: 57, Ct: 02500|TR LOSS: 3.79 LM NLL: 3.33, KL: 66.06|DE LOSS: 3.61|BETA: 0.012547\n",
      "060[s], 08[s], Ep: 58, Ct: 00000|TR LOSS: 3.79 LM NLL: 3.33, KL: 66.05|DE LOSS: 3.62|BETA: 0.012574\n",
      "076[s], 08[s], Ep: 58, Ct: 00500|TR LOSS: 3.79 LM NLL: 3.33, KL: 66.04|DE LOSS: 3.67|BETA: 0.012608\n",
      "075[s], 08[s], Ep: 58, Ct: 01000|TR LOSS: 3.79 LM NLL: 3.33, KL: 66.02|DE LOSS: 3.62|BETA: 0.012643\n",
      "076[s], 08[s], Ep: 58, Ct: 01500|TR LOSS: 3.78 LM NLL: 3.32, KL: 66.01|DE LOSS: 3.62|BETA: 0.012677\n",
      "076[s], 08[s], Ep: 58, Ct: 02000|TR LOSS: 3.78 LM NLL: 3.32, KL: 66.00|DE LOSS: 3.63|BETA: 0.012712\n",
      "076[s], 08[s], Ep: 58, Ct: 02500|TR LOSS: 3.78 LM NLL: 3.32, KL: 65.99|DE LOSS: 3.62|BETA: 0.012746\n",
      "060[s], 08[s], Ep: 59, Ct: 00000|TR LOSS: 3.78 LM NLL: 3.32, KL: 65.98|DE LOSS: 3.65|BETA: 0.012773\n",
      "076[s], 08[s], Ep: 59, Ct: 00500|TR LOSS: 3.78 LM NLL: 3.32, KL: 65.97|DE LOSS: 3.63|BETA: 0.012807\n",
      "075[s], 08[s], Ep: 59, Ct: 01000|TR LOSS: 3.78 LM NLL: 3.32, KL: 65.96|DE LOSS: 3.63|BETA: 0.012841\n",
      "075[s], 08[s], Ep: 59, Ct: 01500|TR LOSS: 3.78 LM NLL: 3.31, KL: 65.94|DE LOSS: 3.63|BETA: 0.012876\n",
      "075[s], 08[s], Ep: 59, Ct: 02000|TR LOSS: 3.78 LM NLL: 3.31, KL: 65.93|DE LOSS: 3.64|BETA: 0.012910\n",
      "075[s], 07[s], Ep: 59, Ct: 02500|TR LOSS: 3.78 LM NLL: 3.31, KL: 65.92|DE LOSS: 3.63|BETA: 0.012945\n",
      "060[s], 08[s], Ep: 60, Ct: 00000|TR LOSS: 3.78 LM NLL: 3.31, KL: 65.91|DE LOSS: 3.64|BETA: 0.012971\n",
      "076[s], 08[s], Ep: 60, Ct: 00500|TR LOSS: 3.78 LM NLL: 3.31, KL: 65.90|DE LOSS: 3.63|BETA: 0.013006\n",
      "076[s], 08[s], Ep: 60, Ct: 01000|TR LOSS: 3.77 LM NLL: 3.30, KL: 65.89|DE LOSS: 3.66|BETA: 0.013040\n",
      "076[s], 08[s], Ep: 60, Ct: 01500|TR LOSS: 3.77 LM NLL: 3.30, KL: 65.87|DE LOSS: 3.64|BETA: 0.013075\n",
      "076[s], 08[s], Ep: 60, Ct: 02000|TR LOSS: 3.77 LM NLL: 3.30, KL: 65.86|DE LOSS: 3.64|BETA: 0.013109\n",
      "075[s], 08[s], Ep: 60, Ct: 02500|TR LOSS: 3.77 LM NLL: 3.30, KL: 65.85|DE LOSS: 3.65|BETA: 0.013144\n",
      "060[s], 08[s], Ep: 61, Ct: 00000|TR LOSS: 3.77 LM NLL: 3.30, KL: 65.84|DE LOSS: 3.64|BETA: 0.013170\n",
      "076[s], 08[s], Ep: 61, Ct: 00500|TR LOSS: 3.77 LM NLL: 3.30, KL: 65.83|DE LOSS: 3.64|BETA: 0.013204\n",
      "076[s], 08[s], Ep: 61, Ct: 01000|TR LOSS: 3.77 LM NLL: 3.29, KL: 65.81|DE LOSS: 3.65|BETA: 0.013239\n",
      "076[s], 08[s], Ep: 61, Ct: 01500|TR LOSS: 3.77 LM NLL: 3.29, KL: 65.80|DE LOSS: 3.64|BETA: 0.013273\n",
      "075[s], 08[s], Ep: 61, Ct: 02000|TR LOSS: 3.77 LM NLL: 3.29, KL: 65.79|DE LOSS: 3.65|BETA: 0.013308\n",
      "075[s], 08[s], Ep: 61, Ct: 02500|TR LOSS: 3.77 LM NLL: 3.29, KL: 65.77|DE LOSS: 3.65|BETA: 0.013342\n",
      "059[s], 08[s], Ep: 62, Ct: 00000|TR LOSS: 3.77 LM NLL: 3.29, KL: 65.76|DE LOSS: 3.64|BETA: 0.013369\n",
      "076[s], 08[s], Ep: 62, Ct: 00500|TR LOSS: 3.76 LM NLL: 3.28, KL: 65.75|DE LOSS: 3.66|BETA: 0.013403\n",
      "075[s], 08[s], Ep: 62, Ct: 01000|TR LOSS: 3.76 LM NLL: 3.28, KL: 65.74|DE LOSS: 3.66|BETA: 0.013438\n",
      "076[s], 08[s], Ep: 62, Ct: 01500|TR LOSS: 3.76 LM NLL: 3.28, KL: 65.73|DE LOSS: 3.64|BETA: 0.013472\n",
      "075[s], 08[s], Ep: 62, Ct: 02000|TR LOSS: 3.76 LM NLL: 3.28, KL: 65.71|DE LOSS: 3.65|BETA: 0.013507\n",
      "076[s], 08[s], Ep: 62, Ct: 02500|TR LOSS: 3.76 LM NLL: 3.28, KL: 65.70|DE LOSS: 3.65|BETA: 0.013541\n",
      "060[s], 08[s], Ep: 63, Ct: 00000|TR LOSS: 3.76 LM NLL: 3.28, KL: 65.69|DE LOSS: 3.67|BETA: 0.013568\n",
      "075[s], 08[s], Ep: 63, Ct: 00500|TR LOSS: 3.76 LM NLL: 3.27, KL: 65.68|DE LOSS: 3.65|BETA: 0.013602\n",
      "076[s], 08[s], Ep: 63, Ct: 01000|TR LOSS: 3.76 LM NLL: 3.27, KL: 65.66|DE LOSS: 3.65|BETA: 0.013636\n",
      "076[s], 08[s], Ep: 63, Ct: 01500|TR LOSS: 3.76 LM NLL: 3.27, KL: 65.65|DE LOSS: 3.65|BETA: 0.013671\n",
      "074[s], 08[s], Ep: 63, Ct: 02000|TR LOSS: 3.76 LM NLL: 3.27, KL: 65.64|DE LOSS: 3.65|BETA: 0.013705\n",
      "076[s], 08[s], Ep: 63, Ct: 02500|TR LOSS: 3.76 LM NLL: 3.27, KL: 65.62|DE LOSS: 3.67|BETA: 0.013740\n",
      "060[s], 08[s], Ep: 64, Ct: 00000|TR LOSS: 3.76 LM NLL: 3.27, KL: 65.61|DE LOSS: 3.68|BETA: 0.013766\n",
      "075[s], 08[s], Ep: 64, Ct: 00500|TR LOSS: 3.75 LM NLL: 3.26, KL: 65.60|DE LOSS: 3.67|BETA: 0.013801\n",
      "075[s], 08[s], Ep: 64, Ct: 01000|TR LOSS: 3.75 LM NLL: 3.26, KL: 65.58|DE LOSS: 3.69|BETA: 0.013835\n",
      "076[s], 08[s], Ep: 64, Ct: 01500|TR LOSS: 3.75 LM NLL: 3.26, KL: 65.57|DE LOSS: 3.67|BETA: 0.013870\n",
      "075[s], 08[s], Ep: 64, Ct: 02000|TR LOSS: 3.75 LM NLL: 3.26, KL: 65.56|DE LOSS: 3.66|BETA: 0.013904\n",
      "075[s], 08[s], Ep: 64, Ct: 02500|TR LOSS: 3.75 LM NLL: 3.26, KL: 65.54|DE LOSS: 3.68|BETA: 0.013939\n",
      "060[s], 08[s], Ep: 65, Ct: 00000|TR LOSS: 3.75 LM NLL: 3.26, KL: 65.53|DE LOSS: 3.66|BETA: 0.013965\n",
      "075[s], 08[s], Ep: 65, Ct: 00500|TR LOSS: 3.75 LM NLL: 3.25, KL: 65.52|DE LOSS: 3.67|BETA: 0.014000\n",
      "075[s], 08[s], Ep: 65, Ct: 01000|TR LOSS: 3.75 LM NLL: 3.25, KL: 65.50|DE LOSS: 3.67|BETA: 0.014034\n",
      "075[s], 08[s], Ep: 65, Ct: 01500|TR LOSS: 3.75 LM NLL: 3.25, KL: 65.49|DE LOSS: 3.68|BETA: 0.014068\n",
      "076[s], 08[s], Ep: 65, Ct: 02000|TR LOSS: 3.75 LM NLL: 3.25, KL: 65.48|DE LOSS: 3.67|BETA: 0.014103\n",
      "075[s], 08[s], Ep: 65, Ct: 02500|TR LOSS: 3.75 LM NLL: 3.25, KL: 65.46|DE LOSS: 3.67|BETA: 0.014137\n",
      "060[s], 08[s], Ep: 66, Ct: 00000|TR LOSS: 3.75 LM NLL: 3.25, KL: 65.45|DE LOSS: 3.68|BETA: 0.014164\n",
      "075[s], 08[s], Ep: 66, Ct: 00500|TR LOSS: 3.75 LM NLL: 3.24, KL: 65.44|DE LOSS: 3.68|BETA: 0.014198\n",
      "075[s], 08[s], Ep: 66, Ct: 01000|TR LOSS: 3.75 LM NLL: 3.24, KL: 65.42|DE LOSS: 3.68|BETA: 0.014233\n",
      "075[s], 08[s], Ep: 66, Ct: 01500|TR LOSS: 3.74 LM NLL: 3.24, KL: 65.41|DE LOSS: 3.68|BETA: 0.014267\n",
      "075[s], 08[s], Ep: 66, Ct: 02000|TR LOSS: 3.74 LM NLL: 3.24, KL: 65.39|DE LOSS: 3.68|BETA: 0.014302\n",
      "075[s], 08[s], Ep: 66, Ct: 02500|TR LOSS: 3.74 LM NLL: 3.24, KL: 65.38|DE LOSS: 3.69|BETA: 0.014336\n",
      "059[s], 08[s], Ep: 67, Ct: 00000|TR LOSS: 3.74 LM NLL: 3.24, KL: 65.37|DE LOSS: 3.69|BETA: 0.014363\n",
      "075[s], 08[s], Ep: 67, Ct: 00500|TR LOSS: 3.74 LM NLL: 3.24, KL: 65.35|DE LOSS: 3.71|BETA: 0.014397\n",
      "075[s], 08[s], Ep: 67, Ct: 01000|TR LOSS: 3.74 LM NLL: 3.23, KL: 65.34|DE LOSS: 3.69|BETA: 0.014432\n",
      "075[s], 08[s], Ep: 67, Ct: 01500|TR LOSS: 3.74 LM NLL: 3.23, KL: 65.33|DE LOSS: 3.68|BETA: 0.014466\n",
      "075[s], 08[s], Ep: 67, Ct: 02000|TR LOSS: 3.74 LM NLL: 3.23, KL: 65.31|DE LOSS: 3.69|BETA: 0.014500\n",
      "075[s], 08[s], Ep: 67, Ct: 02500|TR LOSS: 3.74 LM NLL: 3.23, KL: 65.30|DE LOSS: 3.71|BETA: 0.014535\n",
      "060[s], 08[s], Ep: 68, Ct: 00000|TR LOSS: 3.74 LM NLL: 3.23, KL: 65.28|DE LOSS: 3.69|BETA: 0.014561\n",
      "075[s], 08[s], Ep: 68, Ct: 00500|TR LOSS: 3.74 LM NLL: 3.23, KL: 65.27|DE LOSS: 3.71|BETA: 0.014596\n",
      "075[s], 08[s], Ep: 68, Ct: 01000|TR LOSS: 3.74 LM NLL: 3.22, KL: 65.26|DE LOSS: 3.69|BETA: 0.014630\n",
      "075[s], 08[s], Ep: 68, Ct: 01500|TR LOSS: 3.74 LM NLL: 3.22, KL: 65.24|DE LOSS: 3.70|BETA: 0.014665\n",
      "075[s], 08[s], Ep: 68, Ct: 02000|TR LOSS: 3.74 LM NLL: 3.22, KL: 65.23|DE LOSS: 3.76|BETA: 0.014699\n",
      "075[s], 08[s], Ep: 68, Ct: 02500|TR LOSS: 3.74 LM NLL: 3.22, KL: 65.21|DE LOSS: 3.71|BETA: 0.014734\n",
      "059[s], 08[s], Ep: 69, Ct: 00000|TR LOSS: 3.73 LM NLL: 3.22, KL: 65.20|DE LOSS: 3.71|BETA: 0.014760\n",
      "075[s], 08[s], Ep: 69, Ct: 00500|TR LOSS: 3.73 LM NLL: 3.22, KL: 65.19|DE LOSS: 3.70|BETA: 0.014795\n",
      "075[s], 08[s], Ep: 69, Ct: 01000|TR LOSS: 3.73 LM NLL: 3.22, KL: 65.17|DE LOSS: 3.70|BETA: 0.014829\n",
      "075[s], 08[s], Ep: 69, Ct: 01500|TR LOSS: 3.73 LM NLL: 3.21, KL: 65.16|DE LOSS: 3.70|BETA: 0.014863\n",
      "075[s], 07[s], Ep: 69, Ct: 02000|TR LOSS: 3.73 LM NLL: 3.21, KL: 65.14|DE LOSS: 3.72|BETA: 0.014898\n",
      "075[s], 08[s], Ep: 69, Ct: 02500|TR LOSS: 3.73 LM NLL: 3.21, KL: 65.13|DE LOSS: 3.71|BETA: 0.014932\n",
      "060[s], 08[s], Ep: 70, Ct: 00000|TR LOSS: 3.73 LM NLL: 3.21, KL: 65.11|DE LOSS: 3.71|BETA: 0.014959\n",
      "075[s], 07[s], Ep: 70, Ct: 00500|TR LOSS: 3.73 LM NLL: 3.21, KL: 65.10|DE LOSS: 3.71|BETA: 0.014993\n",
      "075[s], 08[s], Ep: 70, Ct: 01000|TR LOSS: 3.73 LM NLL: 3.21, KL: 65.08|DE LOSS: 3.72|BETA: 0.015028\n",
      "075[s], 08[s], Ep: 70, Ct: 01500|TR LOSS: 3.73 LM NLL: 3.21, KL: 65.07|DE LOSS: 3.71|BETA: 0.015062\n",
      "075[s], 08[s], Ep: 70, Ct: 02000|TR LOSS: 3.73 LM NLL: 3.20, KL: 65.05|DE LOSS: 3.72|BETA: 0.015097\n",
      "075[s], 08[s], Ep: 70, Ct: 02500|TR LOSS: 3.73 LM NLL: 3.20, KL: 65.04|DE LOSS: 3.71|BETA: 0.015131\n",
      "060[s], 08[s], Ep: 71, Ct: 00000|TR LOSS: 3.73 LM NLL: 3.20, KL: 65.03|DE LOSS: 3.72|BETA: 0.015158\n",
      "075[s], 08[s], Ep: 71, Ct: 00500|TR LOSS: 3.73 LM NLL: 3.20, KL: 65.01|DE LOSS: 3.72|BETA: 0.015192\n",
      "075[s], 08[s], Ep: 71, Ct: 01000|TR LOSS: 3.73 LM NLL: 3.20, KL: 65.00|DE LOSS: 3.72|BETA: 0.015227\n",
      "075[s], 08[s], Ep: 71, Ct: 01500|TR LOSS: 3.73 LM NLL: 3.20, KL: 64.98|DE LOSS: 3.72|BETA: 0.015261\n",
      "075[s], 08[s], Ep: 71, Ct: 02000|TR LOSS: 3.73 LM NLL: 3.20, KL: 64.96|DE LOSS: 3.72|BETA: 0.015295\n",
      "075[s], 08[s], Ep: 71, Ct: 02500|TR LOSS: 3.72 LM NLL: 3.19, KL: 64.95|DE LOSS: 3.73|BETA: 0.015330\n",
      "059[s], 08[s], Ep: 72, Ct: 00000|TR LOSS: 3.72 LM NLL: 3.19, KL: 64.94|DE LOSS: 3.73|BETA: 0.015356\n",
      "075[s], 08[s], Ep: 72, Ct: 00500|TR LOSS: 3.72 LM NLL: 3.19, KL: 64.92|DE LOSS: 3.73|BETA: 0.015391\n",
      "075[s], 08[s], Ep: 72, Ct: 01000|TR LOSS: 3.72 LM NLL: 3.19, KL: 64.91|DE LOSS: 3.72|BETA: 0.015425\n",
      "075[s], 08[s], Ep: 72, Ct: 01500|TR LOSS: 3.72 LM NLL: 3.19, KL: 64.89|DE LOSS: 3.73|BETA: 0.015460\n",
      "075[s], 08[s], Ep: 72, Ct: 02000|TR LOSS: 3.72 LM NLL: 3.19, KL: 64.88|DE LOSS: 3.73|BETA: 0.015494\n",
      "075[s], 08[s], Ep: 72, Ct: 02500|TR LOSS: 3.72 LM NLL: 3.19, KL: 64.86|DE LOSS: 3.73|BETA: 0.015529\n",
      "060[s], 08[s], Ep: 73, Ct: 00000|TR LOSS: 3.72 LM NLL: 3.18, KL: 64.85|DE LOSS: 3.74|BETA: 0.015555\n",
      "075[s], 08[s], Ep: 73, Ct: 00500|TR LOSS: 3.72 LM NLL: 3.18, KL: 64.83|DE LOSS: 3.73|BETA: 0.015590\n",
      "075[s], 08[s], Ep: 73, Ct: 01000|TR LOSS: 3.72 LM NLL: 3.18, KL: 64.82|DE LOSS: 3.74|BETA: 0.015624\n",
      "075[s], 08[s], Ep: 73, Ct: 01500|TR LOSS: 3.72 LM NLL: 3.18, KL: 64.80|DE LOSS: 3.74|BETA: 0.015659\n",
      "075[s], 08[s], Ep: 73, Ct: 02000|TR LOSS: 3.72 LM NLL: 3.18, KL: 64.78|DE LOSS: 3.74|BETA: 0.015693\n",
      "075[s], 07[s], Ep: 73, Ct: 02500|TR LOSS: 3.72 LM NLL: 3.18, KL: 64.77|DE LOSS: 3.74|BETA: 0.015727\n",
      "060[s], 08[s], Ep: 74, Ct: 00000|TR LOSS: 3.72 LM NLL: 3.18, KL: 64.76|DE LOSS: 3.75|BETA: 0.015754\n",
      "075[s], 08[s], Ep: 74, Ct: 00500|TR LOSS: 3.72 LM NLL: 3.18, KL: 64.74|DE LOSS: 3.75|BETA: 0.015788\n",
      "074[s], 08[s], Ep: 74, Ct: 01000|TR LOSS: 3.72 LM NLL: 3.17, KL: 64.73|DE LOSS: 3.74|BETA: 0.015823\n",
      "075[s], 08[s], Ep: 74, Ct: 01500|TR LOSS: 3.72 LM NLL: 3.17, KL: 64.71|DE LOSS: 3.76|BETA: 0.015857\n",
      "075[s], 08[s], Ep: 74, Ct: 02000|TR LOSS: 3.72 LM NLL: 3.17, KL: 64.69|DE LOSS: 3.75|BETA: 0.015892\n",
      "075[s], 08[s], Ep: 74, Ct: 02500|TR LOSS: 3.72 LM NLL: 3.17, KL: 64.68|DE LOSS: 3.75|BETA: 0.015926\n",
      "060[s], 08[s], Ep: 75, Ct: 00000|TR LOSS: 3.72 LM NLL: 3.17, KL: 64.67|DE LOSS: 3.74|BETA: 0.015953\n",
      "075[s], 08[s], Ep: 75, Ct: 00500|TR LOSS: 3.71 LM NLL: 3.17, KL: 64.65|DE LOSS: 3.76|BETA: 0.015987\n",
      "075[s], 08[s], Ep: 75, Ct: 01000|TR LOSS: 3.71 LM NLL: 3.17, KL: 64.63|DE LOSS: 3.75|BETA: 0.016022\n",
      "076[s], 08[s], Ep: 75, Ct: 01500|TR LOSS: 3.71 LM NLL: 3.16, KL: 64.62|DE LOSS: 3.75|BETA: 0.016056\n",
      "075[s], 08[s], Ep: 75, Ct: 02000|TR LOSS: 3.71 LM NLL: 3.16, KL: 64.60|DE LOSS: 3.77|BETA: 0.016090\n",
      "075[s], 08[s], Ep: 75, Ct: 02500|TR LOSS: 3.71 LM NLL: 3.16, KL: 64.59|DE LOSS: 3.75|BETA: 0.016125\n",
      "060[s], 08[s], Ep: 76, Ct: 00000|TR LOSS: 3.71 LM NLL: 3.16, KL: 64.57|DE LOSS: 3.77|BETA: 0.016151\n",
      "075[s], 08[s], Ep: 76, Ct: 00500|TR LOSS: 3.71 LM NLL: 3.16, KL: 64.56|DE LOSS: 3.76|BETA: 0.016186\n",
      "075[s], 08[s], Ep: 76, Ct: 01000|TR LOSS: 3.71 LM NLL: 3.16, KL: 64.54|DE LOSS: 3.77|BETA: 0.016220\n",
      "076[s], 08[s], Ep: 76, Ct: 01500|TR LOSS: 3.71 LM NLL: 3.16, KL: 64.53|DE LOSS: 3.76|BETA: 0.016255\n",
      "075[s], 08[s], Ep: 76, Ct: 02000|TR LOSS: 3.71 LM NLL: 3.16, KL: 64.51|DE LOSS: 3.76|BETA: 0.016289\n",
      "075[s], 08[s], Ep: 76, Ct: 02500|TR LOSS: 3.71 LM NLL: 3.15, KL: 64.49|DE LOSS: 3.77|BETA: 0.016324\n",
      "060[s], 08[s], Ep: 77, Ct: 00000|TR LOSS: 3.71 LM NLL: 3.15, KL: 64.48|DE LOSS: 3.76|BETA: 0.016350\n",
      "075[s], 08[s], Ep: 77, Ct: 00500|TR LOSS: 3.71 LM NLL: 3.15, KL: 64.47|DE LOSS: 3.78|BETA: 0.016385\n",
      "075[s], 08[s], Ep: 77, Ct: 01000|TR LOSS: 3.71 LM NLL: 3.15, KL: 64.45|DE LOSS: 3.77|BETA: 0.016419\n",
      "075[s], 08[s], Ep: 77, Ct: 01500|TR LOSS: 3.71 LM NLL: 3.15, KL: 64.43|DE LOSS: 3.77|BETA: 0.016454\n",
      "076[s], 08[s], Ep: 77, Ct: 02000|TR LOSS: 3.71 LM NLL: 3.15, KL: 64.42|DE LOSS: 3.77|BETA: 0.016488\n",
      "075[s], 08[s], Ep: 77, Ct: 02500|TR LOSS: 3.71 LM NLL: 3.15, KL: 64.40|DE LOSS: 3.78|BETA: 0.016522\n",
      "059[s], 08[s], Ep: 78, Ct: 00000|TR LOSS: 3.71 LM NLL: 3.15, KL: 64.39|DE LOSS: 3.77|BETA: 0.016549\n",
      "075[s], 08[s], Ep: 78, Ct: 00500|TR LOSS: 3.71 LM NLL: 3.14, KL: 64.37|DE LOSS: 3.76|BETA: 0.016583\n",
      "076[s], 08[s], Ep: 78, Ct: 01000|TR LOSS: 3.71 LM NLL: 3.14, KL: 64.36|DE LOSS: 3.78|BETA: 0.016618\n",
      "075[s], 08[s], Ep: 78, Ct: 01500|TR LOSS: 3.71 LM NLL: 3.14, KL: 64.34|DE LOSS: 3.79|BETA: 0.016652\n",
      "075[s], 08[s], Ep: 78, Ct: 02000|TR LOSS: 3.71 LM NLL: 3.14, KL: 64.32|DE LOSS: 3.79|BETA: 0.016687\n",
      "076[s], 08[s], Ep: 78, Ct: 02500|TR LOSS: 3.71 LM NLL: 3.14, KL: 64.31|DE LOSS: 3.78|BETA: 0.016721\n",
      "060[s], 08[s], Ep: 79, Ct: 00000|TR LOSS: 3.71 LM NLL: 3.14, KL: 64.29|DE LOSS: 3.78|BETA: 0.016748\n",
      "075[s], 08[s], Ep: 79, Ct: 00500|TR LOSS: 3.70 LM NLL: 3.14, KL: 64.28|DE LOSS: 3.78|BETA: 0.016782\n",
      "076[s], 08[s], Ep: 79, Ct: 01000|TR LOSS: 3.70 LM NLL: 3.14, KL: 64.26|DE LOSS: 3.78|BETA: 0.016817\n",
      "076[s], 08[s], Ep: 79, Ct: 01500|TR LOSS: 3.70 LM NLL: 3.14, KL: 64.25|DE LOSS: 3.79|BETA: 0.016851\n",
      "075[s], 08[s], Ep: 79, Ct: 02000|TR LOSS: 3.70 LM NLL: 3.13, KL: 64.23|DE LOSS: 3.79|BETA: 0.016886\n",
      "075[s], 08[s], Ep: 79, Ct: 02500|TR LOSS: 3.70 LM NLL: 3.13, KL: 64.21|DE LOSS: 3.79|BETA: 0.016920\n",
      "060[s], 08[s], Ep: 80, Ct: 00000|TR LOSS: 3.70 LM NLL: 3.13, KL: 64.20|DE LOSS: 3.79|BETA: 0.016946\n",
      "075[s], 08[s], Ep: 80, Ct: 00500|TR LOSS: 3.70 LM NLL: 3.13, KL: 64.18|DE LOSS: 3.80|BETA: 0.016981\n",
      "076[s], 08[s], Ep: 80, Ct: 01000|TR LOSS: 3.70 LM NLL: 3.13, KL: 64.17|DE LOSS: 3.80|BETA: 0.017015\n",
      "076[s], 08[s], Ep: 80, Ct: 01500|TR LOSS: 3.70 LM NLL: 3.13, KL: 64.15|DE LOSS: 3.79|BETA: 0.017050\n",
      "076[s], 08[s], Ep: 80, Ct: 02000|TR LOSS: 3.70 LM NLL: 3.13, KL: 64.13|DE LOSS: 3.80|BETA: 0.017084\n",
      "076[s], 08[s], Ep: 80, Ct: 02500|TR LOSS: 3.70 LM NLL: 3.13, KL: 64.12|DE LOSS: 3.80|BETA: 0.017119\n",
      "060[s], 08[s], Ep: 81, Ct: 00000|TR LOSS: 3.70 LM NLL: 3.12, KL: 64.10|DE LOSS: 3.80|BETA: 0.017145\n",
      "075[s], 07[s], Ep: 81, Ct: 00500|TR LOSS: 3.70 LM NLL: 3.12, KL: 64.09|DE LOSS: 3.80|BETA: 0.017180\n",
      "075[s], 08[s], Ep: 81, Ct: 01000|TR LOSS: 3.70 LM NLL: 3.12, KL: 64.07|DE LOSS: 3.80|BETA: 0.017214\n",
      "075[s], 08[s], Ep: 81, Ct: 01500|TR LOSS: 3.70 LM NLL: 3.12, KL: 64.05|DE LOSS: 3.80|BETA: 0.017249\n",
      "075[s], 08[s], Ep: 81, Ct: 02000|TR LOSS: 3.70 LM NLL: 3.12, KL: 64.04|DE LOSS: 3.81|BETA: 0.017283\n",
      "075[s], 08[s], Ep: 81, Ct: 02500|TR LOSS: 3.70 LM NLL: 3.12, KL: 64.02|DE LOSS: 3.80|BETA: 0.017318\n",
      "060[s], 08[s], Ep: 82, Ct: 00000|TR LOSS: 3.70 LM NLL: 3.12, KL: 64.01|DE LOSS: 3.82|BETA: 0.017344\n",
      "076[s], 08[s], Ep: 82, Ct: 00500|TR LOSS: 3.70 LM NLL: 3.12, KL: 63.99|DE LOSS: 3.81|BETA: 0.017378\n",
      "076[s], 08[s], Ep: 82, Ct: 01000|TR LOSS: 3.70 LM NLL: 3.12, KL: 63.97|DE LOSS: 3.81|BETA: 0.017413\n",
      "075[s], 08[s], Ep: 82, Ct: 01500|TR LOSS: 3.70 LM NLL: 3.11, KL: 63.96|DE LOSS: 3.82|BETA: 0.017447\n",
      "075[s], 08[s], Ep: 82, Ct: 02000|TR LOSS: 3.70 LM NLL: 3.11, KL: 63.94|DE LOSS: 3.82|BETA: 0.017482\n",
      "075[s], 08[s], Ep: 82, Ct: 02500|TR LOSS: 3.70 LM NLL: 3.11, KL: 63.92|DE LOSS: 3.83|BETA: 0.017516\n",
      "060[s], 08[s], Ep: 83, Ct: 00000|TR LOSS: 3.70 LM NLL: 3.11, KL: 63.91|DE LOSS: 3.81|BETA: 0.017543\n",
      "076[s], 08[s], Ep: 83, Ct: 00500|TR LOSS: 3.70 LM NLL: 3.11, KL: 63.89|DE LOSS: 3.84|BETA: 0.017577\n",
      "075[s], 08[s], Ep: 83, Ct: 01000|TR LOSS: 3.70 LM NLL: 3.11, KL: 63.88|DE LOSS: 3.82|BETA: 0.017612\n",
      "075[s], 08[s], Ep: 83, Ct: 01500|TR LOSS: 3.70 LM NLL: 3.11, KL: 63.86|DE LOSS: 3.85|BETA: 0.017646\n",
      "075[s], 08[s], Ep: 83, Ct: 02000|TR LOSS: 3.70 LM NLL: 3.11, KL: 63.84|DE LOSS: 3.82|BETA: 0.017681\n",
      "075[s], 08[s], Ep: 83, Ct: 02500|TR LOSS: 3.70 LM NLL: 3.11, KL: 63.83|DE LOSS: 3.83|BETA: 0.017715\n",
      "059[s], 08[s], Ep: 84, Ct: 00000|TR LOSS: 3.70 LM NLL: 3.10, KL: 63.81|DE LOSS: 3.83|BETA: 0.017741\n",
      "076[s], 08[s], Ep: 84, Ct: 00500|TR LOSS: 3.69 LM NLL: 3.10, KL: 63.80|DE LOSS: 3.83|BETA: 0.017776\n",
      "075[s], 08[s], Ep: 84, Ct: 01000|TR LOSS: 3.69 LM NLL: 3.10, KL: 63.78|DE LOSS: 3.83|BETA: 0.017810\n",
      "076[s], 08[s], Ep: 84, Ct: 01500|TR LOSS: 3.69 LM NLL: 3.10, KL: 63.76|DE LOSS: 3.82|BETA: 0.017845\n",
      "075[s], 08[s], Ep: 84, Ct: 02000|TR LOSS: 3.69 LM NLL: 3.10, KL: 63.75|DE LOSS: 3.84|BETA: 0.017879\n",
      "076[s], 08[s], Ep: 84, Ct: 02500|TR LOSS: 3.69 LM NLL: 3.10, KL: 63.73|DE LOSS: 3.84|BETA: 0.017914\n",
      "060[s], 08[s], Ep: 85, Ct: 00000|TR LOSS: 3.69 LM NLL: 3.10, KL: 63.72|DE LOSS: 3.84|BETA: 0.017940\n",
      "075[s], 08[s], Ep: 85, Ct: 00500|TR LOSS: 3.69 LM NLL: 3.10, KL: 63.70|DE LOSS: 3.84|BETA: 0.017975\n",
      "075[s], 08[s], Ep: 85, Ct: 01000|TR LOSS: 3.69 LM NLL: 3.10, KL: 63.68|DE LOSS: 3.83|BETA: 0.018009\n",
      "075[s], 08[s], Ep: 85, Ct: 01500|TR LOSS: 3.69 LM NLL: 3.10, KL: 63.67|DE LOSS: 3.83|BETA: 0.018044\n",
      "076[s], 08[s], Ep: 85, Ct: 02000|TR LOSS: 3.69 LM NLL: 3.09, KL: 63.65|DE LOSS: 3.83|BETA: 0.018078\n",
      "075[s], 08[s], Ep: 85, Ct: 02500|TR LOSS: 3.69 LM NLL: 3.09, KL: 63.63|DE LOSS: 3.84|BETA: 0.018113\n",
      "060[s], 08[s], Ep: 86, Ct: 00000|TR LOSS: 3.69 LM NLL: 3.09, KL: 63.62|DE LOSS: 3.84|BETA: 0.018139\n",
      "076[s], 08[s], Ep: 86, Ct: 00500|TR LOSS: 3.69 LM NLL: 3.09, KL: 63.60|DE LOSS: 3.84|BETA: 0.018173\n",
      "076[s], 08[s], Ep: 86, Ct: 01000|TR LOSS: 3.69 LM NLL: 3.09, KL: 63.58|DE LOSS: 3.84|BETA: 0.018208\n",
      "076[s], 08[s], Ep: 86, Ct: 01500|TR LOSS: 3.69 LM NLL: 3.09, KL: 63.57|DE LOSS: 3.84|BETA: 0.018242\n",
      "076[s], 08[s], Ep: 86, Ct: 02000|TR LOSS: 3.69 LM NLL: 3.09, KL: 63.55|DE LOSS: 3.86|BETA: 0.018277\n",
      "075[s], 08[s], Ep: 86, Ct: 02500|TR LOSS: 3.69 LM NLL: 3.09, KL: 63.53|DE LOSS: 3.85|BETA: 0.018311\n",
      "060[s], 08[s], Ep: 87, Ct: 00000|TR LOSS: 3.69 LM NLL: 3.09, KL: 63.52|DE LOSS: 3.85|BETA: 0.018338\n",
      "076[s], 08[s], Ep: 87, Ct: 00500|TR LOSS: 3.69 LM NLL: 3.08, KL: 63.50|DE LOSS: 3.85|BETA: 0.018372\n",
      "076[s], 08[s], Ep: 87, Ct: 01000|TR LOSS: 3.69 LM NLL: 3.08, KL: 63.49|DE LOSS: 3.85|BETA: 0.018407\n",
      "075[s], 08[s], Ep: 87, Ct: 01500|TR LOSS: 3.69 LM NLL: 3.08, KL: 63.47|DE LOSS: 3.86|BETA: 0.018441\n",
      "075[s], 08[s], Ep: 87, Ct: 02000|TR LOSS: 3.69 LM NLL: 3.08, KL: 63.45|DE LOSS: 3.86|BETA: 0.018476\n",
      "075[s], 08[s], Ep: 87, Ct: 02500|TR LOSS: 3.69 LM NLL: 3.08, KL: 63.43|DE LOSS: 3.86|BETA: 0.018510\n",
      "060[s], 08[s], Ep: 88, Ct: 00000|TR LOSS: 3.69 LM NLL: 3.08, KL: 63.42|DE LOSS: 3.86|BETA: 0.018537\n",
      "076[s], 08[s], Ep: 88, Ct: 00500|TR LOSS: 3.69 LM NLL: 3.08, KL: 63.40|DE LOSS: 3.87|BETA: 0.018571\n",
      "076[s], 08[s], Ep: 88, Ct: 01000|TR LOSS: 3.69 LM NLL: 3.08, KL: 63.39|DE LOSS: 3.87|BETA: 0.018605\n",
      "076[s], 08[s], Ep: 88, Ct: 01500|TR LOSS: 3.69 LM NLL: 3.08, KL: 63.37|DE LOSS: 3.86|BETA: 0.018640\n",
      "076[s], 08[s], Ep: 88, Ct: 02000|TR LOSS: 3.69 LM NLL: 3.08, KL: 63.35|DE LOSS: 3.86|BETA: 0.018674\n",
      "076[s], 08[s], Ep: 88, Ct: 02500|TR LOSS: 3.69 LM NLL: 3.07, KL: 63.34|DE LOSS: 3.86|BETA: 0.018709\n",
      "060[s], 08[s], Ep: 89, Ct: 00000|TR LOSS: 3.69 LM NLL: 3.07, KL: 63.32|DE LOSS: 3.89|BETA: 0.018735\n",
      "076[s], 08[s], Ep: 89, Ct: 00500|TR LOSS: 3.69 LM NLL: 3.07, KL: 63.31|DE LOSS: 3.87|BETA: 0.018770\n",
      "075[s], 08[s], Ep: 89, Ct: 01000|TR LOSS: 3.69 LM NLL: 3.07, KL: 63.29|DE LOSS: 3.88|BETA: 0.018804\n",
      "076[s], 08[s], Ep: 89, Ct: 01500|TR LOSS: 3.69 LM NLL: 3.07, KL: 63.27|DE LOSS: 3.88|BETA: 0.018839\n",
      "076[s], 08[s], Ep: 89, Ct: 02000|TR LOSS: 3.69 LM NLL: 3.07, KL: 63.25|DE LOSS: 3.87|BETA: 0.018873\n",
      "075[s], 08[s], Ep: 89, Ct: 02500|TR LOSS: 3.69 LM NLL: 3.07, KL: 63.24|DE LOSS: 3.87|BETA: 0.018908\n",
      "060[s], 08[s], Ep: 90, Ct: 00000|TR LOSS: 3.69 LM NLL: 3.07, KL: 63.22|DE LOSS: 3.87|BETA: 0.018934\n",
      "075[s], 08[s], Ep: 90, Ct: 00500|TR LOSS: 3.69 LM NLL: 3.07, KL: 63.21|DE LOSS: 3.88|BETA: 0.018969\n",
      "075[s], 08[s], Ep: 90, Ct: 01000|TR LOSS: 3.69 LM NLL: 3.07, KL: 63.19|DE LOSS: 3.89|BETA: 0.019003\n",
      "075[s], 08[s], Ep: 90, Ct: 01500|TR LOSS: 3.69 LM NLL: 3.07, KL: 63.17|DE LOSS: 3.87|BETA: 0.019037\n",
      "075[s], 08[s], Ep: 90, Ct: 02000|TR LOSS: 3.69 LM NLL: 3.06, KL: 63.15|DE LOSS: 3.88|BETA: 0.019072\n",
      "075[s], 08[s], Ep: 90, Ct: 02500|TR LOSS: 3.69 LM NLL: 3.06, KL: 63.14|DE LOSS: 3.88|BETA: 0.019106\n",
      "060[s], 08[s], Ep: 91, Ct: 00000|TR LOSS: 3.69 LM NLL: 3.06, KL: 63.12|DE LOSS: 3.88|BETA: 0.019133\n",
      "075[s], 08[s], Ep: 91, Ct: 00500|TR LOSS: 3.69 LM NLL: 3.06, KL: 63.11|DE LOSS: 3.88|BETA: 0.019167\n",
      "075[s], 08[s], Ep: 91, Ct: 01000|TR LOSS: 3.69 LM NLL: 3.06, KL: 63.09|DE LOSS: 3.89|BETA: 0.019202\n",
      "075[s], 08[s], Ep: 91, Ct: 01500|TR LOSS: 3.68 LM NLL: 3.06, KL: 63.07|DE LOSS: 3.89|BETA: 0.019236\n",
      "075[s], 08[s], Ep: 91, Ct: 02000|TR LOSS: 3.68 LM NLL: 3.06, KL: 63.05|DE LOSS: 3.88|BETA: 0.019271\n",
      "076[s], 08[s], Ep: 91, Ct: 02500|TR LOSS: 3.68 LM NLL: 3.06, KL: 63.04|DE LOSS: 3.89|BETA: 0.019305\n",
      "060[s], 08[s], Ep: 92, Ct: 00000|TR LOSS: 3.68 LM NLL: 3.06, KL: 63.02|DE LOSS: 3.90|BETA: 0.019332\n",
      "075[s], 08[s], Ep: 92, Ct: 00500|TR LOSS: 3.68 LM NLL: 3.06, KL: 63.01|DE LOSS: 3.89|BETA: 0.019366\n",
      "075[s], 08[s], Ep: 92, Ct: 01000|TR LOSS: 3.68 LM NLL: 3.06, KL: 62.99|DE LOSS: 3.89|BETA: 0.019400\n",
      "076[s], 08[s], Ep: 92, Ct: 01500|TR LOSS: 3.68 LM NLL: 3.05, KL: 62.97|DE LOSS: 3.90|BETA: 0.019435\n",
      "075[s], 08[s], Ep: 92, Ct: 02000|TR LOSS: 3.68 LM NLL: 3.05, KL: 62.95|DE LOSS: 3.90|BETA: 0.019469\n",
      "076[s], 08[s], Ep: 92, Ct: 02500|TR LOSS: 3.68 LM NLL: 3.05, KL: 62.94|DE LOSS: 3.90|BETA: 0.019504\n",
      "060[s], 08[s], Ep: 93, Ct: 00000|TR LOSS: 3.68 LM NLL: 3.05, KL: 62.92|DE LOSS: 3.91|BETA: 0.019530\n",
      "075[s], 08[s], Ep: 93, Ct: 00500|TR LOSS: 3.68 LM NLL: 3.05, KL: 62.91|DE LOSS: 3.90|BETA: 0.019565\n",
      "076[s], 08[s], Ep: 93, Ct: 01000|TR LOSS: 3.68 LM NLL: 3.05, KL: 62.89|DE LOSS: 3.90|BETA: 0.019599\n",
      "075[s], 08[s], Ep: 93, Ct: 01500|TR LOSS: 3.68 LM NLL: 3.05, KL: 62.87|DE LOSS: 3.91|BETA: 0.019634\n",
      "075[s], 08[s], Ep: 93, Ct: 02000|TR LOSS: 3.68 LM NLL: 3.05, KL: 62.85|DE LOSS: 3.90|BETA: 0.019668\n",
      "076[s], 08[s], Ep: 93, Ct: 02500|TR LOSS: 3.68 LM NLL: 3.05, KL: 62.84|DE LOSS: 3.92|BETA: 0.019703\n",
      "060[s], 08[s], Ep: 94, Ct: 00000|TR LOSS: 3.68 LM NLL: 3.05, KL: 62.82|DE LOSS: 3.90|BETA: 0.019729\n",
      "076[s], 08[s], Ep: 94, Ct: 00500|TR LOSS: 3.68 LM NLL: 3.05, KL: 62.80|DE LOSS: 3.91|BETA: 0.019764\n",
      "075[s], 08[s], Ep: 94, Ct: 01000|TR LOSS: 3.68 LM NLL: 3.04, KL: 62.79|DE LOSS: 3.90|BETA: 0.019798\n",
      "075[s], 08[s], Ep: 94, Ct: 01500|TR LOSS: 3.68 LM NLL: 3.04, KL: 62.77|DE LOSS: 3.91|BETA: 0.019832\n",
      "075[s], 08[s], Ep: 94, Ct: 02000|TR LOSS: 3.68 LM NLL: 3.04, KL: 62.75|DE LOSS: 3.93|BETA: 0.019867\n",
      "076[s], 08[s], Ep: 94, Ct: 02500|TR LOSS: 3.68 LM NLL: 3.04, KL: 62.73|DE LOSS: 3.92|BETA: 0.019901\n",
      "060[s], 08[s], Ep: 95, Ct: 00000|TR LOSS: 3.68 LM NLL: 3.04, KL: 62.72|DE LOSS: 3.92|BETA: 0.019928\n",
      "075[s], 08[s], Ep: 95, Ct: 00500|TR LOSS: 3.68 LM NLL: 3.04, KL: 62.70|DE LOSS: 3.91|BETA: 0.019962\n",
      "075[s], 08[s], Ep: 95, Ct: 01000|TR LOSS: 3.68 LM NLL: 3.04, KL: 62.69|DE LOSS: 3.93|BETA: 0.019997\n",
      "076[s], 08[s], Ep: 95, Ct: 01500|TR LOSS: 3.68 LM NLL: 3.04, KL: 62.67|DE LOSS: 3.93|BETA: 0.020031\n",
      "075[s], 08[s], Ep: 95, Ct: 02000|TR LOSS: 3.68 LM NLL: 3.04, KL: 62.65|DE LOSS: 3.92|BETA: 0.020066\n",
      "076[s], 08[s], Ep: 95, Ct: 02500|TR LOSS: 3.68 LM NLL: 3.04, KL: 62.63|DE LOSS: 3.93|BETA: 0.020100\n",
      "060[s], 08[s], Ep: 96, Ct: 00000|TR LOSS: 3.68 LM NLL: 3.04, KL: 62.62|DE LOSS: 3.93|BETA: 0.020127\n",
      "076[s], 08[s], Ep: 96, Ct: 00500|TR LOSS: 3.68 LM NLL: 3.03, KL: 62.60|DE LOSS: 3.93|BETA: 0.020161\n",
      "076[s], 08[s], Ep: 96, Ct: 01000|TR LOSS: 3.68 LM NLL: 3.03, KL: 62.59|DE LOSS: 3.93|BETA: 0.020196\n",
      "075[s], 08[s], Ep: 96, Ct: 01500|TR LOSS: 3.68 LM NLL: 3.03, KL: 62.57|DE LOSS: 3.93|BETA: 0.020230\n",
      "075[s], 08[s], Ep: 96, Ct: 02000|TR LOSS: 3.68 LM NLL: 3.03, KL: 62.55|DE LOSS: 3.93|BETA: 0.020264\n",
      "075[s], 08[s], Ep: 96, Ct: 02500|TR LOSS: 3.68 LM NLL: 3.03, KL: 62.53|DE LOSS: 3.93|BETA: 0.020299\n",
      "060[s], 08[s], Ep: 97, Ct: 00000|TR LOSS: 3.68 LM NLL: 3.03, KL: 62.52|DE LOSS: 3.93|BETA: 0.020325\n",
      "075[s], 08[s], Ep: 97, Ct: 00500|TR LOSS: 3.68 LM NLL: 3.03, KL: 62.50|DE LOSS: 3.94|BETA: 0.020360\n",
      "076[s], 08[s], Ep: 97, Ct: 01000|TR LOSS: 3.68 LM NLL: 3.03, KL: 62.48|DE LOSS: 3.94|BETA: 0.020394\n",
      "076[s], 08[s], Ep: 97, Ct: 01500|TR LOSS: 3.68 LM NLL: 3.03, KL: 62.47|DE LOSS: 3.94|BETA: 0.020429\n",
      "076[s], 08[s], Ep: 97, Ct: 02000|TR LOSS: 3.68 LM NLL: 3.03, KL: 62.45|DE LOSS: 3.94|BETA: 0.020463\n",
      "075[s], 08[s], Ep: 97, Ct: 02500|TR LOSS: 3.68 LM NLL: 3.03, KL: 62.43|DE LOSS: 3.94|BETA: 0.020498\n",
      "060[s], 08[s], Ep: 98, Ct: 00000|TR LOSS: 3.68 LM NLL: 3.03, KL: 62.42|DE LOSS: 3.94|BETA: 0.020524\n",
      "075[s], 08[s], Ep: 98, Ct: 00500|TR LOSS: 3.68 LM NLL: 3.03, KL: 62.40|DE LOSS: 3.95|BETA: 0.020559\n",
      "075[s], 08[s], Ep: 98, Ct: 01000|TR LOSS: 3.68 LM NLL: 3.02, KL: 62.38|DE LOSS: 3.95|BETA: 0.020593\n",
      "076[s], 08[s], Ep: 98, Ct: 01500|TR LOSS: 3.68 LM NLL: 3.02, KL: 62.37|DE LOSS: 3.95|BETA: 0.020627\n",
      "075[s], 08[s], Ep: 98, Ct: 02000|TR LOSS: 3.68 LM NLL: 3.02, KL: 62.35|DE LOSS: 3.96|BETA: 0.020662\n",
      "075[s], 08[s], Ep: 98, Ct: 02500|TR LOSS: 3.68 LM NLL: 3.02, KL: 62.33|DE LOSS: 3.94|BETA: 0.020696\n",
      "059[s], 08[s], Ep: 99, Ct: 00000|TR LOSS: 3.68 LM NLL: 3.02, KL: 62.32|DE LOSS: 3.96|BETA: 0.020723\n",
      "075[s], 08[s], Ep: 99, Ct: 00500|TR LOSS: 3.68 LM NLL: 3.02, KL: 62.30|DE LOSS: 3.95|BETA: 0.020757\n",
      "075[s], 08[s], Ep: 99, Ct: 01000|TR LOSS: 3.68 LM NLL: 3.02, KL: 62.28|DE LOSS: 3.95|BETA: 0.020792\n",
      "075[s], 08[s], Ep: 99, Ct: 01500|TR LOSS: 3.68 LM NLL: 3.02, KL: 62.26|DE LOSS: 3.95|BETA: 0.020826\n",
      "076[s], 08[s], Ep: 99, Ct: 02000|TR LOSS: 3.68 LM NLL: 3.02, KL: 62.25|DE LOSS: 3.96|BETA: 0.020861\n",
      "075[s], 08[s], Ep: 99, Ct: 02500|TR LOSS: 3.68 LM NLL: 3.02, KL: 62.23|DE LOSS: 3.96|BETA: 0.020895\n",
      "060[s], 08[s], Ep: 100, Ct: 00000|TR LOSS: 3.68 LM NLL: 3.02, KL: 62.21|DE LOSS: 3.96|BETA: 0.020922\n",
      "076[s], 08[s], Ep: 100, Ct: 00500|TR LOSS: 3.68 LM NLL: 3.02, KL: 62.20|DE LOSS: 3.96|BETA: 0.020956\n",
      "075[s], 08[s], Ep: 100, Ct: 01000|TR LOSS: 3.68 LM NLL: 3.01, KL: 62.18|DE LOSS: 3.96|BETA: 0.020991\n",
      "075[s], 08[s], Ep: 100, Ct: 01500|TR LOSS: 3.68 LM NLL: 3.01, KL: 62.16|DE LOSS: 3.96|BETA: 0.021025\n",
      "075[s], 08[s], Ep: 100, Ct: 02000|TR LOSS: 3.68 LM NLL: 3.01, KL: 62.14|DE LOSS: 3.96|BETA: 0.021059\n",
      "075[s], 08[s], Ep: 100, Ct: 02500|TR LOSS: 3.68 LM NLL: 3.01, KL: 62.13|DE LOSS: 3.97|BETA: 0.021094\n",
      "060[s], 08[s], Ep: 101, Ct: 00000|TR LOSS: 3.68 LM NLL: 3.01, KL: 62.11|DE LOSS: 3.97|BETA: 0.021120\n",
      "075[s], 08[s], Ep: 101, Ct: 00500|TR LOSS: 3.68 LM NLL: 3.01, KL: 62.10|DE LOSS: 3.97|BETA: 0.021155\n",
      "075[s], 08[s], Ep: 101, Ct: 01000|TR LOSS: 3.68 LM NLL: 3.01, KL: 62.08|DE LOSS: 3.97|BETA: 0.021189\n",
      "075[s], 08[s], Ep: 101, Ct: 01500|TR LOSS: 3.68 LM NLL: 3.01, KL: 62.06|DE LOSS: 3.96|BETA: 0.021224\n",
      "075[s], 08[s], Ep: 101, Ct: 02000|TR LOSS: 3.68 LM NLL: 3.01, KL: 62.04|DE LOSS: 3.98|BETA: 0.021258\n",
      "075[s], 08[s], Ep: 101, Ct: 02500|TR LOSS: 3.68 LM NLL: 3.01, KL: 62.02|DE LOSS: 3.97|BETA: 0.021293\n",
      "060[s], 08[s], Ep: 102, Ct: 00000|TR LOSS: 3.68 LM NLL: 3.01, KL: 62.01|DE LOSS: 3.98|BETA: 0.021319\n",
      "075[s], 08[s], Ep: 102, Ct: 00500|TR LOSS: 3.68 LM NLL: 3.01, KL: 61.99|DE LOSS: 3.98|BETA: 0.021354\n",
      "075[s], 08[s], Ep: 102, Ct: 01000|TR LOSS: 3.68 LM NLL: 3.01, KL: 61.98|DE LOSS: 3.98|BETA: 0.021388\n",
      "076[s], 08[s], Ep: 102, Ct: 01500|TR LOSS: 3.68 LM NLL: 3.00, KL: 61.96|DE LOSS: 3.99|BETA: 0.021423\n",
      "075[s], 08[s], Ep: 102, Ct: 02000|TR LOSS: 3.68 LM NLL: 3.00, KL: 61.94|DE LOSS: 3.98|BETA: 0.021457\n",
      "075[s], 08[s], Ep: 102, Ct: 02500|TR LOSS: 3.68 LM NLL: 3.00, KL: 61.92|DE LOSS: 3.98|BETA: 0.021491\n",
      "060[s], 08[s], Ep: 103, Ct: 00000|TR LOSS: 3.68 LM NLL: 3.00, KL: 61.91|DE LOSS: 3.98|BETA: 0.021518\n",
      "075[s], 08[s], Ep: 103, Ct: 00500|TR LOSS: 3.68 LM NLL: 3.00, KL: 61.89|DE LOSS: 3.99|BETA: 0.021552\n",
      "075[s], 08[s], Ep: 103, Ct: 01000|TR LOSS: 3.68 LM NLL: 3.00, KL: 61.87|DE LOSS: 3.99|BETA: 0.021587\n",
      "075[s], 08[s], Ep: 103, Ct: 01500|TR LOSS: 3.68 LM NLL: 3.00, KL: 61.86|DE LOSS: 3.99|BETA: 0.021621\n",
      "075[s], 08[s], Ep: 103, Ct: 02000|TR LOSS: 3.68 LM NLL: 3.00, KL: 61.84|DE LOSS: 3.99|BETA: 0.021656\n",
      "075[s], 08[s], Ep: 103, Ct: 02500|TR LOSS: 3.68 LM NLL: 3.00, KL: 61.82|DE LOSS: 3.99|BETA: 0.021690\n",
      "060[s], 08[s], Ep: 104, Ct: 00000|TR LOSS: 3.68 LM NLL: 3.00, KL: 61.81|DE LOSS: 4.00|BETA: 0.021717\n",
      "075[s], 08[s], Ep: 104, Ct: 00500|TR LOSS: 3.68 LM NLL: 3.00, KL: 61.79|DE LOSS: 3.99|BETA: 0.021751\n",
      "075[s], 08[s], Ep: 104, Ct: 01000|TR LOSS: 3.68 LM NLL: 3.00, KL: 61.77|DE LOSS: 3.99|BETA: 0.021786\n",
      "076[s], 08[s], Ep: 104, Ct: 01500|TR LOSS: 3.68 LM NLL: 3.00, KL: 61.75|DE LOSS: 3.99|BETA: 0.021820\n",
      "075[s], 08[s], Ep: 104, Ct: 02000|TR LOSS: 3.68 LM NLL: 3.00, KL: 61.74|DE LOSS: 4.00|BETA: 0.021855\n",
      "075[s], 08[s], Ep: 104, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.99, KL: 61.72|DE LOSS: 4.00|BETA: 0.021889\n",
      "060[s], 08[s], Ep: 105, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.99, KL: 61.70|DE LOSS: 4.00|BETA: 0.021915\n",
      "075[s], 08[s], Ep: 105, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.99, KL: 61.69|DE LOSS: 4.01|BETA: 0.021950\n",
      "075[s], 08[s], Ep: 105, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.99, KL: 61.67|DE LOSS: 4.00|BETA: 0.021984\n",
      "075[s], 08[s], Ep: 105, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.99, KL: 61.65|DE LOSS: 4.00|BETA: 0.022019\n",
      "075[s], 08[s], Ep: 105, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.99, KL: 61.63|DE LOSS: 4.02|BETA: 0.022053\n",
      "075[s], 08[s], Ep: 105, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.99, KL: 61.62|DE LOSS: 4.01|BETA: 0.022088\n",
      "060[s], 08[s], Ep: 106, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.99, KL: 61.60|DE LOSS: 4.01|BETA: 0.022114\n",
      "075[s], 08[s], Ep: 106, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.99, KL: 61.58|DE LOSS: 4.00|BETA: 0.022149\n",
      "075[s], 08[s], Ep: 106, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.99, KL: 61.57|DE LOSS: 4.01|BETA: 0.022183\n",
      "075[s], 08[s], Ep: 106, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.99, KL: 61.55|DE LOSS: 4.01|BETA: 0.022218\n",
      "075[s], 08[s], Ep: 106, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.99, KL: 61.53|DE LOSS: 4.02|BETA: 0.022252\n",
      "075[s], 08[s], Ep: 106, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.99, KL: 61.51|DE LOSS: 4.02|BETA: 0.022286\n",
      "060[s], 08[s], Ep: 107, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.99, KL: 61.50|DE LOSS: 4.02|BETA: 0.022313\n",
      "076[s], 08[s], Ep: 107, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.98, KL: 61.48|DE LOSS: 4.02|BETA: 0.022347\n",
      "076[s], 08[s], Ep: 107, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.98, KL: 61.46|DE LOSS: 4.02|BETA: 0.022382\n",
      "076[s], 08[s], Ep: 107, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.98, KL: 61.45|DE LOSS: 4.02|BETA: 0.022416\n",
      "075[s], 08[s], Ep: 107, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.98, KL: 61.43|DE LOSS: 4.02|BETA: 0.022451\n",
      "075[s], 08[s], Ep: 107, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.98, KL: 61.41|DE LOSS: 4.03|BETA: 0.022485\n",
      "060[s], 08[s], Ep: 108, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.98, KL: 61.40|DE LOSS: 4.03|BETA: 0.022512\n",
      "076[s], 08[s], Ep: 108, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.98, KL: 61.38|DE LOSS: 4.04|BETA: 0.022546\n",
      "075[s], 08[s], Ep: 108, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.98, KL: 61.36|DE LOSS: 4.03|BETA: 0.022581\n",
      "075[s], 08[s], Ep: 108, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.98, KL: 61.34|DE LOSS: 4.02|BETA: 0.022615\n",
      "075[s], 08[s], Ep: 108, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.98, KL: 61.33|DE LOSS: 4.03|BETA: 0.022650\n",
      "075[s], 08[s], Ep: 108, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.98, KL: 61.31|DE LOSS: 4.03|BETA: 0.022684\n",
      "060[s], 08[s], Ep: 109, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.98, KL: 61.29|DE LOSS: 4.04|BETA: 0.022710\n",
      "075[s], 08[s], Ep: 109, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.98, KL: 61.28|DE LOSS: 4.03|BETA: 0.022745\n",
      "075[s], 08[s], Ep: 109, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.98, KL: 61.26|DE LOSS: 4.04|BETA: 0.022779\n",
      "076[s], 08[s], Ep: 109, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.98, KL: 61.24|DE LOSS: 4.04|BETA: 0.022814\n",
      "075[s], 08[s], Ep: 109, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.97, KL: 61.22|DE LOSS: 4.04|BETA: 0.022848\n",
      "076[s], 08[s], Ep: 109, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.97, KL: 61.21|DE LOSS: 4.04|BETA: 0.022883\n",
      "060[s], 08[s], Ep: 110, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.97, KL: 61.19|DE LOSS: 4.04|BETA: 0.022909\n",
      "076[s], 08[s], Ep: 110, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.97, KL: 61.17|DE LOSS: 4.05|BETA: 0.022944\n",
      "076[s], 08[s], Ep: 110, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.97, KL: 61.16|DE LOSS: 4.05|BETA: 0.022978\n",
      "076[s], 08[s], Ep: 110, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.97, KL: 61.14|DE LOSS: 4.05|BETA: 0.023013\n",
      "075[s], 08[s], Ep: 110, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.97, KL: 61.12|DE LOSS: 4.05|BETA: 0.023047\n",
      "075[s], 08[s], Ep: 110, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.97, KL: 61.10|DE LOSS: 4.04|BETA: 0.023082\n",
      "060[s], 08[s], Ep: 111, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.97, KL: 61.09|DE LOSS: 4.06|BETA: 0.023108\n",
      "075[s], 08[s], Ep: 111, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.97, KL: 61.07|DE LOSS: 4.05|BETA: 0.023142\n",
      "075[s], 08[s], Ep: 111, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.97, KL: 61.05|DE LOSS: 4.05|BETA: 0.023177\n",
      "075[s], 08[s], Ep: 111, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.97, KL: 61.04|DE LOSS: 4.07|BETA: 0.023211\n",
      "075[s], 08[s], Ep: 111, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.97, KL: 61.02|DE LOSS: 4.05|BETA: 0.023246\n",
      "075[s], 08[s], Ep: 111, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.97, KL: 61.00|DE LOSS: 4.06|BETA: 0.023280\n",
      "060[s], 08[s], Ep: 112, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.97, KL: 60.99|DE LOSS: 4.06|BETA: 0.023307\n",
      "075[s], 08[s], Ep: 112, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.97, KL: 60.97|DE LOSS: 4.05|BETA: 0.023341\n",
      "075[s], 08[s], Ep: 112, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.95|DE LOSS: 4.06|BETA: 0.023376\n",
      "075[s], 08[s], Ep: 112, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.93|DE LOSS: 4.06|BETA: 0.023410\n",
      "075[s], 08[s], Ep: 112, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.91|DE LOSS: 4.07|BETA: 0.023445\n",
      "075[s], 08[s], Ep: 112, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.90|DE LOSS: 4.07|BETA: 0.023479\n",
      "060[s], 08[s], Ep: 113, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.88|DE LOSS: 4.07|BETA: 0.023506\n",
      "075[s], 08[s], Ep: 113, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.86|DE LOSS: 4.06|BETA: 0.023540\n",
      "076[s], 08[s], Ep: 113, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.85|DE LOSS: 4.07|BETA: 0.023574\n",
      "075[s], 08[s], Ep: 113, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.83|DE LOSS: 4.07|BETA: 0.023609\n",
      "075[s], 08[s], Ep: 113, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.81|DE LOSS: 4.07|BETA: 0.023643\n",
      "075[s], 08[s], Ep: 113, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.79|DE LOSS: 4.08|BETA: 0.023678\n",
      "060[s], 08[s], Ep: 114, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.78|DE LOSS: 4.07|BETA: 0.023704\n",
      "075[s], 08[s], Ep: 114, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.76|DE LOSS: 4.08|BETA: 0.023739\n",
      "075[s], 08[s], Ep: 114, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.74|DE LOSS: 4.09|BETA: 0.023773\n",
      "076[s], 08[s], Ep: 114, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.73|DE LOSS: 4.08|BETA: 0.023808\n",
      "075[s], 08[s], Ep: 114, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.71|DE LOSS: 4.08|BETA: 0.023842\n",
      "076[s], 08[s], Ep: 114, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.69|DE LOSS: 4.08|BETA: 0.023877\n",
      "060[s], 08[s], Ep: 115, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.68|DE LOSS: 4.09|BETA: 0.023903\n",
      "075[s], 08[s], Ep: 115, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.66|DE LOSS: 4.09|BETA: 0.023937\n",
      "075[s], 08[s], Ep: 115, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.64|DE LOSS: 4.09|BETA: 0.023972\n",
      "076[s], 08[s], Ep: 115, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.62|DE LOSS: 4.09|BETA: 0.024006\n",
      "075[s], 08[s], Ep: 115, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.61|DE LOSS: 4.09|BETA: 0.024041\n",
      "076[s], 08[s], Ep: 115, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.59|DE LOSS: 4.09|BETA: 0.024075\n",
      "060[s], 08[s], Ep: 116, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.57|DE LOSS: 4.09|BETA: 0.024102\n",
      "076[s], 08[s], Ep: 116, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.56|DE LOSS: 4.09|BETA: 0.024136\n",
      "075[s], 08[s], Ep: 116, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.54|DE LOSS: 4.10|BETA: 0.024171\n",
      "076[s], 08[s], Ep: 116, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.52|DE LOSS: 4.09|BETA: 0.024205\n",
      "075[s], 08[s], Ep: 116, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.50|DE LOSS: 4.09|BETA: 0.024240\n",
      "075[s], 08[s], Ep: 116, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.48|DE LOSS: 4.09|BETA: 0.024274\n",
      "060[s], 08[s], Ep: 117, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.47|DE LOSS: 4.10|BETA: 0.024301\n",
      "076[s], 08[s], Ep: 117, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.45|DE LOSS: 4.11|BETA: 0.024335\n",
      "076[s], 08[s], Ep: 117, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.44|DE LOSS: 4.11|BETA: 0.024369\n",
      "076[s], 08[s], Ep: 117, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.42|DE LOSS: 4.10|BETA: 0.024404\n",
      "076[s], 08[s], Ep: 117, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.40|DE LOSS: 4.10|BETA: 0.024438\n",
      "075[s], 08[s], Ep: 117, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.38|DE LOSS: 4.11|BETA: 0.024473\n",
      "060[s], 08[s], Ep: 118, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.37|DE LOSS: 4.11|BETA: 0.024499\n",
      "076[s], 08[s], Ep: 118, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.35|DE LOSS: 4.11|BETA: 0.024534\n",
      "076[s], 08[s], Ep: 118, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.33|DE LOSS: 4.11|BETA: 0.024568\n",
      "076[s], 08[s], Ep: 118, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.31|DE LOSS: 4.12|BETA: 0.024603\n",
      "076[s], 08[s], Ep: 118, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.30|DE LOSS: 4.11|BETA: 0.024637\n",
      "075[s], 08[s], Ep: 118, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.28|DE LOSS: 4.11|BETA: 0.024672\n",
      "060[s], 08[s], Ep: 119, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.26|DE LOSS: 4.11|BETA: 0.024698\n",
      "075[s], 08[s], Ep: 119, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.25|DE LOSS: 4.12|BETA: 0.024733\n",
      "076[s], 08[s], Ep: 119, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.23|DE LOSS: 4.11|BETA: 0.024767\n",
      "076[s], 08[s], Ep: 119, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.21|DE LOSS: 4.12|BETA: 0.024801\n",
      "075[s], 08[s], Ep: 119, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.19|DE LOSS: 4.12|BETA: 0.024836\n",
      "076[s], 08[s], Ep: 119, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.18|DE LOSS: 4.13|BETA: 0.024870\n",
      "060[s], 08[s], Ep: 120, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.16|DE LOSS: 4.12|BETA: 0.024897\n",
      "076[s], 08[s], Ep: 120, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.14|DE LOSS: 4.13|BETA: 0.024931\n",
      "076[s], 08[s], Ep: 120, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.13|DE LOSS: 4.13|BETA: 0.024966\n",
      "076[s], 08[s], Ep: 120, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.11|DE LOSS: 4.12|BETA: 0.025000\n",
      "075[s], 08[s], Ep: 120, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.09|DE LOSS: 4.14|BETA: 0.025035\n",
      "075[s], 08[s], Ep: 120, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.07|DE LOSS: 4.13|BETA: 0.025069\n",
      "059[s], 08[s], Ep: 121, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.93, KL: 60.06|DE LOSS: 4.13|BETA: 0.025096\n",
      "075[s], 08[s], Ep: 121, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.93, KL: 60.04|DE LOSS: 4.14|BETA: 0.025130\n",
      "075[s], 08[s], Ep: 121, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.93, KL: 60.02|DE LOSS: 4.13|BETA: 0.025164\n",
      "075[s], 08[s], Ep: 121, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.93, KL: 60.01|DE LOSS: 4.13|BETA: 0.025199\n",
      "076[s], 08[s], Ep: 121, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.93, KL: 59.99|DE LOSS: 4.14|BETA: 0.025233\n",
      "076[s], 08[s], Ep: 121, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.93, KL: 59.97|DE LOSS: 4.14|BETA: 0.025268\n",
      "060[s], 08[s], Ep: 122, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.93, KL: 59.96|DE LOSS: 4.14|BETA: 0.025294\n",
      "076[s], 08[s], Ep: 122, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.93, KL: 59.94|DE LOSS: 4.14|BETA: 0.025329\n",
      "076[s], 08[s], Ep: 122, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.93, KL: 59.92|DE LOSS: 4.14|BETA: 0.025363\n",
      "076[s], 08[s], Ep: 122, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.93, KL: 59.90|DE LOSS: 4.14|BETA: 0.025398\n",
      "075[s], 08[s], Ep: 122, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.93, KL: 59.88|DE LOSS: 4.15|BETA: 0.025432\n",
      "075[s], 08[s], Ep: 122, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.93, KL: 59.87|DE LOSS: 4.14|BETA: 0.025467\n",
      "060[s], 08[s], Ep: 123, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.93, KL: 59.85|DE LOSS: 4.15|BETA: 0.025493\n",
      "075[s], 08[s], Ep: 123, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.93, KL: 59.84|DE LOSS: 4.15|BETA: 0.025528\n",
      "076[s], 08[s], Ep: 123, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.93, KL: 59.82|DE LOSS: 4.15|BETA: 0.025562\n",
      "075[s], 08[s], Ep: 123, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.93, KL: 59.80|DE LOSS: 4.15|BETA: 0.025596\n",
      "075[s], 08[s], Ep: 123, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.93, KL: 59.78|DE LOSS: 4.15|BETA: 0.025631\n",
      "076[s], 08[s], Ep: 123, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.93, KL: 59.76|DE LOSS: 4.15|BETA: 0.025665\n",
      "060[s], 08[s], Ep: 124, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.93, KL: 59.75|DE LOSS: 4.15|BETA: 0.025692\n",
      "075[s], 08[s], Ep: 124, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.73|DE LOSS: 4.16|BETA: 0.025726\n",
      "076[s], 08[s], Ep: 124, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.71|DE LOSS: 4.16|BETA: 0.025761\n",
      "076[s], 08[s], Ep: 124, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.70|DE LOSS: 4.16|BETA: 0.025795\n",
      "075[s], 08[s], Ep: 124, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.68|DE LOSS: 4.16|BETA: 0.025830\n",
      "075[s], 08[s], Ep: 124, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.66|DE LOSS: 4.16|BETA: 0.025864\n",
      "060[s], 08[s], Ep: 125, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.65|DE LOSS: 4.16|BETA: 0.025891\n",
      "076[s], 08[s], Ep: 125, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.63|DE LOSS: 4.16|BETA: 0.025925\n",
      "076[s], 08[s], Ep: 125, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.61|DE LOSS: 4.16|BETA: 0.025960\n",
      "076[s], 08[s], Ep: 125, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.59|DE LOSS: 4.16|BETA: 0.025994\n",
      "075[s], 08[s], Ep: 125, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.58|DE LOSS: 4.17|BETA: 0.026028\n",
      "076[s], 08[s], Ep: 125, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.56|DE LOSS: 4.17|BETA: 0.026063\n",
      "060[s], 08[s], Ep: 126, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.54|DE LOSS: 4.16|BETA: 0.026089\n",
      "075[s], 08[s], Ep: 126, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.53|DE LOSS: 4.17|BETA: 0.026124\n",
      "075[s], 08[s], Ep: 126, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.51|DE LOSS: 4.17|BETA: 0.026158\n",
      "076[s], 08[s], Ep: 126, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.49|DE LOSS: 4.18|BETA: 0.026193\n",
      "076[s], 08[s], Ep: 126, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.47|DE LOSS: 4.17|BETA: 0.026227\n",
      "076[s], 08[s], Ep: 126, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.45|DE LOSS: 4.18|BETA: 0.026262\n",
      "060[s], 08[s], Ep: 127, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.44|DE LOSS: 4.18|BETA: 0.026288\n",
      "075[s], 08[s], Ep: 127, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.42|DE LOSS: 4.18|BETA: 0.026323\n",
      "076[s], 08[s], Ep: 127, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.41|DE LOSS: 4.18|BETA: 0.026357\n",
      "076[s], 08[s], Ep: 127, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.39|DE LOSS: 4.18|BETA: 0.026392\n",
      "076[s], 08[s], Ep: 127, Ct: 02000|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.37|DE LOSS: 4.18|BETA: 0.026426\n",
      "076[s], 08[s], Ep: 127, Ct: 02500|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.35|DE LOSS: 4.18|BETA: 0.026460\n",
      "060[s], 08[s], Ep: 128, Ct: 00000|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.34|DE LOSS: 4.20|BETA: 0.026487\n",
      "075[s], 08[s], Ep: 128, Ct: 00500|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.32|DE LOSS: 4.19|BETA: 0.026521\n",
      "075[s], 08[s], Ep: 128, Ct: 01000|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.30|DE LOSS: 4.19|BETA: 0.026556\n",
      "076[s], 08[s], Ep: 128, Ct: 01500|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.29|DE LOSS: 4.18|BETA: 0.026590\n",
      "075[s], 08[s], Ep: 128, Ct: 02000|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.27|DE LOSS: 4.19|BETA: 0.026625\n",
      "075[s], 08[s], Ep: 128, Ct: 02500|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.25|DE LOSS: 4.20|BETA: 0.026659\n",
      "060[s], 08[s], Ep: 129, Ct: 00000|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.24|DE LOSS: 4.19|BETA: 0.026686\n",
      "074[s], 07[s], Ep: 129, Ct: 00500|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.22|DE LOSS: 4.20|BETA: 0.026720\n",
      "074[s], 07[s], Ep: 129, Ct: 01000|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.20|DE LOSS: 4.20|BETA: 0.026755\n",
      "073[s], 07[s], Ep: 129, Ct: 01500|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.18|DE LOSS: 4.20|BETA: 0.026789\n",
      "070[s], 07[s], Ep: 129, Ct: 02000|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.16|DE LOSS: 4.20|BETA: 0.026823\n",
      "070[s], 07[s], Ep: 129, Ct: 02500|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.15|DE LOSS: 4.19|BETA: 0.026858\n",
      "056[s], 07[s], Ep: 130, Ct: 00000|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.13|DE LOSS: 4.21|BETA: 0.026884\n",
      "070[s], 07[s], Ep: 130, Ct: 00500|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.12|DE LOSS: 4.20|BETA: 0.026919\n",
      "070[s], 07[s], Ep: 130, Ct: 01000|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.10|DE LOSS: 4.20|BETA: 0.026953\n",
      "070[s], 07[s], Ep: 130, Ct: 01500|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.08|DE LOSS: 4.20|BETA: 0.026988\n",
      "070[s], 07[s], Ep: 130, Ct: 02000|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.06|DE LOSS: 4.21|BETA: 0.027022\n",
      "070[s], 07[s], Ep: 130, Ct: 02500|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.04|DE LOSS: 4.20|BETA: 0.027057\n",
      "056[s], 07[s], Ep: 131, Ct: 00000|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.03|DE LOSS: 4.20|BETA: 0.027083\n",
      "070[s], 07[s], Ep: 131, Ct: 00500|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.01|DE LOSS: 4.22|BETA: 0.027118\n",
      "070[s], 07[s], Ep: 131, Ct: 01000|TR LOSS: 3.69 LM NLL: 2.91, KL: 58.99|DE LOSS: 4.21|BETA: 0.027152\n",
      "070[s], 07[s], Ep: 131, Ct: 01500|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.98|DE LOSS: 4.21|BETA: 0.027187\n",
      "070[s], 07[s], Ep: 131, Ct: 02000|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.96|DE LOSS: 4.21|BETA: 0.027221\n",
      "070[s], 07[s], Ep: 131, Ct: 02500|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.94|DE LOSS: 4.22|BETA: 0.027255\n",
      "055[s], 07[s], Ep: 132, Ct: 00000|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.93|DE LOSS: 4.21|BETA: 0.027282\n",
      "070[s], 07[s], Ep: 132, Ct: 00500|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.91|DE LOSS: 4.22|BETA: 0.027316\n",
      "070[s], 07[s], Ep: 132, Ct: 01000|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.89|DE LOSS: 4.22|BETA: 0.027351\n",
      "070[s], 07[s], Ep: 132, Ct: 01500|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.87|DE LOSS: 4.22|BETA: 0.027385\n",
      "070[s], 07[s], Ep: 132, Ct: 02000|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.86|DE LOSS: 4.22|BETA: 0.027420\n",
      "070[s], 07[s], Ep: 132, Ct: 02500|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.84|DE LOSS: 4.22|BETA: 0.027454\n",
      "056[s], 07[s], Ep: 133, Ct: 00000|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.82|DE LOSS: 4.23|BETA: 0.027481\n",
      "070[s], 07[s], Ep: 133, Ct: 00500|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.81|DE LOSS: 4.23|BETA: 0.027515\n",
      "070[s], 07[s], Ep: 133, Ct: 01000|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.79|DE LOSS: 4.22|BETA: 0.027550\n",
      "070[s], 07[s], Ep: 133, Ct: 01500|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.77|DE LOSS: 4.22|BETA: 0.027584\n",
      "070[s], 07[s], Ep: 133, Ct: 02000|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.75|DE LOSS: 4.23|BETA: 0.027619\n",
      "070[s], 07[s], Ep: 133, Ct: 02500|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.74|DE LOSS: 4.23|BETA: 0.027653\n",
      "056[s], 07[s], Ep: 134, Ct: 00000|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.72|DE LOSS: 4.23|BETA: 0.027679\n",
      "070[s], 07[s], Ep: 134, Ct: 00500|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.70|DE LOSS: 4.23|BETA: 0.027714\n",
      "069[s], 07[s], Ep: 134, Ct: 01000|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.69|DE LOSS: 4.23|BETA: 0.027748\n",
      "070[s], 07[s], Ep: 134, Ct: 01500|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.67|DE LOSS: 4.22|BETA: 0.027783\n",
      "070[s], 07[s], Ep: 134, Ct: 02000|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.65|DE LOSS: 4.23|BETA: 0.027817\n",
      "070[s], 07[s], Ep: 134, Ct: 02500|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.63|DE LOSS: 4.23|BETA: 0.027852\n",
      "056[s], 07[s], Ep: 135, Ct: 00000|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.62|DE LOSS: 4.23|BETA: 0.027878\n",
      "070[s], 07[s], Ep: 135, Ct: 00500|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.60|DE LOSS: 4.24|BETA: 0.027913\n",
      "070[s], 07[s], Ep: 135, Ct: 01000|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.58|DE LOSS: 4.24|BETA: 0.027947\n",
      "070[s], 07[s], Ep: 135, Ct: 01500|TR LOSS: 3.69 LM NLL: 2.89, KL: 58.57|DE LOSS: 4.24|BETA: 0.027982\n",
      "070[s], 07[s], Ep: 135, Ct: 02000|TR LOSS: 3.69 LM NLL: 2.89, KL: 58.55|DE LOSS: 4.24|BETA: 0.028016\n",
      "070[s], 07[s], Ep: 135, Ct: 02500|TR LOSS: 3.69 LM NLL: 2.89, KL: 58.53|DE LOSS: 4.25|BETA: 0.028051\n",
      "055[s], 07[s], Ep: 136, Ct: 00000|TR LOSS: 3.69 LM NLL: 2.89, KL: 58.52|DE LOSS: 4.25|BETA: 0.028077\n",
      "070[s], 07[s], Ep: 136, Ct: 00500|TR LOSS: 3.69 LM NLL: 2.89, KL: 58.50|DE LOSS: 4.25|BETA: 0.028111\n",
      "070[s], 07[s], Ep: 136, Ct: 01000|TR LOSS: 3.69 LM NLL: 2.89, KL: 58.48|DE LOSS: 4.24|BETA: 0.028146\n",
      "070[s], 07[s], Ep: 136, Ct: 01500|TR LOSS: 3.69 LM NLL: 2.89, KL: 58.46|DE LOSS: 4.25|BETA: 0.028180\n",
      "070[s], 07[s], Ep: 136, Ct: 02000|TR LOSS: 3.69 LM NLL: 2.89, KL: 58.45|DE LOSS: 4.26|BETA: 0.028215\n",
      "070[s], 07[s], Ep: 136, Ct: 02500|TR LOSS: 3.69 LM NLL: 2.89, KL: 58.43|DE LOSS: 4.26|BETA: 0.028249\n",
      "056[s], 07[s], Ep: 137, Ct: 00000|TR LOSS: 3.69 LM NLL: 2.89, KL: 58.42|DE LOSS: 4.25|BETA: 0.028276\n",
      "070[s], 07[s], Ep: 137, Ct: 00500|TR LOSS: 3.69 LM NLL: 2.89, KL: 58.40|DE LOSS: 4.26|BETA: 0.028310\n",
      "070[s], 07[s], Ep: 137, Ct: 01000|TR LOSS: 3.69 LM NLL: 2.89, KL: 58.38|DE LOSS: 4.25|BETA: 0.028345\n",
      "070[s], 07[s], Ep: 137, Ct: 01500|TR LOSS: 3.69 LM NLL: 2.89, KL: 58.36|DE LOSS: 4.25|BETA: 0.028379\n",
      "069[s], 07[s], Ep: 137, Ct: 02000|TR LOSS: 3.69 LM NLL: 2.89, KL: 58.34|DE LOSS: 4.26|BETA: 0.028414\n",
      "070[s], 07[s], Ep: 137, Ct: 02500|TR LOSS: 3.69 LM NLL: 2.89, KL: 58.33|DE LOSS: 4.26|BETA: 0.028448\n",
      "055[s], 07[s], Ep: 138, Ct: 00000|TR LOSS: 3.69 LM NLL: 2.89, KL: 58.31|DE LOSS: 4.25|BETA: 0.028474\n",
      "070[s], 07[s], Ep: 138, Ct: 00500|TR LOSS: 3.69 LM NLL: 2.89, KL: 58.30|DE LOSS: 4.26|BETA: 0.028509\n",
      "070[s], 07[s], Ep: 138, Ct: 01000|TR LOSS: 3.69 LM NLL: 2.89, KL: 58.28|DE LOSS: 4.26|BETA: 0.028543\n",
      "069[s], 07[s], Ep: 138, Ct: 01500|TR LOSS: 3.69 LM NLL: 2.89, KL: 58.26|DE LOSS: 4.27|BETA: 0.028578\n",
      "069[s], 07[s], Ep: 138, Ct: 02000|TR LOSS: 3.69 LM NLL: 2.89, KL: 58.24|DE LOSS: 4.26|BETA: 0.028612\n",
      "True: the medicine bow national forest in southeast wyoming is seeing its third wildfire of the season\n",
      "Pred: the tonto department of park in southeast missouri is seeing a big floor in the season\n",
      "True: the # acre arapaho fire is the second so far in the laramie peak area in northern albany county\n",
      "Pred: the # magnitude recreation dam was the first known as in the # area area of downtown albany county\n",
      "True: it 's believed lightning started it wednesday night\n",
      "Pred: he 's believed that sparked it monday afternoon\n",
      "True: gov . matt mead says about # area homes were evacuated\n",
      "Pred: gov . matt mead says about # homes homes were evacuated\n",
      "True: it 's only a few miles from the cow camp fire that burned # square miles earlier this month\n",
      "Pred: it 's slightly half a the from the lake creek park , burned # square miles before this month\n",
      "True: the russell 's camp fire has burned about # square miles of the medicine bow in neighboring converse county\n",
      "Pred: the <unk> county <unk> township has burned # # miles miles of the pond home in <unk> , county\n",
      "True: that fire is # percent contained\n",
      "Pred: the fire was # percent contained\n",
      "True: in western wyoming , the <unk> fire remains out of control in the bridger teton national forest\n",
      "Pred: in western and , the area utility are out of control of the rio teton national forest\n",
      "True: that fire has burned about # square miles\n",
      "Pred: no fire has burned about # square feet\n",
      "True: fires also are burning in park , washakie and platte counties\n",
      "Pred: firefighters also been burning in <unk> , and and <unk>\n",
      "True: portland mayoral candidate jefferson smith was charged with assaulting a woman in a <unk> at a party when he was a # year old college student\n",
      "Pred: former judge candidate jesse kilpatrick is charged with assaulting a woman in the courtroom of a businessman that he won in # year old child student\n",
      "True: the misdemeanor charge was dropped when he paid the woman 's medical bills and did community service\n",
      "Pred: the misdemeanor charges was suspended after it dropped the woman 's name cards , also foster service .\n",
      "True: smith acknowledged the willamette week report monday\n",
      "Pred: he says the new last contract monday\n",
      "True: the paper 's website said witnesses recalled the woman was tipped off a couch and charged at smith in the mistaken belief he had done it\n",
      "Pred: the hospital 's mother told police told the woman was behind a a door with someone with gunpoint of a bathroom when that she made it\n",
      "True: one witness said smith tapped her on the forehead\n",
      "Pred: her 's said aguirre entered her during the woman\n",
      "True: smith said she was cut\n",
      "Pred: mayor said she was wrong\n",
      "True: the oregonian says smith ( http : <unk> ) called it `` the worst night of my life .\n",
      "Pred: the herald herald tribune ( http : <unk> ) said `` called the first factor of my life .\n",
      "True: smith 's campaign has been dogged by revelations such as his ejection from two sports leagues after <unk> up opponents and his seven driver 's license suspensions\n",
      "Pred: he 's campaign has been dogged by law that as an business in # # and and being <unk> with of his son 's 's son season\n",
      "True: massachusetts has one of the highest high school graduation rates in the united states , but low income students <unk> lag behind\n",
      "Pred: massachusetts is one of the highest in campuses students rates in the united states , including in schools and and other rates\n",
      "True: a report released on monday by the foundation america 's promise says the state ranked in the top tier of the united states with # percent of students graduating in #\n",
      "Pred: the report released tuesday thursday by the <unk> 's 's foundation , the organization ranked the the middle of of the united states in # in of students in in #\n",
      "True: only about two thirds of low income students received diplomas that year\n",
      "Pred: just about # thirds of millions income taxes have received this year\n",
      "True: hispanic and black students graduated at greater levels in massachusetts and nationally , but are still far behind white students\n",
      "Pred: both and students are for from least places in reading , older , but they still among among among adults\n",
      "True: two thirds of all students graduated boston schools\n",
      "Pred: two thirds of those students from english college\n",
      "True: state education officials say they have invested in professional development since # that focuses on the transition to high school for english language learners , cultural competency and serving special education students\n",
      "Pred: state education officials say they 're visiting in washington county , and and will on the list of # school graduation catholic , , , nursing and and nursing education education\n",
      "True: a similar disparity was reported in other states , particularly hawaii , indiana and texas\n",
      "Pred: a similar case has found in # states , including by , west and counties\n",
      "True: <unk> worldwide inc. said monday that it has amended a credit facility that will increase the amount the trucking company can borrow in the second quarter\n",
      "Pred: <unk> , inc. said monday that it has a the new card that not pay the amount of company that that share in the area quarter\n",
      "True: but the overland park , kan. , company said its liquidity is still being pressured by the high daily cost of running its business\n",
      "Pred: by the national park , and , company says its client information still not used by the end of of of <unk> the company\n",
      "True: but the news sent shares as high as # cents in early trading , but were up # cents at # cents in afternoon trading\n",
      "Pred: but the company 's shares to # # # cents per share , , , was down # cents to $ # in afternoon trading\n",
      "True: under the credit line 's old terms , the company had just $ # million more to borrow as of friday\n",
      "Pred: under the list payment on # , , the company had raised a # million to more it to of monday\n",
      "True: but the company did n't provide a new figure\n",
      "Pred: but the company did n't pay a new contract\n",
      "True: the company was thought to be within days of filing for bankruptcy protection in december , but has gained some steam since that time as the economy began to <unk>\n",
      "Pred: the company was supposed to be paid # of an for its protection in december , but it 's # jobs when it it the the building was to fall\n",
      "True: <unk> said it 's considering several possible steps to boost its liquidity including more cost cuts , more attempts to work with lenders to negotiate loan payments or the sale of assets or business lines\n",
      "Pred: <unk> said it 's important several steps to to eliminate the jobs , which people , , and money to offer with people to help up payments and the the of <unk> and other owners\n",
      "True: companies ordered more heavy machinery , computers and other long lasting manufactured goods in september , a positive sign for the sluggish economy\n",
      "Pred: they that more than goods , including and other stock term goods goods , , in a key for of the <unk> economy\n",
      "True: the commerce department says overall demand for durable goods fell # percent\n",
      "Pred: the dow department says overall demand for durable goods fell # percent\n",
      "True: but that was largely because of a # percent drop in volatile commercial aircraft orders\n",
      "Pred: but that have was rated of a # percent decline in new market beers stocks\n",
      "True: outside of transportation , orders rose # percent\n",
      "Pred: parts of employment , which # # percent\n",
      "True: and business investment in core capital goods rose # percent , the biggest increase in six months\n",
      "Pred: consumer consumer : in private of and rose # percent , a year earlier in # months\n",
      "True: the gain was a sign that companies are sticking with their investment plans despite slow growth and weak consumer spending\n",
      "Pred: the gains was a option that traders are higher into its economy 's , by and and financing economic spending\n",
      "True: a chinese trade delegation has traveled to helena to hear investment pitches and discuss exporting opportunities with representatives from # montana companies\n",
      "Pred: a chinese company center is scheduled to meet to focus on opportunities and for ways to business at businesses in in .\n",
      "True: gov . steve bullock and the montana department of commerce hosted the delegation of more than # chinese investors friday\n",
      "Pred: gov . steve beshear of the new economic of commerce is the <unk> of # than # chinese jets nationwide\n",
      "True: some of the participating montana businesses included <unk> energy , green venture enterprises , <unk> spirits and wide open west adventures\n",
      "Pred: most of the top cities cities , <unk> , , , and , , and and and restaurants range in virginia\n",
      "True: bullock led a delegation of montana business owners to china in september and said at the time discussions were underway to bring chinese investors to the state\n",
      "Pred: brown says the combination of american and to to in , september , <unk> at the offering that that made to attract investors to to the economy\n",
      "True: he says expanding trade relations with china is one of the state 's greatest opportunities to boost montana 's exporting economy and attract foreign direct investments to business projects in the state\n",
      "Pred: he says support medicaid use with india is among of the state 's most opportunities to recruit economic and economy country and boost more companies to to create jobs in the country\n",
      "True: bullock 's trip was the first trade mission by a montana governor in # years\n",
      "Pred: duke 's trip is the first president campaign in the national lottery in # years\n",
      "True: mayor dave bing says detroit has its first negotiated contract with police in three decades , a one year deal with freezing pay and cutting pension benefits\n",
      "Pred: mayor mike bing says city has more worst increase increase with in in # years , the # year contract with other of for other costs costs\n",
      "True: the agreement with the detroit police officers association reduces the amount that current officers will earn under their defined benefit pension plan\n",
      "Pred: the suit in the florida police association who gives the question of legal that could receive more an license health insurance funds\n",
      "True: it also sets up a defined contribution plan for officers hired in the future\n",
      "Pred: it also pays up an its legal to and officers who in the future\n",
      "True: officers will contribute # percent of their pay , and the city # percent\n",
      "Pred: participants have only # percent of their costs , and the revised # percent\n",
      "True: bing spokesman dan lijana says the deal announced tuesday and an earlier agreement with command officers could save detroit as much as $ # million over five years\n",
      "Pred: chief spokesman tom lijana says the event announced thursday that an e in that that says will save land as much as $ # million in five years\n",
      "True: last month , gov . rick snyder signed legislation making a government 's ability to pay the top factor for arbitrators to consider when settling labor disputes involving police officers and firefighters\n",
      "Pred: last week , gov . scott snyder is legislation requiring the state plan ability to pay the pay for for cuts to consider `` he of and and and and and retirees\n",
      "True: police in manchester , n.h. , say a man 's story that he was knocking on residents ' doors offering to shovel their driveways for cash turned suspicious when they discovered there were several warrants out for his arrest\n",
      "Pred: police in san , n.h. . said a man 's name was he was fired and and and injuries to to to into clothes , an on on , he were him were no problems for of his car\n",
      "True: police said they stopped a vehicle on wednesday to question the occupants , whom they believed were seen banging on doors\n",
      "Pred: police said they received a scene without to to see the officers , who they were found found inside on him\n",
      "True: a passenger gave a false name , but was ultimately identified as # year old timothy <unk> of manchester\n",
      "Pred: a man is a similar reporting , but was n't identified as # year old <unk> <unk> of <unk>\n",
      "True: police said there were warrants for his arrest stemming from a report of copper piping and tools taken from a construction site\n",
      "Pred: <unk> said there were planned for an violations after from a series of of metal and others due from the city store\n",
      "True: police said <unk> had been identified as selling tools from the construction site to a local pawn shop , resulting in warrants for receiving stolen property and theft , in addition to others\n",
      "Pred: police said they have had that that stolen of from the sale of for a drug drug shop , which on # for low stolen property and and and and other to others\n",
      "True: oregon gov . john kitzhaber has declared a state of emergency in a coastal county ravaged by last week 's tsunami\n",
      "Pred: vermont gov . andrew kitzhaber has ordered a state of emergency at a new coal plant that # year 's tornado\n",
      "True: kitzhaber 's office announced the emergency declaration for curry county on tuesday\n",
      "Pred: graham 's office says the order surgery for superintendent county on wednesday\n",
      "True: the tsunami that rolled in following an earthquake in japan severely damaged the commercial side of a harbor in brookings\n",
      "Pred: the spill that included down in an earthquake spill which , damaged the largest part of a <unk> in line\n",
      "True: there was also lesser damage to facilities for sport fishing and recreational boats\n",
      "Pred: there have also severe because to use for heating and or other cars\n",
      "True: kitzhaber and u.s. sen. jeff <unk> toured the harbor on saturday\n",
      "Pred: chairman and state president mark <unk> toured the bank on thursday\n",
      "True: want to learn how to fish\n",
      "Pred: visitors to bring how to .\n",
      "True: the oregon department of fish and wildlife has invited new anglers to take part in a family fishing event this saturday at <unk> pond\n",
      "Pred: the american department of fish and volunteers has donated more anglers to get part of the `` owned season on saturday at <unk> cemetery\n",
      "True: the agency says it costs nothing to attend and rods , <unk> and bait will be provided to those who need equipment\n",
      "Pred: the agency says it can spread for open , , , <unk> and residents will be able to prevent who need treatment\n",
      "True: volunteers will also be on hand to answer questions and assist inexperienced anglers\n",
      "Pred: volunteers will also be on on to be `` and operate them anglers\n",
      "True: the pond is located in a <unk> city park and the event is scheduled for # a.m. to # p.m to help new anglers have success , the pond will be stocked with more than # rainbow trout\n",
      "Pred: the monument is based near the park national 's 's a airport is set for # p.m. on # p.m. to help build missions can to to to the will be used to more than # aircraft\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-b57e2659076e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_recon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_kl_batch\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_recon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_kl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mlosses_train\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_recon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_kl_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if len(logs) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "while epoch < config.epochs:\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = get_feed_dict(batch)\n",
    "        if config.warmup > 0 and beta_eval < 1.0: sess.run(update_beta)\n",
    "\n",
    "        _, loss_batch, sent_loss_recon_batch, sent_loss_kl_batch = \\\n",
    "        sess.run([opt, loss, sent_loss_recon, sent_loss_kl], feed_dict = feed_dict)\n",
    "\n",
    "        losses_train += [[loss_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "\n",
    "        if ct%config.log_period==0:\n",
    "            time_dev = time.time()\n",
    "            loss_train, sent_loss_recon_train, sent_loss_kl_train = np.mean(losses_train, 0)\n",
    "            loss_dev = get_loss(sess, dev_batches)\n",
    "            \n",
    "            if loss_dev < loss_min:\n",
    "                loss_min = loss_dev\n",
    "                saver.save(sess, config.modelpath, global_step=epoch*10000+ct)\n",
    "\n",
    "            if config.warmup > 0: beta_eval = beta.eval(session=sess)\n",
    "\n",
    "            clear_output()\n",
    "            time_finish = time.time()\n",
    "            time_log = int(time_finish - time_start)\n",
    "            time_log_dev = int(time_finish - time_dev)\n",
    "            logs += [(time_log, time_log_dev, epoch, ct, loss_train, sent_loss_recon_train, sent_loss_kl_train, loss_dev, beta_eval)]\n",
    "            for log in logs:\n",
    "                print('%03d[s], %02d[s], Ep: %02d, Ct: %05d|TR LOSS: %.2f LM NLL: %.2f, KL: %.2f|DE LOSS: %.2f|BETA: %.6f' %  log)\n",
    "\n",
    "            print_sample(batch)\n",
    "\n",
    "            time_start = time.time()\n",
    "            \n",
    "    epoch += 1\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/rnn_vae/apnews-291000\n"
     ]
    }
   ],
   "source": [
    "ckpt = tf.train.get_checkpoint_state(config.modeldir)\n",
    "model_checkpoint_path = ckpt.all_model_checkpoint_paths[-1]\n",
    "saver.restore(sess, model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(sent_1, sent_2):\n",
    "    sent_tokens_1 = word_tokenize(sent_1)\n",
    "    sent_tokens_2 = word_tokenize(sent_2)\n",
    "    \n",
    "    sent_idxs_1 = [word_to_idx[token] if token in word_to_idx else config.UNK_IDX for token in sent_tokens_1]\n",
    "    sent_idxs_2 = [word_to_idx[token] if token in word_to_idx else config.UNK_IDX for token in sent_tokens_2]\n",
    "    feed_input_token_idxs_list = [sent_idxs_1, sent_idxs_2]\n",
    "    \n",
    "    feed_input_token_idxs = pad_sequences(feed_input_token_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_sent_l = np.array([len(sent_idxs) for sent_idxs in feed_input_token_idxs_list], np.int32)\n",
    "    feed_batch_l = len(feed_sent_l)\n",
    "    \n",
    "    feed_means_infer = sess.run(means_infer, \n",
    "                                                          feed_dict={t_variables['input_token_idxs']: feed_input_token_idxs, \n",
    "                                                                               t_variables['batch_l']: feed_batch_l, t_variables['sent_l']: feed_sent_l})\n",
    "    \n",
    "    n_inter = 10\n",
    "    feed_inter_means_infer = feed_means_infer[0][None, :] + (feed_means_infer[1] - feed_means_infer[0])[None, :] * (np.arange(n_inter+1).astype(np.float32)/n_inter)[:, None]\n",
    "    feed_inter_batch_l = len(feed_inter_means_infer)\n",
    "    \n",
    "    _inter_beam_output_token_idxs = sess.run(inter_beam_output_token_idxs, \n",
    "                                                                                  feed_dict={t_variables['batch_l']: feed_inter_batch_l,\n",
    "                                                                                                       t_variables['keep_prob']: 1.,\n",
    "                                                                                                       inter_means_infer: feed_inter_means_infer})\n",
    "    \n",
    "    output_sents = [' '.join([idx_to_word[idx] for idx in sent_idxs]) for sent_idxs in _inter_beam_output_token_idxs]\n",
    "    \n",
    "    return output_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['two men were in which died injured </p>',\n",
       " 'two men were in which died injured </p>',\n",
       " 'two men were in which died injured </p>',\n",
       " 'two men were in which died injured </p>',\n",
       " 'two men were in which died injured </p>',\n",
       " 'two men were in which died injured </p>',\n",
       " 'four others died and # were injured </p>',\n",
       " 'four others died and # were injured </p>',\n",
       " 'four others died and # were injured </p>',\n",
       " 'five others found two people died injured </p>',\n",
       " 'four others died and # were injured </p>']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_1 = \"two workers died and # were injured\"\n",
    "sent_2 = 'five workers died and # were injured'\n",
    "interpolate(sent_1, sent_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: maryland state police say a man who pointed a firearm at two cecil county sheriff 's deputies was shot by the officers\n",
      "Pred: delaware state police say a man who ran a knife in new york city police say he was hit by his investigation\n",
      "True: it happened thursday about # p.m. in <unk>\n",
      "Pred: it happened thursday about # p.m. in <unk>\n",
      "True: troopers say deputies tried to pull a driver over for a minor traffic violation\n",
      "Pred: troopers say investigators are investigating to a driver suffered a traffic for possible traffic\n",
      "True: police say the driver struck the deputies ' patrol vehicle and fled\n",
      "Pred: police say the driver struck the car police officer fired causing injuries\n",
      "True: the deputies chased the vehicle for about # miles , and it pulled into the driveway of his home in <unk> and ran inside\n",
      "Pred: the woman suffered the gun from about # p.m. and when they responded to the head in one of her home and crashed away\n",
      "True: deputies arrived at the home\n",
      "Pred: deputies arrived at the home\n",
      "True: troopers say the man came to the front door with a firearm pointed at the deputies\n",
      "Pred: troopers say the man went to the hospital with part of a car on his head\n",
      "True: two deputies fired and struck the man\n",
      "Pred: two deputies fired and struck the man\n",
      "True: he was taken to shock trauma in baltimore\n",
      "Pred: he was taken to harborview trauma in anchorage\n",
      "True: the richmond times dispatch is reporting that the number of homicides in and around the capital region jumped # percent in #\n",
      "Pred: the st. louis report reports that took the number of <unk> in <unk> and the <unk> area and nearly # percent in #\n",
      "True: citing its analysis of records , the paper reported monday that there were # homicides in the metro area last year\n",
      "Pred: but for public records , the <unk> , said in that was reported down between the <unk> area since june #\n",
      "True: that 's up from # in #\n",
      "Pred: that 's up from # in #\n",
      "True: homicide numbers had decreased in the region for at least six years\n",
      "Pred: <unk> workers were reported in the community for at least six years\n",
      "True: the paper 's numbers show homicides increased in richmond , petersburg and henrico and chesterfield counties\n",
      "Pred: the survey 's population included <unk> <unk> in <unk> , <unk> and <unk> and <unk> counties\n",
      "True: eighty homicides were recorded in those locations last year , compared with # in #\n",
      "Pred: overall <unk> were born in those deaths last year , compared with # in #\n",
      "True: jay albanese , a virginia commonwealth university criminal justice professor , says the increases may be just a <unk>\n",
      "Pred: jim <unk> , a former federal board of education director , says the numbers should be since a decade\n",
      "True: and he says the # figures represent a small increase\n",
      "Pred: and he says the # census is the other bills\n",
      "True: district of columbia mayor vincent gray wants to build a new , $ # million hospital to replace the city owned hospital east of the <unk> river\n",
      "Pred: ohio 's largest school president barack obama planned to a new two years on tuesday and a <unk> that is in the west virginia community center\n",
      "True: gray says united medical center , which the city took over in # , is `` a money pit for district taxpayers .\n",
      "Pred: spokeswoman with <unk> county officials in america says the money about # year old helped lead to the school commission for <unk> .\n",
      "True: the city has provided $ # million in subsidies to <unk> over the last decade\n",
      "Pred: the state has received $ # million in grants to <unk> over the last month\n",
      "True: the democratic mayor says building a new hospital would be more expensive in the short term but would have greater long term advantages than continuing to renovate the existing hospital\n",
      "Pred: the federal lottery said in a house that will be in place to the # , which he is among six years to have become an economic development in november\n",
      "True: his budget proposal for the upcoming fiscal year will include $ # million for the design of the new hospital\n",
      "Pred: an effort to pass the new york budget was among $ # million for the expansion of the school campus\n",
      "True: the $ # million for its construction would be funded in subsequent years\n",
      "Pred: the $ # million cost for city will be funded in may #\n",
      "True: gray is seeking re election in next week 's democratic primary\n",
      "Pred: <unk> is seeking re election in next week 's democratic primary\n",
      "True: two vermont electric utilities are warning customers about an ongoing bill payment phone scam that threatens customers with <unk> if they do not pay immediately\n",
      "Pred: two virginia state workers are warning tuesday to their federal rules on whether they were in water and that are n't able to do not\n",
      "True: the burlington electric department and green mountain power say burlington customers have received calls claiming to be from gmp\n",
      "Pred: the arkansas weather service , natural gas , whose power , will be <unk> that to go out in\n",
      "True: consumers got a fake toll free number to call , which was answered by a recording claiming to be at gmp\n",
      "Pred: hunters got a good number of being asked by investigators being found dead by a group to be <unk> by them\n",
      "True: the utilities say they follow state rules and do not demand credit card information or alternate payment mechanisms from a customer for any purpose\n",
      "Pred: the nonprofit also say that state health care have not required federal rules and remain in response to <unk> with the state 's #\n",
      "True: customers receiving such calls are urged not to provide personal or financial information\n",
      "Pred: details and further cuts are still trying to provide other issues against work\n",
      "True: they can call their utilities at the number on their electric bill\n",
      "Pred: they 'll discuss on how at the end for new power system\n",
      "True: and they can call the vermont attorney general 's office consumer assistance program at # . #\n",
      "Pred: and they will discuss the county general 's office and public records show in middle #\n",
      "True: the runner up in last year 's gubernatorial race says he will seek another term in the illinois senate from his newly redrawn district\n",
      "Pred: the race running in # years after u.s. sen. chris christie would be another seat with the republican republican party of a special race\n",
      "True: republican sen. bill brady said wednesday he will run in the <unk> # th district , which includes most of bloomington and newly added areas in tazewell , logan , menard and sangamon counties\n",
      "Pred: district mayor bill <unk> says he has been operating on the district # , # , as well as high school 's offices in recent states , including <unk> , <unk> and <unk> counties\n",
      "True: illinois ' legislative boundaries were drawn by democrats as part of the once per decade redistricting process\n",
      "Pred: republican high school voters say more votes , with congress with the # percent cut gop primary\n",
      "True: as a result , brady will no longer represent illinois state university , but he picks up the abraham lincoln capital airport and the fairgrounds in springfield\n",
      "Pred: as a <unk> , it could serve as an independent of <unk> , which he 's part of the school district 's largest in the country war\n",
      "True: in his announcement wednesday , brady outlined a campaign theme similar to his # bid for governor against democrat pat quinn\n",
      "Pred: on tuesday from congress have announced by a governor 's nomination for next to an election and campaign by governor .\n",
      "True: he said democrats still want to tax too much\n",
      "Pred: he said republicans are still to raise taxes money\n",
      "True: a democrat has not yet announced a run against brady\n",
      "Pred: a judge has not yet announced a resolution against <unk>\n",
      "True: a judge has dismissed a complaint accusing south carolina gov . nikki haley of breaking ethics laws while a member of the state house\n",
      "Pred: a judge has dismissed a lawsuit against former republican gov . john christie for federal government as <unk> in the state 's the public\n",
      "True: judge casey manning on wednesday dismissed john rainey 's lawsuit against the first term republican , saying court is not the place to hash out ethics issues\n",
      "Pred: judge william bryant on monday that <unk> court papers against the <unk> in august # , saying he would seek a candidate for him to proceed\n",
      "True: the lawsuit by former state board of economic advisors chairman john rainey centered on haley 's jobs as a fundraiser for lexington medical center and with an engineering firm with state contracts\n",
      "Pred: the letter by president of state university council chairman <unk> <unk> of indianapolis , is planning by a joint member of directors to promote new hampshire and its health and economic development\n",
      "True: it also asked whether it was illegal for haley to seek tens of thousands of dollars from lobbyists , while legislators were in session , for the hospital 's foundation\n",
      "Pred: he says it was asked for ways to have an opportunity with thousands of dollars for other events , including <unk> , which is in a public in exchange\n",
      "True: rainey , a longtime republican activist , did not immediately comment on the dismissal\n",
      "Pred: <unk> , a former republican democrat , did not immediately comment on the race\n",
      "True: haley spokesman rob godfrey called the suit a `` political stunt .\n",
      "Pred: justice spokeswoman dick brown said the death as the party met .\n",
      "True: a woodbury county supervisor pulled a name from a hat , breaking a tie in the election for mayor of cushing\n",
      "Pred: a former county judge gave a relationship with a <unk> woman and a member on the meeting of <unk> 's death\n",
      "True: the sioux city journal reports ( http : //bit.ly/ # <unk> ) supervisor david tripp on tuesday pulled the name of incumbent mayor gary merkel from a hat\n",
      "Pred: the kansas city journal reports ( http : //bit.ly/ # <unk> ) councilman robert <unk> on friday made the case of <unk> michael <unk> 's <unk> of her\n",
      "True: the selection broke a tie with challenger donald joy jr. and means merkel can serve another two year term as mayor of cushing\n",
      "Pred: the winner is a former campaign for <unk> , <unk> and says he 's charged with only two years by state prosecutors since <unk>\n",
      "True: the men had tied with each receiving # votes\n",
      "Pred: the men had worked with one sentence in may\n",
      "True: merkel has been mayor of the # person city since #\n",
      "Pred: johnson has been director on the # as since november #\n",
      "True: cushing is about # miles east of sioux city\n",
      "Pred: talkeetna is about # miles east of pittsburgh city\n",
      "True: <unk> electric co. of fort smith has agreed to pay $ # million to settle a discrimination allegation by the u.s. labor department , but the company says it did nothing wrong\n",
      "Pred: school officials in kansas county have agreed to have $ # million for an effort to the south dakota department of the city , but says he does n't the state police\n",
      "True: the labor department announced the settlement on monday and alleged the company blocked nearly # women and minorities from getting jobs\n",
      "Pred: the district government says the first time thursday because of the company received # percent of residents who pay in state\n",
      "True: <unk> spokeswoman tracy long says the company has made sure its screening process for applicants is in compliance with federal law but says there was no bias complaint from an individual\n",
      "Pred: <unk> <unk> told reporters that the company has made to leave federal rules for people that of his own <unk> , which is asking whether it 's back for #\n",
      "True: the agreement will have <unk> pay $ # million to the # affected applicants and offer jobs to at least # people who were denied employment\n",
      "Pred: the applicants will have spent on one time $ # a year and increase and pay them from # to # , according to <unk> .\n",
      "True: if divided equally , the settlement comes to about $ # per person\n",
      "Pred: if all police show the # would cost about $ # per hour\n",
      "True: the former pastor of a northwestern indiana megachurch awaiting sentencing for a sexual relationship with an underage parishioner allegedly told the girl jesus christ approved of their <unk>\n",
      "Pred: the former chief of a new york man accused of taking a gun in honor to his wife , which is the nation 's death and killing her\n",
      "True: federal prosecutors included letters former first baptist church pastor jack <unk> wrote to his victim in their sentencing memorandum filed wednesday in federal court in hammond\n",
      "Pred: federal prosecutors charged him of <unk> county circuit judge and <unk> `` to enter a `` guilty thursday afternoon and is scheduled in federal circuit court\n",
      "True: the post tribune reports ( http : <unk> ) the # year old <unk> wrote in the one letter to the # year old girl that their relationship `` is exactly what christ <unk> for us .\n",
      "Pred: the news tribune reports ( http : <unk> # ) the # year old <unk> met the <unk> of a month ago after her husband 's `` <unk> `` about `` no `` <unk> `` <unk> .\n",
      "True: <unk> was fired by the # member hammond church in july and later pleaded guilty to taking a minor across state lines with intent to engage in criminal sexual activity\n",
      "Pred: <unk> was convicted in the county jail , in april and three pleaded guilty to be on the theft of child pornography to distribute cocaine and attempted to bank\n",
      "True: he 's scheduled to be sentenced march # and is seeking the minimum # year sentence under his conviction\n",
      "Pred: he 's scheduled to be sentenced next year and ordered to the jury in # days after his conviction\n",
      "True: the president of the national firefighters union is among hundreds of firefighters and supporters who plan to rally on the steps of buffalo city hall\n",
      "Pred: the president of the detroit association says those working for all <unk> and officials are planning to be in the city at <unk> 's park\n",
      "True: harold <unk> , who heads the international association of firefighters , will address the crowd as it gathers thursday morning to recognize firefighters ' service and sacrifice\n",
      "Pred: <unk> <unk> , who owns the university of <unk> , who will discuss the company that about an agreement that are in `` <unk> `` <unk> <unk> .\n",
      "True: organizers say buffalo firefighters face particular hazards because of the city 's large number of abandoned and crumbling houses , which are favorite targets for <unk>\n",
      "Pred: authorities say <unk> <unk> are using <unk> , a group of an <unk> of <unk> in <unk> , but <unk> are <unk> , or other plans\n"
     ]
    }
   ],
   "source": [
    "print_beam_sample(test_batches[10][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
