{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from data_structure import get_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "from topic_model import TopicModel\n",
    "\n",
    "from topic_beam_search_decoder import BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '1', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/apnews/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/rnn_vae', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'apnews', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 1000, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 500, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.1, 'lr')\n",
    "flags.DEFINE_float('reg', 0.1, 'regularization term')\n",
    "flags.DEFINE_float('beta', 0.001, 'initial value of beta')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_integer('warmup', 5000, 'warmup period for KL')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 10, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_topic', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('PAD_IDX', word_to_idx[PAD], 'PAD_IDX')\n",
    "flags.DEFINE_integer('UNK_IDX', word_to_idx[UNK], 'UNK_IDX')\n",
    "flags.DEFINE_integer('BOS_IDX', word_to_idx[BOS], 'BOS_IDX')\n",
    "flags.DEFINE_integer('EOS_IDX', word_to_idx[EOS], 'EOS_IDX')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "maximum_iterations = max([max([instance.max_sent_l for instance in batch]) for ct, batch in dev_batches])\n",
    "flags.DEFINE_integer('maximum_iterations', maximum_iterations, 'maximum_iterations')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build language model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['bow'] = tf.placeholder(tf.float32, [None, config.dim_bow], name='bow')\n",
    "t_variables['input_token_idxs'] = tf.placeholder(tf.int32, [None, None], name='input_token_idxs')\n",
    "t_variables['dec_input_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_input_idxs')\n",
    "t_variables['dec_target_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_target_idxs')\n",
    "t_variables['batch_l'] = tf.placeholder(tf.int32, name='batch_l')\n",
    "t_variables['doc_l'] = tf.placeholder(tf.int32, [None], name='doc_l')\n",
    "t_variables['sent_l'] = tf.placeholder(tf.int32, [None], name='sent_l')\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train', assertion=False):\n",
    "    def token_dropout(sent_idxs):\n",
    "        sent_idxs_dropout = np.asarray(sent_idxs)\n",
    "        sent_idxs_dropout[np.random.rand(len(sent_idxs)) > config.word_keep_prob] = config.UNK_IDX\n",
    "        return list(sent_idxs_dropout)\n",
    "\n",
    "    bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "    \n",
    "    doc_l = np.array([len(instance.token_idxs) for instance in batch])\n",
    "    \n",
    "    feed_input_token_idxs_list = [sent_idxs for instance in batch for sent_idxs in instance.token_idxs]\n",
    "    feed_dec_input_idxs_list = [[config.BOS_IDX] + token_dropout(sent_idxs) for sent_idxs in feed_input_token_idxs_list]\n",
    "    feed_dec_target_idxs_list = [sent_idxs + [config.EOS_IDX]  for sent_idxs in feed_input_token_idxs_list]\n",
    "        \n",
    "    sent_l = np.array([len(sent_idxs) for sent_idxs in feed_input_token_idxs_list], np.int32)\n",
    "    batch_l = len(sent_l)\n",
    "    \n",
    "    feed_input_token_idxs = pad_sequences(feed_input_token_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_input_idxs = pad_sequences(feed_dec_input_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_target_idxs = pad_sequences(feed_dec_target_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    \n",
    "    if assertion:\n",
    "        index = 0\n",
    "        for instance in batch:\n",
    "            for line_idxs in instance.token_idxs:\n",
    "                assert feed_input_token_idxs_list[index] == line_idxs\n",
    "                index += 1\n",
    "        assert feed_input_token_idxs.shape[1] == np.max(sent_l)\n",
    "        assert feed_dec_input_idxs.shape[1] == np.max(sent_l) + 1\n",
    "        assert feed_dec_target_idxs.shape[1] == np.max(sent_l) + 1\n",
    "    \n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "\n",
    "    feed_dict = {\n",
    "                t_variables['bow']: bow, \n",
    "                t_variables['batch_l']: batch_l, t_variables['doc_l']: doc_l, t_variables['sent_l']: sent_l, \n",
    "                t_variables['input_token_idxs']: feed_input_token_idxs, t_variables['dec_input_idxs']: feed_dec_input_idxs, t_variables['dec_target_idxs']: feed_dec_target_idxs, \n",
    "                t_variables['keep_prob']: keep_prob\n",
    "    }\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables, sess_init=False):\n",
    "#     if sess_init:\n",
    "#         sess = tf.Session()\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    if sess_init: sess.close()\n",
    "\n",
    "def debug_value(variables, return_value=False, sess_init=None):\n",
    "#     if sess_init:\n",
    "#         sess = tf.Session()\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "    if sess_init: sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "input_token_idxs = t_variables['input_token_idxs']\n",
    "batch_l = t_variables['batch_l']\n",
    "sent_l = t_variables['sent_l']\n",
    "max_sent_l = tf.reduce_max(sent_l)\n",
    "\n",
    "with tf.variable_scope('shared', reuse=False):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "with tf.variable_scope('sent/enc', reuse=False):\n",
    "    # get word embedding\n",
    "    enc_input = tf.nn.embedding_lookup(embeddings, input_token_idxs)\n",
    "\n",
    "    # get sentence embedding\n",
    "    _, enc_state = dynamic_rnn(enc_input, sent_l, config.dim_hidden, t_variables['keep_prob'])\n",
    "\n",
    "    means_infer = tf.keras.layers.Dense(units=config.dim_latent, name='mean')(enc_state)\n",
    "    logvars_infer = tf.keras.layers.Dense(units=config.dim_latent, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar')(enc_state)\n",
    "\n",
    "    # latent vector from gaussian mixture    \n",
    "    latents_input = sample_latents(means_infer, logvars_infer) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for decoding\n",
    "dec_sent_l = tf.add(sent_l, 1)\n",
    "dec_input_idxs = t_variables['dec_input_idxs']\n",
    "dec_input = tf.nn.embedding_lookup(embeddings, dec_input_idxs)\n",
    "\n",
    "dec_latents_input = tf.tile(tf.expand_dims(latents_input, 1), [1, tf.shape(dec_input)[1], 1])\n",
    "dec_concat_input = tf.concat([dec_input, dec_latents_input], -1)\n",
    "\n",
    "# decode for training\n",
    "with tf.variable_scope('sent/dec/rnn', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32, reuse=False):\n",
    "    dec_cell = tf.contrib.rnn.GRUCell(config.dim_hidden)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob = t_variables['keep_prob'])\n",
    "\n",
    "    dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(latents_input)\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_concat_input, sequence_length=dec_sent_l)\n",
    "\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=dec_cell,\n",
    "        helper=helper,\n",
    "        initial_state=dec_initial_state)\n",
    "\n",
    "    dec_outputs, _, output_sent_l = tf.contrib.seq2seq.dynamic_decode(train_decoder)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(config.n_vocab, use_bias=False, name='out')\n",
    "    output_logits = output_layer(dec_outputs.rnn_output)\n",
    "    \n",
    "    output_token_idxs = tf.argmax(output_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tokens = tf.fill([batch_l], config.BOS_IDX)\n",
    "end_token = config.EOS_IDX\n",
    "\n",
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    infer_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_infer)\n",
    "    beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(infer_dec_initial_state, multiplier=config.beam_width)\n",
    "    beam_latents_input = tf.contrib.seq2seq.tile_batch(means_infer, multiplier=config.beam_width) # added\n",
    "    \n",
    "    beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=beam_latents_input)\n",
    "\n",
    "    beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    beam_output_token_idxs = beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    inter_means_infer = tf.placeholder(tf.float32, [None, config.dim_latent])\n",
    "    \n",
    "    inter_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(inter_means_infer)\n",
    "    inter_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(inter_dec_initial_state, multiplier=config.beam_width)\n",
    "    inter_beam_latents_input = tf.contrib.seq2seq.tile_batch(inter_means_infer, multiplier=config.beam_width) # added\n",
    "    \n",
    "    inter_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=inter_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=inter_beam_latents_input)\n",
    "\n",
    "    inter_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inter_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    inter_beam_output_token_idxs = inter_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language modeling cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target and mask\n",
    "dec_target_idxs = t_variables['dec_target_idxs']\n",
    "dec_mask_tokens = tf.sequence_mask(dec_sent_l, maxlen=max_sent_l+1, dtype=tf.float32)\n",
    "\n",
    "# nll for each token (averaged over batch & sentence)\n",
    "sent_loss_recon = tf.contrib.seq2seq.sequence_loss(output_logits, dec_target_idxs, dec_mask_tokens)\n",
    "\n",
    "sent_loss_kl = compute_kl_loss(means_infer, logvars_infer) # KL divergence b/w latent dist & gaussian std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = tf.Variable(config.beta, name='beta', trainable=False) if config.warmup > 0 else tf.constant(1., name='beta')\n",
    "update_beta = tf.assign_add(beta, 1./(config.warmup*len(train_batches)))\n",
    "loss = sent_loss_recon + beta * sent_loss_kl\n",
    "\n",
    "# define optimizer\n",
    "if config.opt == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif config.opt == 'Adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "    \n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_sents(token_idxs, config, idx_to_word):\n",
    "    sents = []\n",
    "    for sent_idxs in token_idxs:\n",
    "        tokens = []\n",
    "        for idx in sent_idxs:\n",
    "            if idx == config.EOS_IDX: break\n",
    "            tokens.append(idx_to_word[idx])\n",
    "        sent = ' '.join(tokens)\n",
    "        sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch = sess.run(loss, feed_dict = feed_dict)\n",
    "        losses.append(loss_batch)\n",
    "    loss_mean = np.mean(losses)\n",
    "    return loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_batch):\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    pred_token_idxs = sess.run(output_token_idxs, feed_dict = feed_dict)\n",
    "    true_token_idxs = [sent_idxs for instance in sample_batch for sent_idxs in instance.token_idxs]\n",
    "    \n",
    "    assert len(pred_token_idxs) == len(true_token_idxs)\n",
    "    \n",
    "    pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "    true_sents = idxs_to_sents(true_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    for true_sent, pred_sent in zip(true_sents, pred_sents):        \n",
    "        print('True: %s' % true_sent)\n",
    "        print('Pred: %s' % pred_sent)\n",
    "        \n",
    "def print_beam_sample(sample_batch):\n",
    "    feed_dict = get_feed_dict(sample_batch, mode='test')\n",
    "    pred_token_idxs = sess.run(beam_output_token_idxs, feed_dict = feed_dict)\n",
    "    true_token_idxs = [sent_idxs for instance in sample_batch for sent_idxs in instance.token_idxs]\n",
    "    \n",
    "    assert len(pred_token_idxs) == len(true_token_idxs)\n",
    "    \n",
    "    pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "    true_sents = idxs_to_sents(true_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    for true_sent, pred_sent in zip(true_sents, pred_sents):        \n",
    "        print('True: %s' % true_sent)\n",
    "        print('Pred: %s' % pred_sent)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "logs = []\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0\n",
    "saver = tf.train.Saver(max_to_keep=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "028[s], 26[s], Ep: 00, Ct: 00000|TR LOSS: 10.34 LM NLL: 10.34, KL: 0.00|DE LOSS: 10.34|BETA: 0.001000\n",
      "093[s], 26[s], Ep: 00, Ct: 00500|TR LOSS: 7.39 LM NLL: 7.37, KL: 22.99|DE LOSS: 6.88|BETA: 0.001035\n",
      "093[s], 26[s], Ep: 00, Ct: 01000|TR LOSS: 7.13 LM NLL: 7.11, KL: 21.87|DE LOSS: 6.73|BETA: 0.001069\n",
      "093[s], 26[s], Ep: 00, Ct: 01500|TR LOSS: 7.02 LM NLL: 7.00, KL: 21.06|DE LOSS: 6.69|BETA: 0.001104\n",
      "094[s], 26[s], Ep: 00, Ct: 02000|TR LOSS: 6.96 LM NLL: 6.94, KL: 20.39|DE LOSS: 6.67|BETA: 0.001139\n",
      "094[s], 26[s], Ep: 00, Ct: 02500|TR LOSS: 6.91 LM NLL: 6.89, KL: 19.81|DE LOSS: 6.65|BETA: 0.001173\n",
      "060[s], 07[s], Ep: 01, Ct: 00000|TR LOSS: 6.89 LM NLL: 6.87, KL: 19.38|DE LOSS: 6.73|BETA: 0.001200\n",
      "093[s], 26[s], Ep: 01, Ct: 00500|TR LOSS: 6.86 LM NLL: 6.84, KL: 18.80|DE LOSS: 6.59|BETA: 0.001235\n",
      "094[s], 26[s], Ep: 01, Ct: 01000|TR LOSS: 6.83 LM NLL: 6.81, KL: 18.29|DE LOSS: 6.51|BETA: 0.001269\n",
      "094[s], 26[s], Ep: 01, Ct: 01500|TR LOSS: 6.80 LM NLL: 6.78, KL: 17.88|DE LOSS: 6.41|BETA: 0.001304\n",
      "094[s], 26[s], Ep: 01, Ct: 02000|TR LOSS: 6.76 LM NLL: 6.74, KL: 17.73|DE LOSS: 6.37|BETA: 0.001339\n",
      "094[s], 26[s], Ep: 01, Ct: 02500|TR LOSS: 6.73 LM NLL: 6.71, KL: 17.79|DE LOSS: 6.25|BETA: 0.001374\n",
      "078[s], 26[s], Ep: 02, Ct: 00000|TR LOSS: 6.70 LM NLL: 6.68, KL: 18.00|DE LOSS: 6.20|BETA: 0.001400\n",
      "091[s], 25[s], Ep: 02, Ct: 00500|TR LOSS: 6.66 LM NLL: 6.64, KL: 18.45|DE LOSS: 6.09|BETA: 0.001435\n",
      "093[s], 26[s], Ep: 02, Ct: 01000|TR LOSS: 6.62 LM NLL: 6.60, KL: 19.08|DE LOSS: 6.01|BETA: 0.001470\n",
      "093[s], 26[s], Ep: 02, Ct: 01500|TR LOSS: 6.58 LM NLL: 6.56, KL: 19.80|DE LOSS: 5.91|BETA: 0.001504\n",
      "093[s], 26[s], Ep: 02, Ct: 02000|TR LOSS: 6.54 LM NLL: 6.52, KL: 20.59|DE LOSS: 5.87|BETA: 0.001539\n",
      "094[s], 27[s], Ep: 02, Ct: 02500|TR LOSS: 6.50 LM NLL: 6.47, KL: 21.42|DE LOSS: 5.78|BETA: 0.001574\n",
      "078[s], 26[s], Ep: 03, Ct: 00000|TR LOSS: 6.47 LM NLL: 6.44, KL: 22.07|DE LOSS: 5.77|BETA: 0.001600\n",
      "093[s], 26[s], Ep: 03, Ct: 00500|TR LOSS: 6.43 LM NLL: 6.40, KL: 22.95|DE LOSS: 5.60|BETA: 0.001635\n",
      "094[s], 26[s], Ep: 03, Ct: 01000|TR LOSS: 6.39 LM NLL: 6.36, KL: 23.85|DE LOSS: 5.49|BETA: 0.001670\n",
      "075[s], 07[s], Ep: 03, Ct: 01500|TR LOSS: 6.35 LM NLL: 6.32, KL: 24.77|DE LOSS: 5.50|BETA: 0.001704\n",
      "094[s], 26[s], Ep: 03, Ct: 02000|TR LOSS: 6.31 LM NLL: 6.27, KL: 25.69|DE LOSS: 5.34|BETA: 0.001739\n",
      "094[s], 26[s], Ep: 03, Ct: 02500|TR LOSS: 6.27 LM NLL: 6.23, KL: 26.59|DE LOSS: 5.29|BETA: 0.001774\n",
      "078[s], 26[s], Ep: 04, Ct: 00000|TR LOSS: 6.24 LM NLL: 6.20, KL: 27.30|DE LOSS: 5.20|BETA: 0.001800\n",
      "075[s], 07[s], Ep: 04, Ct: 00500|TR LOSS: 6.20 LM NLL: 6.16, KL: 28.22|DE LOSS: 5.21|BETA: 0.001835\n",
      "094[s], 26[s], Ep: 04, Ct: 01000|TR LOSS: 6.16 LM NLL: 6.12, KL: 29.12|DE LOSS: 5.16|BETA: 0.001870\n",
      "094[s], 26[s], Ep: 04, Ct: 01500|TR LOSS: 6.12 LM NLL: 6.08, KL: 30.04|DE LOSS: 4.99|BETA: 0.001904\n",
      "075[s], 08[s], Ep: 04, Ct: 02000|TR LOSS: 6.09 LM NLL: 6.04, KL: 30.96|DE LOSS: 5.00|BETA: 0.001939\n",
      "090[s], 26[s], Ep: 04, Ct: 02500|TR LOSS: 6.05 LM NLL: 6.00, KL: 31.87|DE LOSS: 4.85|BETA: 0.001974\n",
      "056[s], 07[s], Ep: 05, Ct: 00000|TR LOSS: 6.02 LM NLL: 5.97, KL: 32.56|DE LOSS: 4.91|BETA: 0.002001\n",
      "088[s], 25[s], Ep: 05, Ct: 00500|TR LOSS: 5.98 LM NLL: 5.93, KL: 33.45|DE LOSS: 4.77|BETA: 0.002035\n",
      "087[s], 25[s], Ep: 05, Ct: 01000|TR LOSS: 5.95 LM NLL: 5.89, KL: 34.33|DE LOSS: 4.76|BETA: 0.002070\n",
      "090[s], 25[s], Ep: 05, Ct: 01500|TR LOSS: 5.91 LM NLL: 5.85, KL: 35.19|DE LOSS: 4.64|BETA: 0.002105\n",
      "087[s], 25[s], Ep: 05, Ct: 02000|TR LOSS: 5.87 LM NLL: 5.81, KL: 36.03|DE LOSS: 4.60|BETA: 0.002139\n",
      "087[s], 25[s], Ep: 05, Ct: 02500|TR LOSS: 5.84 LM NLL: 5.78, KL: 36.86|DE LOSS: 4.55|BETA: 0.002174\n",
      "073[s], 25[s], Ep: 06, Ct: 00000|TR LOSS: 5.81 LM NLL: 5.75, KL: 37.48|DE LOSS: 4.51|BETA: 0.002201\n",
      "087[s], 25[s], Ep: 06, Ct: 00500|TR LOSS: 5.78 LM NLL: 5.71, KL: 38.28|DE LOSS: 4.48|BETA: 0.002235\n",
      "068[s], 07[s], Ep: 06, Ct: 01000|TR LOSS: 5.75 LM NLL: 5.68, KL: 39.05|DE LOSS: 4.56|BETA: 0.002270\n",
      "087[s], 25[s], Ep: 06, Ct: 01500|TR LOSS: 5.71 LM NLL: 5.64, KL: 39.82|DE LOSS: 4.39|BETA: 0.002305\n",
      "088[s], 26[s], Ep: 06, Ct: 02000|TR LOSS: 5.68 LM NLL: 5.61, KL: 40.58|DE LOSS: 4.39|BETA: 0.002339\n",
      "087[s], 25[s], Ep: 06, Ct: 02500|TR LOSS: 5.65 LM NLL: 5.57, KL: 41.32|DE LOSS: 4.30|BETA: 0.002374\n",
      "074[s], 27[s], Ep: 07, Ct: 00000|TR LOSS: 5.63 LM NLL: 5.55, KL: 41.87|DE LOSS: 4.27|BETA: 0.002401\n",
      "088[s], 25[s], Ep: 07, Ct: 00500|TR LOSS: 5.60 LM NLL: 5.52, KL: 42.58|DE LOSS: 4.22|BETA: 0.002435\n",
      "090[s], 26[s], Ep: 07, Ct: 01000|TR LOSS: 5.57 LM NLL: 5.48, KL: 43.25|DE LOSS: 4.19|BETA: 0.002470\n",
      "091[s], 26[s], Ep: 07, Ct: 01500|TR LOSS: 5.54 LM NLL: 5.45, KL: 43.91|DE LOSS: 4.15|BETA: 0.002505\n",
      "072[s], 07[s], Ep: 07, Ct: 02000|TR LOSS: 5.51 LM NLL: 5.42, KL: 44.55|DE LOSS: 4.30|BETA: 0.002539\n",
      "072[s], 07[s], Ep: 07, Ct: 02500|TR LOSS: 5.48 LM NLL: 5.39, KL: 45.17|DE LOSS: 4.20|BETA: 0.002574\n",
      "076[s], 26[s], Ep: 08, Ct: 00000|TR LOSS: 5.46 LM NLL: 5.37, KL: 45.62|DE LOSS: 4.11|BETA: 0.002601\n",
      "091[s], 26[s], Ep: 08, Ct: 00500|TR LOSS: 5.43 LM NLL: 5.34, KL: 46.20|DE LOSS: 4.08|BETA: 0.002636\n",
      "090[s], 25[s], Ep: 08, Ct: 01000|TR LOSS: 5.41 LM NLL: 5.31, KL: 46.76|DE LOSS: 4.04|BETA: 0.002670\n",
      "072[s], 07[s], Ep: 08, Ct: 01500|TR LOSS: 5.38 LM NLL: 5.28, KL: 47.29|DE LOSS: 4.11|BETA: 0.002705\n",
      "072[s], 07[s], Ep: 08, Ct: 02000|TR LOSS: 5.36 LM NLL: 5.26, KL: 47.81|DE LOSS: 4.13|BETA: 0.002740\n",
      "090[s], 25[s], Ep: 08, Ct: 02500|TR LOSS: 5.33 LM NLL: 5.23, KL: 48.31|DE LOSS: 3.98|BETA: 0.002774\n",
      "074[s], 25[s], Ep: 09, Ct: 00000|TR LOSS: 5.31 LM NLL: 5.21, KL: 48.69|DE LOSS: 3.97|BETA: 0.002801\n",
      "087[s], 25[s], Ep: 09, Ct: 00500|TR LOSS: 5.29 LM NLL: 5.19, KL: 49.16|DE LOSS: 3.97|BETA: 0.002836\n",
      "087[s], 25[s], Ep: 09, Ct: 01000|TR LOSS: 5.27 LM NLL: 5.16, KL: 49.61|DE LOSS: 3.95|BETA: 0.002870\n",
      "087[s], 25[s], Ep: 09, Ct: 01500|TR LOSS: 5.25 LM NLL: 5.14, KL: 50.06|DE LOSS: 3.92|BETA: 0.002905\n",
      "068[s], 07[s], Ep: 09, Ct: 02000|TR LOSS: 5.22 LM NLL: 5.11, KL: 50.49|DE LOSS: 3.94|BETA: 0.002940\n",
      "069[s], 06[s], Ep: 09, Ct: 02500|TR LOSS: 5.20 LM NLL: 5.09, KL: 50.90|DE LOSS: 3.95|BETA: 0.002974\n",
      "054[s], 07[s], Ep: 10, Ct: 00000|TR LOSS: 5.19 LM NLL: 5.07, KL: 51.21|DE LOSS: 4.44|BETA: 0.003001\n",
      "087[s], 25[s], Ep: 10, Ct: 00500|TR LOSS: 5.17 LM NLL: 5.05, KL: 51.60|DE LOSS: 3.87|BETA: 0.003036\n",
      "069[s], 07[s], Ep: 10, Ct: 01000|TR LOSS: 5.15 LM NLL: 5.03, KL: 51.98|DE LOSS: 3.91|BETA: 0.003070\n",
      "068[s], 07[s], Ep: 10, Ct: 01500|TR LOSS: 5.13 LM NLL: 5.01, KL: 52.35|DE LOSS: 3.88|BETA: 0.003105\n",
      "068[s], 07[s], Ep: 10, Ct: 02000|TR LOSS: 5.11 LM NLL: 4.99, KL: 52.71|DE LOSS: 3.93|BETA: 0.003140\n",
      "088[s], 25[s], Ep: 10, Ct: 02500|TR LOSS: 5.09 LM NLL: 4.96, KL: 53.05|DE LOSS: 3.82|BETA: 0.003174\n",
      "084[s], 36[s], Ep: 11, Ct: 00000|TR LOSS: 5.07 LM NLL: 4.95, KL: 53.31|DE LOSS: 3.82|BETA: 0.003201\n",
      "068[s], 07[s], Ep: 11, Ct: 00500|TR LOSS: 5.06 LM NLL: 4.93, KL: 53.64|DE LOSS: 3.91|BETA: 0.003236\n",
      "103[s], 41[s], Ep: 11, Ct: 01000|TR LOSS: 5.04 LM NLL: 4.91, KL: 53.96|DE LOSS: 3.79|BETA: 0.003271\n",
      "068[s], 06[s], Ep: 11, Ct: 01500|TR LOSS: 5.02 LM NLL: 4.89, KL: 54.27|DE LOSS: 3.83|BETA: 0.003305\n",
      "087[s], 25[s], Ep: 11, Ct: 02000|TR LOSS: 5.00 LM NLL: 4.87, KL: 54.57|DE LOSS: 3.78|BETA: 0.003340\n",
      "087[s], 25[s], Ep: 11, Ct: 02500|TR LOSS: 4.99 LM NLL: 4.85, KL: 54.86|DE LOSS: 3.78|BETA: 0.003375\n",
      "073[s], 25[s], Ep: 12, Ct: 00000|TR LOSS: 4.97 LM NLL: 4.84, KL: 55.08|DE LOSS: 3.76|BETA: 0.003401\n",
      "071[s], 07[s], Ep: 12, Ct: 00500|TR LOSS: 4.96 LM NLL: 4.82, KL: 55.35|DE LOSS: 3.81|BETA: 0.003436\n",
      "087[s], 25[s], Ep: 12, Ct: 01000|TR LOSS: 4.94 LM NLL: 4.80, KL: 55.62|DE LOSS: 3.75|BETA: 0.003471\n",
      "070[s], 07[s], Ep: 12, Ct: 01500|TR LOSS: 4.93 LM NLL: 4.79, KL: 55.88|DE LOSS: 3.79|BETA: 0.003505\n",
      "089[s], 26[s], Ep: 12, Ct: 02000|TR LOSS: 4.91 LM NLL: 4.77, KL: 56.14|DE LOSS: 3.74|BETA: 0.003540\n",
      "091[s], 27[s], Ep: 12, Ct: 02500|TR LOSS: 4.90 LM NLL: 4.75, KL: 56.39|DE LOSS: 3.73|BETA: 0.003575\n",
      "060[s], 07[s], Ep: 13, Ct: 00000|TR LOSS: 4.89 LM NLL: 4.74, KL: 56.57|DE LOSS: 3.74|BETA: 0.003601\n",
      "093[s], 26[s], Ep: 13, Ct: 00500|TR LOSS: 4.87 LM NLL: 4.72, KL: 56.81|DE LOSS: 3.73|BETA: 0.003636\n",
      "075[s], 07[s], Ep: 13, Ct: 01000|TR LOSS: 4.86 LM NLL: 4.71, KL: 57.04|DE LOSS: 3.85|BETA: 0.003671\n",
      "093[s], 26[s], Ep: 13, Ct: 01500|TR LOSS: 4.84 LM NLL: 4.69, KL: 57.27|DE LOSS: 3.71|BETA: 0.003705\n",
      "075[s], 07[s], Ep: 13, Ct: 02000|TR LOSS: 4.83 LM NLL: 4.68, KL: 57.48|DE LOSS: 3.72|BETA: 0.003740\n",
      "093[s], 26[s], Ep: 13, Ct: 02500|TR LOSS: 4.82 LM NLL: 4.66, KL: 57.70|DE LOSS: 3.69|BETA: 0.003775\n",
      "079[s], 26[s], Ep: 14, Ct: 00000|TR LOSS: 4.81 LM NLL: 4.65, KL: 57.86|DE LOSS: 3.67|BETA: 0.003801\n",
      "075[s], 07[s], Ep: 14, Ct: 00500|TR LOSS: 4.79 LM NLL: 4.64, KL: 58.06|DE LOSS: 3.68|BETA: 0.003836\n",
      "075[s], 08[s], Ep: 14, Ct: 01000|TR LOSS: 4.78 LM NLL: 4.62, KL: 58.26|DE LOSS: 3.71|BETA: 0.003871\n",
      "094[s], 26[s], Ep: 14, Ct: 01500|TR LOSS: 4.77 LM NLL: 4.61, KL: 58.45|DE LOSS: 3.66|BETA: 0.003906\n",
      "093[s], 26[s], Ep: 14, Ct: 02000|TR LOSS: 4.75 LM NLL: 4.59, KL: 58.64|DE LOSS: 3.66|BETA: 0.003940\n",
      "093[s], 26[s], Ep: 14, Ct: 02500|TR LOSS: 4.74 LM NLL: 4.58, KL: 58.83|DE LOSS: 3.65|BETA: 0.003975\n",
      "059[s], 07[s], Ep: 15, Ct: 00000|TR LOSS: 4.73 LM NLL: 4.57, KL: 58.97|DE LOSS: 3.70|BETA: 0.004002\n",
      "075[s], 07[s], Ep: 15, Ct: 00500|TR LOSS: 4.72 LM NLL: 4.56, KL: 59.14|DE LOSS: 3.69|BETA: 0.004036\n",
      "075[s], 07[s], Ep: 15, Ct: 01000|TR LOSS: 4.71 LM NLL: 4.54, KL: 59.31|DE LOSS: 3.68|BETA: 0.004071\n",
      "094[s], 26[s], Ep: 15, Ct: 01500|TR LOSS: 4.70 LM NLL: 4.53, KL: 59.48|DE LOSS: 3.64|BETA: 0.004106\n",
      "074[s], 07[s], Ep: 15, Ct: 02000|TR LOSS: 4.69 LM NLL: 4.52, KL: 59.65|DE LOSS: 3.65|BETA: 0.004140\n",
      "093[s], 26[s], Ep: 15, Ct: 02500|TR LOSS: 4.68 LM NLL: 4.51, KL: 59.81|DE LOSS: 3.62|BETA: 0.004175\n",
      "059[s], 08[s], Ep: 16, Ct: 00000|TR LOSS: 4.67 LM NLL: 4.50, KL: 59.93|DE LOSS: 3.63|BETA: 0.004202\n",
      "075[s], 07[s], Ep: 16, Ct: 00500|TR LOSS: 4.66 LM NLL: 4.48, KL: 60.08|DE LOSS: 3.63|BETA: 0.004236\n",
      "072[s], 07[s], Ep: 16, Ct: 01000|TR LOSS: 4.65 LM NLL: 4.47, KL: 60.23|DE LOSS: 3.79|BETA: 0.004271\n",
      "076[s], 07[s], Ep: 16, Ct: 01500|TR LOSS: 4.64 LM NLL: 4.46, KL: 60.38|DE LOSS: 3.63|BETA: 0.004306\n",
      "075[s], 07[s], Ep: 16, Ct: 02000|TR LOSS: 4.63 LM NLL: 4.45, KL: 60.52|DE LOSS: 3.69|BETA: 0.004340\n",
      "094[s], 26[s], Ep: 16, Ct: 02500|TR LOSS: 4.62 LM NLL: 4.44, KL: 60.66|DE LOSS: 3.61|BETA: 0.004375\n",
      "059[s], 07[s], Ep: 17, Ct: 00000|TR LOSS: 4.61 LM NLL: 4.43, KL: 60.77|DE LOSS: 3.64|BETA: 0.004402\n",
      "092[s], 26[s], Ep: 17, Ct: 00500|TR LOSS: 4.60 LM NLL: 4.42, KL: 60.90|DE LOSS: 3.60|BETA: 0.004436\n",
      "073[s], 07[s], Ep: 17, Ct: 01000|TR LOSS: 4.59 LM NLL: 4.40, KL: 61.03|DE LOSS: 3.62|BETA: 0.004471\n",
      "073[s], 07[s], Ep: 17, Ct: 01500|TR LOSS: 4.58 LM NLL: 4.39, KL: 61.16|DE LOSS: 3.67|BETA: 0.004506\n",
      "073[s], 07[s], Ep: 17, Ct: 02000|TR LOSS: 4.57 LM NLL: 4.38, KL: 61.29|DE LOSS: 3.62|BETA: 0.004541\n",
      "075[s], 07[s], Ep: 17, Ct: 02500|TR LOSS: 4.56 LM NLL: 4.37, KL: 61.41|DE LOSS: 3.61|BETA: 0.004575\n",
      "078[s], 26[s], Ep: 18, Ct: 00000|TR LOSS: 4.55 LM NLL: 4.36, KL: 61.50|DE LOSS: 3.58|BETA: 0.004602\n",
      "075[s], 07[s], Ep: 18, Ct: 00500|TR LOSS: 4.54 LM NLL: 4.35, KL: 61.62|DE LOSS: 3.58|BETA: 0.004637\n",
      "074[s], 07[s], Ep: 18, Ct: 01000|TR LOSS: 4.54 LM NLL: 4.34, KL: 61.73|DE LOSS: 3.59|BETA: 0.004671\n",
      "092[s], 26[s], Ep: 18, Ct: 01500|TR LOSS: 4.53 LM NLL: 4.33, KL: 61.85|DE LOSS: 3.57|BETA: 0.004706\n",
      "092[s], 26[s], Ep: 18, Ct: 02000|TR LOSS: 4.52 LM NLL: 4.32, KL: 61.96|DE LOSS: 3.56|BETA: 0.004741\n",
      "091[s], 25[s], Ep: 18, Ct: 02500|TR LOSS: 4.51 LM NLL: 4.31, KL: 62.07|DE LOSS: 3.55|BETA: 0.004775\n",
      "059[s], 07[s], Ep: 19, Ct: 00000|TR LOSS: 4.50 LM NLL: 4.31, KL: 62.15|DE LOSS: 3.57|BETA: 0.004802\n",
      "094[s], 26[s], Ep: 19, Ct: 00500|TR LOSS: 4.49 LM NLL: 4.30, KL: 62.25|DE LOSS: 3.55|BETA: 0.004837\n",
      "075[s], 07[s], Ep: 19, Ct: 01000|TR LOSS: 4.49 LM NLL: 4.29, KL: 62.35|DE LOSS: 3.58|BETA: 0.004871\n",
      "075[s], 07[s], Ep: 19, Ct: 01500|TR LOSS: 4.48 LM NLL: 4.28, KL: 62.45|DE LOSS: 3.62|BETA: 0.004906\n",
      "074[s], 07[s], Ep: 19, Ct: 02000|TR LOSS: 4.47 LM NLL: 4.27, KL: 62.55|DE LOSS: 3.61|BETA: 0.004941\n",
      "074[s], 07[s], Ep: 19, Ct: 02500|TR LOSS: 4.46 LM NLL: 4.26, KL: 62.64|DE LOSS: 3.74|BETA: 0.004975\n",
      "080[s], 29[s], Ep: 20, Ct: 00000|TR LOSS: 4.46 LM NLL: 4.25, KL: 62.72|DE LOSS: 3.54|BETA: 0.005002\n",
      "072[s], 07[s], Ep: 20, Ct: 00500|TR LOSS: 4.45 LM NLL: 4.24, KL: 62.81|DE LOSS: 3.56|BETA: 0.005037\n",
      "092[s], 26[s], Ep: 20, Ct: 01000|TR LOSS: 4.44 LM NLL: 4.23, KL: 62.90|DE LOSS: 3.54|BETA: 0.005071\n",
      "094[s], 26[s], Ep: 20, Ct: 01500|TR LOSS: 4.43 LM NLL: 4.22, KL: 62.99|DE LOSS: 3.53|BETA: 0.005106\n",
      "075[s], 07[s], Ep: 20, Ct: 02000|TR LOSS: 4.43 LM NLL: 4.21, KL: 63.07|DE LOSS: 3.63|BETA: 0.005141\n",
      "075[s], 07[s], Ep: 20, Ct: 02500|TR LOSS: 4.42 LM NLL: 4.21, KL: 63.16|DE LOSS: 3.54|BETA: 0.005176\n",
      "077[s], 26[s], Ep: 21, Ct: 00000|TR LOSS: 4.41 LM NLL: 4.20, KL: 63.22|DE LOSS: 3.53|BETA: 0.005202\n",
      "073[s], 07[s], Ep: 21, Ct: 00500|TR LOSS: 4.41 LM NLL: 4.19, KL: 63.30|DE LOSS: 3.54|BETA: 0.005237\n",
      "094[s], 26[s], Ep: 21, Ct: 01000|TR LOSS: 4.40 LM NLL: 4.18, KL: 63.38|DE LOSS: 3.53|BETA: 0.005272\n",
      "076[s], 08[s], Ep: 21, Ct: 01500|TR LOSS: 4.39 LM NLL: 4.17, KL: 63.46|DE LOSS: 3.55|BETA: 0.005306\n",
      "076[s], 08[s], Ep: 21, Ct: 02000|TR LOSS: 4.39 LM NLL: 4.17, KL: 63.53|DE LOSS: 3.56|BETA: 0.005341\n",
      "094[s], 26[s], Ep: 21, Ct: 02500|TR LOSS: 4.38 LM NLL: 4.16, KL: 63.61|DE LOSS: 3.53|BETA: 0.005376\n",
      "060[s], 07[s], Ep: 22, Ct: 00000|TR LOSS: 4.37 LM NLL: 4.15, KL: 63.66|DE LOSS: 3.74|BETA: 0.005402\n",
      "094[s], 26[s], Ep: 22, Ct: 00500|TR LOSS: 4.37 LM NLL: 4.14, KL: 63.74|DE LOSS: 3.52|BETA: 0.005437\n",
      "076[s], 08[s], Ep: 22, Ct: 01000|TR LOSS: 4.36 LM NLL: 4.14, KL: 63.81|DE LOSS: 3.52|BETA: 0.005472\n",
      "076[s], 07[s], Ep: 22, Ct: 01500|TR LOSS: 4.35 LM NLL: 4.13, KL: 63.88|DE LOSS: 3.52|BETA: 0.005506\n",
      "074[s], 08[s], Ep: 22, Ct: 02000|TR LOSS: 4.35 LM NLL: 4.12, KL: 63.94|DE LOSS: 3.55|BETA: 0.005541\n",
      "075[s], 07[s], Ep: 22, Ct: 02500|TR LOSS: 4.34 LM NLL: 4.11, KL: 64.01|DE LOSS: 3.54|BETA: 0.005576\n",
      "058[s], 08[s], Ep: 23, Ct: 00000|TR LOSS: 4.34 LM NLL: 4.11, KL: 64.06|DE LOSS: 3.52|BETA: 0.005602\n",
      "076[s], 08[s], Ep: 23, Ct: 00500|TR LOSS: 4.33 LM NLL: 4.10, KL: 64.12|DE LOSS: 3.65|BETA: 0.005637\n",
      "074[s], 07[s], Ep: 23, Ct: 01000|TR LOSS: 4.32 LM NLL: 4.09, KL: 64.19|DE LOSS: 3.53|BETA: 0.005672\n",
      "073[s], 07[s], Ep: 23, Ct: 01500|TR LOSS: 4.32 LM NLL: 4.08, KL: 64.25|DE LOSS: 3.58|BETA: 0.005706\n",
      "075[s], 07[s], Ep: 23, Ct: 02000|TR LOSS: 4.31 LM NLL: 4.08, KL: 64.31|DE LOSS: 3.53|BETA: 0.005741\n",
      "075[s], 07[s], Ep: 23, Ct: 02500|TR LOSS: 4.31 LM NLL: 4.07, KL: 64.37|DE LOSS: 3.53|BETA: 0.005776\n",
      "079[s], 26[s], Ep: 24, Ct: 00000|TR LOSS: 4.30 LM NLL: 4.06, KL: 64.41|DE LOSS: 3.51|BETA: 0.005802\n",
      "091[s], 26[s], Ep: 24, Ct: 00500|TR LOSS: 4.30 LM NLL: 4.06, KL: 64.47|DE LOSS: 3.50|BETA: 0.005837\n",
      "076[s], 08[s], Ep: 24, Ct: 01000|TR LOSS: 4.29 LM NLL: 4.05, KL: 64.52|DE LOSS: 3.70|BETA: 0.005872\n",
      "074[s], 07[s], Ep: 24, Ct: 01500|TR LOSS: 4.28 LM NLL: 4.04, KL: 64.58|DE LOSS: 3.52|BETA: 0.005907\n",
      "094[s], 26[s], Ep: 24, Ct: 02000|TR LOSS: 4.28 LM NLL: 4.04, KL: 64.63|DE LOSS: 3.50|BETA: 0.005941\n",
      "075[s], 08[s], Ep: 24, Ct: 02500|TR LOSS: 4.27 LM NLL: 4.03, KL: 64.68|DE LOSS: 3.51|BETA: 0.005976\n",
      "077[s], 25[s], Ep: 25, Ct: 00000|TR LOSS: 4.27 LM NLL: 4.02, KL: 64.72|DE LOSS: 3.49|BETA: 0.006003\n",
      "075[s], 07[s], Ep: 25, Ct: 00500|TR LOSS: 4.26 LM NLL: 4.02, KL: 64.77|DE LOSS: 3.49|BETA: 0.006037\n",
      "072[s], 07[s], Ep: 25, Ct: 01000|TR LOSS: 4.26 LM NLL: 4.01, KL: 64.82|DE LOSS: 3.54|BETA: 0.006072\n",
      "075[s], 08[s], Ep: 25, Ct: 01500|TR LOSS: 4.25 LM NLL: 4.00, KL: 64.87|DE LOSS: 3.49|BETA: 0.006107\n",
      "074[s], 07[s], Ep: 25, Ct: 02000|TR LOSS: 4.25 LM NLL: 4.00, KL: 64.91|DE LOSS: 3.49|BETA: 0.006141\n",
      "075[s], 07[s], Ep: 25, Ct: 02500|TR LOSS: 4.24 LM NLL: 3.99, KL: 64.96|DE LOSS: 3.49|BETA: 0.006176\n",
      "059[s], 07[s], Ep: 26, Ct: 00000|TR LOSS: 4.24 LM NLL: 3.99, KL: 65.00|DE LOSS: 3.50|BETA: 0.006203\n",
      "074[s], 07[s], Ep: 26, Ct: 00500|TR LOSS: 4.23 LM NLL: 3.98, KL: 65.04|DE LOSS: 3.52|BETA: 0.006237\n",
      "092[s], 26[s], Ep: 26, Ct: 01000|TR LOSS: 4.23 LM NLL: 3.97, KL: 65.08|DE LOSS: 3.49|BETA: 0.006272\n",
      "075[s], 07[s], Ep: 26, Ct: 01500|TR LOSS: 4.22 LM NLL: 3.97, KL: 65.13|DE LOSS: 3.51|BETA: 0.006307\n",
      "075[s], 07[s], Ep: 26, Ct: 02000|TR LOSS: 4.22 LM NLL: 3.96, KL: 65.17|DE LOSS: 3.50|BETA: 0.006341\n",
      "076[s], 08[s], Ep: 26, Ct: 02500|TR LOSS: 4.21 LM NLL: 3.95, KL: 65.21|DE LOSS: 3.51|BETA: 0.006376\n",
      "078[s], 26[s], Ep: 27, Ct: 00000|TR LOSS: 4.21 LM NLL: 3.95, KL: 65.24|DE LOSS: 3.48|BETA: 0.006403\n",
      "075[s], 08[s], Ep: 27, Ct: 00500|TR LOSS: 4.20 LM NLL: 3.94, KL: 65.28|DE LOSS: 3.51|BETA: 0.006437\n",
      "075[s], 08[s], Ep: 27, Ct: 01000|TR LOSS: 4.20 LM NLL: 3.94, KL: 65.32|DE LOSS: 3.52|BETA: 0.006472\n",
      "075[s], 08[s], Ep: 27, Ct: 01500|TR LOSS: 4.20 LM NLL: 3.93, KL: 65.36|DE LOSS: 3.52|BETA: 0.006507\n",
      "076[s], 08[s], Ep: 27, Ct: 02000|TR LOSS: 4.19 LM NLL: 3.93, KL: 65.39|DE LOSS: 3.49|BETA: 0.006542\n",
      "075[s], 08[s], Ep: 27, Ct: 02500|TR LOSS: 4.19 LM NLL: 3.92, KL: 65.43|DE LOSS: 3.50|BETA: 0.006576\n",
      "060[s], 08[s], Ep: 28, Ct: 00000|TR LOSS: 4.18 LM NLL: 3.92, KL: 65.45|DE LOSS: 3.49|BETA: 0.006603\n",
      "075[s], 07[s], Ep: 28, Ct: 00500|TR LOSS: 4.18 LM NLL: 3.91, KL: 65.49|DE LOSS: 3.49|BETA: 0.006638\n",
      "076[s], 08[s], Ep: 28, Ct: 01000|TR LOSS: 4.17 LM NLL: 3.90, KL: 65.52|DE LOSS: 3.52|BETA: 0.006672\n",
      "076[s], 08[s], Ep: 28, Ct: 01500|TR LOSS: 4.17 LM NLL: 3.90, KL: 65.56|DE LOSS: 4.16|BETA: 0.006707\n",
      "075[s], 08[s], Ep: 28, Ct: 02000|TR LOSS: 4.16 LM NLL: 3.89, KL: 65.59|DE LOSS: 3.51|BETA: 0.006742\n",
      "075[s], 08[s], Ep: 28, Ct: 02500|TR LOSS: 4.16 LM NLL: 3.89, KL: 65.62|DE LOSS: 3.50|BETA: 0.006776\n",
      "060[s], 08[s], Ep: 29, Ct: 00000|TR LOSS: 4.16 LM NLL: 3.88, KL: 65.65|DE LOSS: 3.48|BETA: 0.006803\n",
      "076[s], 07[s], Ep: 29, Ct: 00500|TR LOSS: 4.15 LM NLL: 3.88, KL: 65.68|DE LOSS: 3.49|BETA: 0.006838\n",
      "094[s], 26[s], Ep: 29, Ct: 01000|TR LOSS: 4.15 LM NLL: 3.87, KL: 65.71|DE LOSS: 3.48|BETA: 0.006872\n",
      "076[s], 08[s], Ep: 29, Ct: 01500|TR LOSS: 4.14 LM NLL: 3.87, KL: 65.74|DE LOSS: 3.50|BETA: 0.006907\n",
      "075[s], 08[s], Ep: 29, Ct: 02000|TR LOSS: 4.14 LM NLL: 3.86, KL: 65.76|DE LOSS: 3.53|BETA: 0.006942\n",
      "075[s], 07[s], Ep: 29, Ct: 02500|TR LOSS: 4.14 LM NLL: 3.86, KL: 65.79|DE LOSS: 3.58|BETA: 0.006976\n",
      "060[s], 07[s], Ep: 30, Ct: 00000|TR LOSS: 4.13 LM NLL: 3.85, KL: 65.81|DE LOSS: 3.49|BETA: 0.007003\n",
      "076[s], 08[s], Ep: 30, Ct: 00500|TR LOSS: 4.13 LM NLL: 3.85, KL: 65.84|DE LOSS: 3.52|BETA: 0.007038\n",
      "076[s], 08[s], Ep: 30, Ct: 01000|TR LOSS: 4.13 LM NLL: 3.84, KL: 65.87|DE LOSS: 3.49|BETA: 0.007072\n",
      "075[s], 07[s], Ep: 30, Ct: 01500|TR LOSS: 4.12 LM NLL: 3.84, KL: 65.89|DE LOSS: 3.50|BETA: 0.007107\n",
      "075[s], 07[s], Ep: 30, Ct: 02000|TR LOSS: 4.12 LM NLL: 3.83, KL: 65.92|DE LOSS: 3.48|BETA: 0.007142\n",
      "075[s], 07[s], Ep: 30, Ct: 02500|TR LOSS: 4.11 LM NLL: 3.83, KL: 65.94|DE LOSS: 3.48|BETA: 0.007177\n",
      "060[s], 08[s], Ep: 31, Ct: 00000|TR LOSS: 4.11 LM NLL: 3.82, KL: 65.96|DE LOSS: 3.48|BETA: 0.007203\n",
      "076[s], 07[s], Ep: 31, Ct: 00500|TR LOSS: 4.11 LM NLL: 3.82, KL: 65.98|DE LOSS: 3.54|BETA: 0.007238\n",
      "075[s], 07[s], Ep: 31, Ct: 01000|TR LOSS: 4.10 LM NLL: 3.81, KL: 66.01|DE LOSS: 3.55|BETA: 0.007273\n",
      "075[s], 08[s], Ep: 31, Ct: 01500|TR LOSS: 4.10 LM NLL: 3.81, KL: 66.03|DE LOSS: 3.48|BETA: 0.007307\n",
      "076[s], 08[s], Ep: 31, Ct: 02000|TR LOSS: 4.10 LM NLL: 3.80, KL: 66.05|DE LOSS: 3.50|BETA: 0.007342\n",
      "075[s], 08[s], Ep: 31, Ct: 02500|TR LOSS: 4.09 LM NLL: 3.80, KL: 66.07|DE LOSS: 3.49|BETA: 0.007377\n",
      "058[s], 07[s], Ep: 32, Ct: 00000|TR LOSS: 4.09 LM NLL: 3.79, KL: 66.09|DE LOSS: 3.48|BETA: 0.007403\n",
      "076[s], 07[s], Ep: 32, Ct: 00500|TR LOSS: 4.09 LM NLL: 3.79, KL: 66.11|DE LOSS: 3.54|BETA: 0.007438\n",
      "076[s], 07[s], Ep: 32, Ct: 01000|TR LOSS: 4.08 LM NLL: 3.79, KL: 66.13|DE LOSS: 3.58|BETA: 0.007473\n",
      "075[s], 07[s], Ep: 32, Ct: 01500|TR LOSS: 4.08 LM NLL: 3.78, KL: 66.15|DE LOSS: 3.50|BETA: 0.007507\n",
      "074[s], 07[s], Ep: 32, Ct: 02000|TR LOSS: 4.08 LM NLL: 3.78, KL: 66.17|DE LOSS: 3.55|BETA: 0.007542\n",
      "074[s], 07[s], Ep: 32, Ct: 02500|TR LOSS: 4.07 LM NLL: 3.77, KL: 66.19|DE LOSS: 3.50|BETA: 0.007577\n",
      "059[s], 08[s], Ep: 33, Ct: 00000|TR LOSS: 4.07 LM NLL: 3.77, KL: 66.20|DE LOSS: 3.50|BETA: 0.007603\n",
      "075[s], 08[s], Ep: 33, Ct: 00500|TR LOSS: 4.07 LM NLL: 3.76, KL: 66.22|DE LOSS: 3.48|BETA: 0.007638\n",
      "075[s], 07[s], Ep: 33, Ct: 01000|TR LOSS: 4.06 LM NLL: 3.76, KL: 66.24|DE LOSS: 3.48|BETA: 0.007673\n",
      "075[s], 08[s], Ep: 33, Ct: 01500|TR LOSS: 4.06 LM NLL: 3.75, KL: 66.25|DE LOSS: 3.48|BETA: 0.007707\n",
      "075[s], 08[s], Ep: 33, Ct: 02000|TR LOSS: 4.06 LM NLL: 3.75, KL: 66.27|DE LOSS: 3.49|BETA: 0.007742\n",
      "075[s], 08[s], Ep: 33, Ct: 02500|TR LOSS: 4.05 LM NLL: 3.75, KL: 66.29|DE LOSS: 3.48|BETA: 0.007777\n",
      "060[s], 08[s], Ep: 34, Ct: 00000|TR LOSS: 4.05 LM NLL: 3.74, KL: 66.30|DE LOSS: 3.49|BETA: 0.007803\n",
      "075[s], 08[s], Ep: 34, Ct: 00500|TR LOSS: 4.05 LM NLL: 3.74, KL: 66.31|DE LOSS: 3.50|BETA: 0.007838\n",
      "075[s], 08[s], Ep: 34, Ct: 01000|TR LOSS: 4.04 LM NLL: 3.73, KL: 66.33|DE LOSS: 3.48|BETA: 0.007872\n",
      "075[s], 08[s], Ep: 34, Ct: 01500|TR LOSS: 4.04 LM NLL: 3.73, KL: 66.34|DE LOSS: 3.48|BETA: 0.007907\n",
      "076[s], 08[s], Ep: 34, Ct: 02000|TR LOSS: 4.04 LM NLL: 3.72, KL: 66.36|DE LOSS: 3.49|BETA: 0.007941\n",
      "075[s], 08[s], Ep: 34, Ct: 02500|TR LOSS: 4.03 LM NLL: 3.72, KL: 66.37|DE LOSS: 3.49|BETA: 0.007976\n",
      "059[s], 08[s], Ep: 35, Ct: 00000|TR LOSS: 4.03 LM NLL: 3.72, KL: 66.38|DE LOSS: 3.48|BETA: 0.008002\n",
      "075[s], 08[s], Ep: 35, Ct: 00500|TR LOSS: 4.03 LM NLL: 3.71, KL: 66.39|DE LOSS: 3.49|BETA: 0.008037\n",
      "075[s], 08[s], Ep: 35, Ct: 01000|TR LOSS: 4.03 LM NLL: 3.71, KL: 66.40|DE LOSS: 3.48|BETA: 0.008071\n",
      "075[s], 08[s], Ep: 35, Ct: 01500|TR LOSS: 4.02 LM NLL: 3.70, KL: 66.42|DE LOSS: 3.50|BETA: 0.008106\n",
      "075[s], 08[s], Ep: 35, Ct: 02000|TR LOSS: 4.02 LM NLL: 3.70, KL: 66.43|DE LOSS: 3.52|BETA: 0.008140\n",
      "075[s], 08[s], Ep: 35, Ct: 02500|TR LOSS: 4.02 LM NLL: 3.70, KL: 66.44|DE LOSS: 3.53|BETA: 0.008175\n",
      "060[s], 08[s], Ep: 36, Ct: 00000|TR LOSS: 4.02 LM NLL: 3.69, KL: 66.45|DE LOSS: 3.49|BETA: 0.008201\n",
      "075[s], 07[s], Ep: 36, Ct: 00500|TR LOSS: 4.01 LM NLL: 3.69, KL: 66.46|DE LOSS: 3.49|BETA: 0.008236\n",
      "076[s], 08[s], Ep: 36, Ct: 01000|TR LOSS: 4.01 LM NLL: 3.69, KL: 66.47|DE LOSS: 3.49|BETA: 0.008270\n",
      "076[s], 08[s], Ep: 36, Ct: 01500|TR LOSS: 4.01 LM NLL: 3.68, KL: 66.48|DE LOSS: 3.50|BETA: 0.008304\n",
      "075[s], 08[s], Ep: 36, Ct: 02000|TR LOSS: 4.00 LM NLL: 3.68, KL: 66.49|DE LOSS: 3.50|BETA: 0.008339\n",
      "076[s], 08[s], Ep: 36, Ct: 02500|TR LOSS: 4.00 LM NLL: 3.67, KL: 66.50|DE LOSS: 3.50|BETA: 0.008373\n",
      "060[s], 08[s], Ep: 37, Ct: 00000|TR LOSS: 4.00 LM NLL: 3.67, KL: 66.50|DE LOSS: 3.50|BETA: 0.008400\n",
      "076[s], 08[s], Ep: 37, Ct: 00500|TR LOSS: 4.00 LM NLL: 3.67, KL: 66.51|DE LOSS: 3.50|BETA: 0.008434\n",
      "075[s], 08[s], Ep: 37, Ct: 01000|TR LOSS: 3.99 LM NLL: 3.66, KL: 66.52|DE LOSS: 3.49|BETA: 0.008469\n",
      "076[s], 07[s], Ep: 37, Ct: 01500|TR LOSS: 3.99 LM NLL: 3.66, KL: 66.53|DE LOSS: 3.49|BETA: 0.008503\n",
      "071[s], 07[s], Ep: 37, Ct: 02000|TR LOSS: 3.99 LM NLL: 3.65, KL: 66.54|DE LOSS: 3.52|BETA: 0.008538\n",
      "070[s], 07[s], Ep: 37, Ct: 02500|TR LOSS: 3.99 LM NLL: 3.65, KL: 66.55|DE LOSS: 3.51|BETA: 0.008572\n",
      "055[s], 07[s], Ep: 38, Ct: 00000|TR LOSS: 3.98 LM NLL: 3.65, KL: 66.55|DE LOSS: 3.50|BETA: 0.008599\n",
      "070[s], 07[s], Ep: 38, Ct: 00500|TR LOSS: 3.98 LM NLL: 3.64, KL: 66.56|DE LOSS: 3.50|BETA: 0.008633\n",
      "070[s], 07[s], Ep: 38, Ct: 01000|TR LOSS: 3.98 LM NLL: 3.64, KL: 66.56|DE LOSS: 3.55|BETA: 0.008667\n",
      "070[s], 07[s], Ep: 38, Ct: 01500|TR LOSS: 3.98 LM NLL: 3.64, KL: 66.57|DE LOSS: 3.50|BETA: 0.008702\n",
      "069[s], 07[s], Ep: 38, Ct: 02000|TR LOSS: 3.97 LM NLL: 3.63, KL: 66.58|DE LOSS: 3.50|BETA: 0.008736\n",
      "071[s], 07[s], Ep: 38, Ct: 02500|TR LOSS: 3.97 LM NLL: 3.63, KL: 66.58|DE LOSS: 3.50|BETA: 0.008771\n",
      "056[s], 07[s], Ep: 39, Ct: 00000|TR LOSS: 3.97 LM NLL: 3.63, KL: 66.59|DE LOSS: 3.50|BETA: 0.008797\n",
      "070[s], 07[s], Ep: 39, Ct: 00500|TR LOSS: 3.97 LM NLL: 3.62, KL: 66.59|DE LOSS: 3.56|BETA: 0.008832\n",
      "070[s], 07[s], Ep: 39, Ct: 01000|TR LOSS: 3.96 LM NLL: 3.62, KL: 66.60|DE LOSS: 3.64|BETA: 0.008866\n",
      "070[s], 07[s], Ep: 39, Ct: 01500|TR LOSS: 3.96 LM NLL: 3.62, KL: 66.60|DE LOSS: 3.50|BETA: 0.008901\n",
      "070[s], 07[s], Ep: 39, Ct: 02000|TR LOSS: 3.96 LM NLL: 3.61, KL: 66.60|DE LOSS: 3.50|BETA: 0.008935\n",
      "070[s], 07[s], Ep: 39, Ct: 02500|TR LOSS: 3.96 LM NLL: 3.61, KL: 66.61|DE LOSS: 3.51|BETA: 0.008970\n",
      "055[s], 07[s], Ep: 40, Ct: 00000|TR LOSS: 3.95 LM NLL: 3.61, KL: 66.61|DE LOSS: 3.51|BETA: 0.008996\n",
      "070[s], 07[s], Ep: 40, Ct: 00500|TR LOSS: 3.95 LM NLL: 3.60, KL: 66.61|DE LOSS: 3.53|BETA: 0.009031\n",
      "070[s], 07[s], Ep: 40, Ct: 01000|TR LOSS: 3.95 LM NLL: 3.60, KL: 66.62|DE LOSS: 3.51|BETA: 0.009065\n",
      "070[s], 07[s], Ep: 40, Ct: 01500|TR LOSS: 3.95 LM NLL: 3.60, KL: 66.62|DE LOSS: 3.52|BETA: 0.009099\n",
      "070[s], 07[s], Ep: 40, Ct: 02000|TR LOSS: 3.95 LM NLL: 3.59, KL: 66.62|DE LOSS: 3.50|BETA: 0.009134\n",
      "070[s], 07[s], Ep: 40, Ct: 02500|TR LOSS: 3.94 LM NLL: 3.59, KL: 66.63|DE LOSS: 3.52|BETA: 0.009168\n",
      "056[s], 07[s], Ep: 41, Ct: 00000|TR LOSS: 3.94 LM NLL: 3.59, KL: 66.63|DE LOSS: 3.51|BETA: 0.009195\n",
      "070[s], 07[s], Ep: 41, Ct: 00500|TR LOSS: 3.94 LM NLL: 3.58, KL: 66.63|DE LOSS: 3.51|BETA: 0.009229\n",
      "070[s], 07[s], Ep: 41, Ct: 01000|TR LOSS: 3.94 LM NLL: 3.58, KL: 66.63|DE LOSS: 3.53|BETA: 0.009264\n",
      "070[s], 07[s], Ep: 41, Ct: 01500|TR LOSS: 3.93 LM NLL: 3.58, KL: 66.63|DE LOSS: 3.52|BETA: 0.009298\n",
      "070[s], 07[s], Ep: 41, Ct: 02000|TR LOSS: 3.93 LM NLL: 3.57, KL: 66.63|DE LOSS: 3.52|BETA: 0.009333\n",
      "070[s], 07[s], Ep: 41, Ct: 02500|TR LOSS: 3.93 LM NLL: 3.57, KL: 66.64|DE LOSS: 3.53|BETA: 0.009367\n",
      "056[s], 07[s], Ep: 42, Ct: 00000|TR LOSS: 3.93 LM NLL: 3.57, KL: 66.64|DE LOSS: 3.52|BETA: 0.009394\n",
      "069[s], 07[s], Ep: 42, Ct: 00500|TR LOSS: 3.93 LM NLL: 3.56, KL: 66.64|DE LOSS: 3.54|BETA: 0.009428\n",
      "068[s], 06[s], Ep: 42, Ct: 01000|TR LOSS: 3.92 LM NLL: 3.56, KL: 66.64|DE LOSS: 3.52|BETA: 0.009463\n",
      "070[s], 07[s], Ep: 42, Ct: 01500|TR LOSS: 3.92 LM NLL: 3.56, KL: 66.64|DE LOSS: 3.52|BETA: 0.009497\n",
      "070[s], 07[s], Ep: 42, Ct: 02000|TR LOSS: 3.92 LM NLL: 3.55, KL: 66.64|DE LOSS: 3.54|BETA: 0.009531\n",
      "070[s], 07[s], Ep: 42, Ct: 02500|TR LOSS: 3.92 LM NLL: 3.55, KL: 66.64|DE LOSS: 3.55|BETA: 0.009566\n",
      "056[s], 07[s], Ep: 43, Ct: 00000|TR LOSS: 3.92 LM NLL: 3.55, KL: 66.64|DE LOSS: 3.52|BETA: 0.009592\n",
      "099[s], 15[s], Ep: 43, Ct: 00500|TR LOSS: 3.91 LM NLL: 3.55, KL: 66.64|DE LOSS: 3.52|BETA: 0.009627\n",
      "179[s], 19[s], Ep: 43, Ct: 01000|TR LOSS: 3.91 LM NLL: 3.54, KL: 66.64|DE LOSS: 3.53|BETA: 0.009661\n",
      "213[s], 22[s], Ep: 43, Ct: 01500|TR LOSS: 3.91 LM NLL: 3.54, KL: 66.64|DE LOSS: 3.53|BETA: 0.009696\n",
      "164[s], 17[s], Ep: 43, Ct: 02000|TR LOSS: 3.91 LM NLL: 3.54, KL: 66.63|DE LOSS: 3.54|BETA: 0.009730\n",
      "191[s], 21[s], Ep: 43, Ct: 02500|TR LOSS: 3.91 LM NLL: 3.53, KL: 66.63|DE LOSS: 3.53|BETA: 0.009765\n",
      "170[s], 21[s], Ep: 44, Ct: 00000|TR LOSS: 3.90 LM NLL: 3.53, KL: 66.63|DE LOSS: 3.53|BETA: 0.009791\n",
      "167[s], 17[s], Ep: 44, Ct: 00500|TR LOSS: 3.90 LM NLL: 3.53, KL: 66.63|DE LOSS: 3.53|BETA: 0.009826\n",
      "186[s], 19[s], Ep: 44, Ct: 01000|TR LOSS: 3.90 LM NLL: 3.52, KL: 66.63|DE LOSS: 3.53|BETA: 0.009860\n",
      "217[s], 21[s], Ep: 44, Ct: 01500|TR LOSS: 3.90 LM NLL: 3.52, KL: 66.63|DE LOSS: 3.53|BETA: 0.009895\n",
      "090[s], 07[s], Ep: 44, Ct: 02000|TR LOSS: 3.90 LM NLL: 3.52, KL: 66.62|DE LOSS: 3.55|BETA: 0.009929\n",
      "070[s], 07[s], Ep: 44, Ct: 02500|TR LOSS: 3.90 LM NLL: 3.52, KL: 66.62|DE LOSS: 3.53|BETA: 0.009963\n",
      "055[s], 07[s], Ep: 45, Ct: 00000|TR LOSS: 3.89 LM NLL: 3.51, KL: 66.62|DE LOSS: 3.53|BETA: 0.009990\n",
      "070[s], 07[s], Ep: 45, Ct: 00500|TR LOSS: 3.89 LM NLL: 3.51, KL: 66.62|DE LOSS: 3.53|BETA: 0.010024\n",
      "070[s], 07[s], Ep: 45, Ct: 01000|TR LOSS: 3.89 LM NLL: 3.51, KL: 66.61|DE LOSS: 3.53|BETA: 0.010059\n",
      "070[s], 07[s], Ep: 45, Ct: 01500|TR LOSS: 3.89 LM NLL: 3.51, KL: 66.61|DE LOSS: 3.53|BETA: 0.010093\n",
      "070[s], 07[s], Ep: 45, Ct: 02000|TR LOSS: 3.89 LM NLL: 3.50, KL: 66.61|DE LOSS: 3.54|BETA: 0.010128\n",
      "070[s], 07[s], Ep: 45, Ct: 02500|TR LOSS: 3.88 LM NLL: 3.50, KL: 66.61|DE LOSS: 3.54|BETA: 0.010162\n",
      "055[s], 07[s], Ep: 46, Ct: 00000|TR LOSS: 3.88 LM NLL: 3.50, KL: 66.60|DE LOSS: 3.58|BETA: 0.010189\n",
      "070[s], 07[s], Ep: 46, Ct: 00500|TR LOSS: 3.88 LM NLL: 3.49, KL: 66.60|DE LOSS: 3.57|BETA: 0.010223\n",
      "070[s], 07[s], Ep: 46, Ct: 01000|TR LOSS: 3.88 LM NLL: 3.49, KL: 66.60|DE LOSS: 3.54|BETA: 0.010258\n",
      "070[s], 07[s], Ep: 46, Ct: 01500|TR LOSS: 3.88 LM NLL: 3.49, KL: 66.59|DE LOSS: 3.55|BETA: 0.010292\n",
      "070[s], 07[s], Ep: 46, Ct: 02000|TR LOSS: 3.88 LM NLL: 3.49, KL: 66.59|DE LOSS: 3.59|BETA: 0.010326\n",
      "070[s], 07[s], Ep: 46, Ct: 02500|TR LOSS: 3.87 LM NLL: 3.48, KL: 66.58|DE LOSS: 3.58|BETA: 0.010361\n",
      "056[s], 07[s], Ep: 47, Ct: 00000|TR LOSS: 3.87 LM NLL: 3.48, KL: 66.58|DE LOSS: 3.55|BETA: 0.010387\n",
      "070[s], 07[s], Ep: 47, Ct: 00500|TR LOSS: 3.87 LM NLL: 3.48, KL: 66.57|DE LOSS: 3.58|BETA: 0.010422\n",
      "070[s], 07[s], Ep: 47, Ct: 01000|TR LOSS: 3.87 LM NLL: 3.48, KL: 66.57|DE LOSS: 3.55|BETA: 0.010456\n",
      "070[s], 07[s], Ep: 47, Ct: 01500|TR LOSS: 3.87 LM NLL: 3.47, KL: 66.57|DE LOSS: 3.55|BETA: 0.010491\n",
      "070[s], 07[s], Ep: 47, Ct: 02000|TR LOSS: 3.87 LM NLL: 3.47, KL: 66.56|DE LOSS: 3.55|BETA: 0.010525\n",
      "070[s], 07[s], Ep: 47, Ct: 02500|TR LOSS: 3.86 LM NLL: 3.47, KL: 66.56|DE LOSS: 3.55|BETA: 0.010560\n",
      "056[s], 07[s], Ep: 48, Ct: 00000|TR LOSS: 3.86 LM NLL: 3.47, KL: 66.55|DE LOSS: 3.54|BETA: 0.010586\n",
      "070[s], 07[s], Ep: 48, Ct: 00500|TR LOSS: 3.86 LM NLL: 3.46, KL: 66.55|DE LOSS: 3.55|BETA: 0.010621\n",
      "070[s], 07[s], Ep: 48, Ct: 01000|TR LOSS: 3.86 LM NLL: 3.46, KL: 66.54|DE LOSS: 3.55|BETA: 0.010655\n",
      "070[s], 07[s], Ep: 48, Ct: 01500|TR LOSS: 3.86 LM NLL: 3.46, KL: 66.53|DE LOSS: 3.57|BETA: 0.010690\n",
      "070[s], 07[s], Ep: 48, Ct: 02000|TR LOSS: 3.86 LM NLL: 3.45, KL: 66.53|DE LOSS: 3.57|BETA: 0.010724\n",
      "070[s], 07[s], Ep: 48, Ct: 02500|TR LOSS: 3.86 LM NLL: 3.45, KL: 66.52|DE LOSS: 3.58|BETA: 0.010758\n",
      "056[s], 07[s], Ep: 49, Ct: 00000|TR LOSS: 3.85 LM NLL: 3.45, KL: 66.52|DE LOSS: 3.56|BETA: 0.010785\n",
      "070[s], 07[s], Ep: 49, Ct: 00500|TR LOSS: 3.85 LM NLL: 3.45, KL: 66.51|DE LOSS: 3.56|BETA: 0.010819\n",
      "070[s], 07[s], Ep: 49, Ct: 01000|TR LOSS: 3.85 LM NLL: 3.44, KL: 66.50|DE LOSS: 3.55|BETA: 0.010854\n",
      "070[s], 07[s], Ep: 49, Ct: 01500|TR LOSS: 3.85 LM NLL: 3.44, KL: 66.50|DE LOSS: 3.55|BETA: 0.010888\n",
      "070[s], 07[s], Ep: 49, Ct: 02000|TR LOSS: 3.85 LM NLL: 3.44, KL: 66.49|DE LOSS: 3.56|BETA: 0.010923\n",
      "070[s], 07[s], Ep: 49, Ct: 02500|TR LOSS: 3.85 LM NLL: 3.44, KL: 66.48|DE LOSS: 3.57|BETA: 0.010957\n",
      "055[s], 07[s], Ep: 50, Ct: 00000|TR LOSS: 3.85 LM NLL: 3.44, KL: 66.48|DE LOSS: 3.56|BETA: 0.010984\n",
      "070[s], 07[s], Ep: 50, Ct: 00500|TR LOSS: 3.84 LM NLL: 3.43, KL: 66.47|DE LOSS: 3.61|BETA: 0.011018\n",
      "070[s], 07[s], Ep: 50, Ct: 01000|TR LOSS: 3.84 LM NLL: 3.43, KL: 66.47|DE LOSS: 3.56|BETA: 0.011053\n",
      "070[s], 07[s], Ep: 50, Ct: 01500|TR LOSS: 3.84 LM NLL: 3.43, KL: 66.46|DE LOSS: 3.56|BETA: 0.011087\n",
      "070[s], 07[s], Ep: 50, Ct: 02000|TR LOSS: 3.84 LM NLL: 3.43, KL: 66.45|DE LOSS: 3.57|BETA: 0.011122\n",
      "070[s], 07[s], Ep: 50, Ct: 02500|TR LOSS: 3.84 LM NLL: 3.42, KL: 66.44|DE LOSS: 3.58|BETA: 0.011156\n",
      "056[s], 07[s], Ep: 51, Ct: 00000|TR LOSS: 3.84 LM NLL: 3.42, KL: 66.44|DE LOSS: 3.58|BETA: 0.011182\n",
      "070[s], 07[s], Ep: 51, Ct: 00500|TR LOSS: 3.84 LM NLL: 3.42, KL: 66.43|DE LOSS: 3.57|BETA: 0.011217\n",
      "070[s], 07[s], Ep: 51, Ct: 01000|TR LOSS: 3.83 LM NLL: 3.42, KL: 66.42|DE LOSS: 3.57|BETA: 0.011251\n",
      "070[s], 07[s], Ep: 51, Ct: 01500|TR LOSS: 3.83 LM NLL: 3.41, KL: 66.41|DE LOSS: 3.58|BETA: 0.011286\n",
      "070[s], 07[s], Ep: 51, Ct: 02000|TR LOSS: 3.83 LM NLL: 3.41, KL: 66.41|DE LOSS: 3.59|BETA: 0.011320\n",
      "073[s], 07[s], Ep: 51, Ct: 02500|TR LOSS: 3.83 LM NLL: 3.41, KL: 66.40|DE LOSS: 3.58|BETA: 0.011355\n",
      "058[s], 07[s], Ep: 52, Ct: 00000|TR LOSS: 3.83 LM NLL: 3.41, KL: 66.39|DE LOSS: 3.58|BETA: 0.011381\n",
      "073[s], 07[s], Ep: 52, Ct: 00500|TR LOSS: 3.83 LM NLL: 3.40, KL: 66.38|DE LOSS: 3.58|BETA: 0.011416\n",
      "072[s], 07[s], Ep: 52, Ct: 01000|TR LOSS: 3.83 LM NLL: 3.40, KL: 66.37|DE LOSS: 3.59|BETA: 0.011450\n",
      "074[s], 07[s], Ep: 52, Ct: 01500|TR LOSS: 3.83 LM NLL: 3.40, KL: 66.37|DE LOSS: 3.57|BETA: 0.011485\n",
      "073[s], 07[s], Ep: 52, Ct: 02000|TR LOSS: 3.82 LM NLL: 3.40, KL: 66.36|DE LOSS: 3.58|BETA: 0.011519\n",
      "073[s], 08[s], Ep: 52, Ct: 02500|TR LOSS: 3.82 LM NLL: 3.39, KL: 66.35|DE LOSS: 3.58|BETA: 0.011553\n",
      "060[s], 08[s], Ep: 53, Ct: 00000|TR LOSS: 3.82 LM NLL: 3.39, KL: 66.34|DE LOSS: 3.58|BETA: 0.011580\n",
      "075[s], 07[s], Ep: 53, Ct: 00500|TR LOSS: 3.82 LM NLL: 3.39, KL: 66.33|DE LOSS: 3.58|BETA: 0.011614\n",
      "074[s], 08[s], Ep: 53, Ct: 01000|TR LOSS: 3.82 LM NLL: 3.39, KL: 66.32|DE LOSS: 3.60|BETA: 0.011649\n",
      "075[s], 08[s], Ep: 53, Ct: 01500|TR LOSS: 3.82 LM NLL: 3.39, KL: 66.32|DE LOSS: 3.58|BETA: 0.011683\n",
      "075[s], 08[s], Ep: 53, Ct: 02000|TR LOSS: 3.82 LM NLL: 3.38, KL: 66.31|DE LOSS: 3.59|BETA: 0.011718\n",
      "075[s], 08[s], Ep: 53, Ct: 02500|TR LOSS: 3.82 LM NLL: 3.38, KL: 66.30|DE LOSS: 3.59|BETA: 0.011752\n",
      "060[s], 08[s], Ep: 54, Ct: 00000|TR LOSS: 3.81 LM NLL: 3.38, KL: 66.29|DE LOSS: 3.60|BETA: 0.011779\n",
      "075[s], 08[s], Ep: 54, Ct: 00500|TR LOSS: 3.81 LM NLL: 3.38, KL: 66.28|DE LOSS: 3.59|BETA: 0.011813\n",
      "075[s], 08[s], Ep: 54, Ct: 01000|TR LOSS: 3.81 LM NLL: 3.38, KL: 66.27|DE LOSS: 3.59|BETA: 0.011848\n",
      "075[s], 08[s], Ep: 54, Ct: 01500|TR LOSS: 3.81 LM NLL: 3.37, KL: 66.26|DE LOSS: 3.60|BETA: 0.011882\n",
      "075[s], 08[s], Ep: 54, Ct: 02000|TR LOSS: 3.81 LM NLL: 3.37, KL: 66.25|DE LOSS: 3.66|BETA: 0.011917\n",
      "075[s], 08[s], Ep: 54, Ct: 02500|TR LOSS: 3.81 LM NLL: 3.37, KL: 66.24|DE LOSS: 3.59|BETA: 0.011951\n",
      "060[s], 08[s], Ep: 55, Ct: 00000|TR LOSS: 3.81 LM NLL: 3.37, KL: 66.23|DE LOSS: 3.60|BETA: 0.011977\n",
      "075[s], 08[s], Ep: 55, Ct: 00500|TR LOSS: 3.81 LM NLL: 3.37, KL: 66.22|DE LOSS: 3.60|BETA: 0.012012\n",
      "075[s], 07[s], Ep: 55, Ct: 01000|TR LOSS: 3.80 LM NLL: 3.36, KL: 66.21|DE LOSS: 3.60|BETA: 0.012046\n",
      "075[s], 08[s], Ep: 55, Ct: 01500|TR LOSS: 3.80 LM NLL: 3.36, KL: 66.20|DE LOSS: 3.60|BETA: 0.012081\n",
      "075[s], 08[s], Ep: 55, Ct: 02000|TR LOSS: 3.80 LM NLL: 3.36, KL: 66.19|DE LOSS: 3.69|BETA: 0.012115\n",
      "075[s], 07[s], Ep: 55, Ct: 02500|TR LOSS: 3.80 LM NLL: 3.36, KL: 66.18|DE LOSS: 3.60|BETA: 0.012150\n",
      "059[s], 08[s], Ep: 56, Ct: 00000|TR LOSS: 3.80 LM NLL: 3.35, KL: 66.17|DE LOSS: 3.62|BETA: 0.012176\n",
      "075[s], 08[s], Ep: 56, Ct: 00500|TR LOSS: 3.80 LM NLL: 3.35, KL: 66.16|DE LOSS: 3.60|BETA: 0.012211\n",
      "075[s], 08[s], Ep: 56, Ct: 01000|TR LOSS: 3.80 LM NLL: 3.35, KL: 66.15|DE LOSS: 3.60|BETA: 0.012245\n",
      "075[s], 08[s], Ep: 56, Ct: 01500|TR LOSS: 3.80 LM NLL: 3.35, KL: 66.14|DE LOSS: 3.60|BETA: 0.012280\n",
      "075[s], 08[s], Ep: 56, Ct: 02000|TR LOSS: 3.80 LM NLL: 3.35, KL: 66.13|DE LOSS: 3.63|BETA: 0.012314\n",
      "075[s], 08[s], Ep: 56, Ct: 02500|TR LOSS: 3.79 LM NLL: 3.34, KL: 66.12|DE LOSS: 3.60|BETA: 0.012349\n",
      "060[s], 08[s], Ep: 57, Ct: 00000|TR LOSS: 3.79 LM NLL: 3.34, KL: 66.11|DE LOSS: 3.61|BETA: 0.012375\n",
      "076[s], 08[s], Ep: 57, Ct: 00500|TR LOSS: 3.79 LM NLL: 3.34, KL: 66.10|DE LOSS: 3.61|BETA: 0.012409\n",
      "075[s], 08[s], Ep: 57, Ct: 01000|TR LOSS: 3.79 LM NLL: 3.34, KL: 66.09|DE LOSS: 3.68|BETA: 0.012444\n",
      "076[s], 08[s], Ep: 57, Ct: 01500|TR LOSS: 3.79 LM NLL: 3.34, KL: 66.08|DE LOSS: 3.61|BETA: 0.012478\n",
      "075[s], 08[s], Ep: 57, Ct: 02000|TR LOSS: 3.79 LM NLL: 3.33, KL: 66.07|DE LOSS: 3.62|BETA: 0.012513\n",
      "076[s], 08[s], Ep: 57, Ct: 02500|TR LOSS: 3.79 LM NLL: 3.33, KL: 66.06|DE LOSS: 3.61|BETA: 0.012547\n",
      "060[s], 08[s], Ep: 58, Ct: 00000|TR LOSS: 3.79 LM NLL: 3.33, KL: 66.05|DE LOSS: 3.62|BETA: 0.012574\n",
      "076[s], 08[s], Ep: 58, Ct: 00500|TR LOSS: 3.79 LM NLL: 3.33, KL: 66.04|DE LOSS: 3.67|BETA: 0.012608\n",
      "075[s], 08[s], Ep: 58, Ct: 01000|TR LOSS: 3.79 LM NLL: 3.33, KL: 66.02|DE LOSS: 3.62|BETA: 0.012643\n",
      "076[s], 08[s], Ep: 58, Ct: 01500|TR LOSS: 3.78 LM NLL: 3.32, KL: 66.01|DE LOSS: 3.62|BETA: 0.012677\n",
      "076[s], 08[s], Ep: 58, Ct: 02000|TR LOSS: 3.78 LM NLL: 3.32, KL: 66.00|DE LOSS: 3.63|BETA: 0.012712\n",
      "076[s], 08[s], Ep: 58, Ct: 02500|TR LOSS: 3.78 LM NLL: 3.32, KL: 65.99|DE LOSS: 3.62|BETA: 0.012746\n",
      "060[s], 08[s], Ep: 59, Ct: 00000|TR LOSS: 3.78 LM NLL: 3.32, KL: 65.98|DE LOSS: 3.65|BETA: 0.012773\n",
      "076[s], 08[s], Ep: 59, Ct: 00500|TR LOSS: 3.78 LM NLL: 3.32, KL: 65.97|DE LOSS: 3.63|BETA: 0.012807\n",
      "075[s], 08[s], Ep: 59, Ct: 01000|TR LOSS: 3.78 LM NLL: 3.32, KL: 65.96|DE LOSS: 3.63|BETA: 0.012841\n",
      "075[s], 08[s], Ep: 59, Ct: 01500|TR LOSS: 3.78 LM NLL: 3.31, KL: 65.94|DE LOSS: 3.63|BETA: 0.012876\n",
      "075[s], 08[s], Ep: 59, Ct: 02000|TR LOSS: 3.78 LM NLL: 3.31, KL: 65.93|DE LOSS: 3.64|BETA: 0.012910\n",
      "075[s], 07[s], Ep: 59, Ct: 02500|TR LOSS: 3.78 LM NLL: 3.31, KL: 65.92|DE LOSS: 3.63|BETA: 0.012945\n",
      "060[s], 08[s], Ep: 60, Ct: 00000|TR LOSS: 3.78 LM NLL: 3.31, KL: 65.91|DE LOSS: 3.64|BETA: 0.012971\n",
      "076[s], 08[s], Ep: 60, Ct: 00500|TR LOSS: 3.78 LM NLL: 3.31, KL: 65.90|DE LOSS: 3.63|BETA: 0.013006\n",
      "076[s], 08[s], Ep: 60, Ct: 01000|TR LOSS: 3.77 LM NLL: 3.30, KL: 65.89|DE LOSS: 3.66|BETA: 0.013040\n",
      "076[s], 08[s], Ep: 60, Ct: 01500|TR LOSS: 3.77 LM NLL: 3.30, KL: 65.87|DE LOSS: 3.64|BETA: 0.013075\n",
      "076[s], 08[s], Ep: 60, Ct: 02000|TR LOSS: 3.77 LM NLL: 3.30, KL: 65.86|DE LOSS: 3.64|BETA: 0.013109\n",
      "075[s], 08[s], Ep: 60, Ct: 02500|TR LOSS: 3.77 LM NLL: 3.30, KL: 65.85|DE LOSS: 3.65|BETA: 0.013144\n",
      "060[s], 08[s], Ep: 61, Ct: 00000|TR LOSS: 3.77 LM NLL: 3.30, KL: 65.84|DE LOSS: 3.64|BETA: 0.013170\n",
      "076[s], 08[s], Ep: 61, Ct: 00500|TR LOSS: 3.77 LM NLL: 3.30, KL: 65.83|DE LOSS: 3.64|BETA: 0.013204\n",
      "076[s], 08[s], Ep: 61, Ct: 01000|TR LOSS: 3.77 LM NLL: 3.29, KL: 65.81|DE LOSS: 3.65|BETA: 0.013239\n",
      "076[s], 08[s], Ep: 61, Ct: 01500|TR LOSS: 3.77 LM NLL: 3.29, KL: 65.80|DE LOSS: 3.64|BETA: 0.013273\n",
      "075[s], 08[s], Ep: 61, Ct: 02000|TR LOSS: 3.77 LM NLL: 3.29, KL: 65.79|DE LOSS: 3.65|BETA: 0.013308\n",
      "075[s], 08[s], Ep: 61, Ct: 02500|TR LOSS: 3.77 LM NLL: 3.29, KL: 65.77|DE LOSS: 3.65|BETA: 0.013342\n",
      "059[s], 08[s], Ep: 62, Ct: 00000|TR LOSS: 3.77 LM NLL: 3.29, KL: 65.76|DE LOSS: 3.64|BETA: 0.013369\n",
      "076[s], 08[s], Ep: 62, Ct: 00500|TR LOSS: 3.76 LM NLL: 3.28, KL: 65.75|DE LOSS: 3.66|BETA: 0.013403\n",
      "075[s], 08[s], Ep: 62, Ct: 01000|TR LOSS: 3.76 LM NLL: 3.28, KL: 65.74|DE LOSS: 3.66|BETA: 0.013438\n",
      "076[s], 08[s], Ep: 62, Ct: 01500|TR LOSS: 3.76 LM NLL: 3.28, KL: 65.73|DE LOSS: 3.64|BETA: 0.013472\n",
      "075[s], 08[s], Ep: 62, Ct: 02000|TR LOSS: 3.76 LM NLL: 3.28, KL: 65.71|DE LOSS: 3.65|BETA: 0.013507\n",
      "076[s], 08[s], Ep: 62, Ct: 02500|TR LOSS: 3.76 LM NLL: 3.28, KL: 65.70|DE LOSS: 3.65|BETA: 0.013541\n",
      "060[s], 08[s], Ep: 63, Ct: 00000|TR LOSS: 3.76 LM NLL: 3.28, KL: 65.69|DE LOSS: 3.67|BETA: 0.013568\n",
      "075[s], 08[s], Ep: 63, Ct: 00500|TR LOSS: 3.76 LM NLL: 3.27, KL: 65.68|DE LOSS: 3.65|BETA: 0.013602\n",
      "076[s], 08[s], Ep: 63, Ct: 01000|TR LOSS: 3.76 LM NLL: 3.27, KL: 65.66|DE LOSS: 3.65|BETA: 0.013636\n",
      "076[s], 08[s], Ep: 63, Ct: 01500|TR LOSS: 3.76 LM NLL: 3.27, KL: 65.65|DE LOSS: 3.65|BETA: 0.013671\n",
      "074[s], 08[s], Ep: 63, Ct: 02000|TR LOSS: 3.76 LM NLL: 3.27, KL: 65.64|DE LOSS: 3.65|BETA: 0.013705\n",
      "076[s], 08[s], Ep: 63, Ct: 02500|TR LOSS: 3.76 LM NLL: 3.27, KL: 65.62|DE LOSS: 3.67|BETA: 0.013740\n",
      "060[s], 08[s], Ep: 64, Ct: 00000|TR LOSS: 3.76 LM NLL: 3.27, KL: 65.61|DE LOSS: 3.68|BETA: 0.013766\n",
      "075[s], 08[s], Ep: 64, Ct: 00500|TR LOSS: 3.75 LM NLL: 3.26, KL: 65.60|DE LOSS: 3.67|BETA: 0.013801\n",
      "075[s], 08[s], Ep: 64, Ct: 01000|TR LOSS: 3.75 LM NLL: 3.26, KL: 65.58|DE LOSS: 3.69|BETA: 0.013835\n",
      "076[s], 08[s], Ep: 64, Ct: 01500|TR LOSS: 3.75 LM NLL: 3.26, KL: 65.57|DE LOSS: 3.67|BETA: 0.013870\n",
      "075[s], 08[s], Ep: 64, Ct: 02000|TR LOSS: 3.75 LM NLL: 3.26, KL: 65.56|DE LOSS: 3.66|BETA: 0.013904\n",
      "075[s], 08[s], Ep: 64, Ct: 02500|TR LOSS: 3.75 LM NLL: 3.26, KL: 65.54|DE LOSS: 3.68|BETA: 0.013939\n",
      "060[s], 08[s], Ep: 65, Ct: 00000|TR LOSS: 3.75 LM NLL: 3.26, KL: 65.53|DE LOSS: 3.66|BETA: 0.013965\n",
      "075[s], 08[s], Ep: 65, Ct: 00500|TR LOSS: 3.75 LM NLL: 3.25, KL: 65.52|DE LOSS: 3.67|BETA: 0.014000\n",
      "075[s], 08[s], Ep: 65, Ct: 01000|TR LOSS: 3.75 LM NLL: 3.25, KL: 65.50|DE LOSS: 3.67|BETA: 0.014034\n",
      "075[s], 08[s], Ep: 65, Ct: 01500|TR LOSS: 3.75 LM NLL: 3.25, KL: 65.49|DE LOSS: 3.68|BETA: 0.014068\n",
      "076[s], 08[s], Ep: 65, Ct: 02000|TR LOSS: 3.75 LM NLL: 3.25, KL: 65.48|DE LOSS: 3.67|BETA: 0.014103\n",
      "075[s], 08[s], Ep: 65, Ct: 02500|TR LOSS: 3.75 LM NLL: 3.25, KL: 65.46|DE LOSS: 3.67|BETA: 0.014137\n",
      "060[s], 08[s], Ep: 66, Ct: 00000|TR LOSS: 3.75 LM NLL: 3.25, KL: 65.45|DE LOSS: 3.68|BETA: 0.014164\n",
      "075[s], 08[s], Ep: 66, Ct: 00500|TR LOSS: 3.75 LM NLL: 3.24, KL: 65.44|DE LOSS: 3.68|BETA: 0.014198\n",
      "075[s], 08[s], Ep: 66, Ct: 01000|TR LOSS: 3.75 LM NLL: 3.24, KL: 65.42|DE LOSS: 3.68|BETA: 0.014233\n",
      "075[s], 08[s], Ep: 66, Ct: 01500|TR LOSS: 3.74 LM NLL: 3.24, KL: 65.41|DE LOSS: 3.68|BETA: 0.014267\n",
      "075[s], 08[s], Ep: 66, Ct: 02000|TR LOSS: 3.74 LM NLL: 3.24, KL: 65.39|DE LOSS: 3.68|BETA: 0.014302\n",
      "075[s], 08[s], Ep: 66, Ct: 02500|TR LOSS: 3.74 LM NLL: 3.24, KL: 65.38|DE LOSS: 3.69|BETA: 0.014336\n",
      "059[s], 08[s], Ep: 67, Ct: 00000|TR LOSS: 3.74 LM NLL: 3.24, KL: 65.37|DE LOSS: 3.69|BETA: 0.014363\n",
      "075[s], 08[s], Ep: 67, Ct: 00500|TR LOSS: 3.74 LM NLL: 3.24, KL: 65.35|DE LOSS: 3.71|BETA: 0.014397\n",
      "075[s], 08[s], Ep: 67, Ct: 01000|TR LOSS: 3.74 LM NLL: 3.23, KL: 65.34|DE LOSS: 3.69|BETA: 0.014432\n",
      "075[s], 08[s], Ep: 67, Ct: 01500|TR LOSS: 3.74 LM NLL: 3.23, KL: 65.33|DE LOSS: 3.68|BETA: 0.014466\n",
      "075[s], 08[s], Ep: 67, Ct: 02000|TR LOSS: 3.74 LM NLL: 3.23, KL: 65.31|DE LOSS: 3.69|BETA: 0.014500\n",
      "075[s], 08[s], Ep: 67, Ct: 02500|TR LOSS: 3.74 LM NLL: 3.23, KL: 65.30|DE LOSS: 3.71|BETA: 0.014535\n",
      "060[s], 08[s], Ep: 68, Ct: 00000|TR LOSS: 3.74 LM NLL: 3.23, KL: 65.28|DE LOSS: 3.69|BETA: 0.014561\n",
      "075[s], 08[s], Ep: 68, Ct: 00500|TR LOSS: 3.74 LM NLL: 3.23, KL: 65.27|DE LOSS: 3.71|BETA: 0.014596\n",
      "075[s], 08[s], Ep: 68, Ct: 01000|TR LOSS: 3.74 LM NLL: 3.22, KL: 65.26|DE LOSS: 3.69|BETA: 0.014630\n",
      "075[s], 08[s], Ep: 68, Ct: 01500|TR LOSS: 3.74 LM NLL: 3.22, KL: 65.24|DE LOSS: 3.70|BETA: 0.014665\n",
      "075[s], 08[s], Ep: 68, Ct: 02000|TR LOSS: 3.74 LM NLL: 3.22, KL: 65.23|DE LOSS: 3.76|BETA: 0.014699\n",
      "075[s], 08[s], Ep: 68, Ct: 02500|TR LOSS: 3.74 LM NLL: 3.22, KL: 65.21|DE LOSS: 3.71|BETA: 0.014734\n",
      "059[s], 08[s], Ep: 69, Ct: 00000|TR LOSS: 3.73 LM NLL: 3.22, KL: 65.20|DE LOSS: 3.71|BETA: 0.014760\n",
      "075[s], 08[s], Ep: 69, Ct: 00500|TR LOSS: 3.73 LM NLL: 3.22, KL: 65.19|DE LOSS: 3.70|BETA: 0.014795\n",
      "075[s], 08[s], Ep: 69, Ct: 01000|TR LOSS: 3.73 LM NLL: 3.22, KL: 65.17|DE LOSS: 3.70|BETA: 0.014829\n",
      "075[s], 08[s], Ep: 69, Ct: 01500|TR LOSS: 3.73 LM NLL: 3.21, KL: 65.16|DE LOSS: 3.70|BETA: 0.014863\n",
      "075[s], 07[s], Ep: 69, Ct: 02000|TR LOSS: 3.73 LM NLL: 3.21, KL: 65.14|DE LOSS: 3.72|BETA: 0.014898\n",
      "075[s], 08[s], Ep: 69, Ct: 02500|TR LOSS: 3.73 LM NLL: 3.21, KL: 65.13|DE LOSS: 3.71|BETA: 0.014932\n",
      "060[s], 08[s], Ep: 70, Ct: 00000|TR LOSS: 3.73 LM NLL: 3.21, KL: 65.11|DE LOSS: 3.71|BETA: 0.014959\n",
      "075[s], 07[s], Ep: 70, Ct: 00500|TR LOSS: 3.73 LM NLL: 3.21, KL: 65.10|DE LOSS: 3.71|BETA: 0.014993\n",
      "075[s], 08[s], Ep: 70, Ct: 01000|TR LOSS: 3.73 LM NLL: 3.21, KL: 65.08|DE LOSS: 3.72|BETA: 0.015028\n",
      "075[s], 08[s], Ep: 70, Ct: 01500|TR LOSS: 3.73 LM NLL: 3.21, KL: 65.07|DE LOSS: 3.71|BETA: 0.015062\n",
      "075[s], 08[s], Ep: 70, Ct: 02000|TR LOSS: 3.73 LM NLL: 3.20, KL: 65.05|DE LOSS: 3.72|BETA: 0.015097\n",
      "075[s], 08[s], Ep: 70, Ct: 02500|TR LOSS: 3.73 LM NLL: 3.20, KL: 65.04|DE LOSS: 3.71|BETA: 0.015131\n",
      "060[s], 08[s], Ep: 71, Ct: 00000|TR LOSS: 3.73 LM NLL: 3.20, KL: 65.03|DE LOSS: 3.72|BETA: 0.015158\n",
      "075[s], 08[s], Ep: 71, Ct: 00500|TR LOSS: 3.73 LM NLL: 3.20, KL: 65.01|DE LOSS: 3.72|BETA: 0.015192\n",
      "075[s], 08[s], Ep: 71, Ct: 01000|TR LOSS: 3.73 LM NLL: 3.20, KL: 65.00|DE LOSS: 3.72|BETA: 0.015227\n",
      "075[s], 08[s], Ep: 71, Ct: 01500|TR LOSS: 3.73 LM NLL: 3.20, KL: 64.98|DE LOSS: 3.72|BETA: 0.015261\n",
      "075[s], 08[s], Ep: 71, Ct: 02000|TR LOSS: 3.73 LM NLL: 3.20, KL: 64.96|DE LOSS: 3.72|BETA: 0.015295\n",
      "075[s], 08[s], Ep: 71, Ct: 02500|TR LOSS: 3.72 LM NLL: 3.19, KL: 64.95|DE LOSS: 3.73|BETA: 0.015330\n",
      "059[s], 08[s], Ep: 72, Ct: 00000|TR LOSS: 3.72 LM NLL: 3.19, KL: 64.94|DE LOSS: 3.73|BETA: 0.015356\n",
      "075[s], 08[s], Ep: 72, Ct: 00500|TR LOSS: 3.72 LM NLL: 3.19, KL: 64.92|DE LOSS: 3.73|BETA: 0.015391\n",
      "075[s], 08[s], Ep: 72, Ct: 01000|TR LOSS: 3.72 LM NLL: 3.19, KL: 64.91|DE LOSS: 3.72|BETA: 0.015425\n",
      "075[s], 08[s], Ep: 72, Ct: 01500|TR LOSS: 3.72 LM NLL: 3.19, KL: 64.89|DE LOSS: 3.73|BETA: 0.015460\n",
      "075[s], 08[s], Ep: 72, Ct: 02000|TR LOSS: 3.72 LM NLL: 3.19, KL: 64.88|DE LOSS: 3.73|BETA: 0.015494\n",
      "075[s], 08[s], Ep: 72, Ct: 02500|TR LOSS: 3.72 LM NLL: 3.19, KL: 64.86|DE LOSS: 3.73|BETA: 0.015529\n",
      "060[s], 08[s], Ep: 73, Ct: 00000|TR LOSS: 3.72 LM NLL: 3.18, KL: 64.85|DE LOSS: 3.74|BETA: 0.015555\n",
      "075[s], 08[s], Ep: 73, Ct: 00500|TR LOSS: 3.72 LM NLL: 3.18, KL: 64.83|DE LOSS: 3.73|BETA: 0.015590\n",
      "075[s], 08[s], Ep: 73, Ct: 01000|TR LOSS: 3.72 LM NLL: 3.18, KL: 64.82|DE LOSS: 3.74|BETA: 0.015624\n",
      "075[s], 08[s], Ep: 73, Ct: 01500|TR LOSS: 3.72 LM NLL: 3.18, KL: 64.80|DE LOSS: 3.74|BETA: 0.015659\n",
      "075[s], 08[s], Ep: 73, Ct: 02000|TR LOSS: 3.72 LM NLL: 3.18, KL: 64.78|DE LOSS: 3.74|BETA: 0.015693\n",
      "075[s], 07[s], Ep: 73, Ct: 02500|TR LOSS: 3.72 LM NLL: 3.18, KL: 64.77|DE LOSS: 3.74|BETA: 0.015727\n",
      "060[s], 08[s], Ep: 74, Ct: 00000|TR LOSS: 3.72 LM NLL: 3.18, KL: 64.76|DE LOSS: 3.75|BETA: 0.015754\n",
      "075[s], 08[s], Ep: 74, Ct: 00500|TR LOSS: 3.72 LM NLL: 3.18, KL: 64.74|DE LOSS: 3.75|BETA: 0.015788\n",
      "074[s], 08[s], Ep: 74, Ct: 01000|TR LOSS: 3.72 LM NLL: 3.17, KL: 64.73|DE LOSS: 3.74|BETA: 0.015823\n",
      "075[s], 08[s], Ep: 74, Ct: 01500|TR LOSS: 3.72 LM NLL: 3.17, KL: 64.71|DE LOSS: 3.76|BETA: 0.015857\n",
      "075[s], 08[s], Ep: 74, Ct: 02000|TR LOSS: 3.72 LM NLL: 3.17, KL: 64.69|DE LOSS: 3.75|BETA: 0.015892\n",
      "075[s], 08[s], Ep: 74, Ct: 02500|TR LOSS: 3.72 LM NLL: 3.17, KL: 64.68|DE LOSS: 3.75|BETA: 0.015926\n",
      "060[s], 08[s], Ep: 75, Ct: 00000|TR LOSS: 3.72 LM NLL: 3.17, KL: 64.67|DE LOSS: 3.74|BETA: 0.015953\n",
      "075[s], 08[s], Ep: 75, Ct: 00500|TR LOSS: 3.71 LM NLL: 3.17, KL: 64.65|DE LOSS: 3.76|BETA: 0.015987\n",
      "075[s], 08[s], Ep: 75, Ct: 01000|TR LOSS: 3.71 LM NLL: 3.17, KL: 64.63|DE LOSS: 3.75|BETA: 0.016022\n",
      "076[s], 08[s], Ep: 75, Ct: 01500|TR LOSS: 3.71 LM NLL: 3.16, KL: 64.62|DE LOSS: 3.75|BETA: 0.016056\n",
      "075[s], 08[s], Ep: 75, Ct: 02000|TR LOSS: 3.71 LM NLL: 3.16, KL: 64.60|DE LOSS: 3.77|BETA: 0.016090\n",
      "075[s], 08[s], Ep: 75, Ct: 02500|TR LOSS: 3.71 LM NLL: 3.16, KL: 64.59|DE LOSS: 3.75|BETA: 0.016125\n",
      "060[s], 08[s], Ep: 76, Ct: 00000|TR LOSS: 3.71 LM NLL: 3.16, KL: 64.57|DE LOSS: 3.77|BETA: 0.016151\n",
      "075[s], 08[s], Ep: 76, Ct: 00500|TR LOSS: 3.71 LM NLL: 3.16, KL: 64.56|DE LOSS: 3.76|BETA: 0.016186\n",
      "075[s], 08[s], Ep: 76, Ct: 01000|TR LOSS: 3.71 LM NLL: 3.16, KL: 64.54|DE LOSS: 3.77|BETA: 0.016220\n",
      "076[s], 08[s], Ep: 76, Ct: 01500|TR LOSS: 3.71 LM NLL: 3.16, KL: 64.53|DE LOSS: 3.76|BETA: 0.016255\n",
      "075[s], 08[s], Ep: 76, Ct: 02000|TR LOSS: 3.71 LM NLL: 3.16, KL: 64.51|DE LOSS: 3.76|BETA: 0.016289\n",
      "075[s], 08[s], Ep: 76, Ct: 02500|TR LOSS: 3.71 LM NLL: 3.15, KL: 64.49|DE LOSS: 3.77|BETA: 0.016324\n",
      "060[s], 08[s], Ep: 77, Ct: 00000|TR LOSS: 3.71 LM NLL: 3.15, KL: 64.48|DE LOSS: 3.76|BETA: 0.016350\n",
      "075[s], 08[s], Ep: 77, Ct: 00500|TR LOSS: 3.71 LM NLL: 3.15, KL: 64.47|DE LOSS: 3.78|BETA: 0.016385\n",
      "075[s], 08[s], Ep: 77, Ct: 01000|TR LOSS: 3.71 LM NLL: 3.15, KL: 64.45|DE LOSS: 3.77|BETA: 0.016419\n",
      "075[s], 08[s], Ep: 77, Ct: 01500|TR LOSS: 3.71 LM NLL: 3.15, KL: 64.43|DE LOSS: 3.77|BETA: 0.016454\n",
      "076[s], 08[s], Ep: 77, Ct: 02000|TR LOSS: 3.71 LM NLL: 3.15, KL: 64.42|DE LOSS: 3.77|BETA: 0.016488\n",
      "075[s], 08[s], Ep: 77, Ct: 02500|TR LOSS: 3.71 LM NLL: 3.15, KL: 64.40|DE LOSS: 3.78|BETA: 0.016522\n",
      "059[s], 08[s], Ep: 78, Ct: 00000|TR LOSS: 3.71 LM NLL: 3.15, KL: 64.39|DE LOSS: 3.77|BETA: 0.016549\n",
      "075[s], 08[s], Ep: 78, Ct: 00500|TR LOSS: 3.71 LM NLL: 3.14, KL: 64.37|DE LOSS: 3.76|BETA: 0.016583\n",
      "076[s], 08[s], Ep: 78, Ct: 01000|TR LOSS: 3.71 LM NLL: 3.14, KL: 64.36|DE LOSS: 3.78|BETA: 0.016618\n",
      "075[s], 08[s], Ep: 78, Ct: 01500|TR LOSS: 3.71 LM NLL: 3.14, KL: 64.34|DE LOSS: 3.79|BETA: 0.016652\n",
      "075[s], 08[s], Ep: 78, Ct: 02000|TR LOSS: 3.71 LM NLL: 3.14, KL: 64.32|DE LOSS: 3.79|BETA: 0.016687\n",
      "076[s], 08[s], Ep: 78, Ct: 02500|TR LOSS: 3.71 LM NLL: 3.14, KL: 64.31|DE LOSS: 3.78|BETA: 0.016721\n",
      "060[s], 08[s], Ep: 79, Ct: 00000|TR LOSS: 3.71 LM NLL: 3.14, KL: 64.29|DE LOSS: 3.78|BETA: 0.016748\n",
      "075[s], 08[s], Ep: 79, Ct: 00500|TR LOSS: 3.70 LM NLL: 3.14, KL: 64.28|DE LOSS: 3.78|BETA: 0.016782\n",
      "076[s], 08[s], Ep: 79, Ct: 01000|TR LOSS: 3.70 LM NLL: 3.14, KL: 64.26|DE LOSS: 3.78|BETA: 0.016817\n",
      "076[s], 08[s], Ep: 79, Ct: 01500|TR LOSS: 3.70 LM NLL: 3.14, KL: 64.25|DE LOSS: 3.79|BETA: 0.016851\n",
      "075[s], 08[s], Ep: 79, Ct: 02000|TR LOSS: 3.70 LM NLL: 3.13, KL: 64.23|DE LOSS: 3.79|BETA: 0.016886\n",
      "075[s], 08[s], Ep: 79, Ct: 02500|TR LOSS: 3.70 LM NLL: 3.13, KL: 64.21|DE LOSS: 3.79|BETA: 0.016920\n",
      "060[s], 08[s], Ep: 80, Ct: 00000|TR LOSS: 3.70 LM NLL: 3.13, KL: 64.20|DE LOSS: 3.79|BETA: 0.016946\n",
      "075[s], 08[s], Ep: 80, Ct: 00500|TR LOSS: 3.70 LM NLL: 3.13, KL: 64.18|DE LOSS: 3.80|BETA: 0.016981\n",
      "076[s], 08[s], Ep: 80, Ct: 01000|TR LOSS: 3.70 LM NLL: 3.13, KL: 64.17|DE LOSS: 3.80|BETA: 0.017015\n",
      "076[s], 08[s], Ep: 80, Ct: 01500|TR LOSS: 3.70 LM NLL: 3.13, KL: 64.15|DE LOSS: 3.79|BETA: 0.017050\n",
      "076[s], 08[s], Ep: 80, Ct: 02000|TR LOSS: 3.70 LM NLL: 3.13, KL: 64.13|DE LOSS: 3.80|BETA: 0.017084\n",
      "076[s], 08[s], Ep: 80, Ct: 02500|TR LOSS: 3.70 LM NLL: 3.13, KL: 64.12|DE LOSS: 3.80|BETA: 0.017119\n",
      "060[s], 08[s], Ep: 81, Ct: 00000|TR LOSS: 3.70 LM NLL: 3.12, KL: 64.10|DE LOSS: 3.80|BETA: 0.017145\n",
      "075[s], 07[s], Ep: 81, Ct: 00500|TR LOSS: 3.70 LM NLL: 3.12, KL: 64.09|DE LOSS: 3.80|BETA: 0.017180\n",
      "075[s], 08[s], Ep: 81, Ct: 01000|TR LOSS: 3.70 LM NLL: 3.12, KL: 64.07|DE LOSS: 3.80|BETA: 0.017214\n",
      "075[s], 08[s], Ep: 81, Ct: 01500|TR LOSS: 3.70 LM NLL: 3.12, KL: 64.05|DE LOSS: 3.80|BETA: 0.017249\n",
      "075[s], 08[s], Ep: 81, Ct: 02000|TR LOSS: 3.70 LM NLL: 3.12, KL: 64.04|DE LOSS: 3.81|BETA: 0.017283\n",
      "075[s], 08[s], Ep: 81, Ct: 02500|TR LOSS: 3.70 LM NLL: 3.12, KL: 64.02|DE LOSS: 3.80|BETA: 0.017318\n",
      "060[s], 08[s], Ep: 82, Ct: 00000|TR LOSS: 3.70 LM NLL: 3.12, KL: 64.01|DE LOSS: 3.82|BETA: 0.017344\n",
      "076[s], 08[s], Ep: 82, Ct: 00500|TR LOSS: 3.70 LM NLL: 3.12, KL: 63.99|DE LOSS: 3.81|BETA: 0.017378\n",
      "076[s], 08[s], Ep: 82, Ct: 01000|TR LOSS: 3.70 LM NLL: 3.12, KL: 63.97|DE LOSS: 3.81|BETA: 0.017413\n",
      "075[s], 08[s], Ep: 82, Ct: 01500|TR LOSS: 3.70 LM NLL: 3.11, KL: 63.96|DE LOSS: 3.82|BETA: 0.017447\n",
      "075[s], 08[s], Ep: 82, Ct: 02000|TR LOSS: 3.70 LM NLL: 3.11, KL: 63.94|DE LOSS: 3.82|BETA: 0.017482\n",
      "075[s], 08[s], Ep: 82, Ct: 02500|TR LOSS: 3.70 LM NLL: 3.11, KL: 63.92|DE LOSS: 3.83|BETA: 0.017516\n",
      "060[s], 08[s], Ep: 83, Ct: 00000|TR LOSS: 3.70 LM NLL: 3.11, KL: 63.91|DE LOSS: 3.81|BETA: 0.017543\n",
      "076[s], 08[s], Ep: 83, Ct: 00500|TR LOSS: 3.70 LM NLL: 3.11, KL: 63.89|DE LOSS: 3.84|BETA: 0.017577\n",
      "075[s], 08[s], Ep: 83, Ct: 01000|TR LOSS: 3.70 LM NLL: 3.11, KL: 63.88|DE LOSS: 3.82|BETA: 0.017612\n",
      "075[s], 08[s], Ep: 83, Ct: 01500|TR LOSS: 3.70 LM NLL: 3.11, KL: 63.86|DE LOSS: 3.85|BETA: 0.017646\n",
      "075[s], 08[s], Ep: 83, Ct: 02000|TR LOSS: 3.70 LM NLL: 3.11, KL: 63.84|DE LOSS: 3.82|BETA: 0.017681\n",
      "075[s], 08[s], Ep: 83, Ct: 02500|TR LOSS: 3.70 LM NLL: 3.11, KL: 63.83|DE LOSS: 3.83|BETA: 0.017715\n",
      "059[s], 08[s], Ep: 84, Ct: 00000|TR LOSS: 3.70 LM NLL: 3.10, KL: 63.81|DE LOSS: 3.83|BETA: 0.017741\n",
      "076[s], 08[s], Ep: 84, Ct: 00500|TR LOSS: 3.69 LM NLL: 3.10, KL: 63.80|DE LOSS: 3.83|BETA: 0.017776\n",
      "075[s], 08[s], Ep: 84, Ct: 01000|TR LOSS: 3.69 LM NLL: 3.10, KL: 63.78|DE LOSS: 3.83|BETA: 0.017810\n",
      "076[s], 08[s], Ep: 84, Ct: 01500|TR LOSS: 3.69 LM NLL: 3.10, KL: 63.76|DE LOSS: 3.82|BETA: 0.017845\n",
      "075[s], 08[s], Ep: 84, Ct: 02000|TR LOSS: 3.69 LM NLL: 3.10, KL: 63.75|DE LOSS: 3.84|BETA: 0.017879\n",
      "076[s], 08[s], Ep: 84, Ct: 02500|TR LOSS: 3.69 LM NLL: 3.10, KL: 63.73|DE LOSS: 3.84|BETA: 0.017914\n",
      "060[s], 08[s], Ep: 85, Ct: 00000|TR LOSS: 3.69 LM NLL: 3.10, KL: 63.72|DE LOSS: 3.84|BETA: 0.017940\n",
      "075[s], 08[s], Ep: 85, Ct: 00500|TR LOSS: 3.69 LM NLL: 3.10, KL: 63.70|DE LOSS: 3.84|BETA: 0.017975\n",
      "075[s], 08[s], Ep: 85, Ct: 01000|TR LOSS: 3.69 LM NLL: 3.10, KL: 63.68|DE LOSS: 3.83|BETA: 0.018009\n",
      "075[s], 08[s], Ep: 85, Ct: 01500|TR LOSS: 3.69 LM NLL: 3.10, KL: 63.67|DE LOSS: 3.83|BETA: 0.018044\n",
      "076[s], 08[s], Ep: 85, Ct: 02000|TR LOSS: 3.69 LM NLL: 3.09, KL: 63.65|DE LOSS: 3.83|BETA: 0.018078\n",
      "075[s], 08[s], Ep: 85, Ct: 02500|TR LOSS: 3.69 LM NLL: 3.09, KL: 63.63|DE LOSS: 3.84|BETA: 0.018113\n",
      "060[s], 08[s], Ep: 86, Ct: 00000|TR LOSS: 3.69 LM NLL: 3.09, KL: 63.62|DE LOSS: 3.84|BETA: 0.018139\n",
      "076[s], 08[s], Ep: 86, Ct: 00500|TR LOSS: 3.69 LM NLL: 3.09, KL: 63.60|DE LOSS: 3.84|BETA: 0.018173\n",
      "076[s], 08[s], Ep: 86, Ct: 01000|TR LOSS: 3.69 LM NLL: 3.09, KL: 63.58|DE LOSS: 3.84|BETA: 0.018208\n",
      "076[s], 08[s], Ep: 86, Ct: 01500|TR LOSS: 3.69 LM NLL: 3.09, KL: 63.57|DE LOSS: 3.84|BETA: 0.018242\n",
      "076[s], 08[s], Ep: 86, Ct: 02000|TR LOSS: 3.69 LM NLL: 3.09, KL: 63.55|DE LOSS: 3.86|BETA: 0.018277\n",
      "075[s], 08[s], Ep: 86, Ct: 02500|TR LOSS: 3.69 LM NLL: 3.09, KL: 63.53|DE LOSS: 3.85|BETA: 0.018311\n",
      "060[s], 08[s], Ep: 87, Ct: 00000|TR LOSS: 3.69 LM NLL: 3.09, KL: 63.52|DE LOSS: 3.85|BETA: 0.018338\n",
      "076[s], 08[s], Ep: 87, Ct: 00500|TR LOSS: 3.69 LM NLL: 3.08, KL: 63.50|DE LOSS: 3.85|BETA: 0.018372\n",
      "076[s], 08[s], Ep: 87, Ct: 01000|TR LOSS: 3.69 LM NLL: 3.08, KL: 63.49|DE LOSS: 3.85|BETA: 0.018407\n",
      "075[s], 08[s], Ep: 87, Ct: 01500|TR LOSS: 3.69 LM NLL: 3.08, KL: 63.47|DE LOSS: 3.86|BETA: 0.018441\n",
      "075[s], 08[s], Ep: 87, Ct: 02000|TR LOSS: 3.69 LM NLL: 3.08, KL: 63.45|DE LOSS: 3.86|BETA: 0.018476\n",
      "075[s], 08[s], Ep: 87, Ct: 02500|TR LOSS: 3.69 LM NLL: 3.08, KL: 63.43|DE LOSS: 3.86|BETA: 0.018510\n",
      "060[s], 08[s], Ep: 88, Ct: 00000|TR LOSS: 3.69 LM NLL: 3.08, KL: 63.42|DE LOSS: 3.86|BETA: 0.018537\n",
      "076[s], 08[s], Ep: 88, Ct: 00500|TR LOSS: 3.69 LM NLL: 3.08, KL: 63.40|DE LOSS: 3.87|BETA: 0.018571\n",
      "076[s], 08[s], Ep: 88, Ct: 01000|TR LOSS: 3.69 LM NLL: 3.08, KL: 63.39|DE LOSS: 3.87|BETA: 0.018605\n",
      "076[s], 08[s], Ep: 88, Ct: 01500|TR LOSS: 3.69 LM NLL: 3.08, KL: 63.37|DE LOSS: 3.86|BETA: 0.018640\n",
      "076[s], 08[s], Ep: 88, Ct: 02000|TR LOSS: 3.69 LM NLL: 3.08, KL: 63.35|DE LOSS: 3.86|BETA: 0.018674\n",
      "076[s], 08[s], Ep: 88, Ct: 02500|TR LOSS: 3.69 LM NLL: 3.07, KL: 63.34|DE LOSS: 3.86|BETA: 0.018709\n",
      "060[s], 08[s], Ep: 89, Ct: 00000|TR LOSS: 3.69 LM NLL: 3.07, KL: 63.32|DE LOSS: 3.89|BETA: 0.018735\n",
      "076[s], 08[s], Ep: 89, Ct: 00500|TR LOSS: 3.69 LM NLL: 3.07, KL: 63.31|DE LOSS: 3.87|BETA: 0.018770\n",
      "075[s], 08[s], Ep: 89, Ct: 01000|TR LOSS: 3.69 LM NLL: 3.07, KL: 63.29|DE LOSS: 3.88|BETA: 0.018804\n",
      "076[s], 08[s], Ep: 89, Ct: 01500|TR LOSS: 3.69 LM NLL: 3.07, KL: 63.27|DE LOSS: 3.88|BETA: 0.018839\n",
      "076[s], 08[s], Ep: 89, Ct: 02000|TR LOSS: 3.69 LM NLL: 3.07, KL: 63.25|DE LOSS: 3.87|BETA: 0.018873\n",
      "075[s], 08[s], Ep: 89, Ct: 02500|TR LOSS: 3.69 LM NLL: 3.07, KL: 63.24|DE LOSS: 3.87|BETA: 0.018908\n",
      "060[s], 08[s], Ep: 90, Ct: 00000|TR LOSS: 3.69 LM NLL: 3.07, KL: 63.22|DE LOSS: 3.87|BETA: 0.018934\n",
      "075[s], 08[s], Ep: 90, Ct: 00500|TR LOSS: 3.69 LM NLL: 3.07, KL: 63.21|DE LOSS: 3.88|BETA: 0.018969\n",
      "075[s], 08[s], Ep: 90, Ct: 01000|TR LOSS: 3.69 LM NLL: 3.07, KL: 63.19|DE LOSS: 3.89|BETA: 0.019003\n",
      "075[s], 08[s], Ep: 90, Ct: 01500|TR LOSS: 3.69 LM NLL: 3.07, KL: 63.17|DE LOSS: 3.87|BETA: 0.019037\n",
      "075[s], 08[s], Ep: 90, Ct: 02000|TR LOSS: 3.69 LM NLL: 3.06, KL: 63.15|DE LOSS: 3.88|BETA: 0.019072\n",
      "075[s], 08[s], Ep: 90, Ct: 02500|TR LOSS: 3.69 LM NLL: 3.06, KL: 63.14|DE LOSS: 3.88|BETA: 0.019106\n",
      "060[s], 08[s], Ep: 91, Ct: 00000|TR LOSS: 3.69 LM NLL: 3.06, KL: 63.12|DE LOSS: 3.88|BETA: 0.019133\n",
      "075[s], 08[s], Ep: 91, Ct: 00500|TR LOSS: 3.69 LM NLL: 3.06, KL: 63.11|DE LOSS: 3.88|BETA: 0.019167\n",
      "075[s], 08[s], Ep: 91, Ct: 01000|TR LOSS: 3.69 LM NLL: 3.06, KL: 63.09|DE LOSS: 3.89|BETA: 0.019202\n",
      "075[s], 08[s], Ep: 91, Ct: 01500|TR LOSS: 3.68 LM NLL: 3.06, KL: 63.07|DE LOSS: 3.89|BETA: 0.019236\n",
      "075[s], 08[s], Ep: 91, Ct: 02000|TR LOSS: 3.68 LM NLL: 3.06, KL: 63.05|DE LOSS: 3.88|BETA: 0.019271\n",
      "076[s], 08[s], Ep: 91, Ct: 02500|TR LOSS: 3.68 LM NLL: 3.06, KL: 63.04|DE LOSS: 3.89|BETA: 0.019305\n",
      "060[s], 08[s], Ep: 92, Ct: 00000|TR LOSS: 3.68 LM NLL: 3.06, KL: 63.02|DE LOSS: 3.90|BETA: 0.019332\n",
      "075[s], 08[s], Ep: 92, Ct: 00500|TR LOSS: 3.68 LM NLL: 3.06, KL: 63.01|DE LOSS: 3.89|BETA: 0.019366\n",
      "075[s], 08[s], Ep: 92, Ct: 01000|TR LOSS: 3.68 LM NLL: 3.06, KL: 62.99|DE LOSS: 3.89|BETA: 0.019400\n",
      "076[s], 08[s], Ep: 92, Ct: 01500|TR LOSS: 3.68 LM NLL: 3.05, KL: 62.97|DE LOSS: 3.90|BETA: 0.019435\n",
      "075[s], 08[s], Ep: 92, Ct: 02000|TR LOSS: 3.68 LM NLL: 3.05, KL: 62.95|DE LOSS: 3.90|BETA: 0.019469\n",
      "076[s], 08[s], Ep: 92, Ct: 02500|TR LOSS: 3.68 LM NLL: 3.05, KL: 62.94|DE LOSS: 3.90|BETA: 0.019504\n",
      "060[s], 08[s], Ep: 93, Ct: 00000|TR LOSS: 3.68 LM NLL: 3.05, KL: 62.92|DE LOSS: 3.91|BETA: 0.019530\n",
      "075[s], 08[s], Ep: 93, Ct: 00500|TR LOSS: 3.68 LM NLL: 3.05, KL: 62.91|DE LOSS: 3.90|BETA: 0.019565\n",
      "076[s], 08[s], Ep: 93, Ct: 01000|TR LOSS: 3.68 LM NLL: 3.05, KL: 62.89|DE LOSS: 3.90|BETA: 0.019599\n",
      "075[s], 08[s], Ep: 93, Ct: 01500|TR LOSS: 3.68 LM NLL: 3.05, KL: 62.87|DE LOSS: 3.91|BETA: 0.019634\n",
      "075[s], 08[s], Ep: 93, Ct: 02000|TR LOSS: 3.68 LM NLL: 3.05, KL: 62.85|DE LOSS: 3.90|BETA: 0.019668\n",
      "076[s], 08[s], Ep: 93, Ct: 02500|TR LOSS: 3.68 LM NLL: 3.05, KL: 62.84|DE LOSS: 3.92|BETA: 0.019703\n",
      "060[s], 08[s], Ep: 94, Ct: 00000|TR LOSS: 3.68 LM NLL: 3.05, KL: 62.82|DE LOSS: 3.90|BETA: 0.019729\n",
      "076[s], 08[s], Ep: 94, Ct: 00500|TR LOSS: 3.68 LM NLL: 3.05, KL: 62.80|DE LOSS: 3.91|BETA: 0.019764\n",
      "075[s], 08[s], Ep: 94, Ct: 01000|TR LOSS: 3.68 LM NLL: 3.04, KL: 62.79|DE LOSS: 3.90|BETA: 0.019798\n",
      "075[s], 08[s], Ep: 94, Ct: 01500|TR LOSS: 3.68 LM NLL: 3.04, KL: 62.77|DE LOSS: 3.91|BETA: 0.019832\n",
      "075[s], 08[s], Ep: 94, Ct: 02000|TR LOSS: 3.68 LM NLL: 3.04, KL: 62.75|DE LOSS: 3.93|BETA: 0.019867\n",
      "076[s], 08[s], Ep: 94, Ct: 02500|TR LOSS: 3.68 LM NLL: 3.04, KL: 62.73|DE LOSS: 3.92|BETA: 0.019901\n",
      "060[s], 08[s], Ep: 95, Ct: 00000|TR LOSS: 3.68 LM NLL: 3.04, KL: 62.72|DE LOSS: 3.92|BETA: 0.019928\n",
      "075[s], 08[s], Ep: 95, Ct: 00500|TR LOSS: 3.68 LM NLL: 3.04, KL: 62.70|DE LOSS: 3.91|BETA: 0.019962\n",
      "075[s], 08[s], Ep: 95, Ct: 01000|TR LOSS: 3.68 LM NLL: 3.04, KL: 62.69|DE LOSS: 3.93|BETA: 0.019997\n",
      "076[s], 08[s], Ep: 95, Ct: 01500|TR LOSS: 3.68 LM NLL: 3.04, KL: 62.67|DE LOSS: 3.93|BETA: 0.020031\n",
      "075[s], 08[s], Ep: 95, Ct: 02000|TR LOSS: 3.68 LM NLL: 3.04, KL: 62.65|DE LOSS: 3.92|BETA: 0.020066\n",
      "076[s], 08[s], Ep: 95, Ct: 02500|TR LOSS: 3.68 LM NLL: 3.04, KL: 62.63|DE LOSS: 3.93|BETA: 0.020100\n",
      "060[s], 08[s], Ep: 96, Ct: 00000|TR LOSS: 3.68 LM NLL: 3.04, KL: 62.62|DE LOSS: 3.93|BETA: 0.020127\n",
      "076[s], 08[s], Ep: 96, Ct: 00500|TR LOSS: 3.68 LM NLL: 3.03, KL: 62.60|DE LOSS: 3.93|BETA: 0.020161\n",
      "076[s], 08[s], Ep: 96, Ct: 01000|TR LOSS: 3.68 LM NLL: 3.03, KL: 62.59|DE LOSS: 3.93|BETA: 0.020196\n",
      "075[s], 08[s], Ep: 96, Ct: 01500|TR LOSS: 3.68 LM NLL: 3.03, KL: 62.57|DE LOSS: 3.93|BETA: 0.020230\n",
      "075[s], 08[s], Ep: 96, Ct: 02000|TR LOSS: 3.68 LM NLL: 3.03, KL: 62.55|DE LOSS: 3.93|BETA: 0.020264\n",
      "075[s], 08[s], Ep: 96, Ct: 02500|TR LOSS: 3.68 LM NLL: 3.03, KL: 62.53|DE LOSS: 3.93|BETA: 0.020299\n",
      "060[s], 08[s], Ep: 97, Ct: 00000|TR LOSS: 3.68 LM NLL: 3.03, KL: 62.52|DE LOSS: 3.93|BETA: 0.020325\n",
      "075[s], 08[s], Ep: 97, Ct: 00500|TR LOSS: 3.68 LM NLL: 3.03, KL: 62.50|DE LOSS: 3.94|BETA: 0.020360\n",
      "076[s], 08[s], Ep: 97, Ct: 01000|TR LOSS: 3.68 LM NLL: 3.03, KL: 62.48|DE LOSS: 3.94|BETA: 0.020394\n",
      "076[s], 08[s], Ep: 97, Ct: 01500|TR LOSS: 3.68 LM NLL: 3.03, KL: 62.47|DE LOSS: 3.94|BETA: 0.020429\n",
      "076[s], 08[s], Ep: 97, Ct: 02000|TR LOSS: 3.68 LM NLL: 3.03, KL: 62.45|DE LOSS: 3.94|BETA: 0.020463\n",
      "075[s], 08[s], Ep: 97, Ct: 02500|TR LOSS: 3.68 LM NLL: 3.03, KL: 62.43|DE LOSS: 3.94|BETA: 0.020498\n",
      "060[s], 08[s], Ep: 98, Ct: 00000|TR LOSS: 3.68 LM NLL: 3.03, KL: 62.42|DE LOSS: 3.94|BETA: 0.020524\n",
      "075[s], 08[s], Ep: 98, Ct: 00500|TR LOSS: 3.68 LM NLL: 3.03, KL: 62.40|DE LOSS: 3.95|BETA: 0.020559\n",
      "075[s], 08[s], Ep: 98, Ct: 01000|TR LOSS: 3.68 LM NLL: 3.02, KL: 62.38|DE LOSS: 3.95|BETA: 0.020593\n",
      "076[s], 08[s], Ep: 98, Ct: 01500|TR LOSS: 3.68 LM NLL: 3.02, KL: 62.37|DE LOSS: 3.95|BETA: 0.020627\n",
      "075[s], 08[s], Ep: 98, Ct: 02000|TR LOSS: 3.68 LM NLL: 3.02, KL: 62.35|DE LOSS: 3.96|BETA: 0.020662\n",
      "075[s], 08[s], Ep: 98, Ct: 02500|TR LOSS: 3.68 LM NLL: 3.02, KL: 62.33|DE LOSS: 3.94|BETA: 0.020696\n",
      "059[s], 08[s], Ep: 99, Ct: 00000|TR LOSS: 3.68 LM NLL: 3.02, KL: 62.32|DE LOSS: 3.96|BETA: 0.020723\n",
      "075[s], 08[s], Ep: 99, Ct: 00500|TR LOSS: 3.68 LM NLL: 3.02, KL: 62.30|DE LOSS: 3.95|BETA: 0.020757\n",
      "075[s], 08[s], Ep: 99, Ct: 01000|TR LOSS: 3.68 LM NLL: 3.02, KL: 62.28|DE LOSS: 3.95|BETA: 0.020792\n",
      "075[s], 08[s], Ep: 99, Ct: 01500|TR LOSS: 3.68 LM NLL: 3.02, KL: 62.26|DE LOSS: 3.95|BETA: 0.020826\n",
      "076[s], 08[s], Ep: 99, Ct: 02000|TR LOSS: 3.68 LM NLL: 3.02, KL: 62.25|DE LOSS: 3.96|BETA: 0.020861\n",
      "075[s], 08[s], Ep: 99, Ct: 02500|TR LOSS: 3.68 LM NLL: 3.02, KL: 62.23|DE LOSS: 3.96|BETA: 0.020895\n",
      "060[s], 08[s], Ep: 100, Ct: 00000|TR LOSS: 3.68 LM NLL: 3.02, KL: 62.21|DE LOSS: 3.96|BETA: 0.020922\n",
      "076[s], 08[s], Ep: 100, Ct: 00500|TR LOSS: 3.68 LM NLL: 3.02, KL: 62.20|DE LOSS: 3.96|BETA: 0.020956\n",
      "075[s], 08[s], Ep: 100, Ct: 01000|TR LOSS: 3.68 LM NLL: 3.01, KL: 62.18|DE LOSS: 3.96|BETA: 0.020991\n",
      "075[s], 08[s], Ep: 100, Ct: 01500|TR LOSS: 3.68 LM NLL: 3.01, KL: 62.16|DE LOSS: 3.96|BETA: 0.021025\n",
      "075[s], 08[s], Ep: 100, Ct: 02000|TR LOSS: 3.68 LM NLL: 3.01, KL: 62.14|DE LOSS: 3.96|BETA: 0.021059\n",
      "075[s], 08[s], Ep: 100, Ct: 02500|TR LOSS: 3.68 LM NLL: 3.01, KL: 62.13|DE LOSS: 3.97|BETA: 0.021094\n",
      "060[s], 08[s], Ep: 101, Ct: 00000|TR LOSS: 3.68 LM NLL: 3.01, KL: 62.11|DE LOSS: 3.97|BETA: 0.021120\n",
      "075[s], 08[s], Ep: 101, Ct: 00500|TR LOSS: 3.68 LM NLL: 3.01, KL: 62.10|DE LOSS: 3.97|BETA: 0.021155\n",
      "075[s], 08[s], Ep: 101, Ct: 01000|TR LOSS: 3.68 LM NLL: 3.01, KL: 62.08|DE LOSS: 3.97|BETA: 0.021189\n",
      "075[s], 08[s], Ep: 101, Ct: 01500|TR LOSS: 3.68 LM NLL: 3.01, KL: 62.06|DE LOSS: 3.96|BETA: 0.021224\n",
      "075[s], 08[s], Ep: 101, Ct: 02000|TR LOSS: 3.68 LM NLL: 3.01, KL: 62.04|DE LOSS: 3.98|BETA: 0.021258\n",
      "075[s], 08[s], Ep: 101, Ct: 02500|TR LOSS: 3.68 LM NLL: 3.01, KL: 62.02|DE LOSS: 3.97|BETA: 0.021293\n",
      "060[s], 08[s], Ep: 102, Ct: 00000|TR LOSS: 3.68 LM NLL: 3.01, KL: 62.01|DE LOSS: 3.98|BETA: 0.021319\n",
      "075[s], 08[s], Ep: 102, Ct: 00500|TR LOSS: 3.68 LM NLL: 3.01, KL: 61.99|DE LOSS: 3.98|BETA: 0.021354\n",
      "075[s], 08[s], Ep: 102, Ct: 01000|TR LOSS: 3.68 LM NLL: 3.01, KL: 61.98|DE LOSS: 3.98|BETA: 0.021388\n",
      "076[s], 08[s], Ep: 102, Ct: 01500|TR LOSS: 3.68 LM NLL: 3.00, KL: 61.96|DE LOSS: 3.99|BETA: 0.021423\n",
      "075[s], 08[s], Ep: 102, Ct: 02000|TR LOSS: 3.68 LM NLL: 3.00, KL: 61.94|DE LOSS: 3.98|BETA: 0.021457\n",
      "075[s], 08[s], Ep: 102, Ct: 02500|TR LOSS: 3.68 LM NLL: 3.00, KL: 61.92|DE LOSS: 3.98|BETA: 0.021491\n",
      "060[s], 08[s], Ep: 103, Ct: 00000|TR LOSS: 3.68 LM NLL: 3.00, KL: 61.91|DE LOSS: 3.98|BETA: 0.021518\n",
      "075[s], 08[s], Ep: 103, Ct: 00500|TR LOSS: 3.68 LM NLL: 3.00, KL: 61.89|DE LOSS: 3.99|BETA: 0.021552\n",
      "075[s], 08[s], Ep: 103, Ct: 01000|TR LOSS: 3.68 LM NLL: 3.00, KL: 61.87|DE LOSS: 3.99|BETA: 0.021587\n",
      "075[s], 08[s], Ep: 103, Ct: 01500|TR LOSS: 3.68 LM NLL: 3.00, KL: 61.86|DE LOSS: 3.99|BETA: 0.021621\n",
      "075[s], 08[s], Ep: 103, Ct: 02000|TR LOSS: 3.68 LM NLL: 3.00, KL: 61.84|DE LOSS: 3.99|BETA: 0.021656\n",
      "075[s], 08[s], Ep: 103, Ct: 02500|TR LOSS: 3.68 LM NLL: 3.00, KL: 61.82|DE LOSS: 3.99|BETA: 0.021690\n",
      "060[s], 08[s], Ep: 104, Ct: 00000|TR LOSS: 3.68 LM NLL: 3.00, KL: 61.81|DE LOSS: 4.00|BETA: 0.021717\n",
      "075[s], 08[s], Ep: 104, Ct: 00500|TR LOSS: 3.68 LM NLL: 3.00, KL: 61.79|DE LOSS: 3.99|BETA: 0.021751\n",
      "075[s], 08[s], Ep: 104, Ct: 01000|TR LOSS: 3.68 LM NLL: 3.00, KL: 61.77|DE LOSS: 3.99|BETA: 0.021786\n",
      "076[s], 08[s], Ep: 104, Ct: 01500|TR LOSS: 3.68 LM NLL: 3.00, KL: 61.75|DE LOSS: 3.99|BETA: 0.021820\n",
      "075[s], 08[s], Ep: 104, Ct: 02000|TR LOSS: 3.68 LM NLL: 3.00, KL: 61.74|DE LOSS: 4.00|BETA: 0.021855\n",
      "075[s], 08[s], Ep: 104, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.99, KL: 61.72|DE LOSS: 4.00|BETA: 0.021889\n",
      "060[s], 08[s], Ep: 105, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.99, KL: 61.70|DE LOSS: 4.00|BETA: 0.021915\n",
      "075[s], 08[s], Ep: 105, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.99, KL: 61.69|DE LOSS: 4.01|BETA: 0.021950\n",
      "075[s], 08[s], Ep: 105, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.99, KL: 61.67|DE LOSS: 4.00|BETA: 0.021984\n",
      "075[s], 08[s], Ep: 105, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.99, KL: 61.65|DE LOSS: 4.00|BETA: 0.022019\n",
      "075[s], 08[s], Ep: 105, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.99, KL: 61.63|DE LOSS: 4.02|BETA: 0.022053\n",
      "075[s], 08[s], Ep: 105, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.99, KL: 61.62|DE LOSS: 4.01|BETA: 0.022088\n",
      "060[s], 08[s], Ep: 106, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.99, KL: 61.60|DE LOSS: 4.01|BETA: 0.022114\n",
      "075[s], 08[s], Ep: 106, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.99, KL: 61.58|DE LOSS: 4.00|BETA: 0.022149\n",
      "075[s], 08[s], Ep: 106, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.99, KL: 61.57|DE LOSS: 4.01|BETA: 0.022183\n",
      "075[s], 08[s], Ep: 106, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.99, KL: 61.55|DE LOSS: 4.01|BETA: 0.022218\n",
      "075[s], 08[s], Ep: 106, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.99, KL: 61.53|DE LOSS: 4.02|BETA: 0.022252\n",
      "075[s], 08[s], Ep: 106, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.99, KL: 61.51|DE LOSS: 4.02|BETA: 0.022286\n",
      "060[s], 08[s], Ep: 107, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.99, KL: 61.50|DE LOSS: 4.02|BETA: 0.022313\n",
      "076[s], 08[s], Ep: 107, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.98, KL: 61.48|DE LOSS: 4.02|BETA: 0.022347\n",
      "076[s], 08[s], Ep: 107, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.98, KL: 61.46|DE LOSS: 4.02|BETA: 0.022382\n",
      "076[s], 08[s], Ep: 107, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.98, KL: 61.45|DE LOSS: 4.02|BETA: 0.022416\n",
      "075[s], 08[s], Ep: 107, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.98, KL: 61.43|DE LOSS: 4.02|BETA: 0.022451\n",
      "075[s], 08[s], Ep: 107, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.98, KL: 61.41|DE LOSS: 4.03|BETA: 0.022485\n",
      "060[s], 08[s], Ep: 108, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.98, KL: 61.40|DE LOSS: 4.03|BETA: 0.022512\n",
      "076[s], 08[s], Ep: 108, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.98, KL: 61.38|DE LOSS: 4.04|BETA: 0.022546\n",
      "075[s], 08[s], Ep: 108, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.98, KL: 61.36|DE LOSS: 4.03|BETA: 0.022581\n",
      "075[s], 08[s], Ep: 108, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.98, KL: 61.34|DE LOSS: 4.02|BETA: 0.022615\n",
      "075[s], 08[s], Ep: 108, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.98, KL: 61.33|DE LOSS: 4.03|BETA: 0.022650\n",
      "075[s], 08[s], Ep: 108, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.98, KL: 61.31|DE LOSS: 4.03|BETA: 0.022684\n",
      "060[s], 08[s], Ep: 109, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.98, KL: 61.29|DE LOSS: 4.04|BETA: 0.022710\n",
      "075[s], 08[s], Ep: 109, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.98, KL: 61.28|DE LOSS: 4.03|BETA: 0.022745\n",
      "075[s], 08[s], Ep: 109, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.98, KL: 61.26|DE LOSS: 4.04|BETA: 0.022779\n",
      "076[s], 08[s], Ep: 109, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.98, KL: 61.24|DE LOSS: 4.04|BETA: 0.022814\n",
      "075[s], 08[s], Ep: 109, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.97, KL: 61.22|DE LOSS: 4.04|BETA: 0.022848\n",
      "076[s], 08[s], Ep: 109, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.97, KL: 61.21|DE LOSS: 4.04|BETA: 0.022883\n",
      "060[s], 08[s], Ep: 110, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.97, KL: 61.19|DE LOSS: 4.04|BETA: 0.022909\n",
      "076[s], 08[s], Ep: 110, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.97, KL: 61.17|DE LOSS: 4.05|BETA: 0.022944\n",
      "076[s], 08[s], Ep: 110, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.97, KL: 61.16|DE LOSS: 4.05|BETA: 0.022978\n",
      "076[s], 08[s], Ep: 110, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.97, KL: 61.14|DE LOSS: 4.05|BETA: 0.023013\n",
      "075[s], 08[s], Ep: 110, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.97, KL: 61.12|DE LOSS: 4.05|BETA: 0.023047\n",
      "075[s], 08[s], Ep: 110, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.97, KL: 61.10|DE LOSS: 4.04|BETA: 0.023082\n",
      "060[s], 08[s], Ep: 111, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.97, KL: 61.09|DE LOSS: 4.06|BETA: 0.023108\n",
      "075[s], 08[s], Ep: 111, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.97, KL: 61.07|DE LOSS: 4.05|BETA: 0.023142\n",
      "075[s], 08[s], Ep: 111, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.97, KL: 61.05|DE LOSS: 4.05|BETA: 0.023177\n",
      "075[s], 08[s], Ep: 111, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.97, KL: 61.04|DE LOSS: 4.07|BETA: 0.023211\n",
      "075[s], 08[s], Ep: 111, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.97, KL: 61.02|DE LOSS: 4.05|BETA: 0.023246\n",
      "075[s], 08[s], Ep: 111, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.97, KL: 61.00|DE LOSS: 4.06|BETA: 0.023280\n",
      "060[s], 08[s], Ep: 112, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.97, KL: 60.99|DE LOSS: 4.06|BETA: 0.023307\n",
      "075[s], 08[s], Ep: 112, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.97, KL: 60.97|DE LOSS: 4.05|BETA: 0.023341\n",
      "075[s], 08[s], Ep: 112, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.95|DE LOSS: 4.06|BETA: 0.023376\n",
      "075[s], 08[s], Ep: 112, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.93|DE LOSS: 4.06|BETA: 0.023410\n",
      "075[s], 08[s], Ep: 112, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.91|DE LOSS: 4.07|BETA: 0.023445\n",
      "075[s], 08[s], Ep: 112, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.90|DE LOSS: 4.07|BETA: 0.023479\n",
      "060[s], 08[s], Ep: 113, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.88|DE LOSS: 4.07|BETA: 0.023506\n",
      "075[s], 08[s], Ep: 113, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.86|DE LOSS: 4.06|BETA: 0.023540\n",
      "076[s], 08[s], Ep: 113, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.85|DE LOSS: 4.07|BETA: 0.023574\n",
      "075[s], 08[s], Ep: 113, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.83|DE LOSS: 4.07|BETA: 0.023609\n",
      "075[s], 08[s], Ep: 113, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.81|DE LOSS: 4.07|BETA: 0.023643\n",
      "075[s], 08[s], Ep: 113, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.79|DE LOSS: 4.08|BETA: 0.023678\n",
      "060[s], 08[s], Ep: 114, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.78|DE LOSS: 4.07|BETA: 0.023704\n",
      "075[s], 08[s], Ep: 114, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.76|DE LOSS: 4.08|BETA: 0.023739\n",
      "075[s], 08[s], Ep: 114, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.74|DE LOSS: 4.09|BETA: 0.023773\n",
      "076[s], 08[s], Ep: 114, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.73|DE LOSS: 4.08|BETA: 0.023808\n",
      "075[s], 08[s], Ep: 114, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.71|DE LOSS: 4.08|BETA: 0.023842\n",
      "076[s], 08[s], Ep: 114, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.96, KL: 60.69|DE LOSS: 4.08|BETA: 0.023877\n",
      "060[s], 08[s], Ep: 115, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.68|DE LOSS: 4.09|BETA: 0.023903\n",
      "075[s], 08[s], Ep: 115, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.66|DE LOSS: 4.09|BETA: 0.023937\n",
      "075[s], 08[s], Ep: 115, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.64|DE LOSS: 4.09|BETA: 0.023972\n",
      "076[s], 08[s], Ep: 115, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.62|DE LOSS: 4.09|BETA: 0.024006\n",
      "075[s], 08[s], Ep: 115, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.61|DE LOSS: 4.09|BETA: 0.024041\n",
      "076[s], 08[s], Ep: 115, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.59|DE LOSS: 4.09|BETA: 0.024075\n",
      "060[s], 08[s], Ep: 116, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.57|DE LOSS: 4.09|BETA: 0.024102\n",
      "076[s], 08[s], Ep: 116, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.56|DE LOSS: 4.09|BETA: 0.024136\n",
      "075[s], 08[s], Ep: 116, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.54|DE LOSS: 4.10|BETA: 0.024171\n",
      "076[s], 08[s], Ep: 116, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.52|DE LOSS: 4.09|BETA: 0.024205\n",
      "075[s], 08[s], Ep: 116, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.50|DE LOSS: 4.09|BETA: 0.024240\n",
      "075[s], 08[s], Ep: 116, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.48|DE LOSS: 4.09|BETA: 0.024274\n",
      "060[s], 08[s], Ep: 117, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.47|DE LOSS: 4.10|BETA: 0.024301\n",
      "076[s], 08[s], Ep: 117, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.45|DE LOSS: 4.11|BETA: 0.024335\n",
      "076[s], 08[s], Ep: 117, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.44|DE LOSS: 4.11|BETA: 0.024369\n",
      "076[s], 08[s], Ep: 117, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.42|DE LOSS: 4.10|BETA: 0.024404\n",
      "076[s], 08[s], Ep: 117, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.95, KL: 60.40|DE LOSS: 4.10|BETA: 0.024438\n",
      "075[s], 08[s], Ep: 117, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.38|DE LOSS: 4.11|BETA: 0.024473\n",
      "060[s], 08[s], Ep: 118, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.37|DE LOSS: 4.11|BETA: 0.024499\n",
      "076[s], 08[s], Ep: 118, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.35|DE LOSS: 4.11|BETA: 0.024534\n",
      "076[s], 08[s], Ep: 118, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.33|DE LOSS: 4.11|BETA: 0.024568\n",
      "076[s], 08[s], Ep: 118, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.31|DE LOSS: 4.12|BETA: 0.024603\n",
      "076[s], 08[s], Ep: 118, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.30|DE LOSS: 4.11|BETA: 0.024637\n",
      "075[s], 08[s], Ep: 118, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.28|DE LOSS: 4.11|BETA: 0.024672\n",
      "060[s], 08[s], Ep: 119, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.26|DE LOSS: 4.11|BETA: 0.024698\n",
      "075[s], 08[s], Ep: 119, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.25|DE LOSS: 4.12|BETA: 0.024733\n",
      "076[s], 08[s], Ep: 119, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.23|DE LOSS: 4.11|BETA: 0.024767\n",
      "076[s], 08[s], Ep: 119, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.21|DE LOSS: 4.12|BETA: 0.024801\n",
      "075[s], 08[s], Ep: 119, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.19|DE LOSS: 4.12|BETA: 0.024836\n",
      "076[s], 08[s], Ep: 119, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.18|DE LOSS: 4.13|BETA: 0.024870\n",
      "060[s], 08[s], Ep: 120, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.16|DE LOSS: 4.12|BETA: 0.024897\n",
      "076[s], 08[s], Ep: 120, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.14|DE LOSS: 4.13|BETA: 0.024931\n",
      "076[s], 08[s], Ep: 120, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.13|DE LOSS: 4.13|BETA: 0.024966\n",
      "076[s], 08[s], Ep: 120, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.11|DE LOSS: 4.12|BETA: 0.025000\n",
      "075[s], 08[s], Ep: 120, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.09|DE LOSS: 4.14|BETA: 0.025035\n",
      "075[s], 08[s], Ep: 120, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.94, KL: 60.07|DE LOSS: 4.13|BETA: 0.025069\n",
      "059[s], 08[s], Ep: 121, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.93, KL: 60.06|DE LOSS: 4.13|BETA: 0.025096\n",
      "075[s], 08[s], Ep: 121, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.93, KL: 60.04|DE LOSS: 4.14|BETA: 0.025130\n",
      "075[s], 08[s], Ep: 121, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.93, KL: 60.02|DE LOSS: 4.13|BETA: 0.025164\n",
      "075[s], 08[s], Ep: 121, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.93, KL: 60.01|DE LOSS: 4.13|BETA: 0.025199\n",
      "076[s], 08[s], Ep: 121, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.93, KL: 59.99|DE LOSS: 4.14|BETA: 0.025233\n",
      "076[s], 08[s], Ep: 121, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.93, KL: 59.97|DE LOSS: 4.14|BETA: 0.025268\n",
      "060[s], 08[s], Ep: 122, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.93, KL: 59.96|DE LOSS: 4.14|BETA: 0.025294\n",
      "076[s], 08[s], Ep: 122, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.93, KL: 59.94|DE LOSS: 4.14|BETA: 0.025329\n",
      "076[s], 08[s], Ep: 122, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.93, KL: 59.92|DE LOSS: 4.14|BETA: 0.025363\n",
      "076[s], 08[s], Ep: 122, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.93, KL: 59.90|DE LOSS: 4.14|BETA: 0.025398\n",
      "075[s], 08[s], Ep: 122, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.93, KL: 59.88|DE LOSS: 4.15|BETA: 0.025432\n",
      "075[s], 08[s], Ep: 122, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.93, KL: 59.87|DE LOSS: 4.14|BETA: 0.025467\n",
      "060[s], 08[s], Ep: 123, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.93, KL: 59.85|DE LOSS: 4.15|BETA: 0.025493\n",
      "075[s], 08[s], Ep: 123, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.93, KL: 59.84|DE LOSS: 4.15|BETA: 0.025528\n",
      "076[s], 08[s], Ep: 123, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.93, KL: 59.82|DE LOSS: 4.15|BETA: 0.025562\n",
      "075[s], 08[s], Ep: 123, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.93, KL: 59.80|DE LOSS: 4.15|BETA: 0.025596\n",
      "075[s], 08[s], Ep: 123, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.93, KL: 59.78|DE LOSS: 4.15|BETA: 0.025631\n",
      "076[s], 08[s], Ep: 123, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.93, KL: 59.76|DE LOSS: 4.15|BETA: 0.025665\n",
      "060[s], 08[s], Ep: 124, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.93, KL: 59.75|DE LOSS: 4.15|BETA: 0.025692\n",
      "075[s], 08[s], Ep: 124, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.73|DE LOSS: 4.16|BETA: 0.025726\n",
      "076[s], 08[s], Ep: 124, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.71|DE LOSS: 4.16|BETA: 0.025761\n",
      "076[s], 08[s], Ep: 124, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.70|DE LOSS: 4.16|BETA: 0.025795\n",
      "075[s], 08[s], Ep: 124, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.68|DE LOSS: 4.16|BETA: 0.025830\n",
      "075[s], 08[s], Ep: 124, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.66|DE LOSS: 4.16|BETA: 0.025864\n",
      "060[s], 08[s], Ep: 125, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.65|DE LOSS: 4.16|BETA: 0.025891\n",
      "076[s], 08[s], Ep: 125, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.63|DE LOSS: 4.16|BETA: 0.025925\n",
      "076[s], 08[s], Ep: 125, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.61|DE LOSS: 4.16|BETA: 0.025960\n",
      "076[s], 08[s], Ep: 125, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.59|DE LOSS: 4.16|BETA: 0.025994\n",
      "075[s], 08[s], Ep: 125, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.58|DE LOSS: 4.17|BETA: 0.026028\n",
      "076[s], 08[s], Ep: 125, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.56|DE LOSS: 4.17|BETA: 0.026063\n",
      "060[s], 08[s], Ep: 126, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.54|DE LOSS: 4.16|BETA: 0.026089\n",
      "075[s], 08[s], Ep: 126, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.53|DE LOSS: 4.17|BETA: 0.026124\n",
      "075[s], 08[s], Ep: 126, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.51|DE LOSS: 4.17|BETA: 0.026158\n",
      "076[s], 08[s], Ep: 126, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.49|DE LOSS: 4.18|BETA: 0.026193\n",
      "076[s], 08[s], Ep: 126, Ct: 02000|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.47|DE LOSS: 4.17|BETA: 0.026227\n",
      "076[s], 08[s], Ep: 126, Ct: 02500|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.45|DE LOSS: 4.18|BETA: 0.026262\n",
      "060[s], 08[s], Ep: 127, Ct: 00000|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.44|DE LOSS: 4.18|BETA: 0.026288\n",
      "075[s], 08[s], Ep: 127, Ct: 00500|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.42|DE LOSS: 4.18|BETA: 0.026323\n",
      "076[s], 08[s], Ep: 127, Ct: 01000|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.41|DE LOSS: 4.18|BETA: 0.026357\n",
      "076[s], 08[s], Ep: 127, Ct: 01500|TR LOSS: 3.68 LM NLL: 2.92, KL: 59.39|DE LOSS: 4.18|BETA: 0.026392\n",
      "076[s], 08[s], Ep: 127, Ct: 02000|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.37|DE LOSS: 4.18|BETA: 0.026426\n",
      "076[s], 08[s], Ep: 127, Ct: 02500|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.35|DE LOSS: 4.18|BETA: 0.026460\n",
      "060[s], 08[s], Ep: 128, Ct: 00000|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.34|DE LOSS: 4.20|BETA: 0.026487\n",
      "075[s], 08[s], Ep: 128, Ct: 00500|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.32|DE LOSS: 4.19|BETA: 0.026521\n",
      "075[s], 08[s], Ep: 128, Ct: 01000|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.30|DE LOSS: 4.19|BETA: 0.026556\n",
      "076[s], 08[s], Ep: 128, Ct: 01500|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.29|DE LOSS: 4.18|BETA: 0.026590\n",
      "075[s], 08[s], Ep: 128, Ct: 02000|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.27|DE LOSS: 4.19|BETA: 0.026625\n",
      "075[s], 08[s], Ep: 128, Ct: 02500|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.25|DE LOSS: 4.20|BETA: 0.026659\n",
      "060[s], 08[s], Ep: 129, Ct: 00000|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.24|DE LOSS: 4.19|BETA: 0.026686\n",
      "074[s], 07[s], Ep: 129, Ct: 00500|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.22|DE LOSS: 4.20|BETA: 0.026720\n",
      "074[s], 07[s], Ep: 129, Ct: 01000|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.20|DE LOSS: 4.20|BETA: 0.026755\n",
      "073[s], 07[s], Ep: 129, Ct: 01500|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.18|DE LOSS: 4.20|BETA: 0.026789\n",
      "070[s], 07[s], Ep: 129, Ct: 02000|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.16|DE LOSS: 4.20|BETA: 0.026823\n",
      "070[s], 07[s], Ep: 129, Ct: 02500|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.15|DE LOSS: 4.19|BETA: 0.026858\n",
      "056[s], 07[s], Ep: 130, Ct: 00000|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.13|DE LOSS: 4.21|BETA: 0.026884\n",
      "070[s], 07[s], Ep: 130, Ct: 00500|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.12|DE LOSS: 4.20|BETA: 0.026919\n",
      "070[s], 07[s], Ep: 130, Ct: 01000|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.10|DE LOSS: 4.20|BETA: 0.026953\n",
      "070[s], 07[s], Ep: 130, Ct: 01500|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.08|DE LOSS: 4.20|BETA: 0.026988\n",
      "070[s], 07[s], Ep: 130, Ct: 02000|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.06|DE LOSS: 4.21|BETA: 0.027022\n",
      "070[s], 07[s], Ep: 130, Ct: 02500|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.04|DE LOSS: 4.20|BETA: 0.027057\n",
      "056[s], 07[s], Ep: 131, Ct: 00000|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.03|DE LOSS: 4.20|BETA: 0.027083\n",
      "070[s], 07[s], Ep: 131, Ct: 00500|TR LOSS: 3.69 LM NLL: 2.91, KL: 59.01|DE LOSS: 4.22|BETA: 0.027118\n",
      "070[s], 07[s], Ep: 131, Ct: 01000|TR LOSS: 3.69 LM NLL: 2.91, KL: 58.99|DE LOSS: 4.21|BETA: 0.027152\n",
      "070[s], 07[s], Ep: 131, Ct: 01500|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.98|DE LOSS: 4.21|BETA: 0.027187\n",
      "070[s], 07[s], Ep: 131, Ct: 02000|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.96|DE LOSS: 4.21|BETA: 0.027221\n",
      "070[s], 07[s], Ep: 131, Ct: 02500|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.94|DE LOSS: 4.22|BETA: 0.027255\n",
      "055[s], 07[s], Ep: 132, Ct: 00000|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.93|DE LOSS: 4.21|BETA: 0.027282\n",
      "070[s], 07[s], Ep: 132, Ct: 00500|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.91|DE LOSS: 4.22|BETA: 0.027316\n",
      "070[s], 07[s], Ep: 132, Ct: 01000|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.89|DE LOSS: 4.22|BETA: 0.027351\n",
      "070[s], 07[s], Ep: 132, Ct: 01500|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.87|DE LOSS: 4.22|BETA: 0.027385\n",
      "070[s], 07[s], Ep: 132, Ct: 02000|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.86|DE LOSS: 4.22|BETA: 0.027420\n",
      "070[s], 07[s], Ep: 132, Ct: 02500|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.84|DE LOSS: 4.22|BETA: 0.027454\n",
      "056[s], 07[s], Ep: 133, Ct: 00000|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.82|DE LOSS: 4.23|BETA: 0.027481\n",
      "070[s], 07[s], Ep: 133, Ct: 00500|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.81|DE LOSS: 4.23|BETA: 0.027515\n",
      "070[s], 07[s], Ep: 133, Ct: 01000|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.79|DE LOSS: 4.22|BETA: 0.027550\n",
      "070[s], 07[s], Ep: 133, Ct: 01500|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.77|DE LOSS: 4.22|BETA: 0.027584\n",
      "070[s], 07[s], Ep: 133, Ct: 02000|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.75|DE LOSS: 4.23|BETA: 0.027619\n",
      "070[s], 07[s], Ep: 133, Ct: 02500|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.74|DE LOSS: 4.23|BETA: 0.027653\n",
      "056[s], 07[s], Ep: 134, Ct: 00000|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.72|DE LOSS: 4.23|BETA: 0.027679\n",
      "070[s], 07[s], Ep: 134, Ct: 00500|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.70|DE LOSS: 4.23|BETA: 0.027714\n",
      "069[s], 07[s], Ep: 134, Ct: 01000|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.69|DE LOSS: 4.23|BETA: 0.027748\n",
      "070[s], 07[s], Ep: 134, Ct: 01500|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.67|DE LOSS: 4.22|BETA: 0.027783\n",
      "070[s], 07[s], Ep: 134, Ct: 02000|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.65|DE LOSS: 4.23|BETA: 0.027817\n",
      "070[s], 07[s], Ep: 134, Ct: 02500|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.63|DE LOSS: 4.23|BETA: 0.027852\n",
      "056[s], 07[s], Ep: 135, Ct: 00000|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.62|DE LOSS: 4.23|BETA: 0.027878\n",
      "070[s], 07[s], Ep: 135, Ct: 00500|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.60|DE LOSS: 4.24|BETA: 0.027913\n",
      "070[s], 07[s], Ep: 135, Ct: 01000|TR LOSS: 3.69 LM NLL: 2.90, KL: 58.58|DE LOSS: 4.24|BETA: 0.027947\n",
      "070[s], 07[s], Ep: 135, Ct: 01500|TR LOSS: 3.69 LM NLL: 2.89, KL: 58.57|DE LOSS: 4.24|BETA: 0.027982\n",
      "070[s], 07[s], Ep: 135, Ct: 02000|TR LOSS: 3.69 LM NLL: 2.89, KL: 58.55|DE LOSS: 4.24|BETA: 0.028016\n",
      "070[s], 07[s], Ep: 135, Ct: 02500|TR LOSS: 3.69 LM NLL: 2.89, KL: 58.53|DE LOSS: 4.25|BETA: 0.028051\n",
      "True: new research in wyoming finds that cloud seeding can increase mountain snowfall by up to # percent a year\n",
      "Pred: new mexico on maine that that county seeding will produce by more more up to # percent a year\n",
      "True: in addition , the state funded research suggests cloud seeding has negligible environmental effects\n",
      "Pred: in addition , the state funded grants showed that that has led environmental impacts\n",
      "True: and almost no effect on precipitation in surrounding areas\n",
      "Pred: then there more sign on snow in both areas\n",
      "True: governments around the world and some u.s. states conduct cloud seeding to wring more rain and snow from the sky\n",
      "Pred: as the the world and southern of ports that production that it produce more than and part from the midwest\n",
      "True: but some question its effectiveness and potential environmental threats\n",
      "Pred: all some raises their financial and improve policy recommendations\n",
      "True: wyoming spent about $ # million over nearly # years to determine whether seeding clouds during snowstorms can increase snowpack in several mountain ranges\n",
      "Pred: ohio is more $ # million over $ $ # after settle when it is is efforts will be in in northern areas schools\n",
      "True: scientists , academics and engineers presented their findings wednesday\n",
      "Pred: scientists , police and engineers requested their findings wednesday\n",
      "True: their findings will be presented to the american meteorological society annual meeting next month in phoenix\n",
      "Pred: his duties will be presented to the american legion and in early friday month in february\n",
      "True: a wildfire near slide rock state park in oak creek canyon has forced the evacuations of some businesses and homes in a two mile stretch north of the area\n",
      "Pred: a wildfire near surface national national park near central valley , has caused the evacuation of two homes and businesses in a # mile stretch of of the river\n",
      "True: u.s. forest service officials say the so called slide fire has burned # acres on the western slope of the canyon across the creek from highway # a\n",
      "Pred: state forest service officials say the crews called fire fire has burned # acres of the southern river of the mountains and the state river highway # near\n",
      "True: the fire was reported about # p.m. tuesday\n",
      "Pred: the fire was reported about # p.m. tuesday\n",
      "True: its cause is n't immediately known\n",
      "Pred: his name is n't immediately yet\n",
      "True: highway # a now is closed at the oak creek overlook\n",
      "Pred: multiple # 's fireworks is located at the local community trail\n",
      "True: evacuations have been ordered at west fork , pine flat , halfway , cave springs and other campgrounds and day use areas in the upper canyon\n",
      "Pred: evacuations had been moved at # virginia creek , , , <unk> , and and and other areas and trails water parts in the eastern peninsula\n",
      "True: some <unk> , resorts and other businesses also have been ordered to evacuate by coconino county authorities\n",
      "Pred: some <unk> , <unk> and other counties have have been able to locate as state county authorities\n",
      "True: two <unk> crews , four engines , two helicopters and sedona firefighters are battling the wildfire\n",
      "Pred: two helicopters , , # helicopters and helicopters terrain and helicopters firefighters are battling the blaze\n",
      "True: a lawyer is calling for the release of a u.s. tourist detained in <unk> in connection with the disappearance of his travel companion\n",
      "Pred: a judge is scheduled on the city of a private prison man in connection in connection of the disappearance 's his <unk> body\n",
      "True: attorney michael lopez says there 's no evidence on which to hold gary giordano in the presumed death of # year old robyn gardner\n",
      "Pred: prosecutor <unk> <unk> said he 's no `` `` to is run him testimony in the death death of # year old <unk> owens\n",
      "True: lopez says giordano has provided all the information he can\n",
      "Pred: <unk> says video has `` and the information could can\n",
      "True: giordano 's lawyer released his statement sunday , a day ahead of a detention hearing for his client\n",
      "Pred: court 's lawyer called his # sunday , a third ahead of a brief hearing for his client\n",
      "True: a judge last week ordered giordano held eight days while authorities investigated gardner 's disappearance\n",
      "Pred: a judge last week ordered daniel is monday days later authorities investigated by 's disappearance\n",
      "True: prosecutors have said they will ask for another eight days at a closed hearing monday so that they may continue their investigation\n",
      "Pred: officials said said they did continue for additional additional days of a time jail until that that they are be their investigation\n",
      "True: giordano has said gardner disappeared while snorkeling off the caribbean island\n",
      "Pred: giordano has said dixon body while snorkeling off the national island\n",
      "True: in god we trust `` is being added to a southern california city seal\n",
      "Pred: in addition things hope that was was be to the the park city station\n",
      "True: the ontario city council voted unanimously tuesday night to put those words in a <unk> above the city logo in council chambers\n",
      "Pred: the topeka city council voted unanimously tuesday to to remove out affected in the `` in the city 's 's martin bluffs\n",
      "True: councilman alan <unk> , who crafted the motion , says he was n't surprised , adding , `` i knew they support god .\n",
      "Pred: republican beth <unk> , who says the campaign because he he 'd n't seen , , , but he think he want `` .\n",
      "True: he says the volunteer organization in god we trust america asked cities to display the nation 's motto with their official logos\n",
      "Pred: he says the afghan , and `` `` support his 's him to share the city 's 's to his own <unk>\n",
      "True: <unk> is not concerned about possible lawsuits , saying `` in god we trust `` became the nation 's motto when it was approved by congress in #\n",
      "Pred: <unk> has was paid on running changes , but `` and president 's believe , the the `` 's decision `` it 's on by # in #\n",
      "True: donations will be used to pay the $ # cost of adding the motto to the city hall seal\n",
      "Pred: donations will be used to build the $ # prize of winning the donation to the world 's\n",
      "True: ontario is # miles east of los angeles\n",
      "Pred: wilmington is # miles north of los angeles\n",
      "True: a coroner says a slain western pennsylvania police officer was shot in the head , arm and abdomen by a man who also died at the scene\n",
      "Pred: the coroner says a man west pennsylvania police officers was shot in the head and chest and another of a police who was died at the scene\n",
      "True: westmoreland county coroner kenneth <unk> ( batch ' ah ) released the autopsy results on # year old lower burrell police officer derek kotecki on friday\n",
      "Pred: county district coroner patrick de ( kay ' fee ) identified a report report found about p.m. old william l. police officer <unk> <unk> in saturday\n",
      "True: authorities say kotecki was shot wednesday night by # year old charles post , who was wanted for attempted murder in another shooting earlier this month\n",
      "Pred: authorities say ersland was arrested wednesday night # # year old william l. , , was charged for killing murder , that shooting earlier this month\n",
      "True: district attorney john peck says kotecki was among the first officers to arrive at a dairy queen where police expected to arrest post , who had been on the run since an oct. # shooting\n",
      "Pred: u.s. attorney steve peck says officers was among the victims officers to to at a hospital hospital where he responded to arrest , , who he been in the job since oct. earlier # incident\n",
      "True: instead , police say post shot kotecki before post either took his own life or was shot by other officers\n",
      "Pred: instead , police told <unk> fatally that on shooting as took his gun and and was shot in other evidence\n",
      "True: the coroner is expected to release post 's autopsy results later friday\n",
      "Pred: the coroner is scheduled to resume thursday 's autopsy results later friday\n",
      "True: gov . terry branstad has released tax returns showing that he earned $ # in # and paid just over $ # in federal taxes for the year\n",
      "Pred: gov . terry branstad has appointed proposed returns that it he earned $ # # in and and by over $ of for in spending in the year\n",
      "True: branstad on tuesday continued an annual practice of providing tax details to reporters\n",
      "Pred: branstad 's thursday ordered an additional salary of providing for increases to reporters\n",
      "True: the information released showed he and his wife paid just over $ # in federal taxes , but are getting a refund of nearly $ #\n",
      "Pred: the charges claim , evidence that his client was just over # # in unpaid taxes , which still owed a refund of about $ #\n",
      "True: most of branstad 's # income came from his salary as governor\n",
      "Pred: most notably congress 's # percent are from his job as governor\n",
      "True: he also received a state pension from his earlier service as governor , lieutenant governor and legislator\n",
      "Pred: he also is the civil representative for his bid campaign by governor , attorney commissioner and treasurer\n",
      "True: branstad 's chief of staff , matt <unk> , and his accountant , jamie ward , made more than # pages of state and federal returns available to reporters for review in the governor 's office\n",
      "Pred: office 's office democratic staff , <unk> <unk> , who his former , <unk> <unk> , was it an $ # of requests and failure money to to work monday work and the state 's office\n",
      "True: hartford 's xl center has agreed to improve access for disabled patrons after a federal investigation into a complaint the arena was not complying with the americans with disabilities act\n",
      "Pred: ohio 's military director has agreed to provide medical for marijuana employees after a judge judge into the bill that company has not to with the health with disabilities act\n",
      "True: under terms of the settlement announce tuesday by acting connecticut u.s. attorney deirdre daly , the arena 's management will add accessible bathrooms and seats and make other improvements\n",
      "Pred: under the of the agreement announced tuesday by former in of rep. general <unk> , the city 's efforts is be jobs , and and to provide other improvements\n",
      "True: the initial complaint was brought by new haven lawyer michelle <unk> , an advocate for the disabled , after she attended a # bruce springsteen concert\n",
      "Pred: the court court was released by former county mayor chief smith , a assistant for the sex man who held disappeared a # year grand man\n",
      "True: she says she found several problems , including her view from the arena 's wheelchair seating area , which she says was obstructed\n",
      "Pred: she said she knows many people , including part with from the store 's path , , , which she said she well\n",
      "True: daly says xl center , city and state officials cooperated in addressing the complaint\n",
      "Pred: davis says police center , state manager chief officials assisted <unk> of the investigation\n",
      "True: she says federal authorities will monitor the arena for three years , to ensure compliance\n",
      "Pred: she says city officials will monitor the process for three years ago to ensure service\n",
      "True: savannah tourism officials say recent numbers show that the average visitor for trips to the georgia city is younger and that spending is up\n",
      "Pred: bank ceo inc. say two years made a a new demand for demand to a united market , in and another it is less\n",
      "True: the savannah morning news reports ( http : <unk> # <unk> ) that the average visitor for day trips and overnight visits is in his or her early to mid # s , which is younger than in #\n",
      "Pred: the times morning news reports ( http : //bit.ly/ # <unk> ) that the first of for # on is sundays was to going place home hand life saturday # # s and which is as than # #\n",
      "True: the # tourism numbers also show that the average daily spending per party of overnight visitors is $ #\n",
      "Pred: the two states companies also also that the full # quarter of share needs jobs costs to $ #\n",
      "True: charlie brazil , chairman of the board of visit savannah , says that marketing efforts have been aimed at younger , more affluent visitors\n",
      "Pred: jill <unk> , president of the board 's tuesday <unk> , says that community and have been <unk> at large # more than more\n",
      "True: officials say that visit savannah beefed up its digital marketing efforts in # , <unk> social media such as facebook , twitter , youtube and <unk> more aggressively and launching a mobile internet site\n",
      "Pred: officials say that 's <unk> 's up a own of and in # , and and media outlets as <unk> and and and <unk> and and and than than <unk> a new phone message\n",
      "True: the ann arbor company that owns it says a federal grant is running out and no one has stepped forward yet to offer financial support\n",
      "Pred: the portland arbor press says said and plans the grant grant is underway and of more one year been down to to make support services\n",
      "True: the buoy has spent four summers in lake michigan off ottawa county 's port sheldon township\n",
      "Pred: the dolphin was died three dogs in michigan michigan 's lake city 's airport sheldon park\n",
      "True: the grand rapids press ( http : //bit.ly/ # <unk> # ) says it feeds scientific information to researchers , weather forecasters , the u.s. coast guard and others\n",
      "Pred: the grand rapids press ( http : //bit.ly/ # <unk> # ) says city 's to work to `` , <unk> and and the u.s. fish and and <unk>\n",
      "True: county officials say they 're not interested in paying for it , although port sheldon township says some type of partnership might be possible\n",
      "Pred: county officials say they 've not interested in to for such in some new officials , , some of of them can be hurt\n",
      "True: grand haven state park supervisor joyce rhodes says she can judge waves by looking out a window\n",
      "Pred: miami county medical park school mary <unk> says investigators is show <unk> and police to a window\n",
      "True: a madison man has been sentenced to life in prison for killing his girlfriend 's two young sons\n",
      "Pred: an ohio man has been sentenced to life in prison for killing his girlfriend 's two young daughters\n",
      "True: on the first day of his trial tuesday , david hoem asked a dane county judge to change his pleas to guilty and to sentence him immediately\n",
      "Pred: in the first time of his trial jury , prosecutors <unk> entered a judge judge judge for hear jurors to to his to no no guilty guilty\n",
      "True: hoem is accused of strangling # year old <unk> mcarthur and # year old kevin mcarthur after picking them up from their father 's home last july\n",
      "Pred: perkins is accused of strangling # year old <unk> <unk> , # year old jose <unk> , others up with his his home 's home july month\n",
      "True: wkow tv ( http : <unk> ) reports judge william <unk> sentenced the # year old hoem to two life sentences\n",
      "Pred: kmov tv ( http : <unk> ) reports that william <unk> sentenced the # year old man for first life murder\n"
     ]
    }
   ],
   "source": [
    "if len(logs) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "for epoch in range(config.epochs):\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = get_feed_dict(batch)\n",
    "        if config.warmup > 0 and beta_eval < 1.0: sess.run(update_beta)\n",
    "\n",
    "        _, loss_batch, sent_loss_recon_batch, sent_loss_kl_batch = \\\n",
    "        sess.run([opt, loss, sent_loss_recon, sent_loss_kl], feed_dict = feed_dict)\n",
    "\n",
    "        losses_train += [[loss_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "\n",
    "        if ct%config.log_period==0:\n",
    "            time_dev = time.time()\n",
    "            loss_train, sent_loss_recon_train, sent_loss_kl_train = np.mean(losses_train, 0)\n",
    "            loss_dev = get_loss(sess, dev_batches)\n",
    "            \n",
    "            if loss_dev < loss_min:\n",
    "                loss_min = loss_dev\n",
    "                saver.save(sess, config.modelpath, global_step=epoch*10000+ct)\n",
    "\n",
    "            if config.warmup > 0: beta_eval = beta.eval(session=sess)\n",
    "\n",
    "            clear_output()\n",
    "            time_finish = time.time()\n",
    "            time_log = int(time_finish - time_start)\n",
    "            time_log_dev = int(time_finish - time_dev)\n",
    "            logs += [(time_log, time_log_dev, epoch, ct, loss_train, sent_loss_recon_train, sent_loss_kl_train, loss_dev, beta_eval)]\n",
    "            for log in logs:\n",
    "                print('%03d[s], %02d[s], Ep: %02d, Ct: %05d|TR LOSS: %.2f LM NLL: %.2f, KL: %.2f|DE LOSS: %.2f|BETA: %.6f' %  log)\n",
    "\n",
    "            print_sample(batch)\n",
    "\n",
    "            time_start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(sent_1, sent_2):\n",
    "    sent_tokens_1 = word_tokenize(sent_1)\n",
    "    sent_tokens_2 = word_tokenize(sent_2)\n",
    "    \n",
    "    sent_idxs_1 = [word_to_idx[token] if token in word_to_idx else config.UNK_IDX for token in sent_tokens_1]\n",
    "    sent_idxs_2 = [word_to_idx[token] if token in word_to_idx else config.UNK_IDX for token in sent_tokens_2]\n",
    "    feed_input_token_idxs_list = [sent_idxs_1, sent_idxs_2]\n",
    "    \n",
    "    feed_input_token_idxs = pad_sequences(feed_input_token_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_sent_l = np.array([len(sent_idxs) for sent_idxs in feed_input_token_idxs_list], np.int32)\n",
    "    feed_batch_l = len(feed_sent_l)\n",
    "    \n",
    "    feed_means_infer = sess.run(means_infer, \n",
    "                                                          feed_dict={t_variables['input_token_idxs']: feed_input_token_idxs, \n",
    "                                                                               t_variables['batch_l']: feed_batch_l, t_variables['sent_l']: feed_sent_l})\n",
    "    \n",
    "    n_inter = 10\n",
    "    feed_inter_means_infer = feed_means_infer[0][None, :] + (feed_means_infer[1] - feed_means_infer[0])[None, :] * (np.arange(n_inter+1).astype(np.float32)/n_inter)[:, None]\n",
    "    feed_inter_batch_l = len(feed_inter_means_infer)\n",
    "    \n",
    "    _inter_beam_output_token_idxs = sess.run(inter_beam_output_token_idxs, \n",
    "                                                                                  feed_dict={t_variables['batch_l']: feed_inter_batch_l,\n",
    "                                                                                                       t_variables['keep_prob']: 1.,\n",
    "                                                                                                       inter_means_infer: feed_inter_means_infer})\n",
    "    \n",
    "    output_sents = [' '.join([idx_to_word[idx] for idx in sent_idxs]) for sent_idxs in _inter_beam_output_token_idxs]\n",
    "    \n",
    "    return output_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sent_1 = \"she had been playing near their apartment\"\n",
    "sent_2 = 'he was a german teacher at <unk> central high school'\n",
    "interpolate(sent_1, sent_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: new orleans saints owner tom benson has pledged $ # million to loyola university in new orleans\n",
      "Pred: new orleans based chief financial officer has resigned about # years after learning of new york city\n",
      "True: the money\n",
      "Pred: and a\n",
      "True: being donated through the gayle & tom benson charitable foundation\n",
      "Pred: is held by the <unk> based association board and <unk>\n",
      "True: will create a new home for the university 's jesuit center\n",
      "Pred: will create a new home for the university 's basketball center\n",
      "True: the university says loyola 's former library building , shuttered since # , will be renovated as the tom benson jesuit center\n",
      "Pred: the museum foundation says # of its members , based in # , will be joined by the department of <unk> <unk>\n",
      "True: the # square foot building was originally constructed in #\n",
      "Pred: the # square foot building is officially built in #\n",
      "True: benson , who attended loyola university and received an honorary degree in # , has been a long time supporter of the university\n",
      "Pred: <unk> , who retired and his wife is an active in # , and was given a seat in his family 's history\n",
      "True: in # , his gift of $ # funded the first phase of construction of the gregory r. <unk> chemistry wing in monroe hall\n",
      "Pred: in # , she donated $ # from the <unk> in # to a national park service , new orleans and school district court\n",
      "True: benson also made a $ # million contribution in # to create the jesuit social research institute\n",
      "Pred: <unk> has been a $ # million in # and an open for the national science foundation\n",
      "True: redlands police say a man has been booked for investigation of child molestation and kidnapping after a # year old girl was found naked in his apartment\n",
      "Pred: goodyear police say a man has tried to run for sheriff 's deputies and killing two people in a home shortly after fatally shot four times police\n",
      "True: police spokesman carl baker says the girl 's father went looking for her about # p.m. monday\n",
      "Pred: police spokesman mark <unk> said the body 's family is coming to an area about # p.m.\n",
      "True: she had been playing near their apartment\n",
      "Pred: she had been playing outside his home\n",
      "True: baker says the father found her <unk> cup near the neighbor 's apartment and saw her crying out for him in a window\n",
      "Pred: <unk> says the woman attacked her mother , in the apartment fire , his mother heard from him with his life in her\n",
      "True: baker says the father stormed the apartment , found the man and his daughter naked , grabbed the girl and called police\n",
      "Pred: dearden says the woman 's boyfriend entered the woman , the men and shot him and her the woman and threatened him\n",
      "True: baker identified the man as # year old <unk> <unk> , who surrendered after a # minute standoff and a failed suicide try\n",
      "Pred: trevino testified the suspect in # , and james <unk> , was stabbed in the home on monday after a drug investigation\n",
      "True: baker did not know if the man had an attorney\n",
      "Pred: hernandez wo n't know if the man had an attorney\n",
      "True: redlands is in san bernardino county , about # miles east of los angeles\n",
      "Pred: ewing is in san diego county , about # miles southeast of los angeles\n",
      "True: north carolina 's prisons and other correctional facilities are remembering one of their own with state flags flying at half staff\n",
      "Pred: north carolina 's department of civil rights students and back into their own military and their life in military service\n",
      "True: gov . beverly perdue ordered the honor through for life and service of probation and parole officer jeffrey settle\n",
      "Pred: gov . deval patrick ordered the money for years for <unk> and special security cases of <unk> he died\n",
      "True: the # year old veteran probation officer from <unk> was killed on the job in a vehicle wreck near statesville on thursday\n",
      "Pred: the # year old deputy accused of fatally shot himself in a dispute on the bus and run on school bus said\n",
      "True: he 's being <unk> at a service on sunday\n",
      "Pred: he 's being <unk> at a fire on sunday\n",
      "True: the state correction department says settle was stopped in traffic on u.s. # when his vehicle was struck from behind by a van\n",
      "Pred: the state police department says tests was cited on suspicion of a passenger man who was arrested after driving collided with a tractor\n",
      "True: the state highway patrol will turn over reports to the district attorney to decide whether criminal charges are appropriate\n",
      "Pred: the state highway has agreed to dismiss charges against the city council to report whether criminal charges is warranted\n",
      "True: settle is survived by three children and his wife , a former probation and parole officer\n",
      "Pred: garza is only as # police and two members and the # member of criminal police\n",
      "True: a judge says property owners in allegheny county may see their tax bills split into two payments next year\n",
      "Pred: a company that state workers in illinois counties has no one license to school licenses for # years before\n",
      "True: common pleas court judge r. stanton <unk> jr. suggested the plan monday\n",
      "Pred: former court appointed david h. <unk> jr. , says the investigation began\n",
      "True: it would allow municipalities to bill for half of their yearly tax revenue while a complicated and disputed countywide <unk> is completed\n",
      "Pred: it would allow schools to get public schools at two other cities that includes a state revenue that passed by <unk> voters\n",
      "True: <unk> plans to review a draft of the two bill plan next monday\n",
      "Pred: <unk> plans to unveil a budget of the new law to last monday\n",
      "True: county executive dan onorato objects to the idea , as do some taxpayers\n",
      "Pred: democratic sen. josh <unk> is making the money and he wants some businesses\n",
      "True: but some municipal officials support the idea\n",
      "Pred: but his school officials hope the problem\n",
      "True: under the two payment plan the first bill would make use of existing assessment data from # , while the second would incorporate the new assessment figures and millage rates to <unk> the balance due\n",
      "Pred: under the same time if the state is considering whether to expand medicaid in # , and that a tax increase in the state 's tax rate is now , and a long term has\n",
      "True: a wheatland couple has in a motorcycle crash in kenosha county\n",
      "Pred: a old man was killed a school bus in <unk> county\n",
      "True: sheriff 's authorities say the two teachers were riding a motorcycle on highway p sunday afternoon when an suv pulled out in front of them\n",
      "Pred: sheriff 's officials say the four women were riding a house on thursday morning near his cruiser that left three people in front of the truck\n",
      "True: a union grove school district administrator tells wtmj am ( http : //bit.ly/o # jz ) that high school choir teacher kristin <unk> was a passenger on the bike and died at a hospital\n",
      "Pred: a <unk> county sheriff 's deputy ( <unk> ' <unk> ) reports # year old <unk> <unk> <unk> of st. louis was shot and the death of a three year old child abuse charge\n",
      "True: her husband , david <unk> , was driving the bike and died at the scene\n",
      "Pred: three <unk> <unk> <unk> , who was injured the other dogs died at the scene\n",
      "True: he was a german teacher at <unk> central high school\n",
      "Pred: he was a former officer in justice in west virginia\n",
      "True: union grove administrator al <unk> says kristin was loved by many students and graduates and was a former miss oshkosh\n",
      "Pred: baylor university professor james <unk> says he died as possible , and other students are <unk> the school in <unk>\n",
      "True: a # year old woman is the state 's latest lottery jackpot winner\n",
      "Pred: a # year old woman is the state 's highest suit since september\n",
      "True: officials say brenda romero , of <unk> , held the winning ticket worth $ # from the may # lotto drawing\n",
      "Pred: prosecutors say <unk> <unk> and <unk> <unk> sold his wife $ # a year and # percent in the three months\n",
      "True: she received $ # after federal and state taxes were withheld\n",
      "Pred: she received $ # after federal and state licenses are unconstitutional\n",
      "True: the winning numbers were : # , # , # , # , # and #\n",
      "Pred: the winning numbers were : # , # , # , # and # in #\n",
      "True: officials said monday that romero purchased the ticket from murphy usa no . # in lafayette\n",
      "Pred: officials said monday that <unk> bought the three people near brown 's built near san francisco\n",
      "True: the store will get a bonus of $ # , which is # percent of the prize , for selling the ticket\n",
      "Pred: the <unk> will receive a total of # million , who spent # percent of the children and more than # members\n",
      "True: a north alabama judge has ordered a new sentence for a man who was convicted of three counts of rape and avoided prison time\n",
      "Pred: a st. louis judge is considering a # sentence for a man who was convicted of murder in other crimes and seven years old\n",
      "True: <unk> tv ( http : //bit.ly/ # <unk> ) reported tuesday that limestone county circuit court judge james <unk> ordered a new sentence for # year old austin clem\n",
      "Pred: <unk> tv ( http : <unk> # <unk> ) reports that friday 's circuit court judge william <unk> jr. entered a plea in # in # on thursday\n",
      "True: clem was convicted in september of repeatedly raping one of his former neighbors\n",
      "Pred: peterson was convicted in february and that included in his death of <unk>\n",
      "True: investigators have said clem 's attacks on the now # year old woman dated back to when she was # years old\n",
      "Pred: investigators also said that person 's body the baby # year old baby disappeared while going home after she spent # years\n",
      "True: <unk> initially handed down a sentence ordering clem to spend two years with a community based corrections program , allowing him to work and to live at home , plus three years ' probation\n",
      "Pred: <unk> previously pleaded guilty a case that more than doubled in for a string of <unk> and <unk> <unk> that he used to pay for school students and at least six months in jail\n",
      "True: he was ordered to pay restitution , stay away from the victim and register as a sex offender\n",
      "Pred: he was ordered to pay restitution and pleaded guilty as the health care and children under the influence\n",
      "True: the white house will showcase work tuesday by more than # startup teams of women , minorities and young people all underrepresented in entrepreneurship\n",
      "Pred: the new campaign has attracted attention to # p.m. to its graduation , with students , and those who represent # students in scholarships\n",
      "True: the exhibits range from early stage technology to consumer products\n",
      "Pred: the artifacts range from early stages and will public services\n",
      "True: obama also will announce the winners of small business administration competitions and other efforts to increase diversity in entrepreneurship and technology\n",
      "Pred: obama also will be the <unk> of private sector , and its leaders are looking for better research and urban development\n",
      "True: the white house says about # percent of america 's venture capital backed startups are led by women ; around # percent are led by african americans\n",
      "Pred: the city 's <unk> includes $ # billion and president of public broadcasting , which is one of <unk> 's largest high rate and to other jobs\n",
      "True: about # percent of u.s. based venture capital investors are women\n",
      "Pred: about # percent of u.s. trade research site , are business\n",
      "True: a richmond woman faces prison time for health care fraud and other offenses\n",
      "Pred: a philadelphia man faces trial on providing child pornography fraud and other crimes\n",
      "True: veronica sharon cunningham will be sentenced thursday in federal court in richmond\n",
      "Pred: <unk> ' attorney will be arraigned friday in his court in <unk>\n",
      "True: a jury in march convicted her on # counts of health care fraud , eight counts of falsifying patient health care records and one count of filing a false tax return\n",
      "Pred: a jury convicted # counts of felony count of grand larceny , accusing false of possession of fraud and other crimes as part of his lawsuit against two tax returns\n",
      "True: cunningham owned and operated community neurological services , which administered intravenous immune <unk> to patients suffering from immune deficiency disorders\n",
      "Pred: <unk> <unk> , with wildlife service , <unk> , provides electronic devices that are eligible for children from hospital\n",
      "True: she regularly billed insurance companies and the medicare and medicaid programs for intravenous immune <unk> not actually administered\n",
      "Pred: she is accused , as much of the health insurance exchange is from <unk> , which is now\n",
      "True: gas prices in massachusetts have dipped # cents in the last week\n",
      "Pred: gas prices in texas have decreased # cents in the last week\n",
      "True: aaa southern new england reported monday that the average price of a gallon of self serve , regular gasoline was $ #\n",
      "Pred: aaa southern new england said thursday that the average price of a gallon of self serve regular , fell to # cents\n",
      "True: that # cents less than the national average of $ # per gallon , but # cents higher than at this time last year\n",
      "Pred: last week 's # to $ # the average of # cents per gallon ) , about # cents more than # bushels this week\n",
      "True: the prices in the aaa survey released monday ranged from $ # to $ # per gallon\n",
      "Pred: the price of the corn price was based valued at $ # to $ # per gallon\n"
     ]
    }
   ],
   "source": [
    "print_beam_sample(test_batches[4][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
