{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_test_batches'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-4f8fac4d9773>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_structure\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_test_batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcomponents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_latents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_kl_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_rnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_bi_rnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_test_batches'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "# %matplotlib nbagg\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import hmean\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from data_structure import get_batches, get_test_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "\n",
    "from topic_beam_search_decoder import BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '1', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/bags/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/topic_vae', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'bags', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 1000, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 3000, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "# flags.DEFINE_string('opt', 'Adam', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.01, 'lr')\n",
    "flags.DEFINE_float('reg', 1., 'regularization term')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_bool('warmup', True, 'flg of warming up')\n",
    "flags.DEFINE_integer('epochs_cycle', 5, 'number of epochs within a cycle')\n",
    "flags.DEFINE_float('r_cycle', 0.5, 'proportion used to increase beta within a cycle')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 20, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_bow', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_hidden_topic', 512, 'dim_hidden_topic')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "flags.DEFINE_bool('bidirectional', True, 'flg of bidirectional encoding')\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_batches(instances, batch_size, iterator=False):\n",
    "    iter_instances = iter(instances)\n",
    "    n_batch = len(instances)//batch_size\n",
    "    \n",
    "    batches = [(i_batch, [next(iter_instances) for i_doc in range(batch_size)]) for i_batch in range(n_batch)]\n",
    "    \n",
    "    if iterator: batches = iter(batches)\n",
    "    return batches\n",
    "\n",
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "flags.DEFINE_integer('cycle_steps', len(train_batches)*config.epochs_cycle, 'number of steps for each cycle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     10,
     24,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables, model):\n",
    "    sample_batch = dev_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, model, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "def check_shape(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()\n",
    "    \n",
    "def check_value(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doubly rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoublyRNNCell:\n",
    "    def __init__(self, dim_hidden, output_layer=None):\n",
    "        self.dim_hidden = dim_hidden\n",
    "        \n",
    "        self.ancestral_layer=tf.layers.Dense(units=dim_hidden, activation=tf.nn.tanh, name='ancestral')\n",
    "        self.fraternal_layer=tf.layers.Dense(units=dim_hidden, activation=tf.nn.tanh, name='fraternal')\n",
    "        self.hidden_layer = tf.layers.Dense(units=dim_hidden, name='hidden')\n",
    "        \n",
    "        self.output_layer=output_layer\n",
    "        \n",
    "    def __call__(self, state_ancestral, state_fraternal, reuse=True):\n",
    "        with tf.variable_scope('input', reuse=reuse):\n",
    "            state_ancestral = self.ancestral_layer(state_ancestral)\n",
    "            state_fraternal = self.fraternal_layer(state_fraternal)\n",
    "\n",
    "        with tf.variable_scope('output', reuse=reuse):\n",
    "            state_hidden = self.hidden_layer(state_ancestral + state_fraternal)\n",
    "            if self.output_layer is not None: \n",
    "                output = self.output_layer(state_hidden)\n",
    "            else:\n",
    "                output = state_hidden\n",
    "            \n",
    "        return output, state_hidden\n",
    "    \n",
    "    def get_initial_state(self, name):\n",
    "        initial_state = tf.get_variable(name, [1, self.dim_hidden], dtype=tf.float32)\n",
    "        return initial_state\n",
    "    \n",
    "    def get_zero_state(self, name):\n",
    "        zero_state = tf.zeros([1, self.dim_hidden], dtype=tf.float32, name=name)\n",
    "        return zero_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubly_rnn(dim_hidden, tree_idxs, initial_state_parent=None, initial_state_sibling=None, output_layer=None, name=''):\n",
    "    outputs, states_parent = {}, {}\n",
    "    \n",
    "    with tf.variable_scope(name, reuse=False):\n",
    "        doubly_rnn_cell = DoublyRNNCell(dim_hidden, output_layer)\n",
    "\n",
    "        if initial_state_parent is None: \n",
    "            initial_state_parent = doubly_rnn_cell.get_initial_state('init_state_parent')\n",
    "#             initial_state_parent = doubly_rnn_cell.get_zero_state('init_state_parent')\n",
    "        if initial_state_sibling is None: \n",
    "#             initial_state_sibling = doubly_rnn_cell.get_initial_state('init_state_sibling')\n",
    "            initial_state_sibling = doubly_rnn_cell.get_zero_state('init_state_sibling')\n",
    "        output, state_sibling = doubly_rnn_cell(initial_state_parent, initial_state_sibling, reuse=False)\n",
    "        outputs[0], states_parent[0] = output, state_sibling\n",
    "\n",
    "        for parent_idx, child_idxs in tree_idxs.items():\n",
    "            state_parent = states_parent[parent_idx]\n",
    "            state_sibling = initial_state_sibling\n",
    "            for child_idx in child_idxs:\n",
    "                output, state_sibling = doubly_rnn_cell(state_parent, state_sibling)\n",
    "                outputs[child_idx], states_parent[child_idx] = output, state_sibling\n",
    "\n",
    "    return outputs, states_parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell:\n",
    "    def __init__(self, dim_hidden, output_layer=None):\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.hidden_layer = tf.layers.Dense(units=dim_hidden, activation=tf.nn.tanh, name='hidden')\n",
    "        self.output_layer=output_layer\n",
    "        \n",
    "    def __call__(self, state, reuse=True):\n",
    "        with tf.variable_scope('output', reuse=reuse):\n",
    "            state_hidden = self.hidden_layer(state)\n",
    "            if self.output_layer is not None: \n",
    "                output = self.output_layer(state_hidden)\n",
    "            else:\n",
    "                output = state_hidden\n",
    "            \n",
    "        return output, state_hidden\n",
    "    \n",
    "    def get_initial_state(self, name):\n",
    "        initial_state = tf.get_variable(name, [1, self.dim_hidden], dtype=tf.float32)\n",
    "        return initial_state\n",
    "    \n",
    "    def get_zero_state(self, name):\n",
    "        zero_state = tf.zeros([1, self.dim_hidden], dtype=tf.float32, name=name)\n",
    "        return zero_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(dim_hidden, max_depth, initial_state=None, output_layer=None, name=''):\n",
    "    outputs, states_hidden = [], []\n",
    "    \n",
    "    with tf.variable_scope(name, reuse=False):\n",
    "        rnn_cell = RNNCell(dim_hidden, output_layer)\n",
    "\n",
    "        if initial_state is not None: \n",
    "            state_hidden = initial_state\n",
    "        else:\n",
    "            state_hidden = rnn_cell.get_initial_state('init_state')\n",
    "#             state_hidden = rnn_cell.get_zero_state('init_state_parent')\n",
    "        \n",
    "        for depth in range(max_depth):\n",
    "            if depth == 0:                \n",
    "                output, state_hidden = rnn_cell(state_hidden, reuse=False)\n",
    "            else:\n",
    "                output, state_hidden = rnn_cell(state_hidden, reuse=True)\n",
    "            outputs.append(output)\n",
    "            states_hidden.append(state_hidden)\n",
    "\n",
    "    outputs = tf.concat(outputs, 1)\n",
    "    states_hidden = tf.concat(states_hidden, 0)\n",
    "    return outputs, states_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nCRP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "code_folding": [
     1,
     2,
     25,
     48,
     63,
     232
    ]
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, config, tree_idxs):\n",
    "        def get_depth(parent_idx=0, tree_depth=None, depth=1):\n",
    "            if tree_depth is None: tree_depth={0: depth}\n",
    "\n",
    "            child_idxs = tree_idxs[parent_idx]\n",
    "            depth +=1\n",
    "            for child_idx in child_idxs:\n",
    "                tree_depth[child_idx] = depth\n",
    "                if child_idx in tree_idxs: get_depth(child_idx, tree_depth, depth)\n",
    "            return tree_depth\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        self.t_variables = {}\n",
    "        \n",
    "        self.tree_idxs = tree_idxs\n",
    "        self.topic_idxs = [0] + [idx for child_idxs in tree_idxs.values() for idx in child_idxs]\n",
    "        self.child_to_parent_idxs = {child_idx: parent_idx for parent_idx, child_idxs in self.tree_idxs.items() for child_idx in child_idxs}\n",
    "        self.tree_depth = get_depth()\n",
    "        self.n_depth = max(self.tree_depth.values())\n",
    "        \n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        def nCRP(tree_sticks_topic):\n",
    "            tree_prob_topic = {}\n",
    "            tree_prob_leaf = {}\n",
    "            # calculate topic probability and save\n",
    "            tree_prob_topic[0] = 1.\n",
    "\n",
    "            for parent_idx, child_idxs in self.tree_idxs.items():\n",
    "                rest_prob_topic = tree_prob_topic[parent_idx]\n",
    "                for child_idx in child_idxs:\n",
    "                    stick_topic = tree_sticks_topic[child_idx]\n",
    "                    if child_idx == child_idxs[-1]:\n",
    "                        prob_topic = rest_prob_topic * 1.\n",
    "                    else:\n",
    "                        prob_topic = rest_prob_topic * stick_topic\n",
    "\n",
    "                    if not child_idx in self.tree_idxs: # leaf childs\n",
    "                        tree_prob_leaf[child_idx] = prob_topic\n",
    "                    else:\n",
    "                        tree_prob_topic[child_idx] = prob_topic\n",
    "\n",
    "                    rest_prob_topic -= prob_topic\n",
    "            return tree_prob_leaf\n",
    "\n",
    "        def sbp(sticks_depth, max_depth):\n",
    "            prob_depth_list = []\n",
    "            rest_prob_depth = 1.\n",
    "            for depth in range(max_depth):\n",
    "                stick_depth = tf.expand_dims(sticks_depth[:, depth], 1)\n",
    "                if depth == max_depth -1:\n",
    "                    prob_depth = rest_prob_depth * 1.\n",
    "                else:\n",
    "                    prob_depth = rest_prob_depth * stick_depth\n",
    "                prob_depth_list.append(prob_depth)\n",
    "                rest_prob_depth -= prob_depth\n",
    "\n",
    "            prob_depth = tf.concat(prob_depth_list, 1)\n",
    "            return prob_depth\n",
    "       \n",
    "        def get_prob_topic(tree_prob_leaf, prob_depth):\n",
    "            def get_ancestor_idxs(leaf_idx, ancestor_idxs = None):\n",
    "                if ancestor_idxs is None: ancestor_idxs = [leaf_idx]\n",
    "                parent_idx = self.child_to_parent_idxs[leaf_idx]\n",
    "                ancestor_idxs += [parent_idx]\n",
    "                if parent_idx in self.child_to_parent_idxs: get_ancestor_idxs(parent_idx, ancestor_idxs)\n",
    "                return ancestor_idxs[::-1]\n",
    "            \n",
    "            tree_prob_topic = defaultdict(float)\n",
    "            leaf_ancestor_idxs = {leaf_idx: get_ancestor_idxs(leaf_idx) for leaf_idx in tree_prob_leaf}\n",
    "            for leaf_idx, ancestor_idxs in leaf_ancestor_idxs.items():\n",
    "                prob_leaf = tree_prob_leaf[leaf_idx]\n",
    "                for i, ancestor_idx in enumerate(ancestor_idxs):\n",
    "                    prob_ancestor = prob_leaf * tf.expand_dims(prob_depth[:, i], -1)\n",
    "                    tree_prob_topic[ancestor_idx] += prob_ancestor\n",
    "            prob_topic = tf.concat([tree_prob_topic[topic_idx] for topic_idx in self.topic_idxs], -1)\n",
    "            return prob_topic\n",
    "        \n",
    "        def get_tree_topic_bow(tree_topic_embeddings):\n",
    "            def softmax_with_temperature(logits, axis=None, name=None, temperature=1.):\n",
    "                if axis is None:\n",
    "                    axis = -1\n",
    "                return tf.exp(logits / temperature) / tf.reduce_sum(tf.exp(logits / temperature), axis=axis)\n",
    "\n",
    "            tree_topic_bow = {}\n",
    "            for topic_idx, depth in self.tree_depth.items():\n",
    "                topic_embedding = tree_topic_embeddings[topic_idx]\n",
    "                temperature = tf.constant(1. ** (1./depth), dtype=tf.float32)\n",
    "                logits = tf.matmul(topic_embedding, self.bow_embeddings, transpose_b=True)\n",
    "                tree_topic_bow[topic_idx] = softmax_with_temperature(logits, axis=-1, temperature=temperature)\n",
    "            return tree_topic_bow\n",
    "        \n",
    "        def get_topic_loss_reg(tree_topic_embeddings):\n",
    "            def get_tree_mask_reg(all_child_idxs):        \n",
    "                tree_mask_reg = np.zeros([len(all_child_idxs), len(all_child_idxs)], dtype=np.float32)\n",
    "                for parent_idx, child_idxs in self.tree_idxs.items():\n",
    "#                     neighbor_idxs = child_idxs + [parent_idx] if parent_idx in all_child_idxs else child_idxs\n",
    "                    neighbor_idxs = child_idxs\n",
    "                    for neighbor_idx1 in neighbor_idxs:\n",
    "                        for neighbor_idx2 in neighbor_idxs:\n",
    "                            neighbor_index1 = all_child_idxs.index(neighbor_idx1)\n",
    "                            neighbor_index2 = all_child_idxs.index(neighbor_idx2)\n",
    "                            tree_mask_reg[neighbor_index1, neighbor_index2] = tree_mask_reg[neighbor_index2, neighbor_index1] = 1.\n",
    "                return tree_mask_reg\n",
    "            \n",
    "            all_child_idxs = list(self.child_to_parent_idxs.keys())\n",
    "            self.diff_topic_embeddings = tf.concat([tree_topic_embeddings[child_idx] - tree_topic_embeddings[self.child_to_parent_idxs[child_idx]] for child_idx in all_child_idxs], axis=0)\n",
    "            diff_topic_embeddings_norm = self.diff_topic_embeddings / tf.norm(self.diff_topic_embeddings, axis=1, keepdims=True)\n",
    "            self.topic_dots = tf.clip_by_value(tf.matmul(diff_topic_embeddings_norm, tf.transpose(diff_topic_embeddings_norm)), -1., 1.)        \n",
    "\n",
    "            self.tree_mask_reg = get_tree_mask_reg(all_child_idxs)\n",
    "            self.topic_losses_reg = tf.square(self.topic_dots - tf.eye(len(all_child_idxs))) * self.tree_mask_reg\n",
    "            self.topic_loss_reg = tf.reduce_sum(self.topic_losses_reg) / tf.reduce_sum(self.tree_mask_reg)\n",
    "            return self.topic_loss_reg\n",
    "        \n",
    "#         def get_topic_loss_reg(tree_topic_embeddings):\n",
    "#             def get_tree_mask_reg(all_child_idxs):        \n",
    "#                 tree_mask_reg = np.zeros([len(all_child_idxs), len(all_child_idxs)], dtype=np.float32)\n",
    "#                 for parent_idx, child_idxs in self.tree_idxs.items():\n",
    "#                     neighbor_idxs = child_idxs + [parent_idx] if parent_idx in all_child_idxs else child_idxs\n",
    "#                     for neighbor_idx1 in neighbor_idxs:\n",
    "#                         for neighbor_idx2 in neighbor_idxs:\n",
    "#                             neighbor_index1 = all_child_idxs.index(neighbor_idx1)\n",
    "#                             neighbor_index2 = all_child_idxs.index(neighbor_idx2)\n",
    "#                             tree_mask_reg[neighbor_index1, neighbor_index2] = tree_mask_reg[neighbor_index2, neighbor_index1] = 1.\n",
    "#                 return tree_mask_reg\n",
    "            \n",
    "#             self.topic_losses_reg = []\n",
    "#             for parent_idx, child_idxs in self.tree_idxs.items():\n",
    "#                 diff_topic_embeddings = tf.concat([tree_topic_embeddings[child_idx] - tree_topic_embeddings[parent_idx] for child_idx in child_idxs], axis=0)\n",
    "#                 diff_topic_embeddings_norm = diff_topic_embeddings / tf.norm(diff_topic_embeddings, axis=1, keepdims=True)\n",
    "#                 topic_dots = tf.clip_by_value(tf.matmul(diff_topic_embeddings_norm, tf.transpose(diff_topic_embeddings_norm)), -1., 1.)        \n",
    "\n",
    "#                 mean_angles = tf.asin(tf.sqrt(tf.clip_by_value(tf.linalg.det(topic_dots), 0, 1)))\n",
    "#                 var_angles = tf.square(tf.constant(np.pi/2., dtype=tf.float32)-mean_angles)\n",
    "\n",
    "#                 topic_loss_reg = var_angles - mean_angles\n",
    "#                 self.topic_losses_reg.append(topic_loss_reg)\n",
    "\n",
    "#             self.topic_loss_reg = tf.reduce_sum(self.topic_losses_reg)\n",
    "#             return self.topic_loss_reg\n",
    "           \n",
    "        # -------------- Build Model --------------\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.t_variables['bow'] = tf.placeholder(tf.float32, [None, self.config.dim_bow])\n",
    "        self.t_variables['keep_prob'] = tf.placeholder(tf.float32)\n",
    "        \n",
    "        # encode bow\n",
    "        with tf.variable_scope('topic/enc', reuse=False):\n",
    "            hidden_bow_ = tf.layers.Dense(units=self.config.dim_hidden_bow, activation=tf.nn.tanh, name='hidden_bow')(self.t_variables['bow'])\n",
    "            hidden_bow = tf.layers.Dropout(self.t_variables['keep_prob'])(hidden_bow_)\n",
    "            means_bow = tf.layers.Dense(units=self.config.dim_latent_bow, name='mean_bow')(hidden_bow)\n",
    "            logvars_bow = tf.layers.Dense(units=self.config.dim_latent_bow, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_bow')(hidden_bow)\n",
    "            latents_bow = sample_latents(means_bow, logvars_bow) # sample latent vectors\n",
    "            prob_layer = lambda h: tf.nn.sigmoid(tf.matmul(latents_bow, h, transpose_b=True))\n",
    "\n",
    "            tree_sticks_topic, tree_states_sticks_topic = doubly_rnn(self.config.dim_latent_bow, self.tree_idxs, output_layer=prob_layer, name='sticks_topic')\n",
    "            tree_prob_leaf = nCRP(tree_sticks_topic)\n",
    "            self.tree_prob_leaf = tree_prob_leaf\n",
    "#             prob_depth = tf.layers.Dense(units=self.n_depth, activation=tf.nn.softmax, name='prob_depth')(latents_bow) # inference of topic probabilities\n",
    "            sticks_depth, _ = rnn(config.dim_latent_bow, self.n_depth, output_layer=prob_layer, name='prob_depth')\n",
    "            prob_depth = sbp(sticks_depth, self.n_depth)\n",
    "            self.prob_depth = prob_depth\n",
    "\n",
    "            prob_topic = get_prob_topic(tree_prob_leaf, prob_depth)\n",
    "            self.prob_topic = prob_topic # n_batch x n_topic\n",
    "\n",
    "        # decode bow\n",
    "        with tf.variable_scope('shared', reuse=False):\n",
    "            self.bow_embeddings = tf.get_variable('emb', [self.config.dim_bow, self.config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "        with tf.variable_scope('topic/dec', reuse=False):\n",
    "        #     tree_topic_embeddings, tree_states_topic_embeddings = doubly_rnn(self.config.dim_emb, self.tree_idxs, name='emb_topic')\n",
    "            emb_layer = lambda h: tf.layers.Dense(units=self.config.dim_emb, name='output')(tf.nn.tanh(h))\n",
    "            tree_topic_embeddings, tree_states_topic_embeddings = doubly_rnn(self.config.dim_emb, self.tree_idxs, output_layer=emb_layer, name='emb_topic')\n",
    "#             topic_embeddings = tf.get_variable('topic_emb', [len(self.topic_idxs), self.config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of topics\n",
    "#             tree_topic_embeddings = {topic_idx: tf.expand_dims(topic_embeddings[self.topic_idxs.index(topic_idx)], 0) for topic_idx in self.topic_idxs}\n",
    "\n",
    "            self.tree_topic_bow = get_tree_topic_bow(tree_topic_embeddings) # bow vectors for each topic\n",
    "\n",
    "            topic_bow = tf.concat([self.tree_topic_bow[topic_idx] for topic_idx in self.topic_idxs], 0) # KxV\n",
    "            self.topic_bow = topic_bow\n",
    "            logits_bow = tf_log(tf.matmul(prob_topic, topic_bow)) # predicted bow distribution N_Batch x  V\n",
    "            self.logits_bow = logits_bow\n",
    "            \n",
    "        # define losses\n",
    "        self.topic_losses_recon = -tf.reduce_sum(tf.multiply(self.t_variables['bow'], logits_bow), 1)\n",
    "        self.topic_loss_recon = tf.reduce_mean(self.topic_losses_recon) # negative log likelihood of each words\n",
    "\n",
    "        self.topic_loss_kl = compute_kl_loss(means_bow, logvars_bow) # KL divergence b/w latent dist & gaussian std\n",
    "        \n",
    "        self.topic_embeddings = tf.concat([tree_topic_embeddings[topic_idx] for topic_idx in self.topic_idxs], 0) # temporary\n",
    "        self.topic_loss_reg = get_topic_loss_reg(tree_topic_embeddings)\n",
    "\n",
    "#         self.topic_embeddings = tf.concat([tree_topic_embeddings[topic_idx] for topic_idx in self.topic_idxs], 0)\n",
    "#         topic_embeddings_norm = self.topic_embeddings / tf.norm(self.topic_embeddings, axis=1, keepdims=True)\n",
    "#         self.topic_dots = tf.clip_by_value(tf.matmul(topic_embeddings_norm, tf.transpose(topic_embeddings_norm)), -1., 1.)\n",
    "#         self.mean_angles = tf.asin(tf.sqrt(tf.linalg.det(self.topic_dots)))\n",
    "#         self.var_angles = tf.square(tf.constant(np.pi/2., dtype=tf.float32)-self.mean_angles)\n",
    "#         self.topic_loss_reg = self.var_angles - self.mean_angles\n",
    "\n",
    "        self.global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "\n",
    "        self.loss = self.topic_loss_recon + self.topic_loss_kl + self.config.reg * self.topic_loss_reg\n",
    "\n",
    "        # define optimizer\n",
    "        if self.config.opt == 'Adam':\n",
    "            optimizer = tf.train.AdamOptimizer(self.config.lr)\n",
    "        elif self.config.opt == 'Adagrad':\n",
    "            optimizer = tf.train.AdagradOptimizer(self.config.lr)\n",
    "\n",
    "        self.grad_vars = optimizer.compute_gradients(self.loss)\n",
    "        self.clipped_grad_vars = [(tf.clip_by_value(grad, -self.config.grad_clip, self.config.grad_clip), var) for grad, var in self.grad_vars]\n",
    "        self.opt = optimizer.apply_gradients(self.clipped_grad_vars, global_step=self.global_step)\n",
    "\n",
    "        # monitor\n",
    "        self.n_bow = tf.reduce_sum(self.t_variables['bow'], 1)\n",
    "        self.topic_ppls = tf.divide(self.topic_losses_recon, tf.maximum(1e-5, self.n_bow))\n",
    "        self.topics_freq_bow_indices = tf.nn.top_k(topic_bow, 10, name='topic_freq_bow').indices\n",
    "    \n",
    "        # growth criteria\n",
    "#         self.dist_bow = -tf.matmul(self.t_variables['bow'], tf.log(topic_bow), transpose_b=True)\n",
    "#         self.rads_bow = tf.divide(tf.multiply(self.dist_bow, prob_topic), tf.expand_dims(self.n_bow, -1))\n",
    "        self.n_topics = tf.multiply(tf.expand_dims(self.n_bow, -1), prob_topic)\n",
    "        \n",
    "        self.arcs_bow = tf.acos(tf.matmul(tf.linalg.l2_normalize(self.bow_embeddings, axis=-1), tf.linalg.l2_normalize(self.topic_embeddings, axis=-1), transpose_b=True)) # n_vocab x n_topic\n",
    "        self.rads_bow = tf.multiply(tf.matmul(self.t_variables['bow'], self.arcs_bow), self.prob_topic) # n_batch x n_topic\n",
    "    \n",
    "    def get_feed_dict(self, batch, mode='train'):\n",
    "        bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "        keep_prob = self.config.keep_prob if mode == 'train' else 1.0\n",
    "        feed_dict = {\n",
    "                    self.t_variables['bow']: bow, \n",
    "                    self.t_variables['keep_prob']: keep_prob\n",
    "        }\n",
    "        return  feed_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_loss(sess, batches, model):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    rads_bow_list = []\n",
    "    prob_topic_list = []\n",
    "    n_bow_list = []\n",
    "    n_topics_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = model.get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, ppls_batch, rads_bow_batch, prob_topic_batch, n_bow_batch, n_topics_batch \\\n",
    "            = sess.run([model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_reg, model.topic_ppls, model.rads_bow, model.prob_topic, model.n_bow, model.n_topics], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "        rads_bow_list.append(rads_bow_batch)\n",
    "        prob_topic_list.append(prob_topic_batch)\n",
    "        n_bow_list.append(n_bow_batch)\n",
    "        n_topics_list.append(n_topics_batch)\n",
    "    loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    \n",
    "    probs_topic = np.concatenate(prob_topic_list, 0)\n",
    "    \n",
    "    n_bow = np.concatenate(n_bow_list, 0)\n",
    "    n_topics = np.concatenate(n_topics_list, 0)\n",
    "    probs_topic_mean = np.sum(n_topics, 0) / np.sum(n_bow)\n",
    "    \n",
    "    rads_bow = np.concatenate(rads_bow_list, 0)\n",
    "    rads_bow_mean = np.cos(np.sum(rads_bow, 0) / np.sum(n_topics, 0))\n",
    "    \n",
    "    return loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, ppl_mean, rads_bow_mean, probs_topic_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def print_topic_sample(tree_idxs, sess=None, model=None, topic_rad_bow=None, topic_prob_topic=None, parent_idx=0, topics_freq_bow_idxs=None, depth = 0):\n",
    "    if topics_freq_bow_idxs is None:\n",
    "        topics_freq_bow_idxs = bow_idxs[sess.run(model.topics_freq_bow_indices)]\n",
    "        topic_freq_bow_idxs = topics_freq_bow_idxs[model.topic_idxs.index(parent_idx)]\n",
    "        rad_bow = topic_rad_bow[parent_idx]\n",
    "        prob_topic = topic_prob_topic[parent_idx]\n",
    "        print(parent_idx, 'R: %.3f' % rad_bow, 'P: %.3f' % prob_topic, ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "    \n",
    "    child_idxs = tree_idxs[parent_idx]\n",
    "    depth += 1\n",
    "    for child_idx in child_idxs:\n",
    "        topic_freq_bow_idxs = topics_freq_bow_idxs[model.topic_idxs.index(child_idx)]\n",
    "        rad_bow = topic_rad_bow[child_idx]\n",
    "        prob_topic = topic_prob_topic[child_idx]\n",
    "        print('  '*depth, child_idx, 'R: %.2f' % rad_bow, 'P: %.3f' % prob_topic, ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        \n",
    "        if child_idx in tree_idxs: print_topic_sample(tree_idxs, model=model, topic_rad_bow=topic_rad_bow, topic_prob_topic=topic_prob_topic, parent_idx=child_idx, topics_freq_bow_idxs=topics_freq_bow_idxs, depth=depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topic_specialization(sess, model):\n",
    "    topics_bow = sess.run(model.topic_bow)\n",
    "    norm_bow = np.sum([instance.bow for instance in instances_test], 0)\n",
    "    topics_vec = topics_bow / np.linalg.norm(topics_bow, axis=1, keepdims=True)\n",
    "    norm_vec = norm_bow / np.linalg.norm(norm_bow)\n",
    "\n",
    "    topics_spec = 1 - topics_vec.dot(norm_vec)\n",
    "\n",
    "    depth_topic_idxs = defaultdict(list)\n",
    "    for topic_idx, depth in model.tree_depth.items():\n",
    "        depth_topic_idxs[depth].append(topic_idx)\n",
    "\n",
    "    for depth, topic_idxs in depth_topic_idxs.items():\n",
    "        topic_indices = np.array([model.topic_idxs.index(topic_idx) for topic_idx in topic_idxs])\n",
    "        depth_spec = np.mean(topics_spec[topic_indices])\n",
    "        print(depth, depth_spec)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_hierarchical_affinity(sess, model):\n",
    "    def get_cos_sim(parent_to_child_idxs):\n",
    "        parent_child_bows = {parent_idx: np.concatenate([normed_tree_topic_bow[child_idx] for child_idx in child_idxs], 0) for parent_idx, child_idxs in parent_to_child_idxs.items()}\n",
    "        cos_sim = np.mean([np.mean(normed_tree_topic_bow[parent_idx].dot(child_bows.T)) for parent_idx, child_bows in parent_child_bows.items()])\n",
    "        return cos_sim    \n",
    "    \n",
    "    tree_topic_bow = sess.run(model.tree_topic_bow)\n",
    "    normed_tree_topic_bow = {topic_idx: topic_bow/np.linalg.norm(topic_bow) for topic_idx, topic_bow in tree_topic_bow.items()}\n",
    "\n",
    "    third_child_idxs = [child_idx for child_idx, depth in model.tree_depth.items() if depth==3]\n",
    "    second_parent_to_child_idxs = {parent_idx:child_idxs for parent_idx, child_idxs in model.tree_idxs.items() if model.tree_depth[parent_idx] == 2}\n",
    "    second_parent_to_unchild_idxs = {parent_idx: [child_idx for child_idx in third_child_idxs if child_idx not in child_idxs] for parent_idx, child_idxs in second_parent_to_child_idxs.items()}\n",
    "    \n",
    "    child_cos_sim = get_cos_sim(second_parent_to_child_idxs)\n",
    "    unchild_cos_sim = get_cos_sim(second_parent_to_unchild_idxs)\n",
    "    print('child %.3f, not-child: %.3f' % (child_cos_sim, unchild_cos_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recur_topic_idxs(model, parent_idx, recur_topic_idxs = None):\n",
    "    if recur_topic_idxs is None: recur_topic_idxs = [parent_idx]\n",
    "\n",
    "    if parent_idx in model.tree_idxs:\n",
    "        child_idxs = model.tree_idxs[parent_idx]\n",
    "        recur_topic_idxs += child_idxs\n",
    "        for child_idx in child_idxs:\n",
    "            if child_idx in model.tree_idxs: get_recur_topic_idxs(model, child_idx, recur_topic_idxs)\n",
    "    return recur_topic_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def update_tree(recur_prob_topic, topic_prob_topic, model, add_threshold=0.3, remove_threshold=0.1):    \n",
    "    assert len(model.topic_idxs) == len(recur_prob_topic) == len(topic_prob_topic)\n",
    "    update_tree_flg = False\n",
    "    \n",
    "    def add_topic(topic_idx, tree_idxs):\n",
    "        if topic_idx in tree_idxs:\n",
    "            child_idx = min([10*topic_idx+i for i in range(1, 10) if 10*topic_idx+i not in tree_idxs[topic_idx]])\n",
    "            tree_idxs[topic_idx].append(child_idx)        \n",
    "        else:\n",
    "            child_idx = 10*topic_idx+1\n",
    "            tree_idxs[topic_idx] = [10*topic_idx+1]\n",
    "        return tree_idxs, child_idx\n",
    "    \n",
    "    added_tree_idxs = copy.deepcopy(model.tree_idxs)\n",
    "    for parent_idx, child_idxs in model.tree_idxs.items():\n",
    "#         rad_bow = topic_rad_bow[parent_idx]\n",
    "        rad_bow = topic_prob_topic[parent_idx]\n",
    "        if rad_bow > add_threshold:\n",
    "            update_tree_flg = True\n",
    "            for depth in range(model.tree_depth[parent_idx], model.n_depth):\n",
    "                added_tree_idxs, parent_idx = add_topic(parent_idx, added_tree_idxs)\n",
    "    \n",
    "    def remove_topic(parent_idx, child_idx, tree_idxs):\n",
    "        if parent_idx in tree_idxs:\n",
    "            tree_idxs[parent_idx].remove(child_idx)\n",
    "            if child_idx in tree_idxs:\n",
    "                tree_idxs.pop(child_idx)    \n",
    "        return tree_idxs\n",
    "    \n",
    "    removed_tree_idxs = copy.deepcopy(added_tree_idxs)\n",
    "    for parent_idx, child_idxs in model.tree_idxs.items():\n",
    "#         probs_child = np.array([topic_prob_topic[child_idx] for child_idx in child_idxs])\n",
    "        probs_child = np.array([recur_prob_topic[child_idx] for child_idx in child_idxs])\n",
    "#         prob_child = np.min(probs_child)\n",
    "#         child_idx = child_idxs[np.argmin(probs_child)]\n",
    "        for prob_child, child_idx in zip(probs_child, child_idxs):\n",
    "            if prob_child < remove_threshold:\n",
    "                update_tree_flg = True\n",
    "                removed_tree_idxs = remove_topic(parent_idx, child_idx, removed_tree_idxs)\n",
    "                if parent_idx in removed_tree_idxs:\n",
    "                    if len(removed_tree_idxs[parent_idx]) == 0:\n",
    "                        ancestor_idx = model.child_to_parent_idxs[parent_idx]\n",
    "                        removed_tree_idxs = remove_topic(ancestor_idx, parent_idx, removed_tree_idxs)\n",
    "    return removed_tree_idxs, update_tree_flg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','TM','','','','VALID:','TM','','',''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL','REG','LOSS','PPL','NLL','KL','REG']]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree_idxs = {0:[1, 2], \n",
    "#           1:[10, 11], 2:[20, 21]}\n",
    "\n",
    "tree_idxs = {0:[1, 2, 3], \n",
    "              1:[11, 12], 2:[21, 22], 3:[31, 32]}\n",
    "\n",
    "# tree_idxs = {0:[1, 2, 3], \n",
    "#               1:[10, 11, 12], 2:[20, 21, 22], 3:[30, 31, 32]}\n",
    "\n",
    "\n",
    "if 'sess' in globals(): sess.close()\n",
    "model = Model(config, tree_idxs)\n",
    "sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "name_variables = {tensor.name: variable for tensor, variable in zip(tf.global_variables(), sess.run(tf.global_variables()))}\n",
    "saver = tf.train.Saver(max_to_keep=10)\n",
    "update_tree_flg = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>VALID:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>49</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>111.50</td>\n",
       "      <td>493</td>\n",
       "      <td>110.52</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.98</td>\n",
       "      <td>465</td>\n",
       "      <td>102.77</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>53</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>111.09</td>\n",
       "      <td>474</td>\n",
       "      <td>109.84</td>\n",
       "      <td>1.24</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.86</td>\n",
       "      <td>452</td>\n",
       "      <td>102.38</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>49</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>110.86</td>\n",
       "      <td>463</td>\n",
       "      <td>109.44</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.53</td>\n",
       "      <td>438</td>\n",
       "      <td>101.93</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>54</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>110.71</td>\n",
       "      <td>456</td>\n",
       "      <td>109.18</td>\n",
       "      <td>1.53</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.62</td>\n",
       "      <td>440</td>\n",
       "      <td>101.89</td>\n",
       "      <td>1.72</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>54</td>\n",
       "      <td>50</td>\n",
       "      <td>49</td>\n",
       "      <td>110.60</td>\n",
       "      <td>450</td>\n",
       "      <td>108.97</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.44</td>\n",
       "      <td>430</td>\n",
       "      <td>101.60</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>57</td>\n",
       "      <td>60</td>\n",
       "      <td>59</td>\n",
       "      <td>110.51</td>\n",
       "      <td>446</td>\n",
       "      <td>108.81</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.45</td>\n",
       "      <td>427</td>\n",
       "      <td>101.54</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35000</th>\n",
       "      <td>56</td>\n",
       "      <td>70</td>\n",
       "      <td>69</td>\n",
       "      <td>110.43</td>\n",
       "      <td>442</td>\n",
       "      <td>108.67</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.44</td>\n",
       "      <td>426</td>\n",
       "      <td>101.46</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40000</th>\n",
       "      <td>64</td>\n",
       "      <td>80</td>\n",
       "      <td>79</td>\n",
       "      <td>110.36</td>\n",
       "      <td>439</td>\n",
       "      <td>108.55</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.53</td>\n",
       "      <td>424</td>\n",
       "      <td>101.50</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45000</th>\n",
       "      <td>59</td>\n",
       "      <td>90</td>\n",
       "      <td>89</td>\n",
       "      <td>110.31</td>\n",
       "      <td>436</td>\n",
       "      <td>108.46</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.30</td>\n",
       "      <td>422</td>\n",
       "      <td>101.32</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000</th>\n",
       "      <td>81</td>\n",
       "      <td>100</td>\n",
       "      <td>99</td>\n",
       "      <td>110.26</td>\n",
       "      <td>434</td>\n",
       "      <td>108.37</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.27</td>\n",
       "      <td>415</td>\n",
       "      <td>101.17</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55000</th>\n",
       "      <td>60</td>\n",
       "      <td>110</td>\n",
       "      <td>109</td>\n",
       "      <td>110.22</td>\n",
       "      <td>432</td>\n",
       "      <td>108.29</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.14</td>\n",
       "      <td>414</td>\n",
       "      <td>101.06</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60000</th>\n",
       "      <td>59</td>\n",
       "      <td>120</td>\n",
       "      <td>119</td>\n",
       "      <td>110.18</td>\n",
       "      <td>430</td>\n",
       "      <td>108.22</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.14</td>\n",
       "      <td>412</td>\n",
       "      <td>100.99</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65000</th>\n",
       "      <td>53</td>\n",
       "      <td>130</td>\n",
       "      <td>129</td>\n",
       "      <td>110.15</td>\n",
       "      <td>428</td>\n",
       "      <td>108.17</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.27</td>\n",
       "      <td>414</td>\n",
       "      <td>101.17</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70000</th>\n",
       "      <td>66</td>\n",
       "      <td>140</td>\n",
       "      <td>139</td>\n",
       "      <td>110.12</td>\n",
       "      <td>427</td>\n",
       "      <td>108.11</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.07</td>\n",
       "      <td>412</td>\n",
       "      <td>100.90</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75000</th>\n",
       "      <td>55</td>\n",
       "      <td>150</td>\n",
       "      <td>149</td>\n",
       "      <td>110.09</td>\n",
       "      <td>426</td>\n",
       "      <td>108.07</td>\n",
       "      <td>2.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.12</td>\n",
       "      <td>413</td>\n",
       "      <td>101.00</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80000</th>\n",
       "      <td>69</td>\n",
       "      <td>160</td>\n",
       "      <td>159</td>\n",
       "      <td>110.07</td>\n",
       "      <td>425</td>\n",
       "      <td>108.03</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.10</td>\n",
       "      <td>411</td>\n",
       "      <td>100.95</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85000</th>\n",
       "      <td>67</td>\n",
       "      <td>170</td>\n",
       "      <td>169</td>\n",
       "      <td>110.05</td>\n",
       "      <td>424</td>\n",
       "      <td>107.99</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>102.85</td>\n",
       "      <td>403</td>\n",
       "      <td>100.73</td>\n",
       "      <td>2.12</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90000</th>\n",
       "      <td>63</td>\n",
       "      <td>180</td>\n",
       "      <td>179</td>\n",
       "      <td>110.04</td>\n",
       "      <td>423</td>\n",
       "      <td>107.96</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.24</td>\n",
       "      <td>414</td>\n",
       "      <td>101.10</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95000</th>\n",
       "      <td>71</td>\n",
       "      <td>190</td>\n",
       "      <td>189</td>\n",
       "      <td>110.02</td>\n",
       "      <td>422</td>\n",
       "      <td>107.93</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.00</td>\n",
       "      <td>408</td>\n",
       "      <td>100.78</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>48</td>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>110.02</td>\n",
       "      <td>422</td>\n",
       "      <td>107.92</td>\n",
       "      <td>2.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.29</td>\n",
       "      <td>416</td>\n",
       "      <td>101.20</td>\n",
       "      <td>2.09</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105000</th>\n",
       "      <td>58</td>\n",
       "      <td>210</td>\n",
       "      <td>209</td>\n",
       "      <td>110.01</td>\n",
       "      <td>421</td>\n",
       "      <td>107.90</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>102.95</td>\n",
       "      <td>406</td>\n",
       "      <td>100.79</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110000</th>\n",
       "      <td>57</td>\n",
       "      <td>220</td>\n",
       "      <td>219</td>\n",
       "      <td>109.99</td>\n",
       "      <td>421</td>\n",
       "      <td>107.88</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.15</td>\n",
       "      <td>411</td>\n",
       "      <td>100.98</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115000</th>\n",
       "      <td>65</td>\n",
       "      <td>230</td>\n",
       "      <td>229</td>\n",
       "      <td>109.98</td>\n",
       "      <td>420</td>\n",
       "      <td>107.86</td>\n",
       "      <td>2.12</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.12</td>\n",
       "      <td>410</td>\n",
       "      <td>100.90</td>\n",
       "      <td>2.22</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120000</th>\n",
       "      <td>60</td>\n",
       "      <td>240</td>\n",
       "      <td>239</td>\n",
       "      <td>109.97</td>\n",
       "      <td>419</td>\n",
       "      <td>107.83</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.09</td>\n",
       "      <td>410</td>\n",
       "      <td>100.88</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125000</th>\n",
       "      <td>62</td>\n",
       "      <td>250</td>\n",
       "      <td>249</td>\n",
       "      <td>109.96</td>\n",
       "      <td>419</td>\n",
       "      <td>107.81</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.00</td>\n",
       "      <td>405</td>\n",
       "      <td>100.80</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130000</th>\n",
       "      <td>61</td>\n",
       "      <td>260</td>\n",
       "      <td>259</td>\n",
       "      <td>109.95</td>\n",
       "      <td>418</td>\n",
       "      <td>107.80</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.10</td>\n",
       "      <td>409</td>\n",
       "      <td>100.88</td>\n",
       "      <td>2.22</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135000</th>\n",
       "      <td>57</td>\n",
       "      <td>270</td>\n",
       "      <td>269</td>\n",
       "      <td>109.94</td>\n",
       "      <td>418</td>\n",
       "      <td>107.78</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.03</td>\n",
       "      <td>406</td>\n",
       "      <td>100.83</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140000</th>\n",
       "      <td>60</td>\n",
       "      <td>280</td>\n",
       "      <td>279</td>\n",
       "      <td>109.93</td>\n",
       "      <td>418</td>\n",
       "      <td>107.76</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.09</td>\n",
       "      <td>410</td>\n",
       "      <td>100.92</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145000</th>\n",
       "      <td>61</td>\n",
       "      <td>290</td>\n",
       "      <td>289</td>\n",
       "      <td>109.92</td>\n",
       "      <td>417</td>\n",
       "      <td>107.75</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.16</td>\n",
       "      <td>409</td>\n",
       "      <td>100.94</td>\n",
       "      <td>2.22</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150000</th>\n",
       "      <td>67</td>\n",
       "      <td>300</td>\n",
       "      <td>299</td>\n",
       "      <td>109.91</td>\n",
       "      <td>417</td>\n",
       "      <td>107.73</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.17</td>\n",
       "      <td>413</td>\n",
       "      <td>100.94</td>\n",
       "      <td>2.23</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350000</th>\n",
       "      <td>63</td>\n",
       "      <td>701</td>\n",
       "      <td>200</td>\n",
       "      <td>109.76</td>\n",
       "      <td>409</td>\n",
       "      <td>107.44</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.21</td>\n",
       "      <td>412</td>\n",
       "      <td>100.94</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355000</th>\n",
       "      <td>58</td>\n",
       "      <td>711</td>\n",
       "      <td>210</td>\n",
       "      <td>109.76</td>\n",
       "      <td>409</td>\n",
       "      <td>107.44</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.02</td>\n",
       "      <td>403</td>\n",
       "      <td>100.73</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360000</th>\n",
       "      <td>63</td>\n",
       "      <td>721</td>\n",
       "      <td>220</td>\n",
       "      <td>109.75</td>\n",
       "      <td>409</td>\n",
       "      <td>107.43</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.02</td>\n",
       "      <td>404</td>\n",
       "      <td>100.69</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365000</th>\n",
       "      <td>53</td>\n",
       "      <td>731</td>\n",
       "      <td>230</td>\n",
       "      <td>109.76</td>\n",
       "      <td>409</td>\n",
       "      <td>107.43</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.20</td>\n",
       "      <td>412</td>\n",
       "      <td>101.00</td>\n",
       "      <td>2.19</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370000</th>\n",
       "      <td>62</td>\n",
       "      <td>741</td>\n",
       "      <td>240</td>\n",
       "      <td>109.75</td>\n",
       "      <td>409</td>\n",
       "      <td>107.43</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>102.89</td>\n",
       "      <td>401</td>\n",
       "      <td>100.60</td>\n",
       "      <td>2.28</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375000</th>\n",
       "      <td>59</td>\n",
       "      <td>751</td>\n",
       "      <td>250</td>\n",
       "      <td>109.75</td>\n",
       "      <td>409</td>\n",
       "      <td>107.43</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>102.96</td>\n",
       "      <td>403</td>\n",
       "      <td>100.68</td>\n",
       "      <td>2.28</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380000</th>\n",
       "      <td>62</td>\n",
       "      <td>761</td>\n",
       "      <td>260</td>\n",
       "      <td>109.75</td>\n",
       "      <td>409</td>\n",
       "      <td>107.42</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.07</td>\n",
       "      <td>406</td>\n",
       "      <td>100.75</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385000</th>\n",
       "      <td>57</td>\n",
       "      <td>771</td>\n",
       "      <td>270</td>\n",
       "      <td>109.75</td>\n",
       "      <td>409</td>\n",
       "      <td>107.42</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.02</td>\n",
       "      <td>407</td>\n",
       "      <td>100.73</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390000</th>\n",
       "      <td>63</td>\n",
       "      <td>781</td>\n",
       "      <td>280</td>\n",
       "      <td>109.74</td>\n",
       "      <td>409</td>\n",
       "      <td>107.41</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.03</td>\n",
       "      <td>407</td>\n",
       "      <td>100.70</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395000</th>\n",
       "      <td>51</td>\n",
       "      <td>791</td>\n",
       "      <td>290</td>\n",
       "      <td>109.74</td>\n",
       "      <td>409</td>\n",
       "      <td>107.42</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.12</td>\n",
       "      <td>411</td>\n",
       "      <td>100.91</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400000</th>\n",
       "      <td>60</td>\n",
       "      <td>801</td>\n",
       "      <td>300</td>\n",
       "      <td>109.74</td>\n",
       "      <td>409</td>\n",
       "      <td>107.41</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.03</td>\n",
       "      <td>404</td>\n",
       "      <td>100.75</td>\n",
       "      <td>2.28</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405000</th>\n",
       "      <td>58</td>\n",
       "      <td>811</td>\n",
       "      <td>310</td>\n",
       "      <td>109.74</td>\n",
       "      <td>409</td>\n",
       "      <td>107.41</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>102.92</td>\n",
       "      <td>403</td>\n",
       "      <td>100.64</td>\n",
       "      <td>2.28</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410000</th>\n",
       "      <td>62</td>\n",
       "      <td>821</td>\n",
       "      <td>320</td>\n",
       "      <td>109.74</td>\n",
       "      <td>408</td>\n",
       "      <td>107.40</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>102.94</td>\n",
       "      <td>403</td>\n",
       "      <td>100.63</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415000</th>\n",
       "      <td>59</td>\n",
       "      <td>831</td>\n",
       "      <td>330</td>\n",
       "      <td>109.74</td>\n",
       "      <td>408</td>\n",
       "      <td>107.40</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.06</td>\n",
       "      <td>403</td>\n",
       "      <td>100.76</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420000</th>\n",
       "      <td>58</td>\n",
       "      <td>841</td>\n",
       "      <td>340</td>\n",
       "      <td>109.73</td>\n",
       "      <td>408</td>\n",
       "      <td>107.40</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.05</td>\n",
       "      <td>404</td>\n",
       "      <td>100.75</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425000</th>\n",
       "      <td>58</td>\n",
       "      <td>851</td>\n",
       "      <td>350</td>\n",
       "      <td>109.73</td>\n",
       "      <td>408</td>\n",
       "      <td>107.39</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.01</td>\n",
       "      <td>404</td>\n",
       "      <td>100.71</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430000</th>\n",
       "      <td>62</td>\n",
       "      <td>861</td>\n",
       "      <td>360</td>\n",
       "      <td>109.73</td>\n",
       "      <td>408</td>\n",
       "      <td>107.39</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.08</td>\n",
       "      <td>407</td>\n",
       "      <td>100.78</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435000</th>\n",
       "      <td>59</td>\n",
       "      <td>871</td>\n",
       "      <td>370</td>\n",
       "      <td>109.73</td>\n",
       "      <td>408</td>\n",
       "      <td>107.39</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>102.98</td>\n",
       "      <td>402</td>\n",
       "      <td>100.68</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440000</th>\n",
       "      <td>62</td>\n",
       "      <td>881</td>\n",
       "      <td>380</td>\n",
       "      <td>109.72</td>\n",
       "      <td>408</td>\n",
       "      <td>107.38</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>102.99</td>\n",
       "      <td>403</td>\n",
       "      <td>100.68</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445000</th>\n",
       "      <td>60</td>\n",
       "      <td>891</td>\n",
       "      <td>390</td>\n",
       "      <td>109.72</td>\n",
       "      <td>408</td>\n",
       "      <td>107.38</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.04</td>\n",
       "      <td>402</td>\n",
       "      <td>100.75</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450000</th>\n",
       "      <td>59</td>\n",
       "      <td>901</td>\n",
       "      <td>400</td>\n",
       "      <td>109.72</td>\n",
       "      <td>408</td>\n",
       "      <td>107.38</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.01</td>\n",
       "      <td>403</td>\n",
       "      <td>100.72</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455000</th>\n",
       "      <td>57</td>\n",
       "      <td>911</td>\n",
       "      <td>410</td>\n",
       "      <td>109.72</td>\n",
       "      <td>408</td>\n",
       "      <td>107.37</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.09</td>\n",
       "      <td>408</td>\n",
       "      <td>100.81</td>\n",
       "      <td>2.28</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460000</th>\n",
       "      <td>62</td>\n",
       "      <td>921</td>\n",
       "      <td>420</td>\n",
       "      <td>109.72</td>\n",
       "      <td>408</td>\n",
       "      <td>107.37</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>102.85</td>\n",
       "      <td>398</td>\n",
       "      <td>100.52</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465000</th>\n",
       "      <td>51</td>\n",
       "      <td>931</td>\n",
       "      <td>430</td>\n",
       "      <td>109.72</td>\n",
       "      <td>408</td>\n",
       "      <td>107.37</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.22</td>\n",
       "      <td>415</td>\n",
       "      <td>101.02</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470000</th>\n",
       "      <td>63</td>\n",
       "      <td>941</td>\n",
       "      <td>440</td>\n",
       "      <td>109.72</td>\n",
       "      <td>408</td>\n",
       "      <td>107.37</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.07</td>\n",
       "      <td>404</td>\n",
       "      <td>100.78</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475000</th>\n",
       "      <td>59</td>\n",
       "      <td>951</td>\n",
       "      <td>450</td>\n",
       "      <td>109.72</td>\n",
       "      <td>408</td>\n",
       "      <td>107.37</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>102.81</td>\n",
       "      <td>397</td>\n",
       "      <td>100.50</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480000</th>\n",
       "      <td>62</td>\n",
       "      <td>961</td>\n",
       "      <td>460</td>\n",
       "      <td>109.71</td>\n",
       "      <td>407</td>\n",
       "      <td>107.36</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>102.85</td>\n",
       "      <td>399</td>\n",
       "      <td>100.54</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485000</th>\n",
       "      <td>60</td>\n",
       "      <td>971</td>\n",
       "      <td>470</td>\n",
       "      <td>109.71</td>\n",
       "      <td>407</td>\n",
       "      <td>107.36</td>\n",
       "      <td>2.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>102.92</td>\n",
       "      <td>401</td>\n",
       "      <td>100.63</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490000</th>\n",
       "      <td>78</td>\n",
       "      <td>981</td>\n",
       "      <td>480</td>\n",
       "      <td>109.71</td>\n",
       "      <td>407</td>\n",
       "      <td>107.36</td>\n",
       "      <td>2.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>102.98</td>\n",
       "      <td>405</td>\n",
       "      <td>100.65</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495000</th>\n",
       "      <td>58</td>\n",
       "      <td>991</td>\n",
       "      <td>490</td>\n",
       "      <td>109.71</td>\n",
       "      <td>407</td>\n",
       "      <td>107.35</td>\n",
       "      <td>2.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.00</td>\n",
       "      <td>403</td>\n",
       "      <td>100.70</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       TRAIN:   TM                      VALID:   TM          \\\n",
       "       Time   Ep   Ct    LOSS  PPL     NLL    KL   REG    LOSS  PPL     NLL   \n",
       "5000     49   10    9  111.50  493  110.52  0.98  0.01  103.98  465  102.77   \n",
       "10000    53   20   19  111.09  474  109.84  1.24  0.01  103.86  452  102.38   \n",
       "15000    49   30   29  110.86  463  109.44  1.41  0.01  103.53  438  101.93   \n",
       "20000    54   40   39  110.71  456  109.18  1.53  0.01  103.62  440  101.89   \n",
       "25000    54   50   49  110.60  450  108.97  1.61  0.01  103.44  430  101.60   \n",
       "30000    57   60   59  110.51  446  108.81  1.69  0.01  103.45  427  101.54   \n",
       "35000    56   70   69  110.43  442  108.67  1.75  0.01  103.44  426  101.46   \n",
       "40000    64   80   79  110.36  439  108.55  1.80  0.01  103.53  424  101.50   \n",
       "45000    59   90   89  110.31  436  108.46  1.84  0.01  103.30  422  101.32   \n",
       "50000    81  100   99  110.26  434  108.37  1.88  0.01  103.27  415  101.17   \n",
       "55000    60  110  109  110.22  432  108.29  1.92  0.01  103.14  414  101.06   \n",
       "60000    59  120  119  110.18  430  108.22  1.95  0.01  103.14  412  100.99   \n",
       "65000    53  130  129  110.15  428  108.17  1.97  0.01  103.27  414  101.17   \n",
       "70000    66  140  139  110.12  427  108.11  2.00  0.01  103.07  412  100.90   \n",
       "75000    55  150  149  110.09  426  108.07  2.02  0.01  103.12  413  101.00   \n",
       "80000    69  160  159  110.07  425  108.03  2.03  0.01  103.10  411  100.95   \n",
       "85000    67  170  169  110.05  424  107.99  2.05  0.01  102.85  403  100.73   \n",
       "90000    63  180  179  110.04  423  107.96  2.07  0.01  103.24  414  101.10   \n",
       "95000    71  190  189  110.02  422  107.93  2.08  0.01  103.00  408  100.78   \n",
       "100000   48  200  199  110.02  422  107.92  2.09  0.01  103.29  416  101.20   \n",
       "105000   58  210  209  110.01  421  107.90  2.10  0.01  102.95  406  100.79   \n",
       "110000   57  220  219  109.99  421  107.88  2.11  0.01  103.15  411  100.98   \n",
       "115000   65  230  229  109.98  420  107.86  2.12  0.01  103.12  410  100.90   \n",
       "120000   60  240  239  109.97  419  107.83  2.13  0.01  103.09  410  100.88   \n",
       "125000   62  250  249  109.96  419  107.81  2.14  0.01  103.00  405  100.80   \n",
       "130000   61  260  259  109.95  418  107.80  2.15  0.00  103.10  409  100.88   \n",
       "135000   57  270  269  109.94  418  107.78  2.16  0.00  103.03  406  100.83   \n",
       "140000   60  280  279  109.93  418  107.76  2.16  0.00  103.09  410  100.92   \n",
       "145000   61  290  289  109.92  417  107.75  2.17  0.00  103.16  409  100.94   \n",
       "150000   67  300  299  109.91  417  107.73  2.18  0.00  103.17  413  100.94   \n",
       "...     ...  ...  ...     ...  ...     ...   ...   ...     ...  ...     ...   \n",
       "350000   63  701  200  109.76  409  107.44  2.31  0.00  103.21  412  100.94   \n",
       "355000   58  711  210  109.76  409  107.44  2.31  0.00  103.02  403  100.73   \n",
       "360000   63  721  220  109.75  409  107.43  2.31  0.00  103.02  404  100.69   \n",
       "365000   53  731  230  109.76  409  107.43  2.31  0.00  103.20  412  101.00   \n",
       "370000   62  741  240  109.75  409  107.43  2.32  0.00  102.89  401  100.60   \n",
       "375000   59  751  250  109.75  409  107.43  2.32  0.00  102.96  403  100.68   \n",
       "380000   62  761  260  109.75  409  107.42  2.32  0.00  103.07  406  100.75   \n",
       "385000   57  771  270  109.75  409  107.42  2.32  0.00  103.02  407  100.73   \n",
       "390000   63  781  280  109.74  409  107.41  2.32  0.00  103.03  407  100.70   \n",
       "395000   51  791  290  109.74  409  107.42  2.32  0.00  103.12  411  100.91   \n",
       "400000   60  801  300  109.74  409  107.41  2.32  0.00  103.03  404  100.75   \n",
       "405000   58  811  310  109.74  409  107.41  2.32  0.00  102.92  403  100.64   \n",
       "410000   62  821  320  109.74  408  107.40  2.33  0.00  102.94  403  100.63   \n",
       "415000   59  831  330  109.74  408  107.40  2.33  0.00  103.06  403  100.76   \n",
       "420000   58  841  340  109.73  408  107.40  2.33  0.00  103.05  404  100.75   \n",
       "425000   58  851  350  109.73  408  107.39  2.33  0.00  103.01  404  100.71   \n",
       "430000   62  861  360  109.73  408  107.39  2.33  0.00  103.08  407  100.78   \n",
       "435000   59  871  370  109.73  408  107.39  2.33  0.00  102.98  402  100.68   \n",
       "440000   62  881  380  109.72  408  107.38  2.33  0.00  102.99  403  100.68   \n",
       "445000   60  891  390  109.72  408  107.38  2.34  0.00  103.04  402  100.75   \n",
       "450000   59  901  400  109.72  408  107.38  2.34  0.00  103.01  403  100.72   \n",
       "455000   57  911  410  109.72  408  107.37  2.34  0.00  103.09  408  100.81   \n",
       "460000   62  921  420  109.72  408  107.37  2.34  0.00  102.85  398  100.52   \n",
       "465000   51  931  430  109.72  408  107.37  2.34  0.00  103.22  415  101.02   \n",
       "470000   63  941  440  109.72  408  107.37  2.34  0.00  103.07  404  100.78   \n",
       "475000   59  951  450  109.72  408  107.37  2.34  0.00  102.81  397  100.50   \n",
       "480000   62  961  460  109.71  407  107.36  2.34  0.00  102.85  399  100.54   \n",
       "485000   60  971  470  109.71  407  107.36  2.35  0.00  102.92  401  100.63   \n",
       "490000   78  981  480  109.71  407  107.36  2.35  0.00  102.98  405  100.65   \n",
       "495000   58  991  490  109.71  407  107.35  2.35  0.00  103.00  403  100.70   \n",
       "\n",
       "                    \n",
       "          KL   REG  \n",
       "5000    1.20  0.00  \n",
       "10000   1.47  0.00  \n",
       "15000   1.60  0.00  \n",
       "20000   1.72  0.00  \n",
       "25000   1.84  0.01  \n",
       "30000   1.91  0.00  \n",
       "35000   1.98  0.00  \n",
       "40000   2.03  0.00  \n",
       "45000   1.98  0.00  \n",
       "50000   2.10  0.00  \n",
       "55000   2.08  0.00  \n",
       "60000   2.14  0.00  \n",
       "65000   2.10  0.00  \n",
       "70000   2.17  0.00  \n",
       "75000   2.13  0.00  \n",
       "80000   2.15  0.00  \n",
       "85000   2.12  0.00  \n",
       "90000   2.14  0.00  \n",
       "95000   2.21  0.00  \n",
       "100000  2.09  0.00  \n",
       "105000  2.16  0.00  \n",
       "110000  2.17  0.00  \n",
       "115000  2.22  0.00  \n",
       "120000  2.20  0.00  \n",
       "125000  2.20  0.00  \n",
       "130000  2.22  0.00  \n",
       "135000  2.20  0.00  \n",
       "140000  2.17  0.00  \n",
       "145000  2.22  0.00  \n",
       "150000  2.23  0.00  \n",
       "...      ...   ...  \n",
       "350000  2.27  0.00  \n",
       "355000  2.29  0.00  \n",
       "360000  2.33  0.00  \n",
       "365000  2.19  0.00  \n",
       "370000  2.28  0.00  \n",
       "375000  2.28  0.00  \n",
       "380000  2.32  0.00  \n",
       "385000  2.29  0.00  \n",
       "390000  2.32  0.00  \n",
       "395000  2.21  0.00  \n",
       "400000  2.28  0.00  \n",
       "405000  2.28  0.00  \n",
       "410000  2.31  0.00  \n",
       "415000  2.30  0.00  \n",
       "420000  2.29  0.00  \n",
       "425000  2.30  0.00  \n",
       "430000  2.30  0.00  \n",
       "435000  2.30  0.00  \n",
       "440000  2.32  0.00  \n",
       "445000  2.29  0.00  \n",
       "450000  2.29  0.00  \n",
       "455000  2.28  0.00  \n",
       "460000  2.33  0.00  \n",
       "465000  2.20  0.00  \n",
       "470000  2.29  0.00  \n",
       "475000  2.30  0.00  \n",
       "480000  2.31  0.00  \n",
       "485000  2.30  0.00  \n",
       "490000  2.33  0.00  \n",
       "495000  2.30  0.00  \n",
       "\n",
       "[99 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 R: 1.000 P: 0.423 carry pockets room pocket small perfect strap nice hold space\n",
      "   1 R: 0.21 P: 0.086 - inch sleeve : zipper quality nice price bit made\n",
      "     11 R: 0.04 P: 0.038 & ; pro surface tablet drive air perfectly hard protection\n",
      "     12 R: 0.09 P: 0.090 sleeve protection air inside ipad protect pro perfectly soft chromebook\n",
      "   2 R: 0.16 P: 0.063 price bought quality amazon ... $ purchase item buy happy\n",
      "     23 R: 0.06 P: 0.059 color love perfectly mac picture ! pink blue pro cover\n",
      "     21 R: 0.04 P: 0.041 ! love perfect ... absolutely recommend awesome buy compliments cute\n",
      "   3 R: 0.09 P: 0.038 months broke year years zipper handle bought strap quality weeks\n",
      "     31 R: 0.05 P: 0.052 bottom top part piece plastic corners back cracked feet months\n",
      "   5 R: 0.11 P: 0.043 smell item return $ cheap money received ! back days\n",
      "     51 R: 0.07 P: 0.066 cover keyboard hard screen pro air mac apple protector bottom\n",
      "1 0.32184213\n",
      "2 0.5701278\n",
      "3 0.6095932\n",
      "child 0.202, not-child: 0.112\n",
      "{0: [1, 2, 3, 5, 4], 1: [12, 13], 2: [23, 22], 3: [31], 5: [51], 4: [41]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>VALID:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>49</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>111.50</td>\n",
       "      <td>493</td>\n",
       "      <td>110.52</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.98</td>\n",
       "      <td>465</td>\n",
       "      <td>102.77</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>53</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>111.09</td>\n",
       "      <td>474</td>\n",
       "      <td>109.84</td>\n",
       "      <td>1.24</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.86</td>\n",
       "      <td>452</td>\n",
       "      <td>102.38</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>49</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>110.86</td>\n",
       "      <td>463</td>\n",
       "      <td>109.44</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.53</td>\n",
       "      <td>438</td>\n",
       "      <td>101.93</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>54</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>110.71</td>\n",
       "      <td>456</td>\n",
       "      <td>109.18</td>\n",
       "      <td>1.53</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.62</td>\n",
       "      <td>440</td>\n",
       "      <td>101.89</td>\n",
       "      <td>1.72</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>54</td>\n",
       "      <td>50</td>\n",
       "      <td>49</td>\n",
       "      <td>110.60</td>\n",
       "      <td>450</td>\n",
       "      <td>108.97</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.44</td>\n",
       "      <td>430</td>\n",
       "      <td>101.60</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>57</td>\n",
       "      <td>60</td>\n",
       "      <td>59</td>\n",
       "      <td>110.51</td>\n",
       "      <td>446</td>\n",
       "      <td>108.81</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.45</td>\n",
       "      <td>427</td>\n",
       "      <td>101.54</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35000</th>\n",
       "      <td>56</td>\n",
       "      <td>70</td>\n",
       "      <td>69</td>\n",
       "      <td>110.43</td>\n",
       "      <td>442</td>\n",
       "      <td>108.67</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.44</td>\n",
       "      <td>426</td>\n",
       "      <td>101.46</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40000</th>\n",
       "      <td>64</td>\n",
       "      <td>80</td>\n",
       "      <td>79</td>\n",
       "      <td>110.36</td>\n",
       "      <td>439</td>\n",
       "      <td>108.55</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.53</td>\n",
       "      <td>424</td>\n",
       "      <td>101.50</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45000</th>\n",
       "      <td>59</td>\n",
       "      <td>90</td>\n",
       "      <td>89</td>\n",
       "      <td>110.31</td>\n",
       "      <td>436</td>\n",
       "      <td>108.46</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.30</td>\n",
       "      <td>422</td>\n",
       "      <td>101.32</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000</th>\n",
       "      <td>81</td>\n",
       "      <td>100</td>\n",
       "      <td>99</td>\n",
       "      <td>110.26</td>\n",
       "      <td>434</td>\n",
       "      <td>108.37</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.27</td>\n",
       "      <td>415</td>\n",
       "      <td>101.17</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55000</th>\n",
       "      <td>60</td>\n",
       "      <td>110</td>\n",
       "      <td>109</td>\n",
       "      <td>110.22</td>\n",
       "      <td>432</td>\n",
       "      <td>108.29</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.14</td>\n",
       "      <td>414</td>\n",
       "      <td>101.06</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60000</th>\n",
       "      <td>59</td>\n",
       "      <td>120</td>\n",
       "      <td>119</td>\n",
       "      <td>110.18</td>\n",
       "      <td>430</td>\n",
       "      <td>108.22</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.14</td>\n",
       "      <td>412</td>\n",
       "      <td>100.99</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65000</th>\n",
       "      <td>53</td>\n",
       "      <td>130</td>\n",
       "      <td>129</td>\n",
       "      <td>110.15</td>\n",
       "      <td>428</td>\n",
       "      <td>108.17</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.27</td>\n",
       "      <td>414</td>\n",
       "      <td>101.17</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70000</th>\n",
       "      <td>66</td>\n",
       "      <td>140</td>\n",
       "      <td>139</td>\n",
       "      <td>110.12</td>\n",
       "      <td>427</td>\n",
       "      <td>108.11</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.07</td>\n",
       "      <td>412</td>\n",
       "      <td>100.90</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75000</th>\n",
       "      <td>55</td>\n",
       "      <td>150</td>\n",
       "      <td>149</td>\n",
       "      <td>110.09</td>\n",
       "      <td>426</td>\n",
       "      <td>108.07</td>\n",
       "      <td>2.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.12</td>\n",
       "      <td>413</td>\n",
       "      <td>101.00</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80000</th>\n",
       "      <td>69</td>\n",
       "      <td>160</td>\n",
       "      <td>159</td>\n",
       "      <td>110.07</td>\n",
       "      <td>425</td>\n",
       "      <td>108.03</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.10</td>\n",
       "      <td>411</td>\n",
       "      <td>100.95</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85000</th>\n",
       "      <td>67</td>\n",
       "      <td>170</td>\n",
       "      <td>169</td>\n",
       "      <td>110.05</td>\n",
       "      <td>424</td>\n",
       "      <td>107.99</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>102.85</td>\n",
       "      <td>403</td>\n",
       "      <td>100.73</td>\n",
       "      <td>2.12</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90000</th>\n",
       "      <td>63</td>\n",
       "      <td>180</td>\n",
       "      <td>179</td>\n",
       "      <td>110.04</td>\n",
       "      <td>423</td>\n",
       "      <td>107.96</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.24</td>\n",
       "      <td>414</td>\n",
       "      <td>101.10</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95000</th>\n",
       "      <td>71</td>\n",
       "      <td>190</td>\n",
       "      <td>189</td>\n",
       "      <td>110.02</td>\n",
       "      <td>422</td>\n",
       "      <td>107.93</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.00</td>\n",
       "      <td>408</td>\n",
       "      <td>100.78</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>48</td>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>110.02</td>\n",
       "      <td>422</td>\n",
       "      <td>107.92</td>\n",
       "      <td>2.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.29</td>\n",
       "      <td>416</td>\n",
       "      <td>101.20</td>\n",
       "      <td>2.09</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105000</th>\n",
       "      <td>58</td>\n",
       "      <td>210</td>\n",
       "      <td>209</td>\n",
       "      <td>110.01</td>\n",
       "      <td>421</td>\n",
       "      <td>107.90</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>102.95</td>\n",
       "      <td>406</td>\n",
       "      <td>100.79</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110000</th>\n",
       "      <td>57</td>\n",
       "      <td>220</td>\n",
       "      <td>219</td>\n",
       "      <td>109.99</td>\n",
       "      <td>421</td>\n",
       "      <td>107.88</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.15</td>\n",
       "      <td>411</td>\n",
       "      <td>100.98</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115000</th>\n",
       "      <td>65</td>\n",
       "      <td>230</td>\n",
       "      <td>229</td>\n",
       "      <td>109.98</td>\n",
       "      <td>420</td>\n",
       "      <td>107.86</td>\n",
       "      <td>2.12</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.12</td>\n",
       "      <td>410</td>\n",
       "      <td>100.90</td>\n",
       "      <td>2.22</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120000</th>\n",
       "      <td>60</td>\n",
       "      <td>240</td>\n",
       "      <td>239</td>\n",
       "      <td>109.97</td>\n",
       "      <td>419</td>\n",
       "      <td>107.83</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.09</td>\n",
       "      <td>410</td>\n",
       "      <td>100.88</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125000</th>\n",
       "      <td>62</td>\n",
       "      <td>250</td>\n",
       "      <td>249</td>\n",
       "      <td>109.96</td>\n",
       "      <td>419</td>\n",
       "      <td>107.81</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.00</td>\n",
       "      <td>405</td>\n",
       "      <td>100.80</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130000</th>\n",
       "      <td>61</td>\n",
       "      <td>260</td>\n",
       "      <td>259</td>\n",
       "      <td>109.95</td>\n",
       "      <td>418</td>\n",
       "      <td>107.80</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.10</td>\n",
       "      <td>409</td>\n",
       "      <td>100.88</td>\n",
       "      <td>2.22</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135000</th>\n",
       "      <td>57</td>\n",
       "      <td>270</td>\n",
       "      <td>269</td>\n",
       "      <td>109.94</td>\n",
       "      <td>418</td>\n",
       "      <td>107.78</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.03</td>\n",
       "      <td>406</td>\n",
       "      <td>100.83</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140000</th>\n",
       "      <td>60</td>\n",
       "      <td>280</td>\n",
       "      <td>279</td>\n",
       "      <td>109.93</td>\n",
       "      <td>418</td>\n",
       "      <td>107.76</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.09</td>\n",
       "      <td>410</td>\n",
       "      <td>100.92</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145000</th>\n",
       "      <td>61</td>\n",
       "      <td>290</td>\n",
       "      <td>289</td>\n",
       "      <td>109.92</td>\n",
       "      <td>417</td>\n",
       "      <td>107.75</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.16</td>\n",
       "      <td>409</td>\n",
       "      <td>100.94</td>\n",
       "      <td>2.22</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150000</th>\n",
       "      <td>67</td>\n",
       "      <td>300</td>\n",
       "      <td>299</td>\n",
       "      <td>109.91</td>\n",
       "      <td>417</td>\n",
       "      <td>107.73</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.17</td>\n",
       "      <td>413</td>\n",
       "      <td>100.94</td>\n",
       "      <td>2.23</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350000</th>\n",
       "      <td>63</td>\n",
       "      <td>701</td>\n",
       "      <td>200</td>\n",
       "      <td>109.76</td>\n",
       "      <td>409</td>\n",
       "      <td>107.44</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.21</td>\n",
       "      <td>412</td>\n",
       "      <td>100.94</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355000</th>\n",
       "      <td>58</td>\n",
       "      <td>711</td>\n",
       "      <td>210</td>\n",
       "      <td>109.76</td>\n",
       "      <td>409</td>\n",
       "      <td>107.44</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.02</td>\n",
       "      <td>403</td>\n",
       "      <td>100.73</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360000</th>\n",
       "      <td>63</td>\n",
       "      <td>721</td>\n",
       "      <td>220</td>\n",
       "      <td>109.75</td>\n",
       "      <td>409</td>\n",
       "      <td>107.43</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.02</td>\n",
       "      <td>404</td>\n",
       "      <td>100.69</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365000</th>\n",
       "      <td>53</td>\n",
       "      <td>731</td>\n",
       "      <td>230</td>\n",
       "      <td>109.76</td>\n",
       "      <td>409</td>\n",
       "      <td>107.43</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.20</td>\n",
       "      <td>412</td>\n",
       "      <td>101.00</td>\n",
       "      <td>2.19</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370000</th>\n",
       "      <td>62</td>\n",
       "      <td>741</td>\n",
       "      <td>240</td>\n",
       "      <td>109.75</td>\n",
       "      <td>409</td>\n",
       "      <td>107.43</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>102.89</td>\n",
       "      <td>401</td>\n",
       "      <td>100.60</td>\n",
       "      <td>2.28</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375000</th>\n",
       "      <td>59</td>\n",
       "      <td>751</td>\n",
       "      <td>250</td>\n",
       "      <td>109.75</td>\n",
       "      <td>409</td>\n",
       "      <td>107.43</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>102.96</td>\n",
       "      <td>403</td>\n",
       "      <td>100.68</td>\n",
       "      <td>2.28</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380000</th>\n",
       "      <td>62</td>\n",
       "      <td>761</td>\n",
       "      <td>260</td>\n",
       "      <td>109.75</td>\n",
       "      <td>409</td>\n",
       "      <td>107.42</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.07</td>\n",
       "      <td>406</td>\n",
       "      <td>100.75</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385000</th>\n",
       "      <td>57</td>\n",
       "      <td>771</td>\n",
       "      <td>270</td>\n",
       "      <td>109.75</td>\n",
       "      <td>409</td>\n",
       "      <td>107.42</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.02</td>\n",
       "      <td>407</td>\n",
       "      <td>100.73</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390000</th>\n",
       "      <td>63</td>\n",
       "      <td>781</td>\n",
       "      <td>280</td>\n",
       "      <td>109.74</td>\n",
       "      <td>409</td>\n",
       "      <td>107.41</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.03</td>\n",
       "      <td>407</td>\n",
       "      <td>100.70</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395000</th>\n",
       "      <td>51</td>\n",
       "      <td>791</td>\n",
       "      <td>290</td>\n",
       "      <td>109.74</td>\n",
       "      <td>409</td>\n",
       "      <td>107.42</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.12</td>\n",
       "      <td>411</td>\n",
       "      <td>100.91</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400000</th>\n",
       "      <td>60</td>\n",
       "      <td>801</td>\n",
       "      <td>300</td>\n",
       "      <td>109.74</td>\n",
       "      <td>409</td>\n",
       "      <td>107.41</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.03</td>\n",
       "      <td>404</td>\n",
       "      <td>100.75</td>\n",
       "      <td>2.28</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405000</th>\n",
       "      <td>58</td>\n",
       "      <td>811</td>\n",
       "      <td>310</td>\n",
       "      <td>109.74</td>\n",
       "      <td>409</td>\n",
       "      <td>107.41</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>102.92</td>\n",
       "      <td>403</td>\n",
       "      <td>100.64</td>\n",
       "      <td>2.28</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410000</th>\n",
       "      <td>62</td>\n",
       "      <td>821</td>\n",
       "      <td>320</td>\n",
       "      <td>109.74</td>\n",
       "      <td>408</td>\n",
       "      <td>107.40</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>102.94</td>\n",
       "      <td>403</td>\n",
       "      <td>100.63</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415000</th>\n",
       "      <td>59</td>\n",
       "      <td>831</td>\n",
       "      <td>330</td>\n",
       "      <td>109.74</td>\n",
       "      <td>408</td>\n",
       "      <td>107.40</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.06</td>\n",
       "      <td>403</td>\n",
       "      <td>100.76</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420000</th>\n",
       "      <td>58</td>\n",
       "      <td>841</td>\n",
       "      <td>340</td>\n",
       "      <td>109.73</td>\n",
       "      <td>408</td>\n",
       "      <td>107.40</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.05</td>\n",
       "      <td>404</td>\n",
       "      <td>100.75</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425000</th>\n",
       "      <td>58</td>\n",
       "      <td>851</td>\n",
       "      <td>350</td>\n",
       "      <td>109.73</td>\n",
       "      <td>408</td>\n",
       "      <td>107.39</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.01</td>\n",
       "      <td>404</td>\n",
       "      <td>100.71</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430000</th>\n",
       "      <td>62</td>\n",
       "      <td>861</td>\n",
       "      <td>360</td>\n",
       "      <td>109.73</td>\n",
       "      <td>408</td>\n",
       "      <td>107.39</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.08</td>\n",
       "      <td>407</td>\n",
       "      <td>100.78</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435000</th>\n",
       "      <td>59</td>\n",
       "      <td>871</td>\n",
       "      <td>370</td>\n",
       "      <td>109.73</td>\n",
       "      <td>408</td>\n",
       "      <td>107.39</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>102.98</td>\n",
       "      <td>402</td>\n",
       "      <td>100.68</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440000</th>\n",
       "      <td>62</td>\n",
       "      <td>881</td>\n",
       "      <td>380</td>\n",
       "      <td>109.72</td>\n",
       "      <td>408</td>\n",
       "      <td>107.38</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>102.99</td>\n",
       "      <td>403</td>\n",
       "      <td>100.68</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445000</th>\n",
       "      <td>60</td>\n",
       "      <td>891</td>\n",
       "      <td>390</td>\n",
       "      <td>109.72</td>\n",
       "      <td>408</td>\n",
       "      <td>107.38</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.04</td>\n",
       "      <td>402</td>\n",
       "      <td>100.75</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450000</th>\n",
       "      <td>59</td>\n",
       "      <td>901</td>\n",
       "      <td>400</td>\n",
       "      <td>109.72</td>\n",
       "      <td>408</td>\n",
       "      <td>107.38</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.01</td>\n",
       "      <td>403</td>\n",
       "      <td>100.72</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455000</th>\n",
       "      <td>57</td>\n",
       "      <td>911</td>\n",
       "      <td>410</td>\n",
       "      <td>109.72</td>\n",
       "      <td>408</td>\n",
       "      <td>107.37</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.09</td>\n",
       "      <td>408</td>\n",
       "      <td>100.81</td>\n",
       "      <td>2.28</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460000</th>\n",
       "      <td>62</td>\n",
       "      <td>921</td>\n",
       "      <td>420</td>\n",
       "      <td>109.72</td>\n",
       "      <td>408</td>\n",
       "      <td>107.37</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>102.85</td>\n",
       "      <td>398</td>\n",
       "      <td>100.52</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465000</th>\n",
       "      <td>51</td>\n",
       "      <td>931</td>\n",
       "      <td>430</td>\n",
       "      <td>109.72</td>\n",
       "      <td>408</td>\n",
       "      <td>107.37</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.22</td>\n",
       "      <td>415</td>\n",
       "      <td>101.02</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470000</th>\n",
       "      <td>63</td>\n",
       "      <td>941</td>\n",
       "      <td>440</td>\n",
       "      <td>109.72</td>\n",
       "      <td>408</td>\n",
       "      <td>107.37</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.07</td>\n",
       "      <td>404</td>\n",
       "      <td>100.78</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475000</th>\n",
       "      <td>59</td>\n",
       "      <td>951</td>\n",
       "      <td>450</td>\n",
       "      <td>109.72</td>\n",
       "      <td>408</td>\n",
       "      <td>107.37</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>102.81</td>\n",
       "      <td>397</td>\n",
       "      <td>100.50</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480000</th>\n",
       "      <td>62</td>\n",
       "      <td>961</td>\n",
       "      <td>460</td>\n",
       "      <td>109.71</td>\n",
       "      <td>407</td>\n",
       "      <td>107.36</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>102.85</td>\n",
       "      <td>399</td>\n",
       "      <td>100.54</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485000</th>\n",
       "      <td>60</td>\n",
       "      <td>971</td>\n",
       "      <td>470</td>\n",
       "      <td>109.71</td>\n",
       "      <td>407</td>\n",
       "      <td>107.36</td>\n",
       "      <td>2.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>102.92</td>\n",
       "      <td>401</td>\n",
       "      <td>100.63</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490000</th>\n",
       "      <td>78</td>\n",
       "      <td>981</td>\n",
       "      <td>480</td>\n",
       "      <td>109.71</td>\n",
       "      <td>407</td>\n",
       "      <td>107.36</td>\n",
       "      <td>2.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>102.98</td>\n",
       "      <td>405</td>\n",
       "      <td>100.65</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495000</th>\n",
       "      <td>58</td>\n",
       "      <td>991</td>\n",
       "      <td>490</td>\n",
       "      <td>109.71</td>\n",
       "      <td>407</td>\n",
       "      <td>107.35</td>\n",
       "      <td>2.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.00</td>\n",
       "      <td>403</td>\n",
       "      <td>100.70</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       TRAIN:   TM                      VALID:   TM          \\\n",
       "       Time   Ep   Ct    LOSS  PPL     NLL    KL   REG    LOSS  PPL     NLL   \n",
       "5000     49   10    9  111.50  493  110.52  0.98  0.01  103.98  465  102.77   \n",
       "10000    53   20   19  111.09  474  109.84  1.24  0.01  103.86  452  102.38   \n",
       "15000    49   30   29  110.86  463  109.44  1.41  0.01  103.53  438  101.93   \n",
       "20000    54   40   39  110.71  456  109.18  1.53  0.01  103.62  440  101.89   \n",
       "25000    54   50   49  110.60  450  108.97  1.61  0.01  103.44  430  101.60   \n",
       "30000    57   60   59  110.51  446  108.81  1.69  0.01  103.45  427  101.54   \n",
       "35000    56   70   69  110.43  442  108.67  1.75  0.01  103.44  426  101.46   \n",
       "40000    64   80   79  110.36  439  108.55  1.80  0.01  103.53  424  101.50   \n",
       "45000    59   90   89  110.31  436  108.46  1.84  0.01  103.30  422  101.32   \n",
       "50000    81  100   99  110.26  434  108.37  1.88  0.01  103.27  415  101.17   \n",
       "55000    60  110  109  110.22  432  108.29  1.92  0.01  103.14  414  101.06   \n",
       "60000    59  120  119  110.18  430  108.22  1.95  0.01  103.14  412  100.99   \n",
       "65000    53  130  129  110.15  428  108.17  1.97  0.01  103.27  414  101.17   \n",
       "70000    66  140  139  110.12  427  108.11  2.00  0.01  103.07  412  100.90   \n",
       "75000    55  150  149  110.09  426  108.07  2.02  0.01  103.12  413  101.00   \n",
       "80000    69  160  159  110.07  425  108.03  2.03  0.01  103.10  411  100.95   \n",
       "85000    67  170  169  110.05  424  107.99  2.05  0.01  102.85  403  100.73   \n",
       "90000    63  180  179  110.04  423  107.96  2.07  0.01  103.24  414  101.10   \n",
       "95000    71  190  189  110.02  422  107.93  2.08  0.01  103.00  408  100.78   \n",
       "100000   48  200  199  110.02  422  107.92  2.09  0.01  103.29  416  101.20   \n",
       "105000   58  210  209  110.01  421  107.90  2.10  0.01  102.95  406  100.79   \n",
       "110000   57  220  219  109.99  421  107.88  2.11  0.01  103.15  411  100.98   \n",
       "115000   65  230  229  109.98  420  107.86  2.12  0.01  103.12  410  100.90   \n",
       "120000   60  240  239  109.97  419  107.83  2.13  0.01  103.09  410  100.88   \n",
       "125000   62  250  249  109.96  419  107.81  2.14  0.01  103.00  405  100.80   \n",
       "130000   61  260  259  109.95  418  107.80  2.15  0.00  103.10  409  100.88   \n",
       "135000   57  270  269  109.94  418  107.78  2.16  0.00  103.03  406  100.83   \n",
       "140000   60  280  279  109.93  418  107.76  2.16  0.00  103.09  410  100.92   \n",
       "145000   61  290  289  109.92  417  107.75  2.17  0.00  103.16  409  100.94   \n",
       "150000   67  300  299  109.91  417  107.73  2.18  0.00  103.17  413  100.94   \n",
       "...     ...  ...  ...     ...  ...     ...   ...   ...     ...  ...     ...   \n",
       "350000   63  701  200  109.76  409  107.44  2.31  0.00  103.21  412  100.94   \n",
       "355000   58  711  210  109.76  409  107.44  2.31  0.00  103.02  403  100.73   \n",
       "360000   63  721  220  109.75  409  107.43  2.31  0.00  103.02  404  100.69   \n",
       "365000   53  731  230  109.76  409  107.43  2.31  0.00  103.20  412  101.00   \n",
       "370000   62  741  240  109.75  409  107.43  2.32  0.00  102.89  401  100.60   \n",
       "375000   59  751  250  109.75  409  107.43  2.32  0.00  102.96  403  100.68   \n",
       "380000   62  761  260  109.75  409  107.42  2.32  0.00  103.07  406  100.75   \n",
       "385000   57  771  270  109.75  409  107.42  2.32  0.00  103.02  407  100.73   \n",
       "390000   63  781  280  109.74  409  107.41  2.32  0.00  103.03  407  100.70   \n",
       "395000   51  791  290  109.74  409  107.42  2.32  0.00  103.12  411  100.91   \n",
       "400000   60  801  300  109.74  409  107.41  2.32  0.00  103.03  404  100.75   \n",
       "405000   58  811  310  109.74  409  107.41  2.32  0.00  102.92  403  100.64   \n",
       "410000   62  821  320  109.74  408  107.40  2.33  0.00  102.94  403  100.63   \n",
       "415000   59  831  330  109.74  408  107.40  2.33  0.00  103.06  403  100.76   \n",
       "420000   58  841  340  109.73  408  107.40  2.33  0.00  103.05  404  100.75   \n",
       "425000   58  851  350  109.73  408  107.39  2.33  0.00  103.01  404  100.71   \n",
       "430000   62  861  360  109.73  408  107.39  2.33  0.00  103.08  407  100.78   \n",
       "435000   59  871  370  109.73  408  107.39  2.33  0.00  102.98  402  100.68   \n",
       "440000   62  881  380  109.72  408  107.38  2.33  0.00  102.99  403  100.68   \n",
       "445000   60  891  390  109.72  408  107.38  2.34  0.00  103.04  402  100.75   \n",
       "450000   59  901  400  109.72  408  107.38  2.34  0.00  103.01  403  100.72   \n",
       "455000   57  911  410  109.72  408  107.37  2.34  0.00  103.09  408  100.81   \n",
       "460000   62  921  420  109.72  408  107.37  2.34  0.00  102.85  398  100.52   \n",
       "465000   51  931  430  109.72  408  107.37  2.34  0.00  103.22  415  101.02   \n",
       "470000   63  941  440  109.72  408  107.37  2.34  0.00  103.07  404  100.78   \n",
       "475000   59  951  450  109.72  408  107.37  2.34  0.00  102.81  397  100.50   \n",
       "480000   62  961  460  109.71  407  107.36  2.34  0.00  102.85  399  100.54   \n",
       "485000   60  971  470  109.71  407  107.36  2.35  0.00  102.92  401  100.63   \n",
       "490000   78  981  480  109.71  407  107.36  2.35  0.00  102.98  405  100.65   \n",
       "495000   58  991  490  109.71  407  107.35  2.35  0.00  103.00  403  100.70   \n",
       "\n",
       "                    \n",
       "          KL   REG  \n",
       "5000    1.20  0.00  \n",
       "10000   1.47  0.00  \n",
       "15000   1.60  0.00  \n",
       "20000   1.72  0.00  \n",
       "25000   1.84  0.01  \n",
       "30000   1.91  0.00  \n",
       "35000   1.98  0.00  \n",
       "40000   2.03  0.00  \n",
       "45000   1.98  0.00  \n",
       "50000   2.10  0.00  \n",
       "55000   2.08  0.00  \n",
       "60000   2.14  0.00  \n",
       "65000   2.10  0.00  \n",
       "70000   2.17  0.00  \n",
       "75000   2.13  0.00  \n",
       "80000   2.15  0.00  \n",
       "85000   2.12  0.00  \n",
       "90000   2.14  0.00  \n",
       "95000   2.21  0.00  \n",
       "100000  2.09  0.00  \n",
       "105000  2.16  0.00  \n",
       "110000  2.17  0.00  \n",
       "115000  2.22  0.00  \n",
       "120000  2.20  0.00  \n",
       "125000  2.20  0.00  \n",
       "130000  2.22  0.00  \n",
       "135000  2.20  0.00  \n",
       "140000  2.17  0.00  \n",
       "145000  2.22  0.00  \n",
       "150000  2.23  0.00  \n",
       "...      ...   ...  \n",
       "350000  2.27  0.00  \n",
       "355000  2.29  0.00  \n",
       "360000  2.33  0.00  \n",
       "365000  2.19  0.00  \n",
       "370000  2.28  0.00  \n",
       "375000  2.28  0.00  \n",
       "380000  2.32  0.00  \n",
       "385000  2.29  0.00  \n",
       "390000  2.32  0.00  \n",
       "395000  2.21  0.00  \n",
       "400000  2.28  0.00  \n",
       "405000  2.28  0.00  \n",
       "410000  2.31  0.00  \n",
       "415000  2.30  0.00  \n",
       "420000  2.29  0.00  \n",
       "425000  2.30  0.00  \n",
       "430000  2.30  0.00  \n",
       "435000  2.30  0.00  \n",
       "440000  2.32  0.00  \n",
       "445000  2.29  0.00  \n",
       "450000  2.29  0.00  \n",
       "455000  2.28  0.00  \n",
       "460000  2.33  0.00  \n",
       "465000  2.20  0.00  \n",
       "470000  2.29  0.00  \n",
       "475000  2.30  0.00  \n",
       "480000  2.31  0.00  \n",
       "485000  2.30  0.00  \n",
       "490000  2.33  0.00  \n",
       "495000  2.30  0.00  \n",
       "\n",
       "[99 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 R: 1.000 P: 0.409 carry pockets room pocket small perfect nice strap hold back\n",
      "   1 R: 0.21 P: 0.090 - inch sleeve : zipper quality nice price bit tight\n",
      "     12 R: 0.04 P: 0.038 & ; pro drive tablet surface perfectly air hard protection\n",
      "     13 R: 0.08 P: 0.078 sleeve protection air inside ipad protect pro perfectly chromebook soft\n",
      "   2 R: 0.17 P: 0.069 price bought quality amazon ... $ purchase item happy buy\n",
      "     23 R: 0.06 P: 0.061 color love perfectly mac picture ! pink blue colors pro\n",
      "     22 R: 0.04 P: 0.040 ! love perfect ... absolutely recommend awesome buy compliments cute\n",
      "   3 R: 0.09 P: 0.040 months broke zipper years year started bought weeks handle quality\n",
      "     31 R: 0.05 P: 0.048 bottom top part piece corners plastic back feet months cracked\n",
      "   5 R: 0.07 P: 0.028 smell cheap return air $ item bad 'm days pay\n",
      "     51 R: 0.04 P: 0.043 cover keyboard hard pro scratches mac apple screen shell nice\n",
      "   4 R: 0.06 P: 0.022 return received money item back seller $ ! ordered cheap\n",
      "     41 R: 0.04 P: 0.035 cover keyboard hard screen apple easy pro protector bottom nice\n"
     ]
    }
   ],
   "source": [
    "if len(log_df) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "while epoch < config.epochs:\n",
    "    # train\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = model.get_feed_dict(batch)\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, ppls_batch, global_step_log = \\\n",
    "        sess.run([model.opt, model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_reg, model.topic_ppls, tf.train.get_global_step()], feed_dict = feed_dict)\n",
    "\n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        # validate\n",
    "#         if global_step_log % config.log_period == 0:\n",
    "        if global_step_log % 5000 == 0:            \n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, rads_bow_dev, probs_topic_dev = get_loss(sess, dev_batches, model)\n",
    "\n",
    "            # log\n",
    "            clear_output()\n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_reg_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_reg_dev],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "\n",
    "            # visualize topic\n",
    "#             topic_rad_bow = {topic_idx: rad_bow for topic_idx, rad_bow in zip(model.topic_idxs, rads_bow_dev)}\n",
    "            topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "            \n",
    "            recur_topic_idxs = {parent_idx: get_recur_topic_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "            recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in recur_topic_idxs.items()}\n",
    "            \n",
    "            print_topic_sample(tree_idxs, sess, model, topic_prob_topic=topic_prob_topic, topic_rad_bow=recur_prob_topic)\n",
    "            print_topic_specialization(sess, model)\n",
    "            print_hierarchical_affinity(sess, model)\n",
    "            time_start = time.time()\n",
    "\n",
    "            # update tree\n",
    "#             tree_idxs, update_tree_flg = update_tree(topic_rad_bow, topic_prob_topic, model, add_threshold=0.2, remove_threshold=0.05)\n",
    "            tree_idxs, update_tree_flg = update_tree(recur_prob_topic, topic_prob_topic, model, add_threshold=0.05, remove_threshold=0.05)\n",
    "            if update_tree_flg:\n",
    "                print(tree_idxs)\n",
    "                name_variables = {tensor.name: variable for tensor, variable in zip(tf.global_variables(), sess.run(tf.global_variables()))} # store paremeters\n",
    "                if 'sess' in globals(): sess.close()\n",
    "                model = Model(config, tree_idxs)\n",
    "                sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "                name_tensors = {tensor.name: tensor for tensor in tf.global_variables()}\n",
    "                sess.run([name_tensors[name].assign(variable) for name, variable in name_variables.items()]) # restore parameters\n",
    "\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "    epoch += 1\n",
    "\n",
    "loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, rads_bow_dev, probs_topic_dev = get_loss(sess, dev_batches, model)\n",
    "topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "\n",
    "recur_topic_idxs = {parent_idx: get_recur_topic_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in recur_topic_idxs.items()}\n",
    "display(log_df)\n",
    "print_topic_sample(tree_idxs, sess, model, topic_prob_topic=topic_prob_topic, topic_rad_bow=recur_prob_topic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
