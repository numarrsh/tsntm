{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from data_structure import get_batches\n",
    "from hntm import HierarchicalNeuralTopicModel\n",
    "from tree import get_descendant_idxs\n",
    "from evaluation import validate, print_hierarchical_affinity, print_topic_sample, print_topic_specialization\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '3', 'visible gpu')\n",
    "flags.DEFINE_integer('seed', seed, 'random seed')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/20news/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/topic_vae', 'directory of model')\n",
    "flags.DEFINE_string('modelname', '20news', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 1000, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 3000, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.01, 'lr')\n",
    "flags.DEFINE_float('reg', 1., 'regularization term')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_bow', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "\n",
    "flags.DEFINE_float('depth_temperature', 1., 'dropout rate')\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))\n",
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     10
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables, model):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, model, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    return _variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','TM','','','','VALID:','TM','','',''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL','REG','LOSS','PPL','NLL','KL','REG']]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_idxs = {0:[1, 2, 3, 4, 5], \n",
    "              1:[11, 12], 2:[21, 22], 3:[31, 32], 4:[41, 42], 5:[51, 52]}\n",
    "\n",
    "if 'sess' in globals(): sess.close()\n",
    "model = HierarchicalNeuralTopicModel(config, tree_idxs)\n",
    "sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "name_variables = {tensor.name: variable for tensor, variable in zip(tf.global_variables(), sess.run(tf.global_variables()))}\n",
    "saver = tf.train.Saver(max_to_keep=10)\n",
    "update_tree_flg = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>VALID:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>68</td>\n",
       "      <td>28</td>\n",
       "      <td>99</td>\n",
       "      <td>596.18</td>\n",
       "      <td>858</td>\n",
       "      <td>592.99</td>\n",
       "      <td>3.01</td>\n",
       "      <td>0.18</td>\n",
       "      <td>589.53</td>\n",
       "      <td>1041</td>\n",
       "      <td>585.22</td>\n",
       "      <td>4.16</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>76</td>\n",
       "      <td>57</td>\n",
       "      <td>24</td>\n",
       "      <td>595.55</td>\n",
       "      <td>845</td>\n",
       "      <td>591.52</td>\n",
       "      <td>3.88</td>\n",
       "      <td>0.14</td>\n",
       "      <td>572.94</td>\n",
       "      <td>905</td>\n",
       "      <td>568.24</td>\n",
       "      <td>4.62</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>64</td>\n",
       "      <td>85</td>\n",
       "      <td>124</td>\n",
       "      <td>593.34</td>\n",
       "      <td>832</td>\n",
       "      <td>588.96</td>\n",
       "      <td>4.27</td>\n",
       "      <td>0.11</td>\n",
       "      <td>571.12</td>\n",
       "      <td>864</td>\n",
       "      <td>566.28</td>\n",
       "      <td>4.81</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>56</td>\n",
       "      <td>114</td>\n",
       "      <td>49</td>\n",
       "      <td>593.66</td>\n",
       "      <td>827</td>\n",
       "      <td>589.18</td>\n",
       "      <td>4.39</td>\n",
       "      <td>0.09</td>\n",
       "      <td>571.30</td>\n",
       "      <td>866</td>\n",
       "      <td>566.75</td>\n",
       "      <td>4.53</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>54</td>\n",
       "      <td>142</td>\n",
       "      <td>149</td>\n",
       "      <td>593.42</td>\n",
       "      <td>824</td>\n",
       "      <td>588.91</td>\n",
       "      <td>4.42</td>\n",
       "      <td>0.09</td>\n",
       "      <td>570.57</td>\n",
       "      <td>868</td>\n",
       "      <td>566.30</td>\n",
       "      <td>4.20</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>59</td>\n",
       "      <td>171</td>\n",
       "      <td>74</td>\n",
       "      <td>593.12</td>\n",
       "      <td>820</td>\n",
       "      <td>588.57</td>\n",
       "      <td>4.45</td>\n",
       "      <td>0.09</td>\n",
       "      <td>568.40</td>\n",
       "      <td>838</td>\n",
       "      <td>563.85</td>\n",
       "      <td>4.47</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35000</th>\n",
       "      <td>65</td>\n",
       "      <td>199</td>\n",
       "      <td>174</td>\n",
       "      <td>592.93</td>\n",
       "      <td>816</td>\n",
       "      <td>588.33</td>\n",
       "      <td>4.51</td>\n",
       "      <td>0.09</td>\n",
       "      <td>566.68</td>\n",
       "      <td>832</td>\n",
       "      <td>561.96</td>\n",
       "      <td>4.66</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40000</th>\n",
       "      <td>50</td>\n",
       "      <td>228</td>\n",
       "      <td>99</td>\n",
       "      <td>592.71</td>\n",
       "      <td>815</td>\n",
       "      <td>588.13</td>\n",
       "      <td>4.50</td>\n",
       "      <td>0.08</td>\n",
       "      <td>569.85</td>\n",
       "      <td>848</td>\n",
       "      <td>565.59</td>\n",
       "      <td>4.23</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45000</th>\n",
       "      <td>59</td>\n",
       "      <td>257</td>\n",
       "      <td>24</td>\n",
       "      <td>592.70</td>\n",
       "      <td>813</td>\n",
       "      <td>588.11</td>\n",
       "      <td>4.51</td>\n",
       "      <td>0.08</td>\n",
       "      <td>567.18</td>\n",
       "      <td>826</td>\n",
       "      <td>562.57</td>\n",
       "      <td>4.54</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      TRAIN:   TM                      VALID:    TM          \\\n",
       "      Time   Ep   Ct    LOSS  PPL     NLL    KL   REG    LOSS   PPL     NLL   \n",
       "5000    68   28   99  596.18  858  592.99  3.01  0.18  589.53  1041  585.22   \n",
       "10000   76   57   24  595.55  845  591.52  3.88  0.14  572.94   905  568.24   \n",
       "15000   64   85  124  593.34  832  588.96  4.27  0.11  571.12   864  566.28   \n",
       "20000   56  114   49  593.66  827  589.18  4.39  0.09  571.30   866  566.75   \n",
       "25000   54  142  149  593.42  824  588.91  4.42  0.09  570.57   868  566.30   \n",
       "30000   59  171   74  593.12  820  588.57  4.45  0.09  568.40   838  563.85   \n",
       "35000   65  199  174  592.93  816  588.33  4.51  0.09  566.68   832  561.96   \n",
       "40000   50  228   99  592.71  815  588.13  4.50  0.08  569.85   848  565.59   \n",
       "45000   59  257   24  592.70  813  588.11  4.51  0.08  567.18   826  562.57   \n",
       "\n",
       "                   \n",
       "         KL   REG  \n",
       "5000   4.16  0.15  \n",
       "10000  4.62  0.07  \n",
       "15000  4.81  0.04  \n",
       "20000  4.53  0.03  \n",
       "25000  4.20  0.07  \n",
       "30000  4.47  0.08  \n",
       "35000  4.66  0.06  \n",
       "40000  4.23  0.03  \n",
       "45000  4.54  0.07  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 R: 1.000 P: 0.349 write article get one know think like go say see\n",
      "   1 R: 0.375 P: 0.129 file use program image window available version information also run\n",
      "     14 R: 0.045 P: 0.045 game play team win player year season hockey la league\n",
      "     12 R: 0.083 P: 0.083 use card drive problem thanks system work get driver windows\n",
      "     13 R: 0.117 P: 0.117 god one say jesus christian people believe think write religion\n",
      "   4 R: 0.101 P: 0.048 space launch use nasa system satellite new program research earth\n",
      "     42 R: 0.053 P: 0.053 people turkish say armenian armenians one kill war woman turkey\n",
      "   6 R: 0.056 P: 0.032 key use chip encryption system clipper government phone device ground\n",
      "     61 R: 0.003 P: 0.003 bike ride dog motorcycle helmet bmw dod car road rider\n",
      "     62 R: 0.022 P: 0.022 car bike buy drive speed engine dod power driver model\n",
      "   2 R: 0.118 P: 0.057 government gun people law use state crime drug tax weapon\n",
      "     21 R: 0.061 P: 0.061 president people say make israel work stephanopoulos think country state\n",
      "1 0.12612305641101362\n",
      "2 0.5185396241215926\n",
      "3 0.5408077908782272\n",
      "child 0.303, not-child: 0.239\n",
      "{0: [1, 4, 6, 2, 3], 1: [12, 13, 11], 4: [42], 6: [62], 2: [21, 22], 3: [31]}\n"
     ]
    }
   ],
   "source": [
    "if len(log_df) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "while epoch < config.epochs:    \n",
    "    # train\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = model.get_feed_dict(batch)\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, ppls_batch, global_step_log = \\\n",
    "        sess.run([model.opt, model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_reg, model.topic_ppls, tf.train.get_global_step()], feed_dict = feed_dict)\n",
    "\n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        # validate\n",
    "        if global_step_log % 5000 == 0:            \n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, rads_bow_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "\n",
    "            # log\n",
    "            clear_output()\n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_reg_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_reg_dev],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "\n",
    "            # visualize topic\n",
    "            topics_freq_idxs = bow_idxs[sess.run(model.topics_freq_bow_indices)]\n",
    "            topic_freq_token = {topic_idx: ' '.join([idx_to_word[idx] for idx in topic_freq_idxs]) for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "            topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "            descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "            recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "            \n",
    "            print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_token=topic_freq_token)\n",
    "            print_topic_specialization(sess, model, instances_test)\n",
    "            print_hierarchical_affinity(sess, model)\n",
    "            time_start = time.time()\n",
    "\n",
    "            # update tree\n",
    "            tree_idxs, update_tree_flg = model.update_tree(topic_prob_topic, recur_prob_topic, add_threshold=0.05, remove_threshold=0.05)\n",
    "            if update_tree_flg:\n",
    "                print(tree_idxs)\n",
    "                name_variables = {tensor.name: variable for tensor, variable in zip(tf.global_variables(), sess.run(tf.global_variables()))} # store paremeters\n",
    "                if 'sess' in globals(): sess.close()\n",
    "                model = HierarchicalNeuralTopicModel(config, tree_idxs)\n",
    "                sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "                name_tensors = {tensor.name: tensor for tensor in tf.global_variables()}\n",
    "                sess.run([name_tensors[name].assign(variable) for name, variable in name_variables.items()]) # restore parameters\n",
    "\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "    epoch += 1\n",
    "\n",
    "loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, rads_bow_dev, probs_topic_dev = get_loss(sess, dev_batches, model)\n",
    "topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "\n",
    "descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "display(log_df)\n",
    "print_topic_sample(tree_idxs, sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
