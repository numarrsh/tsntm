{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "import _pickle as cPickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from scipy.special import gammaln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/bags/instances.pkl', 'path of data')\n",
    "\n",
    "flags.DEFINE_integer('n_depth', 3, 'depth of tree')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "config = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))\n",
    "docs_bow = [instance.bow for instance in instances_train]\n",
    "docs_raw = [[[bow_index]*int(doc_bow[bow_index]) for bow_index in np.where(doc_bow > 0)[0]] for doc_bow in docs_bow]\n",
    "docs_words = [[idx for idxs in doc for idx in idxs] for doc in docs_raw]\n",
    "words = [word for doc_words in docs_words for word in doc_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs_bow = [instance.bow for instance in instances_test]\n",
    "test_docs_raw = [[[bow_index]*int(test_doc_bow[bow_index]) for bow_index in np.where(test_doc_bow > 0)[0]] for test_doc_bow in test_docs_bow]\n",
    "test_docs_words = [[idx for idxs in doc for idx in idxs] for doc in test_docs_raw][:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31943, 1035, 568401)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_doc = len(docs_words)\n",
    "n_vocab = len(np.unique(words))\n",
    "n_words = len(words)\n",
    "assert n_vocab == len(bow_idxs)\n",
    "n_doc, n_vocab, n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assign docs to tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Topic:\n",
    "    def __init__(self, idx, sibling_idx, parent, depth, n_doc, n_vocab):\n",
    "        self.idx = idx\n",
    "        self.sibling_idx = sibling_idx\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.depth = depth\n",
    "        self.cnt_doc = 0\n",
    "        self.n_doc = n_doc\n",
    "        self.n_vocab = n_vocab\n",
    "        self.cnt_words = np.zeros(n_vocab) # Number of Words over Documents\n",
    "        self.set_prob_words()\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def sample_child(self, doc, train=True):\n",
    "        s_child_prior = self.get_s_child_prior(gam)\n",
    "        s_child_likelihood = self.get_s_child_likelihood(doc, eta)\n",
    "        p_child = np.array(s_child_prior * s_child_likelihood) / np.sum(s_child_prior * s_child_likelihood)\n",
    "        \n",
    "        child_index = np.random.multinomial(1, p_child).argmax()\n",
    "        if verbose: print('Depth: ', self.depth, 'p_child: ', p_child, 'selected:', child_index)\n",
    "        \n",
    "        if child_index < len(self.children):\n",
    "            child = self.children[child_index]\n",
    "        else:\n",
    "            child = self.get_new_child()\n",
    "            if train: self.children += [child]\n",
    "        return child\n",
    "    \n",
    "    def init_sample_child(self, train=True):\n",
    "        s_child_prior = self.get_s_child_prior(gam)\n",
    "        p_child = np.array(s_child_prior) / np.sum(s_child_prior)\n",
    "        \n",
    "        child_index = np.random.multinomial(1, p_child).argmax()\n",
    "        if verbose: print('Depth: ', self.depth, 'p_child: ', p_child, 'selected:', child_index)\n",
    "\n",
    "        if child_index < len(self.children):\n",
    "            child = self.children[child_index]\n",
    "        else:\n",
    "            child = self.get_new_child()\n",
    "            if train: self.children += [child]\n",
    "        return child\n",
    "    \n",
    "    def get_probs_child(self, doc):\n",
    "        s_child_prior = self.get_s_child_prior(gam)\n",
    "        s_child_likelihood = self.get_s_child_likelihood(doc, eta)\n",
    "        p_child = np.array(s_child_prior * s_child_likelihood) / np.sum(s_child_prior * s_child_likelihood)\n",
    "        return p_child\n",
    "    \n",
    "    def get_s_child_prior(self, gam):\n",
    "        s_child_prior = [child.cnt_doc for child in self.children]\n",
    "        s_child_prior += [gam]\n",
    "        return s_child_prior\n",
    "    \n",
    "    def get_s_child_likelihood(self, doc, eta):\n",
    "        if len(self.children) > 0:\n",
    "            children_cnt_words = np.concatenate([np.array([child.cnt_words for child in self.children]), np.zeros([1, self.n_vocab])], 0) # (Children+1) x Vocabulary\n",
    "        else:\n",
    "            children_cnt_words = np.zeros([1, self.n_vocab]) # (Children+1) x Vocabulary\n",
    "        \n",
    "        cnt_words_doc = doc.cnt_words[None, :] # 1 x Vocabulary\n",
    "\n",
    "        logits_likelihood = gammaln(np.sum(children_cnt_words, -1) + n_vocab*eta) \\\n",
    "                            - np.sum(gammaln(children_cnt_words + eta), -1) \\\n",
    "                            - gammaln(np.sum(children_cnt_words + cnt_words_doc, -1) + n_vocab*eta) \\\n",
    "                            + np.sum(gammaln(children_cnt_words + cnt_words_doc + eta), -1)\n",
    "        s_child_likelihood = np.exp(logits_likelihood)\n",
    "        return s_child_likelihood\n",
    "    \n",
    "    def get_new_child(self):\n",
    "        sibling_idx = max([child.sibling_idx for child in self.children]) + 1 if len(self.children) > 0 else 1\n",
    "        idx = self.idx + '-' + str(sibling_idx)\n",
    "        depth = self.depth+1\n",
    "        child = Topic(idx=idx, sibling_idx=sibling_idx, parent=self, depth=depth, n_doc=self.n_doc, n_vocab=self.n_vocab)        \n",
    "        return child\n",
    "        \n",
    "    def get_children(self):\n",
    "        child = self.get_new_child()\n",
    "        children = self.children + [child]\n",
    "        return children\n",
    "    \n",
    "    def delete_topic(self):\n",
    "        self.parent.children.remove(self)\n",
    "        \n",
    "    def set_prob_words(self):\n",
    "        cnt_words = self.cnt_words + eta\n",
    "        self.prob_words = cnt_words / np.sum(cnt_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc:\n",
    "    def __init__(self, idx, words, bow, n_depth):\n",
    "        self.idx = idx\n",
    "        self.words = words\n",
    "        self.cnt_words = bow\n",
    "        assert len(words) == np.sum(bow)\n",
    "        \n",
    "        self.topics = [] # Depth\n",
    "        self.word_depths = [] # Word Indices\n",
    "        self.depth_cnt_words = np.zeros([n_depth, n_vocab])\n",
    "                \n",
    "    def get_probs_depth(self, word_idx):\n",
    "        s_docs = np.sum(self.depth_cnt_words, -1) + alpha # Depth\n",
    "        s_words = np.array([topic.cnt_words[word_idx] for topic in self.topics]) + eta # Depth\n",
    "        z_words = np.array([np.sum(topic.cnt_words) for topic in self.topics]) + n_vocab*eta # Depth\n",
    "        assert s_docs.shape == s_words.shape == z_words.shape\n",
    "\n",
    "        s_depths = s_docs*s_words/z_words\n",
    "        p_depths = s_depths/np.sum(s_depths) # Depth\n",
    "        return p_depths\n",
    "    \n",
    "    def sample_depth(self, word_idx):\n",
    "        prob_depths = self.get_probs_depth(word_idx)\n",
    "        word_depth = np.argmax(np.random.multinomial(1, prob_depths))\n",
    "        return word_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample doc path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p({\\bf c}_{m}\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf w}, {\\bf c}_{-m}, {\\bf z})\\propto p({\\bf w}_{m}\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf c}, {\\bf w}_{-m}, {\\bf z})\\cdot p({\\bf c}_{m}\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf c}_{-m})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p({\\bf w}_{m}\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf c}, {\\bf w}_{-m}, {\\bf z})=\\prod_{\\ell=1}^{L}\\left(\\frac{\\Gamma(n_{c_{m,\\ell},-m}^{(\\cdot)}+W\\eta)}{\\prod_{w}\\Gamma(n_{c_{m,\\ell},-m}^{(w)}+\\eta)}\\frac{\\prod_{w}\\Gamma(n_{c_{m,\\ell},-m}^{(w)}+n_{c_{m,\\ell},m}^{(w)}+\\eta)}{\\Gamma(n_{c_{m,\\ell},-m}^{(\\cdot)}+n_{c_{m,\\ell},m}^{(\\cdot)}+W\\eta)}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_doc_topics(docs, topic_root, train=True):\n",
    "    for doc in docs:\n",
    "        topic = topic_root\n",
    "        doc.topics = [topic]\n",
    "        if train: topic.cnt_doc += 1 # increment count of docs\n",
    "\n",
    "        for depth in range(1, n_depth):\n",
    "            topic = topic.init_sample_child(train=train)\n",
    "            doc.topics += [topic]\n",
    "            if train: topic.cnt_doc += 1 # increment count of docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_doc_topics(docs, topic_root, train=True):\n",
    "    for doc in docs:\n",
    "        if train:\n",
    "            for depth in range(1, n_depth):\n",
    "                topic = doc.topics[depth]\n",
    "                topic.cnt_doc -= 1 # decrement count of docs\n",
    "                assert topic.cnt_doc >= 0\n",
    "                topic.cnt_words -= doc.depth_cnt_words[depth] # decrement count of words\n",
    "                assert np.min(topic.cnt_words) >= 0\n",
    "\n",
    "                if topic.cnt_doc == 0: \n",
    "                    topic.delete_topic()\n",
    "                    assert np.sum(topic.cnt_words) == 0\n",
    "\n",
    "        topic = topic_root\n",
    "        doc.topics = [topic]\n",
    "        for depth in range(1, n_depth):\n",
    "            topic = topic.sample_child(doc, train=train)\n",
    "            doc.topics += [topic]\n",
    "            if train: topic.cnt_doc += 1 # increment count of docs\n",
    "            if train: topic.cnt_words += doc.depth_cnt_words[depth] # increment count of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assign words to topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "p(z_{i}=j\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf z}_{-i},{\\bf w})\\propto (n_{-i,j}^{(d_{i})}+\\alpha)\\frac{n_{-i,j}^{(w_{i})}+\\eta}{n_{-i,j}^{(\\cdot)}+W\\eta}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_word_topics(docs, train=True):\n",
    "    for doc in docs:\n",
    "        if doc.idx % 10000 == 0: print(doc.idx, end=' ')\n",
    "        for word_index, word_idx in enumerate(doc.words):\n",
    "            # sample depth of word\n",
    "            new_depth = doc.sample_depth(word_idx)\n",
    "            new_topic = doc.topics[new_depth]\n",
    "            \n",
    "            # increment count of words\n",
    "            doc.depth_cnt_words[new_depth, word_idx] += 1\n",
    "            if train: new_topic.cnt_words[word_idx] += 1\n",
    "            doc.word_depths.append(new_depth) # for reference when sampling\n",
    "            \n",
    "        assert len(doc.word_depths) == len(doc.words) == np.sum(doc.depth_cnt_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_word_topics(docs, train=True):\n",
    "    for doc in docs:\n",
    "        if doc.idx % 10000 == 0: print(doc.idx, end=' ')\n",
    "        for word_index, word_idx in enumerate(doc.words):\n",
    "            # refer depth of word\n",
    "            old_depth = doc.word_depths[word_index]\n",
    "            old_topic = doc.topics[old_depth]\n",
    "            \n",
    "            # decrement count of words\n",
    "            doc.depth_cnt_words[old_depth, word_idx] -= 1\n",
    "            if train: old_topic.cnt_words[word_idx] -= 1            \n",
    "            \n",
    "            # sample depth of word\n",
    "            new_depth = doc.sample_depth(word_idx)\n",
    "            new_topic = doc.topics[new_depth]\n",
    "            \n",
    "            # increment count of words\n",
    "            doc.depth_cnt_words[new_depth, word_idx] += 1\n",
    "            if train: new_topic.cnt_words[word_idx] += 1\n",
    "            doc.word_depths[word_index] = new_depth # for sample\n",
    "            \n",
    "        assert len(doc.word_depths) == len(doc.words) == np.sum(doc.depth_cnt_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recur_cnt_words(topic):\n",
    "    cnt_words = np.sum(topic.cnt_words)\n",
    "    for child in topic.children:\n",
    "        cnt_words += recur_cnt_words(child)\n",
    "    return cnt_words\n",
    "    \n",
    "def assert_sum_cnt_words(topic_root):\n",
    "    sum_cnt_words = recur_cnt_words(topic_root)\n",
    "    assert sum_cnt_words == sum([len(doc.words) for doc in docs])\n",
    "    \n",
    "def nearly_equal(val, thre):\n",
    "    return (val > thre-1e-5) and (val < thre+1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.array([10., 5., 1.])\n",
    "gam = 0.01\n",
    "eta = 1.\n",
    "n_depth = 3\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_sample = 100\n",
    "docs = [Doc(idx=doc_idx, words=doc_words, bow=doc_bow, n_depth=config.n_depth) for doc_idx, (doc_words, doc_bow) in enumerate(zip(docs_words, docs_bow))]\n",
    "topic_root = Topic(idx='0', sibling_idx=0, parent=None, depth=0, n_doc=n_doc, n_vocab=n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_train(docs, topic_root):\n",
    "    init_doc_topics(docs=docs, topic_root=topic_root)\n",
    "    init_word_topics(docs=docs)\n",
    "    assert_sum_cnt_words(topic_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_train(docs, topic_root):\n",
    "    sample_doc_topics(docs=docs, topic_root=topic_root)\n",
    "    sample_word_topics(docs=docs)\n",
    "    assert_sum_cnt_words(topic_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_perplexity(docs, topic_root):\n",
    "    def set_prob_words(topic):\n",
    "        topic.set_prob_words()\n",
    "        for topic_child in topic.children:\n",
    "            set_prob_words(topic_child)\n",
    "            \n",
    "    # set Probabilty of Words\n",
    "    set_prob_words(topic_root)\n",
    "    \n",
    "    logit_docs, n_words = 0, 0\n",
    "    for doc in docs:\n",
    "        # Path Probability for each document\n",
    "        topic = topic_root\n",
    "        probs_paths= [{topic: 1.}]\n",
    "        for depth in range(1, n_depth):\n",
    "            probs_path = {}\n",
    "            for topic, prob_path in probs_paths[-1].items():\n",
    "                topics_child = topic.get_children()\n",
    "                probs_child = topic.get_probs_child(doc)\n",
    "                probs_path_child = prob_path * probs_child\n",
    "                for topic_child, prob_path_child in zip(topics_child, probs_path_child):\n",
    "                    probs_path[topic_child] = prob_path_child\n",
    "            probs_paths.append(probs_path)    \n",
    "            \n",
    "        assert nearly_equal(np.sum([sum(probs_path.values()) for probs_path in probs_paths]), n_depth)        \n",
    "\n",
    "        # Depth Probability for Each Word\n",
    "        probs_depths = []\n",
    "        for word_index, word_idx in enumerate(doc.words):\n",
    "            probs_depth = doc.get_probs_depth(word_idx)\n",
    "            probs_depths.append(probs_depth)\n",
    "            \n",
    "        assert nearly_equal(np.sum(probs_depths), len(doc.words))\n",
    "    \n",
    "        # Likelihood of Doc\n",
    "        assert len(probs_depths) == len(doc.words)\n",
    "        logit_doc = 0\n",
    "        for prob_depths, word_idx in zip(probs_depths, doc.words):\n",
    "#             prob_topics, prob_word_topics = [], []\n",
    "            prob_word = 0\n",
    "            for prob_paths, prob_depth in zip(probs_paths, prob_depths):\n",
    "                for topic, prob_path in prob_paths.items():\n",
    "                    prob_topic = prob_path * prob_depth # scalar\n",
    "                    prob_word_topic = topic.prob_words[word_idx] # scalar\n",
    "#                     prob_topics.append(prob_topic)\n",
    "#                     prob_word_topics.append(prob_word_topic)\n",
    "                    prob_word += prob_topic * prob_word_topic\n",
    "            logit_word = np.log(prob_word)\n",
    "            logit_doc += logit_word\n",
    "        logit_docs += logit_doc\n",
    "        n_words += len(doc.words)\n",
    "#         assert nearly_equal(sum(prob_topics), 1.)\n",
    "        \n",
    "    perplexity = np.exp(-logit_docs/n_words)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_test(test_docs, topic_root):\n",
    "    init_doc_topics(docs=test_docs, topic_root=topic_root, train=False)\n",
    "    init_word_topics(docs=test_docs, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_test(test_docs, topic_root):\n",
    "    sample_doc_topics(docs=test_docs, topic_root=topic_root, train=False)\n",
    "    sample_word_topics(docs=test_docs, train=False)\n",
    "    assert_sum_cnt_words(topic_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 : ['0-1', '0-2', '0-3', '0-4', '0-5', '0-6', '0-7', '0-8', '0-9', '0-10', '0-11', '0-12', '0-13', '0-14', '0-15', '0-16', '0-17', '0-18', '0-19', '0-20', '0-21', '0-22', '0-23', '0-24', '0-25', '0-26', '0-27', '0-28', '0-29', '0-30', '0-31', '0-32', '0-33', '0-34', '0-35', '0-36', '0-37', '0-38', '0-39', '0-40', '0-41', '0-42', '0-43', '0-44', '0-45', '0-46', '0-47', '0-48', '0-49', '0-50', '0-51', '0-52', '0-53', '0-54', '0-55', '0-56', '0-57', '0-58', '0-59', '0-60', '0-61', '0-62', '0-63', '0-64'] 31943 261116.0 ['!', 'nice', 'bought', 'price', 'quality', 'love', \"'m\", 'made', 'perfect', 'recommend']\n",
      "   0-1 : ['0-1-4', '0-1-6'] 356 2940.0 ['color', 'picture', 'blue', 'pink', 'green', 'purple', 'black', 'shown', 'received', 'red']\n",
      "     0-1-4 : [] 222 141.0 ['packaging', 'fingerprints', 'knew', 'weird', 'expecting', '-', 'cut', 'wont', 'spot', 'correctly']\n",
      "     0-1-6 : [] 134 90.0 ['darker', 'package', 'version', 'file', 'tend', 'tight', 'blue', 'weird', 'important', 'picked']\n",
      "   0-2 : ['0-2-1', '0-2-2'] 142 1356.0 ['security', 'airport', 'tsa', 'travel', 'remove', 'friendly', 'pockets', 'open', 'feature', 'lock']\n",
      "     0-2-1 : [] 83 90.0 ['unzip', 'lock', 'concerned', 'trips', 'avoid', 'check', 'ultrabook', 'red', 'trouble', 'ended']\n",
      "     0-2-2 : [] 59 57.0 ['traveling', 'feature', 'held', 'stuck', 'feeling', 'compartments', 'prevent', 'sizes', 'size', 'future']\n",
      "   0-3 : ['0-3-1', '0-3-2'] 288 2713.0 ['zipper', 'zippers', 'broke', 'zip', 'open', 'close', 'compartment', 'broken', 'months', 'main']\n",
      "     0-3-1 : [] 225 154.0 ['full', 'broken', 'pull', 'easily', 'flap', 'compartments', 'started', 'stitching', 'anymore', 'falling']\n",
      "     0-3-2 : [] 63 74.0 ['wide', 'strap', 'reasonable', 'test', 'cool', 'serves', 'edge', 'zipped', 'memory', 'fabric']\n",
      "   0-4 : ['0-4-2'] 756 6753.0 ['cover', 'keyboard', 'color', '!', 'screen', 'protector', 'love', 'keys', 'hard', 'type']\n",
      "     0-4-2 : [] 756 588.0 ['free', 'corners', 'snaps', 'clear', 'fingerprints', 'people', 'edges', 'minor', 'sleeve', 'began']\n",
      "   0-5 : ['0-5-1', '0-5-2'] 582 4258.0 ['inch', 'sleeve', 'size', 'laptops', 'big', 'small', 'inches', 'dimensions', 'asus', 'bigger']\n",
      "     0-5-1 : [] 393 252.0 ['difference', \"'\", 'searched', 'originally', 'huge', 'find', 'reviews', 'compared', 'dimensions', 'lenovo']\n",
      "     0-5-2 : [] 189 112.0 ['mba', 'colors', 'sized', 'positive', 'width', 'work', 'notebook', 'close', 'mail', 'returned']\n",
      "   0-6 : ['0-6-2'] 197 1777.0 ['cards', 'sd', 'memory', 'card', 'slots', 'small', 'plastic', 'camera', 'pockets', 'compact']\n",
      "     0-6-2 : [] 197 102.0 ['well-made', 'shipping', 'phone', 'eventually', 'lots', 'accessible', 'place', 'works', 'storage', 'tear']\n",
      "   0-7 : ['0-7-1', '0-7-3'] 237 2293.0 ['broke', 'zipper', 'months', 'handle', 'amazon', 'warranty', 'week', 'days', 'year', 'service']\n",
      "     0-7-1 : [] 126 100.0 ['began', 'coming', 'key', 'constantly', 'saved', 'live', 'purchasing', 'netbook', 'class', 'realized']\n",
      "     0-7-3 : [] 111 63.0 ['replace', 'snapped', 'hours', 'system', 'loves', 'rolls', 'bother', 'sticks', 'car', 'outer']\n",
      "   0-8 : ['0-8-1', '0-8-2'] 347 3303.0 ['bottom', 'top', 'cover', 'part', 'piece', 'back', 'snap', 'stay', 'place', 'air']\n",
      "     0-8-1 : [] 239 208.0 ['returning', 'make', 'damaged', 'pieces', 'forward', 'found', 'company', 'clips', 'chromebook', 'opened']\n",
      "     0-8-2 : [] 108 88.0 ['fan', 'edge', 'broken', 'worth', 'bottom', 'install', 'notice', 'sit', 'grab', 'guess']\n",
      "   0-9 : ['0-9-1', '0-9-2'] 426 3787.0 ['months', 'bottom', 'corners', 'cover', 'cracked', \"'ve\", 'speck', 'started', 'crack', 'color']\n",
      "     0-9-1 : [] 349 262.0 ['corners', 'normal', 'rubberized', 'loved', 'recently', 'sides', 'life', 'owned', 'abuse', 'shape']\n",
      "     0-9-2 : [] 77 73.0 ['rain', 'finish', 'suggest', 'weeks', 'water', 'days', 'cost', 'couple', 'covered', 'protector']\n",
      "   0-10 : ['0-10-1', '0-10-4'] 268 2327.0 ['...', 'handle', 'wheels', 'years', 'year', 'rolling', 'model', 'ago', 'longer', 'bags']\n",
      "     0-10-1 : [] 150 149.0 ['+', 'floor', 'briefcase', 'lasted', 'stay', 'note', 'sizes', 'portion', 'hoping', 'daily']\n",
      "     0-10-4 : [] 118 85.0 ['attached', '...', 'figure', 'dropped', 'compartment', 'construction', 'cut', 'roll', 'replacement', 'easy']\n",
      "   0-11 : ['0-11-2', '0-11-3'] 752 6099.0 ['sleeve', 'netbook', 'protection', 'mouse', 'perfect', 'inch', 'hp', 'power', 'room', 'protect']\n",
      "     0-11-2 : [] 483 355.0 ['inspiron', 'dell', 'cell', 'mouse', 'wife', 'drop', 'battery', 'perfectly', 'bring', 'number']\n",
      "     0-11-3 : [] 269 229.0 ['accommodate', 'pocket', 'samsung', 'highly', 'spare', 'satisfied', 'travel', 'papers', 'orange', 'needed']\n",
      "   0-12 : ['0-12-2', '0-12-3'] 720 5286.0 ['smell', '!', 'sleeve', 'air', 'strong', 'days', 'bad', 'smells', '&', 'chemical']\n",
      "     0-12-2 : [] 533 463.0 ['fast', 'mentioned', 'material', 'returning', 'zipper', 'mention', 'people', 'time', 'protective', 'type']\n",
      "     0-12-3 : [] 187 171.0 ['sony', 'box', 'series', 'happy', 'pay', 'delivered', 'toshiba', 'snug', 'vaio', 'print']\n",
      "   0-13 : ['0-13-2'] 221 2139.0 ['cover', 'keyboard', 'screen', 'protector', 'mouse', 'keys', 'hard', 'pad', 'money', 'key']\n",
      "     0-13-2 : [] 221 168.0 ['break', 'cheaper', 'fine', 'positive', 'noticed', 'adapter', 'gift', 'pay', 'fault', 'excellent']\n",
      "   0-14 : ['0-14-1', '0-14-2'] 150 1512.0 ['usb', 'card', 'drive', 'power', 'works', 'ports', 'external', 'port', 'cable', 'work']\n",
      "     0-14-1 : [] 103 79.0 ['inspiron', 'port', 'reason', 'covered', 'intended', 'fully', 'lock', 'adapter', 'properly', 'minor']\n",
      "     0-14-2 : [] 47 56.0 ['description', 'fan', 'toshiba', 'listed', 'slides', 'turned', 'website', 'small', 'add', 'end']\n",
      "   0-15 : ['0-15-2'] 94 937.0 ['player', 'dvd', 'portable', 'car', 'sony', 'seat', 'straps', 'cords', 'front', 'storage']\n",
      "     0-15-2 : [] 94 55.0 ['red', 'doubt', 'options', 'pink', 'kids', 'handles', 'sony', 'flaw', 'hard', 'system']\n",
      "   0-16 : ['0-16-4', '0-16-5'] 753 6804.0 ['carry', 'books', 'back', 'stuff', 'pack', 'pockets', 'comfortable', 'straps', 'school', 'lot']\n",
      "     0-16-4 : [] 127 117.0 ['compact', \"'ve\", 'make', 'small', 'compartments', 'products', 'inch', 'seam', 'time', 'minimal']\n",
      "     0-16-5 : [] 626 537.0 ['laptops', 'supplies', 'smaller', 'walk', 'things', 'spacious', 'places', 'student', 'feel', 'cords']\n",
      "   0-17 : ['0-17-2', '0-17-3'] 253 2261.0 ['strap', 'shoulder', 'broke', 'handle', 'plastic', 'straps', 'design', 'metal', 'heavy', 'carrying']\n",
      "     0-17-2 : [] 175 161.0 ['began', 'clips', 'months', 'noticed', 'realized', 'drop', 'pay', 'longer', 'absolutely', 'wife']\n",
      "     0-17-3 : [] 78 82.0 ['messenger', 'keeping', 'give', 'fix', 'spacious', 'metal', 'reviews', 'asus', 'happened', 'stitching']\n",
      "   0-18 : ['0-18-1', '0-18-2', '0-18-3'] 381 3343.0 ['pro', 'sleeve', ';', '&', 'retina', 'air', 'inch', 'display', 'mbp', 'loose']\n",
      "     0-18-1 : [] 29 22.0 ['disappointed', 'cushioning', 'mentioned', 'type', 'tad', 'online', 'send', 'decent', 'bought', 'ultrabook']\n",
      "     0-18-2 : [] 266 195.0 ['incase', 'protects', 'fitting', 'inside', 'device', 'adjust', 'air', 'uncomfortable', 'correct', 'stretch']\n",
      "     0-18-3 : [] 86 56.0 ['loose', 'amazon', 'amount', 'returned', 'annoying', 'pro', 'properly', 'inexpensive', 'experience', 'provided']\n",
      "   0-19 : ['0-19-2', '0-19-3'] 1064 9109.0 ['cover', 'mac', 'pro', 'hard', 'keyboard', 'apple', 'color', 'shell', 'speck', '!']\n",
      "     0-19-2 : [] 15 9.0 ['affordable', 'absolutely', 'dollars', 'trouble', 'lasted', 'classy', 'quality', 'online', 'fitting', 'fingerprints']\n",
      "     0-19-3 : [] 1049 893.0 ['aluminum', 'ports', 'stand', 'legs', 'website', 'typing', 'wanted', 'apple', 'recently', 'ipearl']\n",
      "   0-20 : ['0-20-1', '0-20-2'] 391 3253.0 ['return', 'cover', 'ordered', 'pro', 'item', 'wrong', 'received', 'back', 'mac', 'seller']\n",
      "     0-20-1 : [] 190 120.0 ['sides', 'arrived', 'im', 'item', 'back', 'paid', 'shipping', 'returned', 'products', 'stated']\n",
      "     0-20-2 : [] 201 126.0 ['size', 'stitching', 'purchased', '/', 'inch', 'times', 'returning', 'corner', 'immediately', 'display']\n",
      "   0-21 : ['0-21-2', '0-21-5'] 239 2325.0 ['&', ';', 'sleeve', 'size', 'description', 'item', 'big', 'laptops', 'dimensions', 'hp']\n",
      "     0-21-2 : [] 49 27.0 ['finally', 'receive', 'paperwork', 'helps', 'ipad', 'experience', 'security', 'pouches', 'today', 'main']\n",
      "     0-21-5 : [] 190 142.0 ['dell', 'seller', 'small', 'tablet', 'spot', 'beautiful', 'stated', 'pattern', 'phone', 'photo']\n",
      "   0-22 : ['0-22-1', '0-22-6'] 1117 8595.0 ['!', 'love', 'sleeve', 'pocket', 'perfect', 'charger', 'color', 'inside', 'perfectly', 'inch']\n",
      "     0-22-1 : [] 321 243.0 ['zippered', 'walk', 'slim', 'asus', 'excellent', 'mail', 'scratch', 'brown', 'extended', 'hp']\n",
      "     0-22-6 : [] 796 654.0 ['spare', 'ipod', 'acer', 'notebooks', 'scratched', 'pen', 'plug', 'soft', 'ipad', 'inside']\n",
      "   0-23 : ['0-23-1', '0-23-2'] 239 2579.0 ['bottom', 'rubber', 'feet', '--', 'cover', ':', 'edges', 'plastic', 'edge', 'sharp']\n",
      "     0-23-1 : [] 147 133.0 ['worried', 'slides', 'typing', 'tend', 'bother', 'cracks', 'table', 'walk', 'happened', 'ports']\n",
      "     0-23-2 : [] 92 66.0 ['scratching', 'sliding', 'feet', 'started', 'scratch', 'extremely', 'comment', 'tend', 'cut', 'pay']\n",
      "   0-24 : ['0-24-1', '0-24-2', '0-24-3'] 259 2243.0 ['velcro', 'flap', '-', 'leather', 'closed', 'open', 'closure', \"'\", 'design', 'strap']\n",
      "     0-24-1 : [] 79 96.0 ['thinking', 'slot', 'velcro', 'samsonite', 'strong', 'desk', 'pink', 'prevent', 'bright', 'cd']\n",
      "     0-24-2 : [] 59 65.0 ['velcro', \"'\", '-', 'organized', 'close', 'section', 'tall', 'back', 'bumps', 'entire']\n",
      "     0-24-3 : [] 121 129.0 ['close', 'mba', 'body', 'portion', 'lower', 'broke', 'pictures', 'included', 'base', 'time']\n",
      "   0-25 : ['0-25-3', '0-25-4'] 113 1180.0 ['drive', 'hard', 'cable', 'usb', 'drives', 'external', 'sleeves', 'rugged', 'inside', 'portable']\n",
      "     0-25-3 : [] 69 47.0 ['cd', 'completely', 'scratched', 'expensive', 'doesnt', 'barely', 'avoid', 'inch', 'initially', 'cords']\n",
      "     0-25-4 : [] 44 37.0 ['original', 'dimensions', 'house', 'send', 'key', 'set', 'putting', 'mini', 'cord', 'inside']\n",
      "   0-26 : ['0-26-2'] 273 3045.0 ['camera', 'lenses', 'canon', 'lens', 'equipment', 'straps', 'room', 'gear', 'pack', 'tripod']\n",
      "     0-26-2 : [] 273 236.0 ['pleased', 'open', 'tripod', 'single', 'expensive', 'couple', 'bags', 'imagine', 'sony', 'originally']\n",
      "   0-27 : ['0-27-3'] 177 2045.0 ['&', ';', 'cover', 'bottom', 'color', 'keyboard', 'top', 'pro', 'part', 'feet']\n",
      "     0-27-3 : [] 177 222.0 ['desk', 'white', 'snap', 'fingerprints', 'version', 'open', 'corners', 'returning', 'prevent', 'slide']\n",
      "   0-28 : ['0-28-3', '0-28-5'] 702 4964.0 ['item', 'amazon', 'arrived', 'received', 'shipping', 'return', 'time', 'quality', 'purchase', 'service']\n",
      "     0-28-3 : [] 545 489.0 ['days', 'description', '!', 'initially', 'purchased', 'including', 'originally', 'expected', 'care', 'brown']\n",
      "     0-28-5 : [] 157 116.0 ['products', 'nicer', 'wide', 'local', 'person', 'outer', 'port', 'rolling', 'standard', 'shows']\n",
      "   0-29 : ['0-29-1', '0-29-2', '0-29-3'] 413 4518.0 ['sleeve', 'protection', '-', 'padding', 'neoprene', ':', 'air', 'zipper', 'foam', 'thin']\n",
      "     0-29-1 : [] 167 197.0 ['offers', 'loose', 'cushioning', 'logo', 'glove', 'pocket', 'plenty', 'lenovo', 'thinner', 'clear']\n",
      "     0-29-2 : [] 98 116.0 ['aluminum', 'rugged', 'support', 'snug', 'expect', '+', 'cheaper', 'protecting', 'reviewers', 'tab']\n",
      "     0-29-3 : [] 148 135.0 ['cable', 'messenger', 'width', 'usb', 'fitting', 'stars', 'stretch', 'add', 'fine', 'basic']\n",
      "   0-30 : ['0-30-1'] 234 3016.0 ['camera', 'lens', 'lenses', 'compartment', 'flash', 'carry', 'accessories', 'pack', 'gear', 'room']\n",
      "     0-30-1 : [] 234 216.0 ['gear', 'strap', 'multiple', 'grip', 'messenger', 'carries', 'places', 'walk', 'foam', 'cell']\n",
      "   0-31 : ['0-31-1', '0-31-2', '0-31-3'] 231 3039.0 ['-', ':', 'pros', 'cons', 'strap', 'pockets', 'shoulder', 'space', 'straps', 'compartment']\n",
      "     0-31-1 : [] 94 131.0 ['months', 'heavy', 'weeks', 'pack', 'design', '-', 'compartment', '$', 'specifically', 'stands']\n",
      "     0-31-2 : [] 112 136.0 ['stuck', 'light', 'holds', 'moving', '+', 'files', 'tear', 'books', '?', 'days']\n",
      "     0-31-3 : [] 25 33.0 ['attach', 'rolling', 'smooth', ';', 'security', 'barely', 'width', 'wide', 'shape', 'leaves']\n",
      "   0-32 : ['0-32-2', '0-32-3'] 160 1660.0 ['water', 'pockets', 'bottle', 'compartment', 'pack', 'main', 'side', 'zipper', 'zippers', 'rain']\n",
      "     0-32-2 : [] 104 107.0 ['gear', 'elastic', 'purchasing', 'pros', 'lid', 'pad', 'review', 'disappointing', 'duty', 'bright']\n",
      "     0-32-3 : [] 56 91.0 ['fitting', 'replacement', 'textbooks', 'expected', 'build', 'stitching', 'velcro', 'edge', 'years', 'metal']\n",
      "   0-33 : ['0-33-5', '0-33-6'] 162 1435.0 ['tablet', 'cover', 'stand', 'screen', 'tab', 'keyboard', 'ipad', 'surface', 'edges', 'angle']\n",
      "     0-33-5 : [] 86 98.0 ['port', 'breaking', 'cool', 'expensive', 'photo', 'adjustable', 'mention', 'today', 'elastic', 'blue']\n",
      "     0-33-6 : [] 76 95.0 ['finger', 'held', 'feature', 'protective', 'asus', 'useless', 'coming', 'compact', 'type', 'fact']\n",
      "   0-34 : ['0-34-1', '0-34-2', '0-34-3'] 606 5836.0 ['!', '...', 'cover', 'bottom', 'money', '$', 'top', 'buy', 'speck', 'scratches']\n",
      "     0-34-1 : [] 176 159.0 ['back', 'samsung', 'looked', 'hassle', 'fault', 'send', 'plain', 'spend', 'ordered', 'refund']\n",
      "     0-34-2 : [] 290 224.0 ['dust', 'caught', 'cracks', 'care', 'metal', 'manufacturer', 'purchasing', 'bucks', 'wo', 'flimsy']\n",
      "     0-34-3 : [] 140 125.0 ['fell', 'customer', 'machine', 'happen', 'test', 'move', 'sits', 'holes', 'feeling', 'hit']\n",
      "   0-35 : ['0-35-2', '0-35-3'] 218 1938.0 ['handle', 'seat', 'wheels', 'travel', 'clothes', 'overhead', 'business', 'rolling', 'luggage', 'trip']\n",
      "     0-35-2 : [] 150 124.0 ['single', 'trip', 'standard', 'sort', 'lasted', 'zippers', 'fairly', 'description', 'dell', 'expect']\n",
      "     0-35-3 : [] 68 58.0 ['days', 'organization', 'notebook', 'larger', 'travels', 'important', 'made', 'wearing', 'making', 'including']\n",
      "   0-36 : ['0-36-1', '0-36-2', '0-36-3'] 143 1349.0 ['netbook', 'battery', 'life', 'acer', 'toshiba', 'cord', 'power', 'pocket', 'pouch', 'hours']\n",
      "     0-36-1 : [] 8 5.0 ['additional', 'rubberized', 'colors', 'bit', 'lid', 'fix', 'finger', 'fingerprints', 'finish', 'fitting']\n",
      "     0-36-2 : [] 93 88.0 ['files', 'air', 'originally', 'prefer', 'note', 'board', 'concern', 'features', 'nicely', 'feature']\n",
      "     0-36-3 : [] 42 52.0 ['covered', 'wife', 'neoprene', 'prefer', 'real', 'suggest', 'reasonable', 'imagine', 'owned', 'specifically']\n",
      "   0-37 : ['0-37-1', '0-37-2'] 1276 10384.0 ['!', 'color', 'love', 'cover', 'keyboard', 'perfectly', 'pro', 'mac', 'apple', 'ordered']\n",
      "     0-37-1 : [] 406 293.0 ['christmas', 'gift', 'snap', 'appearance', 'video', 'wait', 'trouble', 'totally', 'damaged', 'noticed']\n",
      "     0-37-2 : [] 870 703.0 ['purchase', 'chromebook', 'retina', 'perfect', 'wanted', 'seller', 'true', 'durable', 'happy', 'pro']\n",
      "   0-38 : ['0-38-1', '0-38-2', '0-38-3'] 681 5700.0 ['years', 'year', 'bought', 'school', 'quality', 'months', \"'ve\", 'ago', 'straps', 'pack']\n",
      "     0-38-1 : [] 423 311.0 ['mine', 'figured', 'travels', 'wheels', 'update', 'original', 'swissgear', 'problems', 'tough', 'comfort']\n",
      "     0-38-2 : [] 90 72.0 ['contacted', 'lot', 'cable', 'canon', 'organization', 'minor', 'plug', 'service', 'happy', 'uncomfortable']\n",
      "     0-38-3 : [] 168 133.0 ['zippers', 'model', 'lots', 'products', 'regret', 'weight', 'live', 'weekend', 'picked', 'photo']\n",
      "   0-39 : ['0-39-1', '0-39-2'] 630 5365.0 ['ipad', 'carry', 'pocket', 'sleeve', 'protection', 'room', 'cover', 'air', 'charger', 'extra']\n",
      "     0-39-1 : [] 87 75.0 ['green', 'likes', 'days', 'double', 'samsonite', 'simple', 'clothes', 'trust', 'including', 'covers']\n",
      "     0-39-2 : [] 543 449.0 ['extras', 'simply', 'place', 'works', 'intended', 'extra', 'noticed', 'basic', 'keys', 'protection']\n",
      "   0-40 : ['0-40-1', '0-40-2'] 204 2120.0 ['cover', 'bottom', 'apple', 'side', 'logo', 'cut', 'top', 'mac', 'pro', 'plastic']\n",
      "     0-40-1 : [] 59 48.0 ['usage', 'middle', 'video', 'noticeable', 'pieces', 'installed', 'received', 'turned', 'missing', 'securely']\n",
      "     0-40-2 : [] 145 110.0 ['glove', 'texture', 'happen', 'stays', 'lock', 'heavier', 'fact', 'weight', 'center', 'uncomfortable']\n",
      "   0-41 : ['0-41-2'] 115 1161.0 [';', '&', 'handle', 'pull', 'straps', 'airport', 'wheels', 'large', 'carry', 'section']\n",
      "     0-41-2 : [] 115 155.0 ['weight', 'falls', 'item', 'walking', 'picture', 'arrived', 'fault', 'built', 'style', 'plastic']\n",
      "   0-42 : ['0-42-1', '0-42-2'] 667 6500.0 ['pocket', 'power', 'sleeve', 'mouse', 'cord', 'netbook', 'inside', 'zipper', 'charger', 'side']\n",
      "     0-42-1 : [] 631 531.0 ['items', 'accessories', 'scratch', 'perfectly', 'chromebook', 'bonus', 'prevent', 'edge', 'stick', 'tiny']\n",
      "     0-42-2 : [] 36 42.0 ['separate', 'corners', 'seams', 'christmas', 'match', 'vaio', 'constructed', 'carry', 'nicely', 'receive']\n",
      "   0-43 : ['0-43-1', '0-43-4', '0-43-6'] 1408 10227.0 ['sleeve', '&', ';', 'pro', 'inside', 'air', 'protection', 'nice', 'material', 'soft']\n",
      "     0-43-1 : [] 472 360.0 ['mba', 'sleeve', 'snug', 'scratch', 'lightweight', 'protective', 'display', 'closed', 'handles', 'smell']\n",
      "     0-43-4 : [] 217 189.0 ['-inch', 'squeeze', 'duty', 'clean', 'water', 'slightly', 'book', 'textbooks', 'practical', 'careful']\n",
      "     0-43-6 : [] 719 606.0 ['feels', 'inch', 'scratched', 'protecting', 'material', 'snug', 'higher', 'touch', 'pretty', 'bit']\n",
      "   0-44 : ['0-44-1', '0-44-3'] 620 5359.0 ['carry', 'easy', 'travel', 'airport', 'security', 'back', 'work', 'pack', 'pockets', 'items']\n",
      "     0-44-1 : [] 18 28.0 ['disappointing', 'happened', 'star', 'easier', 'held', 'wear', 'process', 'adjust', 'home', 'close']\n",
      "     0-44-3 : [] 602 449.0 ['folders', 'carried', 'feature', 'works', 'line', 'clips', 'daily', 'purse', 'extended', '!']\n",
      "   0-45 : ['0-45-3', '0-45-4'] 412 3871.0 ['back', 'straps', 'strap', 'shoulder', 'padding', 'top', 'pack', 'handle', 'heavy', 'carry']\n",
      "     0-45-3 : [] 203 233.0 ['protects', 'rating', 'designed', 'storage', 'barely', 'access', 'point', 'seam', 'well-made', 'regular']\n",
      "     0-45-4 : [] 209 245.0 ['handle', 'made', 'piece', 'trips', 'suitcase', 'careful', 'solid', 'place', 'form', 'material']\n",
      "   0-46 : ['0-46-1'] 606 5652.0 ['!', 'cover', 'love', '&', ';', 'color', 'keyboard', 'easy', 'mac', 'air']\n",
      "     0-46-1 : [] 606 473.0 ['chromebook', 'angle', 'open', 'surprised', 'super', 'comfort', 'lap', 'snapped', 'make', 'mac']\n",
      "   0-47 : ['0-47-2', '0-47-3'] 332 3285.0 ['work', 'handle', 'carry', 'wheels', 'office', 'day', \"'ve\", 'daily', 'years', 'shoulder']\n",
      "     0-47-2 : [] 143 158.0 ['warranty', 'hand', 'meant', 'find', 'pieces', 'shoulders', 'office', 'duty', 'carrying', 'width']\n",
      "     0-47-3 : [] 189 226.0 ['professional', 'flap', 'happen', 'shoulder', 'suitcase', 'loved', 'work', 'durability', 'dirt', 'heavier']\n",
      "   0-48 : ['0-48-1', '0-48-2', '0-48-3'] 635 5598.0 ['cover', 'protection', 'air', 'scratches', 'mac', 'weight', 'protect', ':', '...', 'bottom']\n",
      "     0-48-1 : [] 361 281.0 ['logo', 'negative', 'bulk', 'metal', 'system', 'shut', 'fall', 'card', 'fault', 'full']\n",
      "     0-48-2 : [] 146 147.0 ['rear', 'lots', 'drawback', 'personal', 'nice', 'section', 'fan', 'covers', 'notice', 'adequate']\n",
      "     0-48-3 : [] 128 123.0 ['cons', 'end', 'long', 'neoprene', 'delivery', 'problems', 'nicer', 'feels', 'layer', 'feet']\n",
      "   0-49 : ['0-49-1', '0-49-3'] 278 3387.0 ['pocket', 'pockets', 'inside', 'zipper', 'main', 'strap', 'open', 'top', 'front', 'compartment']\n",
      "     0-49-1 : [] 224 203.0 ['area', 'side', 'swiss', 'idea', 'weird', 'loose', 'putting', 'appears', 'issue', 'seam']\n",
      "     0-49-3 : [] 54 60.0 ['type', 'style', 'deep', 'section', 'underneath', 'brand', 'snap', 'opening', 'snaps', 'secure']\n",
      "   0-50 : ['0-50-3', '0-50-4'] 318 2468.0 ['sleeve', '&', ';', 'tablet', 'protect', 'protection', 'scratches', 'protecting', 'chromebook', 'job']\n",
      "     0-50-3 : [] 116 92.0 ['built', 'mbp', 'sit', 'exterior', 'class', 'eventually', 'securely', 'thing', 'based', 'constantly']\n",
      "     0-50-4 : [] 202 182.0 ['cover', 'colors', 'screen', 'function', 'protecting', 'hand', 'covers', 'pockets', 'complaints', 'fun']\n",
      "   0-51 : ['0-51-2', '0-51-4', '0-51-5'] 283 3006.0 ['strap', 'straps', 'months', 'shoulder', 'handle', 'started', 'weeks', 'return', 'stitching', 'broke']\n",
      "     0-51-2 : [] 67 73.0 ['cheaply', 'plane', 'box', 'binder', 'started', '...', 'section', 'hold', 'appears', 'set']\n",
      "     0-51-4 : [] 56 56.0 ['business', 'means', 'update', 'bad', 'messenger', 'condition', 'closed', 'pulled', 'point', 'tabs']\n",
      "     0-51-5 : [] 160 122.0 ['real', 'starting', 'bad', 'daily', 'amazon', 'excited', 'support', 'size', 'couple', 'binder']\n",
      "   0-52 : ['0-52-2', '0-52-4'] 1761 12015.0 ['!', 'carry', 'recommend', 'room', 'books', 'love', '&', 'school', 'highly', 'bought']\n",
      "     0-52-2 : [] 290 185.0 ['shoulders', 'offered', 'comment', 'adequate', 'pens', 'gear', 'walk', 'junk', 'back', 'satisfied']\n",
      "     0-52-4 : [] 1471 1231.0 ['wife', 'purchased', 'likes', 'gift', 'briefcase', 'loves', 'trip', 'travels', 'perfect', 'asked']\n",
      "   0-53 : ['0-53-4', '0-53-5'] 1887 13105.0 ['price', '$', 'quality', 'room', 'pockets', 'mouse', 'pocket', 'plenty', 'carry', 'accessories']\n",
      "     0-53-4 : [] 1235 1371.0 ['leather', 'quality', 'keyboard', 'duty', 'local', 'sony', 'box', 'personally', 'comfortably', 'cases']\n",
      "     0-53-5 : [] 652 657.0 ['amazon', 'samsonite', 'price', 'cheaper', 'surprised', 'print', 'years', 'surface', 'materials', 'cushion']\n",
      "   0-54 : ['0-54-3', '0-54-5'] 296 3037.0 ['power', 'pocket', 'mouse', 'room', 'drive', 'pro', 'charger', 'mac', 'usb', 'external']\n",
      "     0-54-3 : [] 126 87.0 ['including', 'cable', 'elastic', 'photo', 'wrong', 'chargers', 'feet', 'things', 'needed', 'tote']\n",
      "     0-54-5 : [] 170 112.0 ['portable', 'starting', 'drive', 'separate', 'making', 'display', 'front', 'pouches', 'outer', 'storage']\n",
      "   0-55 : ['0-55-2', '0-55-3'] 763 6654.0 ['cover', 'bottom', 'color', 'nice', 'top', 'apple', 'easily', 'logo', 'feel', 'plastic']\n",
      "     0-55-2 : [] 608 456.0 ['beautiful', 'starting', 'easier', 'helps', 'pick', 'tiny', 'move', 'clean', 'shell', 'slightly']\n",
      "     0-55-3 : [] 155 126.0 ['fitting', 'underneath', 'contacted', 'pull', 'accessory', 'careful', 'keyboard', '+', 'file', 'installed']\n",
      "   0-56 : ['0-56-1', '0-56-2', '0-56-3'] 451 4443.0 ['room', 'inch', 'mouse', 'power', 'pockets', 'strap', 'plenty', 'accessories', 'compartment', 'size']\n",
      "     0-56-1 : [] 313 266.0 ['toshiba', 'house', 'included', 'left', 'choose', 'paper', 'screen', 'form', 'battery', 'adjust']\n",
      "     0-56-2 : [] 33 29.0 ['keyboard', 'adjustable', 'version', 'vaio', 'imagine', 'ton', 'offer', 'package', 'system', 'surprise']\n",
      "     0-56-3 : [] 105 81.0 ['cd', 'cord', 'inch', 'important', 'special', 'papers', 'stay', 'past', 'glove', 'holder']\n",
      "   0-57 : ['0-57-1', '0-57-4'] 104 936.0 ['pockets', 'review', 'phone', 'pocket', 'front', 'solid', 'zippers', 'pack', 'pictures', 'secure']\n",
      "     0-57-1 : [] 42 33.0 ['durability', 'strong', 'check', 'rugged', 'daily', 'months', 'access', 'internal', 'online', 'amazon']\n",
      "     0-57-4 : [] 62 60.0 ['canvas', 'searched', 'piece', 'single', '!', 'nice', 'overnight', 'suggest', 'surprised', 'tablet']\n",
      "   0-58 : ['0-58-1'] 170 1729.0 ['&', ';', 'strap', 'plastic', 'shoulder', 'velcro', 'school', 'straps', 'comfortable', 'find']\n",
      "     0-58-1 : [] 170 172.0 ['sized', 'samsonite', 'thin', 'places', 'junk', 'straps', 'rubberized', 'actual', 'school', 'fabric']\n",
      "   0-59 : ['0-59-2', '0-59-3'] 148 2089.0 ['pocket', 'compartment', ':', 'main', 'compartments', 'bottle', 'zipper', 'front', 'large', 'books']\n",
      "     0-59-2 : [] 93 126.0 ['papers', 'left', 'close', 'smoothly', '?', 'plane', 'form', 'tiny', 'replacement', 'stitching']\n",
      "     0-59-3 : [] 55 62.0 ['lighter', 'tsa', 'bags', 'large', 'mentioned', 'swiss', 'purse', 'handle', 'read', 'convenient']\n",
      "   0-60 : ['0-60-3', '0-60-4'] 317 3966.0 ['pocket', 'compartment', 'pockets', 'small', 'hold', 'carry', 'pens', 'room', 'items', 'front']\n",
      "     0-60-3 : [] 288 298.0 ['change', 'returned', 'falling', 'set', 'cord', 'multiple', 'overnight', 'store', 'pockets', 'phone']\n",
      "     0-60-4 : [] 29 44.0 ['long', 'lot', 'move', 'wallet', 'everyday', 'flexible', 'damaged', 'comfort', 'market', 'searching']\n",
      "   0-61 : ['0-61-1', '0-61-4'] 148 1553.0 ['&', ';', 'pocket', 'pockets', 'ipad', 'padded', 'slot', 'tablet', 'phone', 'pouch']\n",
      "     0-61-1 : [] 42 46.0 ['reading', 'pieces', 'finally', 'secure', 'materials', 'kids', 'mbp', 'separate', 'organized', 'taking']\n",
      "     0-61-4 : [] 106 180.0 ['iphone', 'lid', 'original', 'thought', 'pleased', 'opens', 'roomy', 'cool', 'fantastic', 'starting']\n",
      "   0-62 : ['0-62-2'] 218 2542.0 ['&', ';', 'pocket', 'mouse', 'power', 'room', 'cord', 'carry', 'strap', 'charger']\n",
      "     0-62-2 : [] 218 209.0 ['inside', 'charger', 'bulk', 'person', 'papers', 'ton', 'choose', 'acer', 'aluminum', 'times']\n",
      "   0-63 : ['0-63-1', '0-63-2', '0-63-3', '0-63-4', '0-63-5', '0-63-6'] 2203 21370.0 ['pockets', 'strap', 'carry', '&', ';', 'shoulder', 'pocket', 'room', 'plenty', 'comfortable']\n",
      "     0-63-1 : [] 286 346.0 ['tote', 'color', 'empty', 'purse', 'compartments', 'section', 'love', 'hands', 'abuse', 'spacious']\n",
      "     0-63-2 : [] 247 378.0 ['leather', 'professional', 'suitcase', 'beautiful', 'blue', 'purse', 'briefcase', 'kindle', 'dark', 'sturdy']\n",
      "     0-63-3 : [] 327 366.0 ['pouch', 'deep', 'papers', 'retina', 'stay', 'organization', 'exterior', 'lining', 'inside', 'couple']\n",
      "     0-63-4 : [] 1257 1365.0 ['&', 'large', 'short', 'roller', 'pads', 'backpacks', 'samsonite', 'convenient', 'laptops', 'organize']\n",
      "     0-63-5 : [] 80 101.0 ['stuff', 'pros', 'capacity', 'handle', 'photos', 'pattern', 'stitching', 'system', 'customer', 'trips']\n",
      "     0-63-6 : [] 6 9.0 ['provide', 'receive', 'basically', 'world', 'difference', 'products', 'bother', 'compartment', 'searching', 'flat']\n",
      "   0-64 : ['0-64-1', '0-64-2', '0-64-3'] 1348 8822.0 ['sleeve', 'ipad', 'zipper', 'padding', 'pro', '!', 'cheap', 'chromebook', 'air', '$']\n",
      "     0-64-1 : [] 685 867.0 ['logic', 'worry', 'green', 'broke', '-inch', \"'\", 'super', 'suggest', 'wife', 'sits']\n",
      "     0-64-2 : [] 562 742.0 ['leather', 'flap', 'worth', 'student', 'interior', 'im', 'class', 'single', 'scratching', 'basically']\n",
      "     0-64-3 : [] 101 217.0 ['vibrant', 'leaving', 'nice', 'live', 'thin', 'finish', 'tote', 'key', 'orange', 'device']\n"
     ]
    }
   ],
   "source": [
    "def print_child_idxs(topic):\n",
    "    topic_freq_words = [idx_to_word[bow_idxs[bow_index]] for bow_index in np.argsort(topic.cnt_words)[::-1][:10]]\n",
    "    print('  '*topic.depth, topic.idx, ':', [child.idx for child in topic.children], topic.cnt_doc, np.sum(topic.cnt_words), topic_freq_words)\n",
    "    for topic in topic.children:\n",
    "        print_child_idxs(topic)\n",
    "print_child_idxs(topic_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 100\n",
    "alpha = np.array([10., 5., 1.])\n",
    "gam = 0.01\n",
    "eta = 1.\n",
    "n_depth = 3\n",
    "verbose = False\n",
    "topic_root = Topic(idx='0', sibling_idx=0, parent=None, depth=0, n_doc=n_doc, n_vocab=n_vocab)\n",
    "docs = [Doc(idx=doc_idx, words=doc_words, bow=doc_bow, n_depth=config.n_depth) for doc_idx, (doc_words, doc_bow) in enumerate(zip(docs_words, docs_bow))]\n",
    "test_docs = [Doc(idx=doc_idx, words=doc_words, bow=doc_bow, n_depth=config.n_depth) for doc_idx, (doc_words, doc_bow) in enumerate(zip(test_docs_words, test_docs_bow))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10000 20000 30000 0 Perplexity: 384.68\n",
      "0 10000 20000 30000 0 Perplexity: 333.69\n",
      "0 10000 20000 30000 0 Perplexity: 313.95\n",
      "0 10000 20000 30000 0 Perplexity: 302.79\n",
      "0 10000 20000 30000 0 Perplexity: 302.68\n",
      "0 10000 20000 30000 0 Perplexity: 299.52\n",
      "0 10000 20000 30000 0 Perplexity: 297.26\n",
      "0 10000 20000 30000 0 Perplexity: 300.96\n",
      "0 10000 20000 30000 0 Perplexity: 301.02\n",
      "0 10000 20000 30000 0 Perplexity: 296.26\n",
      "0 10000 20000 30000 0 Perplexity: 288.02\n",
      "0 10000 20000 30000 0 Perplexity: 283.37\n",
      "0 10000 20000 30000 0 Perplexity: 291.71\n",
      "0 10000 20000 30000 0 Perplexity: 284.27\n",
      "0 10000 20000 30000 0 Perplexity: 283.77\n",
      "0 10000 20000 30000 0 Perplexity: 282.31\n",
      "0 10000 20000 30000 0 Perplexity: 279.27\n",
      "0 10000 20000 30000 0 Perplexity: 278.99\n",
      "0 10000 20000 30000 0 Perplexity: 274.78\n",
      "0 10000 20000 30000 0 Perplexity: 284.89\n",
      "0 10000 20000 30000 0 Perplexity: 286.40\n",
      "0 10000 20000 30000 0 Perplexity: 278.99\n",
      "0 10000 20000 30000 0 Perplexity: 273.92\n",
      "0 10000 20000 30000 0 Perplexity: 273.73\n",
      "0 10000 20000 30000 0 Perplexity: 268.64\n",
      "0 10000 20000 30000 0 Perplexity: 270.98\n",
      "0 10000 20000 30000 0 Perplexity: 269.68\n",
      "0 10000 20000 30000 0 Perplexity: 269.72\n",
      "0 10000 20000 30000 0 Perplexity: 273.63\n",
      "0 10000 20000 30000 0 Perplexity: 270.19\n",
      "0 10000 20000 30000 0 Perplexity: 272.09\n",
      "0 10000 20000 30000 0 Perplexity: 277.34\n",
      "0 10000 20000 30000 0 Perplexity: 276.52\n",
      "0 10000 20000 30000 0 Perplexity: 269.58\n",
      "0 10000 20000 30000 0 Perplexity: 272.65\n",
      "0 10000 20000 30000 0 Perplexity: 277.70\n",
      "0 10000 20000 30000 0 Perplexity: 277.18\n",
      "0 10000 20000 30000 0 Perplexity: 272.17\n",
      "0 10000 20000 30000 0 Perplexity: 273.57\n",
      "0 10000 20000 30000 0 Perplexity: 278.71\n",
      "0 10000 20000 30000 0 Perplexity: 276.33\n",
      "0 10000 20000 30000 0 Perplexity: 274.92\n",
      "0 10000 20000 30000 0 Perplexity: 271.44\n",
      "0 10000 20000 30000 0 Perplexity: 271.57\n",
      "0 10000 20000 30000 0 Perplexity: 265.00\n",
      "0 10000 20000 30000 0 Perplexity: 266.04\n",
      "0 10000 20000 30000 0 Perplexity: 274.68\n",
      "0 10000 20000 30000 0 Perplexity: 265.93\n",
      "0 10000 20000 30000 0 Perplexity: 269.21\n",
      "0 10000 20000 30000 0 Perplexity: 274.59\n",
      "0 10000 20000 30000 0 Perplexity: 266.54\n",
      "0 10000 20000 30000 0 Perplexity: 262.88\n",
      "0 10000 20000 30000 0 Perplexity: 262.30\n",
      "0 10000 20000 30000 0 Perplexity: 265.28\n",
      "0 10000 20000 30000 0 Perplexity: 261.34\n",
      "0 10000 20000 30000 0 Perplexity: 265.93\n",
      "0 10000 20000 30000 0 Perplexity: 272.39\n",
      "0 10000 20000 30000 0 Perplexity: 269.83\n",
      "0 10000 20000 30000 0 Perplexity: 263.21\n",
      "0 10000 20000 30000 0 Perplexity: 268.45\n",
      "0 10000 20000 30000 0 Perplexity: 269.97\n",
      "0 10000 20000 30000 0 Perplexity: 267.46\n",
      "0 10000 20000 30000 0 Perplexity: 266.99\n",
      "0 10000 20000 30000 0 Perplexity: 268.51\n",
      "0 10000 20000 30000 0 Perplexity: 266.07\n",
      "0 10000 20000 30000 0 Perplexity: 264.17\n",
      "0 10000 20000 30000 0 Perplexity: 262.76\n",
      "0 10000 20000 30000 0 Perplexity: 261.11\n",
      "0 10000 20000 30000 0 Perplexity: 260.38\n",
      "0 10000 20000 30000 0 Perplexity: 265.23\n",
      "0 10000 20000 30000 0 Perplexity: 267.46\n",
      "0 10000 20000 30000 0 Perplexity: 260.87\n",
      "0 10000 20000 30000 0 Perplexity: 258.07\n",
      "0 10000 20000 30000 0 Perplexity: 259.59\n",
      "0 10000 20000 30000 0 Perplexity: 268.87\n",
      "0 10000 20000 30000 0 Perplexity: 260.98\n",
      "0 10000 20000 30000 0 Perplexity: 261.50\n",
      "0 10000 20000 30000 0 Perplexity: 259.41\n",
      "0 10000 20000 30000 0 Perplexity: 260.54\n",
      "0 10000 20000 30000 0 Perplexity: 265.74\n",
      "0 10000 20000 30000 0 Perplexity: 261.34\n",
      "0 10000 20000 30000 0 Perplexity: 260.82\n",
      "0 10000 20000 30000 0 Perplexity: 262.85\n",
      "0 10000 20000 30000 0 Perplexity: 261.41\n",
      "0 10000 20000 30000 0 Perplexity: 262.14\n",
      "0 10000 20000 30000 0 Perplexity: 261.37\n",
      "0 10000 20000 30000 0 Perplexity: 269.57\n",
      "0 10000 20000 30000 0 Perplexity: 274.80\n",
      "0 10000 20000 30000 0 Perplexity: 264.25\n",
      "0 10000 20000 30000 0 Perplexity: 259.93\n",
      "0 10000 20000 30000 0 Perplexity: 260.81\n",
      "0 10000 20000 30000 0 Perplexity: 261.63\n",
      "0 10000 20000 30000 0 Perplexity: 260.62\n",
      "0 10000 20000 30000 0 Perplexity: 262.02\n",
      "0 10000 20000 30000 0 Perplexity: 260.83\n",
      "0 10000 20000 30000 0 Perplexity: 258.37\n",
      "0 10000 20000 30000 0 Perplexity: 262.91\n",
      "0 10000 20000 30000 0 Perplexity: 264.63\n",
      "0 10000 20000 30000 0 Perplexity: 257.81\n",
      "0 10000 20000 30000 0 Perplexity: 259.14\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_sample):\n",
    "    if i == 0:\n",
    "        init_train(docs, topic_root)\n",
    "        init_test(test_docs, topic_root)        \n",
    "        assert_sum_cnt_words(topic_root)\n",
    "    else:\n",
    "        sample_train(docs, topic_root)\n",
    "        sample_test(test_docs, topic_root)                \n",
    "        assert_sum_cnt_words(topic_root)\n",
    "        \n",
    "    perplexity = eval_perplexity(test_docs, topic_root)\n",
    "    print('Perplexity: %.2f' % perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_freq_tokens(topic, topics_freq_tokens):\n",
    "    topic_freq_tokens = ' '.join([idx_to_word[bow_idxs[bow_index]] for bow_index in np.argsort(topic.cnt_words)[::-1][:10]])\n",
    "    topics_freq_tokens.append(topic_freq_tokens)\n",
    "    for child in topic.children:\n",
    "        add_freq_tokens(child, topics_freq_tokens)\n",
    "\n",
    "topics_freq_tokens = []\n",
    "add_freq_tokens(topic_root, topics_freq_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_coherence = 'npmi/data/bags/cgs.txt'\n",
    "with open(path_coherence, 'w') as f:\n",
    "    f.write('\\n'.join(topics_freq_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get specialization score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_bow = np.sum([instance.bow for instance in instances_train], 0)\n",
    "norm_vec = norm_bow / np.linalg.norm(norm_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spec(topic, depth_specs=None):\n",
    "    if depth_specs is None: depth_specs = defaultdict(list)\n",
    "    topic_vec = topic.prob_words / np.linalg.norm(topic.prob_words)\n",
    "    topic_spec = 1 - topic_vec.dot(norm_vec)\n",
    "    depth_specs[topic.depth].append(topic_spec)\n",
    "    for child in topic.children:\n",
    "        depth_specs = add_spec(child, depth_specs)\n",
    "    return depth_specs\n",
    "\n",
    "depth_specs = add_spec(topic_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0749372385946312]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth_specs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0749372385946312\n",
      "1 0.6226005207697187\n",
      "2 0.49158613401802365\n",
      "3 nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "for depth, specs in depth_specs.items():\n",
    "    spec = np.mean(specs)\n",
    "    print(depth, spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_arc(topic, depth_arcs=None):\n",
    "    if depth_arcs is None: depth_arcs = defaultdict(list)\n",
    "    topic_vec = topic.prob_words / np.linalg.norm(topic.prob_words)\n",
    "    topic_arc = np.arccos(topic_vec.dot(norm_vec))\n",
    "    depth_arcs[topic.depth].append(topic_arc)\n",
    "    for child in topic.children:\n",
    "        depth_arcs = add_arc(child, depth_arcs)\n",
    "    return depth_arcs\n",
    "\n",
    "depth_arcs = add_arc(topic_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0749372385946312\n",
      "1 0.6196820668592284\n",
      "2 0.4913526867429402\n"
     ]
    }
   ],
   "source": [
    "for depth, arcs in depth_arcs.items():\n",
    "    spec = 1 - np.cos(np.mean(arcs))\n",
    "    print(depth, spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
