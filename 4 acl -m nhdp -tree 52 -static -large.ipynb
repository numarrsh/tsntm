{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from data_structure import get_batches\n",
    "from hntm import HierarchicalNeuralTopicModel\n",
    "from tree import get_descendant_idxs\n",
    "from evaluation import validate, get_hierarchical_affinity, get_topic_specialization, print_topic_sample\n",
    "from configure import get_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(nb_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu\n",
    "np.random.seed(config.seed)\n",
    "random.seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.path_data,'rb'))\n",
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)\n",
    "config.dim_bow = len(bow_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0,
     10
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables, model):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, model, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    return _variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = []\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "ppl_min = np.inf\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','','','','','VALID:','','','','','TEST:','', 'SPEC:', '', '', 'HIER:', ''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL','REG','LOSS','PPL','NLL','KL','REG','LOSS','PPL', '1', '2', '3', 'CHILD', 'OTHER']]))))\n",
    "\n",
    "cmd_rm = 'rm -r %s' % config.dir_model\n",
    "res = subprocess.call(cmd_rm.split())\n",
    "cmd_mk = 'mkdir %s' % config.dir_model\n",
    "res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "def update_checkpoint(config, checkpoint, global_step):\n",
    "    checkpoint.append(config.path_model + '-%i' % global_step)\n",
    "    if len(checkpoint) > config.max_to_keep:\n",
    "        path_model = checkpoint.pop(0) + '.*'\n",
    "        for p in glob.glob(path_model):\n",
    "            os.remove(p)\n",
    "    cPickle.dump(checkpoint, open(config.path_checkpoint, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "model = HierarchicalNeuralTopicModel(config)\n",
    "sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(max_to_keep=config.max_to_keep)\n",
    "update_tree_flg = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train & validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th colspan=\"4\" halign=\"left\"></th>\n",
       "      <th>VALID:</th>\n",
       "      <th colspan=\"4\" halign=\"left\"></th>\n",
       "      <th>TEST:</th>\n",
       "      <th></th>\n",
       "      <th>SPEC:</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "      <th>HIER:</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>CHILD</th>\n",
       "      <th>OTHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>80</td>\n",
       "      <td>8</td>\n",
       "      <td>463</td>\n",
       "      <td>9167.40</td>\n",
       "      <td>1851</td>\n",
       "      <td>9145.84</td>\n",
       "      <td>21.31</td>\n",
       "      <td>0.28</td>\n",
       "      <td>8881.64</td>\n",
       "      <td>1726</td>\n",
       "      <td>8857.70</td>\n",
       "      <td>23.73</td>\n",
       "      <td>0.21</td>\n",
       "      <td>8881.08</td>\n",
       "      <td>1726</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>80</td>\n",
       "      <td>17</td>\n",
       "      <td>360</td>\n",
       "      <td>9126.04</td>\n",
       "      <td>1788</td>\n",
       "      <td>9103.30</td>\n",
       "      <td>22.52</td>\n",
       "      <td>0.23</td>\n",
       "      <td>8870.14</td>\n",
       "      <td>1709</td>\n",
       "      <td>8846.55</td>\n",
       "      <td>23.42</td>\n",
       "      <td>0.18</td>\n",
       "      <td>8870.38</td>\n",
       "      <td>1709</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>78</td>\n",
       "      <td>26</td>\n",
       "      <td>257</td>\n",
       "      <td>9108.23</td>\n",
       "      <td>1762</td>\n",
       "      <td>9085.20</td>\n",
       "      <td>22.81</td>\n",
       "      <td>0.21</td>\n",
       "      <td>8867.22</td>\n",
       "      <td>1704</td>\n",
       "      <td>8843.71</td>\n",
       "      <td>23.34</td>\n",
       "      <td>0.17</td>\n",
       "      <td>8866.46</td>\n",
       "      <td>1703</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>77</td>\n",
       "      <td>35</td>\n",
       "      <td>154</td>\n",
       "      <td>9098.05</td>\n",
       "      <td>1748</td>\n",
       "      <td>9074.91</td>\n",
       "      <td>22.91</td>\n",
       "      <td>0.20</td>\n",
       "      <td>8864.86</td>\n",
       "      <td>1701</td>\n",
       "      <td>8841.53</td>\n",
       "      <td>23.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>8865.50</td>\n",
       "      <td>1702</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>75</td>\n",
       "      <td>44</td>\n",
       "      <td>51</td>\n",
       "      <td>9091.08</td>\n",
       "      <td>1738</td>\n",
       "      <td>9068.03</td>\n",
       "      <td>22.95</td>\n",
       "      <td>0.19</td>\n",
       "      <td>8863.46</td>\n",
       "      <td>1699</td>\n",
       "      <td>8840.34</td>\n",
       "      <td>22.95</td>\n",
       "      <td>0.16</td>\n",
       "      <td>8862.96</td>\n",
       "      <td>1698</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325000</th>\n",
       "      <td>78</td>\n",
       "      <td>573</td>\n",
       "      <td>108</td>\n",
       "      <td>9051.54</td>\n",
       "      <td>1683</td>\n",
       "      <td>9030.67</td>\n",
       "      <td>22.01</td>\n",
       "      <td>0.12</td>\n",
       "      <td>8862.93</td>\n",
       "      <td>1697</td>\n",
       "      <td>8841.59</td>\n",
       "      <td>21.25</td>\n",
       "      <td>0.09</td>\n",
       "      <td>8861.52</td>\n",
       "      <td>1694</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330000</th>\n",
       "      <td>78</td>\n",
       "      <td>582</td>\n",
       "      <td>5</td>\n",
       "      <td>9051.37</td>\n",
       "      <td>1683</td>\n",
       "      <td>9030.55</td>\n",
       "      <td>22.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>8863.21</td>\n",
       "      <td>1698</td>\n",
       "      <td>8841.97</td>\n",
       "      <td>21.15</td>\n",
       "      <td>0.09</td>\n",
       "      <td>8861.52</td>\n",
       "      <td>1694</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335000</th>\n",
       "      <td>78</td>\n",
       "      <td>590</td>\n",
       "      <td>469</td>\n",
       "      <td>9051.24</td>\n",
       "      <td>1682</td>\n",
       "      <td>9030.44</td>\n",
       "      <td>21.99</td>\n",
       "      <td>0.12</td>\n",
       "      <td>8862.44</td>\n",
       "      <td>1696</td>\n",
       "      <td>8841.24</td>\n",
       "      <td>21.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>8861.52</td>\n",
       "      <td>1694</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340000</th>\n",
       "      <td>78</td>\n",
       "      <td>599</td>\n",
       "      <td>366</td>\n",
       "      <td>9051.12</td>\n",
       "      <td>1682</td>\n",
       "      <td>9030.35</td>\n",
       "      <td>21.98</td>\n",
       "      <td>0.12</td>\n",
       "      <td>8862.90</td>\n",
       "      <td>1697</td>\n",
       "      <td>8841.64</td>\n",
       "      <td>21.17</td>\n",
       "      <td>0.09</td>\n",
       "      <td>8861.52</td>\n",
       "      <td>1694</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345000</th>\n",
       "      <td>77</td>\n",
       "      <td>608</td>\n",
       "      <td>263</td>\n",
       "      <td>9051.00</td>\n",
       "      <td>1682</td>\n",
       "      <td>9030.25</td>\n",
       "      <td>21.97</td>\n",
       "      <td>0.12</td>\n",
       "      <td>8863.67</td>\n",
       "      <td>1697</td>\n",
       "      <td>8842.49</td>\n",
       "      <td>21.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>8861.52</td>\n",
       "      <td>1694</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        TRAIN:                               VALID:        \\\n",
       "       Time   Ep   Ct     LOSS   PPL      NLL     KL   REG     LOSS   PPL   \n",
       "5000     80    8  463  9167.40  1851  9145.84  21.31  0.28  8881.64  1726   \n",
       "10000    80   17  360  9126.04  1788  9103.30  22.52  0.23  8870.14  1709   \n",
       "15000    78   26  257  9108.23  1762  9085.20  22.81  0.21  8867.22  1704   \n",
       "20000    77   35  154  9098.05  1748  9074.91  22.91  0.20  8864.86  1701   \n",
       "25000    75   44   51  9091.08  1738  9068.03  22.95  0.19  8863.46  1699   \n",
       "...     ...  ...  ...      ...   ...      ...    ...   ...      ...   ...   \n",
       "325000   78  573  108  9051.54  1683  9030.67  22.01  0.12  8862.93  1697   \n",
       "330000   78  582    5  9051.37  1683  9030.55  22.00  0.12  8863.21  1698   \n",
       "335000   78  590  469  9051.24  1682  9030.44  21.99  0.12  8862.44  1696   \n",
       "340000   78  599  366  9051.12  1682  9030.35  21.98  0.12  8862.90  1697   \n",
       "345000   77  608  263  9051.00  1682  9030.25  21.97  0.12  8863.67  1697   \n",
       "\n",
       "                                TEST:       SPEC:             HIER:        \n",
       "            NLL     KL   REG     LOSS   PPL     1     2     3 CHILD OTHER  \n",
       "5000    8857.70  23.73  0.21  8881.08  1726  0.41  0.45  0.46  0.31  0.22  \n",
       "10000   8846.55  23.42  0.18  8870.38  1709  0.44  0.46  0.48  0.30  0.21  \n",
       "15000   8843.71  23.34  0.17  8866.46  1703  0.45  0.46  0.49  0.30  0.20  \n",
       "20000   8841.53  23.16  0.16  8865.50  1702  0.47  0.47  0.49  0.28  0.20  \n",
       "25000   8840.34  22.95  0.16  8862.96  1698  0.51  0.47  0.49  0.28  0.20  \n",
       "...         ...    ...   ...      ...   ...   ...   ...   ...   ...   ...  \n",
       "325000  8841.59  21.25  0.09  8861.52  1694  0.54  0.48  0.49  0.28  0.20  \n",
       "330000  8841.97  21.15  0.09  8861.52  1694  0.54  0.48  0.49  0.28  0.20  \n",
       "335000  8841.24  21.11  0.09  8861.52  1694  0.53  0.48  0.48  0.28  0.20  \n",
       "340000  8841.64  21.17  0.09  8861.52  1694  0.53  0.48  0.48  0.28  0.20  \n",
       "345000  8842.49  21.09  0.09  8861.52  1694  0.53  0.48  0.49  0.28  0.20  \n",
       "\n",
       "[69 rows x 20 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 R: 1.000 P: 0.065 user dialogue knowledge utterance generation domain action state systems human\n",
      "   1 R: 0.184 P: 0.045 models embeddings neural vector network training vectors representations embedding representation\n",
      "     11 R: 0.076 P: 0.076 document documents question topic query sentences questions answer method similarity\n",
      "     12 R: 0.063 P: 0.063 sentiment features tweets negative positive classification analysis social opinion polarity\n",
      "   2 R: 0.212 P: 0.065 tree parsing dependency parser grammar node trees parse rules nodes\n",
      "     21 R: 0.044 P: 0.044 event relations annotation relation discourse events argument semantic annotated temporal\n",
      "     22 R: 0.103 P: 0.103 verb semantic syntactic structure noun lexical verbs rules grammar phrase\n",
      "   3 R: 0.208 P: 0.079 training models algorithm probability features feature function learning performance distribution\n",
      "     31 R: 0.077 P: 0.077 features feature training performance entity learning classification classifier entities class\n",
      "     32 R: 0.052 P: 0.052 errors pos character chinese error segmentation tag tags tagging method\n",
      "   4 R: 0.183 P: 0.061 translation source target alignment phrase english pairs training parallel sentences\n",
      "     41 R: 0.069 P: 0.069 sense semantic similarity wordnet lexical senses terms context relations method\n",
      "     42 R: 0.053 P: 0.053 languages english morphological translation arabic lexicon dictionary forms form spanish\n",
      "   5 R: 0.148 P: 0.034 evaluation systems human sentences scores test quality score metrics mt\n",
      "     51 R: 0.071 P: 0.071 annotation web resources tools user research project tool linguistic processing\n",
      "     52 R: 0.042 P: 0.042 speech speakers speaker study features spoken recognition models native participants\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8ea66be3d146>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m# validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_recon_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_kl_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_reg_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mppl_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mppls_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mloss_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_recon_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_kl_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_reg_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppl_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs_topic_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   3116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3117\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 3118\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   3119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m     \"\"\"\n\u001b[0;32m--> 591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "while epoch < config.n_epochs:\n",
    "    # train\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = model.get_feed_dict(batch)\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, ppls_batch, global_step_log = \\\n",
    "        sess.run([model.opt, model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_reg, model.topic_ppls, tf.train.get_global_step()], feed_dict = feed_dict)\n",
    "\n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if global_step_log % config.log_period == 0:\n",
    "            # validate\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "\n",
    "            # test\n",
    "            if ppl_dev < ppl_min:\n",
    "                ppl_min = ppl_dev\n",
    "                loss_test, _, _, _, ppl_test, _ = validate(sess, test_batches, model)\n",
    "                saver.save(sess, config.path_model, global_step=global_step_log)\n",
    "                cPickle.dump(config, open(config.path_config % global_step_log, 'wb'))\n",
    "                update_checkpoint(config, checkpoint, global_step_log)\n",
    "            \n",
    "            # visualize topic\n",
    "            topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "            topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "            topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "            topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "            descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "            recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "            \n",
    "            depth_specs = get_topic_specialization(sess, model, instances_test)\n",
    "            hierarchical_affinities = get_hierarchical_affinity(sess, model)\n",
    "            \n",
    "            # log\n",
    "            clear_output()\n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_reg_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_reg_dev, \\\n",
    "                    '%.2f'%loss_test, '%.0f'%ppl_test, \\\n",
    "                    '%.2f'%depth_specs[1], '%.2f'%depth_specs[2], '%.2f'%depth_specs[3], \\\n",
    "                    '%.2f'%hierarchical_affinities[0], '%.2f'%hierarchical_affinities[1]],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "            cPickle.dump(log_df, open(os.path.join(config.path_log), 'wb'))\n",
    "            print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)\n",
    "\n",
    "            # update tree\n",
    "            if not config.static:\n",
    "                config.tree_idxs, update_tree_flg = model.update_tree(topic_prob_topic, recur_prob_topic)\n",
    "                if update_tree_flg:\n",
    "                    print(config.tree_idxs)\n",
    "                    name_variables = {tensor.name: variable for tensor, variable in zip(tf.global_variables(), sess.run(tf.global_variables()))} # store paremeters\n",
    "                    if 'sess' in globals(): sess.close()\n",
    "                    model = HierarchicalNeuralTopicModel(config)\n",
    "                    sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "                    name_tensors = {tensor.name: tensor for tensor in tf.global_variables()}\n",
    "                    sess.run([name_tensors[name].assign(variable) for name, variable in name_variables.items()]) # restore parameters\n",
    "                    saver = tf.train.Saver(max_to_keep=1)\n",
    "                \n",
    "            time_start = time.time()\n",
    "\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "    epoch += 1\n",
    "\n",
    "loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "display(log_df)\n",
    "print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_topics = []\n",
    "years = []\n",
    "for i, train_batch in train_batches:\n",
    "    probs_topics_batch = sess.run(model.prob_topic, feed_dict=model.get_feed_dict(train_batch, mode='test'))\n",
    "    years_batch = [instance.year for instance in train_batch]\n",
    "    probs_topics += [probs_topics_batch]\n",
    "    years += years_batch\n",
    "probs_topics = np.concatenate(probs_topics)\n",
    "years = np.array(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36288, 16), (36288,))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_years = years.dot(probs_topics) / np.sum(probs_topics, 0)\n",
    "topic_year = {model.topic_idxs[i]: year for i, year in enumerate(topic_years)}\n",
    "probs_topics.shape, years.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 2006.1405287293783,\n",
       " 1: 2015.073065684627,\n",
       " 2: 2006.6358226095088,\n",
       " 3: 2010.2605308626469,\n",
       " 4: 2011.1153424999409,\n",
       " 5: 2010.2201559271937,\n",
       " 11: 2010.638231762466,\n",
       " 12: 2013.6412160983643,\n",
       " 21: 2009.3432210176409,\n",
       " 22: 2003.0377479786164,\n",
       " 31: 2010.7845425693845,\n",
       " 32: 2007.4959372377405,\n",
       " 41: 2008.9745451250799,\n",
       " 42: 2009.024079592822,\n",
       " 51: 2007.963976828227,\n",
       " 52: 2008.7701026307714}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topic_sample(sess, model, topic_prob_topic=None, recur_prob_topic=None, topic_freq_tokens=None, topic_year=None, parent_idx=0, depth=0):\n",
    "    if depth == 0: # print root\n",
    "        assert len(topic_prob_topic) == len(recur_prob_topic) == len(topic_freq_tokens) == len(topic_year)\n",
    "        freq_tokens = topic_freq_tokens[parent_idx]\n",
    "        recur_topic = recur_prob_topic[parent_idx]\n",
    "        prob_topic = topic_prob_topic[parent_idx]\n",
    "        year = topic_year[parent_idx]\n",
    "        print(parent_idx, 'Avg Year: %i' % year, ' '.join(freq_tokens))\n",
    "    \n",
    "    child_idxs = model.tree_idxs[parent_idx]\n",
    "    depth += 1\n",
    "    for child_idx in child_idxs:\n",
    "        freq_tokens = topic_freq_tokens[child_idx]\n",
    "        recur_topic = recur_prob_topic[child_idx]\n",
    "        prob_topic = topic_prob_topic[child_idx]\n",
    "        year = topic_year[child_idx]\n",
    "        print('  '*depth, child_idx, 'Avg Year: %i' % year, ' '.join(freq_tokens))\n",
    "        \n",
    "        if child_idx in model.tree_idxs: \n",
    "            print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens, topic_year=topic_year, parent_idx=child_idx, depth=depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Avg Year: 2006 user dialogue knowledge utterance generation domain action state systems human\n",
      "   1 Avg Year: 2015 models embeddings neural vector network training vectors representations embedding representation\n",
      "     11 Avg Year: 2010 document documents question topic query sentences questions answer method similarity\n",
      "     12 Avg Year: 2013 sentiment features tweets negative positive classification analysis social opinion polarity\n",
      "   2 Avg Year: 2006 tree parsing dependency parser grammar node trees parse rules nodes\n",
      "     21 Avg Year: 2009 event relations annotation relation discourse events argument semantic annotated temporal\n",
      "     22 Avg Year: 2003 verb semantic syntactic structure noun lexical verbs rules grammar phrase\n",
      "   3 Avg Year: 2010 training models algorithm probability features feature function learning performance distribution\n",
      "     31 Avg Year: 2010 features feature training performance entity learning classification classifier entities class\n",
      "     32 Avg Year: 2007 errors pos character chinese error segmentation tag tags tagging method\n",
      "   4 Avg Year: 2011 translation source target alignment phrase english pairs training parallel sentences\n",
      "     41 Avg Year: 2008 sense semantic similarity wordnet lexical senses terms context relations method\n",
      "     42 Avg Year: 2009 languages english morphological translation arabic lexicon dictionary forms form spanish\n",
      "   5 Avg Year: 2010 evaluation systems human sentences scores test quality score metrics mt\n",
      "     51 Avg Year: 2007 annotation web resources tools user research project tool linguistic processing\n",
      "     52 Avg Year: 2008 speech speakers speaker study features spoken recognition models native participants\n"
     ]
    }
   ],
   "source": [
    "print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens, topic_year=topic_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
