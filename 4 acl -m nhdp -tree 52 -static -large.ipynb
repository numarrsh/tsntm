{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from data_structure import get_batches\n",
    "from hntm import HierarchicalNeuralTopicModel\n",
    "from tree import get_descendant_idxs\n",
    "from evaluation import validate, get_hierarchical_affinity, get_topic_specialization, print_topic_sample\n",
    "from configure import get_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(nb_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu\n",
    "np.random.seed(config.seed)\n",
    "random.seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.path_data,'rb'))\n",
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)\n",
    "config.dim_bow = len(bow_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     10
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables, model):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, model, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    return _variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = []\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "ppl_min = np.inf\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','','','','','VALID:','','','','','TEST:','', 'SPEC:', '', '', 'HIER:', ''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL','REG','LOSS','PPL','NLL','KL','REG','LOSS','PPL', '1', '2', '3', 'CHILD', 'OTHER']]))))\n",
    "\n",
    "cmd_rm = 'rm -r %s' % config.dir_model\n",
    "res = subprocess.call(cmd_rm.split())\n",
    "cmd_mk = 'mkdir %s' % config.dir_model\n",
    "res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "def update_checkpoint(config, checkpoint, global_step):\n",
    "    checkpoint.append(config.path_model + '-%i' % global_step)\n",
    "    if len(checkpoint) > config.max_to_keep:\n",
    "        path_model = checkpoint.pop(0) + '.*'\n",
    "        for p in glob.glob(path_model):\n",
    "            os.remove(p)\n",
    "    cPickle.dump(checkpoint, open(config.path_checkpoint, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "model = HierarchicalNeuralTopicModel(config)\n",
    "sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(max_to_keep=config.max_to_keep)\n",
    "update_tree_flg = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train & validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th colspan=\"4\" halign=\"left\"></th>\n",
       "      <th>VALID:</th>\n",
       "      <th colspan=\"4\" halign=\"left\"></th>\n",
       "      <th>TEST:</th>\n",
       "      <th></th>\n",
       "      <th>SPEC:</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "      <th>HIER:</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>CHILD</th>\n",
       "      <th>OTHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>100</td>\n",
       "      <td>8</td>\n",
       "      <td>463</td>\n",
       "      <td>9175.77</td>\n",
       "      <td>1876</td>\n",
       "      <td>9157.67</td>\n",
       "      <td>17.82</td>\n",
       "      <td>0.27</td>\n",
       "      <td>9156.08</td>\n",
       "      <td>1791</td>\n",
       "      <td>9136.12</td>\n",
       "      <td>19.77</td>\n",
       "      <td>0.19</td>\n",
       "      <td>9156.00</td>\n",
       "      <td>1791</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>92</td>\n",
       "      <td>17</td>\n",
       "      <td>360</td>\n",
       "      <td>9131.72</td>\n",
       "      <td>1809</td>\n",
       "      <td>9112.23</td>\n",
       "      <td>19.30</td>\n",
       "      <td>0.22</td>\n",
       "      <td>9136.03</td>\n",
       "      <td>1764</td>\n",
       "      <td>9115.48</td>\n",
       "      <td>20.40</td>\n",
       "      <td>0.15</td>\n",
       "      <td>9135.53</td>\n",
       "      <td>1763</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>91</td>\n",
       "      <td>26</td>\n",
       "      <td>257</td>\n",
       "      <td>9111.77</td>\n",
       "      <td>1780</td>\n",
       "      <td>9091.72</td>\n",
       "      <td>19.87</td>\n",
       "      <td>0.19</td>\n",
       "      <td>9131.22</td>\n",
       "      <td>1756</td>\n",
       "      <td>9110.29</td>\n",
       "      <td>20.80</td>\n",
       "      <td>0.13</td>\n",
       "      <td>9130.96</td>\n",
       "      <td>1756</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>91</td>\n",
       "      <td>35</td>\n",
       "      <td>154</td>\n",
       "      <td>9099.94</td>\n",
       "      <td>1763</td>\n",
       "      <td>9079.66</td>\n",
       "      <td>20.15</td>\n",
       "      <td>0.18</td>\n",
       "      <td>9127.53</td>\n",
       "      <td>1751</td>\n",
       "      <td>9106.58</td>\n",
       "      <td>20.83</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9127.03</td>\n",
       "      <td>1751</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>89</td>\n",
       "      <td>44</td>\n",
       "      <td>51</td>\n",
       "      <td>9091.84</td>\n",
       "      <td>1752</td>\n",
       "      <td>9071.42</td>\n",
       "      <td>20.31</td>\n",
       "      <td>0.17</td>\n",
       "      <td>9124.96</td>\n",
       "      <td>1748</td>\n",
       "      <td>9104.15</td>\n",
       "      <td>20.69</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9125.23</td>\n",
       "      <td>1749</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>89</td>\n",
       "      <td>52</td>\n",
       "      <td>515</td>\n",
       "      <td>9086.05</td>\n",
       "      <td>1744</td>\n",
       "      <td>9065.49</td>\n",
       "      <td>20.40</td>\n",
       "      <td>0.16</td>\n",
       "      <td>9124.32</td>\n",
       "      <td>1747</td>\n",
       "      <td>9103.63</td>\n",
       "      <td>20.56</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9123.97</td>\n",
       "      <td>1746</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35000</th>\n",
       "      <td>88</td>\n",
       "      <td>61</td>\n",
       "      <td>412</td>\n",
       "      <td>9081.68</td>\n",
       "      <td>1738</td>\n",
       "      <td>9061.18</td>\n",
       "      <td>20.46</td>\n",
       "      <td>0.15</td>\n",
       "      <td>9123.49</td>\n",
       "      <td>1745</td>\n",
       "      <td>9102.74</td>\n",
       "      <td>20.63</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9123.47</td>\n",
       "      <td>1746</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40000</th>\n",
       "      <td>88</td>\n",
       "      <td>70</td>\n",
       "      <td>309</td>\n",
       "      <td>9078.19</td>\n",
       "      <td>1732</td>\n",
       "      <td>9057.69</td>\n",
       "      <td>20.51</td>\n",
       "      <td>0.15</td>\n",
       "      <td>9123.15</td>\n",
       "      <td>1745</td>\n",
       "      <td>9102.42</td>\n",
       "      <td>20.61</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9123.47</td>\n",
       "      <td>1746</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45000</th>\n",
       "      <td>85</td>\n",
       "      <td>79</td>\n",
       "      <td>206</td>\n",
       "      <td>9075.20</td>\n",
       "      <td>1728</td>\n",
       "      <td>9054.66</td>\n",
       "      <td>20.54</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9121.44</td>\n",
       "      <td>1743</td>\n",
       "      <td>9100.74</td>\n",
       "      <td>20.59</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9121.59</td>\n",
       "      <td>1743</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000</th>\n",
       "      <td>84</td>\n",
       "      <td>88</td>\n",
       "      <td>103</td>\n",
       "      <td>9072.60</td>\n",
       "      <td>1725</td>\n",
       "      <td>9052.08</td>\n",
       "      <td>20.57</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9120.72</td>\n",
       "      <td>1743</td>\n",
       "      <td>9100.03</td>\n",
       "      <td>20.58</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9121.20</td>\n",
       "      <td>1743</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55000</th>\n",
       "      <td>75</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>9070.25</td>\n",
       "      <td>1722</td>\n",
       "      <td>9049.79</td>\n",
       "      <td>20.59</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9121.31</td>\n",
       "      <td>1744</td>\n",
       "      <td>9100.64</td>\n",
       "      <td>20.57</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9121.20</td>\n",
       "      <td>1743</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60000</th>\n",
       "      <td>75</td>\n",
       "      <td>105</td>\n",
       "      <td>464</td>\n",
       "      <td>9068.35</td>\n",
       "      <td>1719</td>\n",
       "      <td>9047.88</td>\n",
       "      <td>20.61</td>\n",
       "      <td>0.13</td>\n",
       "      <td>9121.82</td>\n",
       "      <td>1743</td>\n",
       "      <td>9101.33</td>\n",
       "      <td>20.39</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9121.20</td>\n",
       "      <td>1743</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65000</th>\n",
       "      <td>77</td>\n",
       "      <td>114</td>\n",
       "      <td>361</td>\n",
       "      <td>9066.80</td>\n",
       "      <td>1716</td>\n",
       "      <td>9046.26</td>\n",
       "      <td>20.63</td>\n",
       "      <td>0.13</td>\n",
       "      <td>9121.00</td>\n",
       "      <td>1743</td>\n",
       "      <td>9100.34</td>\n",
       "      <td>20.56</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9121.20</td>\n",
       "      <td>1743</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70000</th>\n",
       "      <td>80</td>\n",
       "      <td>123</td>\n",
       "      <td>258</td>\n",
       "      <td>9065.33</td>\n",
       "      <td>1714</td>\n",
       "      <td>9044.70</td>\n",
       "      <td>20.64</td>\n",
       "      <td>0.13</td>\n",
       "      <td>9121.51</td>\n",
       "      <td>1744</td>\n",
       "      <td>9100.82</td>\n",
       "      <td>20.58</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9121.20</td>\n",
       "      <td>1743</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75000</th>\n",
       "      <td>76</td>\n",
       "      <td>132</td>\n",
       "      <td>155</td>\n",
       "      <td>9063.95</td>\n",
       "      <td>1712</td>\n",
       "      <td>9043.28</td>\n",
       "      <td>20.65</td>\n",
       "      <td>0.13</td>\n",
       "      <td>9118.39</td>\n",
       "      <td>1740</td>\n",
       "      <td>9097.67</td>\n",
       "      <td>20.62</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9119.24</td>\n",
       "      <td>1741</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80000</th>\n",
       "      <td>76</td>\n",
       "      <td>141</td>\n",
       "      <td>52</td>\n",
       "      <td>9062.66</td>\n",
       "      <td>1711</td>\n",
       "      <td>9041.94</td>\n",
       "      <td>20.66</td>\n",
       "      <td>0.13</td>\n",
       "      <td>9120.56</td>\n",
       "      <td>1742</td>\n",
       "      <td>9099.88</td>\n",
       "      <td>20.58</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9119.24</td>\n",
       "      <td>1741</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85000</th>\n",
       "      <td>77</td>\n",
       "      <td>149</td>\n",
       "      <td>516</td>\n",
       "      <td>9061.53</td>\n",
       "      <td>1709</td>\n",
       "      <td>9040.78</td>\n",
       "      <td>20.66</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9119.63</td>\n",
       "      <td>1740</td>\n",
       "      <td>9099.04</td>\n",
       "      <td>20.50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9119.24</td>\n",
       "      <td>1741</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90000</th>\n",
       "      <td>83</td>\n",
       "      <td>158</td>\n",
       "      <td>413</td>\n",
       "      <td>9060.53</td>\n",
       "      <td>1707</td>\n",
       "      <td>9039.76</td>\n",
       "      <td>20.67</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9120.26</td>\n",
       "      <td>1742</td>\n",
       "      <td>9099.58</td>\n",
       "      <td>20.58</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9119.24</td>\n",
       "      <td>1741</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95000</th>\n",
       "      <td>79</td>\n",
       "      <td>167</td>\n",
       "      <td>310</td>\n",
       "      <td>9059.63</td>\n",
       "      <td>1706</td>\n",
       "      <td>9038.84</td>\n",
       "      <td>20.67</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9121.33</td>\n",
       "      <td>1743</td>\n",
       "      <td>9100.68</td>\n",
       "      <td>20.55</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9119.24</td>\n",
       "      <td>1741</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>78</td>\n",
       "      <td>176</td>\n",
       "      <td>207</td>\n",
       "      <td>9058.73</td>\n",
       "      <td>1705</td>\n",
       "      <td>9037.93</td>\n",
       "      <td>20.67</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9120.45</td>\n",
       "      <td>1742</td>\n",
       "      <td>9099.85</td>\n",
       "      <td>20.50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9119.24</td>\n",
       "      <td>1741</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105000</th>\n",
       "      <td>76</td>\n",
       "      <td>185</td>\n",
       "      <td>104</td>\n",
       "      <td>9057.89</td>\n",
       "      <td>1704</td>\n",
       "      <td>9037.09</td>\n",
       "      <td>20.67</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9120.83</td>\n",
       "      <td>1743</td>\n",
       "      <td>9100.09</td>\n",
       "      <td>20.63</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9119.24</td>\n",
       "      <td>1741</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110000</th>\n",
       "      <td>77</td>\n",
       "      <td>194</td>\n",
       "      <td>1</td>\n",
       "      <td>9057.06</td>\n",
       "      <td>1703</td>\n",
       "      <td>9036.25</td>\n",
       "      <td>20.67</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9121.86</td>\n",
       "      <td>1744</td>\n",
       "      <td>9101.22</td>\n",
       "      <td>20.54</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9119.24</td>\n",
       "      <td>1741</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115000</th>\n",
       "      <td>77</td>\n",
       "      <td>202</td>\n",
       "      <td>465</td>\n",
       "      <td>9056.35</td>\n",
       "      <td>1702</td>\n",
       "      <td>9035.53</td>\n",
       "      <td>20.67</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9122.33</td>\n",
       "      <td>1745</td>\n",
       "      <td>9101.74</td>\n",
       "      <td>20.49</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9119.24</td>\n",
       "      <td>1741</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120000</th>\n",
       "      <td>77</td>\n",
       "      <td>211</td>\n",
       "      <td>362</td>\n",
       "      <td>9055.74</td>\n",
       "      <td>1701</td>\n",
       "      <td>9034.89</td>\n",
       "      <td>20.67</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9121.30</td>\n",
       "      <td>1744</td>\n",
       "      <td>9100.71</td>\n",
       "      <td>20.49</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9119.24</td>\n",
       "      <td>1741</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125000</th>\n",
       "      <td>77</td>\n",
       "      <td>220</td>\n",
       "      <td>259</td>\n",
       "      <td>9055.14</td>\n",
       "      <td>1700</td>\n",
       "      <td>9034.20</td>\n",
       "      <td>20.67</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9120.69</td>\n",
       "      <td>1743</td>\n",
       "      <td>9100.08</td>\n",
       "      <td>20.52</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9119.24</td>\n",
       "      <td>1741</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130000</th>\n",
       "      <td>87</td>\n",
       "      <td>229</td>\n",
       "      <td>156</td>\n",
       "      <td>9054.56</td>\n",
       "      <td>1699</td>\n",
       "      <td>9033.53</td>\n",
       "      <td>20.67</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9120.14</td>\n",
       "      <td>1742</td>\n",
       "      <td>9099.58</td>\n",
       "      <td>20.47</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9119.24</td>\n",
       "      <td>1741</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135000</th>\n",
       "      <td>75</td>\n",
       "      <td>238</td>\n",
       "      <td>53</td>\n",
       "      <td>9053.98</td>\n",
       "      <td>1698</td>\n",
       "      <td>9032.86</td>\n",
       "      <td>20.67</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9120.11</td>\n",
       "      <td>1742</td>\n",
       "      <td>9099.58</td>\n",
       "      <td>20.43</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9119.24</td>\n",
       "      <td>1741</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140000</th>\n",
       "      <td>77</td>\n",
       "      <td>246</td>\n",
       "      <td>517</td>\n",
       "      <td>9053.46</td>\n",
       "      <td>1697</td>\n",
       "      <td>9032.27</td>\n",
       "      <td>20.66</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9120.68</td>\n",
       "      <td>1743</td>\n",
       "      <td>9100.11</td>\n",
       "      <td>20.48</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9119.24</td>\n",
       "      <td>1741</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145000</th>\n",
       "      <td>79</td>\n",
       "      <td>255</td>\n",
       "      <td>414</td>\n",
       "      <td>9053.00</td>\n",
       "      <td>1697</td>\n",
       "      <td>9031.74</td>\n",
       "      <td>20.66</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9120.62</td>\n",
       "      <td>1742</td>\n",
       "      <td>9100.20</td>\n",
       "      <td>20.33</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9119.24</td>\n",
       "      <td>1741</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150000</th>\n",
       "      <td>86</td>\n",
       "      <td>264</td>\n",
       "      <td>311</td>\n",
       "      <td>9052.58</td>\n",
       "      <td>1696</td>\n",
       "      <td>9031.25</td>\n",
       "      <td>20.65</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9122.20</td>\n",
       "      <td>1744</td>\n",
       "      <td>9101.71</td>\n",
       "      <td>20.40</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9119.24</td>\n",
       "      <td>1741</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155000</th>\n",
       "      <td>82</td>\n",
       "      <td>273</td>\n",
       "      <td>208</td>\n",
       "      <td>9052.14</td>\n",
       "      <td>1695</td>\n",
       "      <td>9030.76</td>\n",
       "      <td>20.65</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9121.59</td>\n",
       "      <td>1745</td>\n",
       "      <td>9101.08</td>\n",
       "      <td>20.41</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9119.24</td>\n",
       "      <td>1741</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160000</th>\n",
       "      <td>86</td>\n",
       "      <td>282</td>\n",
       "      <td>105</td>\n",
       "      <td>9051.72</td>\n",
       "      <td>1695</td>\n",
       "      <td>9030.29</td>\n",
       "      <td>20.64</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9121.12</td>\n",
       "      <td>1743</td>\n",
       "      <td>9100.60</td>\n",
       "      <td>20.43</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9119.24</td>\n",
       "      <td>1741</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165000</th>\n",
       "      <td>78</td>\n",
       "      <td>291</td>\n",
       "      <td>2</td>\n",
       "      <td>9051.28</td>\n",
       "      <td>1694</td>\n",
       "      <td>9029.79</td>\n",
       "      <td>20.64</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9121.95</td>\n",
       "      <td>1745</td>\n",
       "      <td>9101.50</td>\n",
       "      <td>20.36</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9119.24</td>\n",
       "      <td>1741</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170000</th>\n",
       "      <td>78</td>\n",
       "      <td>299</td>\n",
       "      <td>466</td>\n",
       "      <td>9050.89</td>\n",
       "      <td>1693</td>\n",
       "      <td>9029.36</td>\n",
       "      <td>20.64</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9123.14</td>\n",
       "      <td>1746</td>\n",
       "      <td>9102.60</td>\n",
       "      <td>20.44</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9119.24</td>\n",
       "      <td>1741</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175000</th>\n",
       "      <td>78</td>\n",
       "      <td>308</td>\n",
       "      <td>363</td>\n",
       "      <td>9050.56</td>\n",
       "      <td>1693</td>\n",
       "      <td>9028.99</td>\n",
       "      <td>20.63</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9121.28</td>\n",
       "      <td>1744</td>\n",
       "      <td>9100.80</td>\n",
       "      <td>20.38</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9119.24</td>\n",
       "      <td>1741</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180000</th>\n",
       "      <td>78</td>\n",
       "      <td>317</td>\n",
       "      <td>260</td>\n",
       "      <td>9050.23</td>\n",
       "      <td>1692</td>\n",
       "      <td>9028.61</td>\n",
       "      <td>20.63</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9122.27</td>\n",
       "      <td>1745</td>\n",
       "      <td>9101.91</td>\n",
       "      <td>20.28</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9119.24</td>\n",
       "      <td>1741</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185000</th>\n",
       "      <td>77</td>\n",
       "      <td>326</td>\n",
       "      <td>157</td>\n",
       "      <td>9049.88</td>\n",
       "      <td>1692</td>\n",
       "      <td>9028.23</td>\n",
       "      <td>20.62</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9120.53</td>\n",
       "      <td>1743</td>\n",
       "      <td>9100.08</td>\n",
       "      <td>20.36</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9119.24</td>\n",
       "      <td>1741</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190000</th>\n",
       "      <td>81</td>\n",
       "      <td>335</td>\n",
       "      <td>54</td>\n",
       "      <td>9049.53</td>\n",
       "      <td>1691</td>\n",
       "      <td>9027.84</td>\n",
       "      <td>20.62</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9122.55</td>\n",
       "      <td>1745</td>\n",
       "      <td>9102.09</td>\n",
       "      <td>20.37</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9119.24</td>\n",
       "      <td>1741</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195000</th>\n",
       "      <td>81</td>\n",
       "      <td>343</td>\n",
       "      <td>518</td>\n",
       "      <td>9049.21</td>\n",
       "      <td>1691</td>\n",
       "      <td>9027.49</td>\n",
       "      <td>20.62</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9122.39</td>\n",
       "      <td>1745</td>\n",
       "      <td>9102.05</td>\n",
       "      <td>20.25</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9119.24</td>\n",
       "      <td>1741</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200000</th>\n",
       "      <td>80</td>\n",
       "      <td>352</td>\n",
       "      <td>415</td>\n",
       "      <td>9048.92</td>\n",
       "      <td>1690</td>\n",
       "      <td>9027.18</td>\n",
       "      <td>20.62</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9122.32</td>\n",
       "      <td>1744</td>\n",
       "      <td>9101.89</td>\n",
       "      <td>20.34</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9119.24</td>\n",
       "      <td>1741</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205000</th>\n",
       "      <td>81</td>\n",
       "      <td>361</td>\n",
       "      <td>312</td>\n",
       "      <td>9048.66</td>\n",
       "      <td>1690</td>\n",
       "      <td>9026.90</td>\n",
       "      <td>20.61</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9122.04</td>\n",
       "      <td>1745</td>\n",
       "      <td>9101.53</td>\n",
       "      <td>20.42</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9119.24</td>\n",
       "      <td>1741</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210000</th>\n",
       "      <td>79</td>\n",
       "      <td>370</td>\n",
       "      <td>209</td>\n",
       "      <td>9048.39</td>\n",
       "      <td>1690</td>\n",
       "      <td>9026.60</td>\n",
       "      <td>20.61</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9122.55</td>\n",
       "      <td>1746</td>\n",
       "      <td>9102.04</td>\n",
       "      <td>20.42</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9119.24</td>\n",
       "      <td>1741</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215000</th>\n",
       "      <td>79</td>\n",
       "      <td>379</td>\n",
       "      <td>106</td>\n",
       "      <td>9048.12</td>\n",
       "      <td>1689</td>\n",
       "      <td>9026.31</td>\n",
       "      <td>20.61</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9123.29</td>\n",
       "      <td>1747</td>\n",
       "      <td>9102.76</td>\n",
       "      <td>20.44</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9119.24</td>\n",
       "      <td>1741</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220000</th>\n",
       "      <td>87</td>\n",
       "      <td>388</td>\n",
       "      <td>3</td>\n",
       "      <td>9047.83</td>\n",
       "      <td>1689</td>\n",
       "      <td>9026.00</td>\n",
       "      <td>20.61</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9122.92</td>\n",
       "      <td>1746</td>\n",
       "      <td>9102.47</td>\n",
       "      <td>20.37</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9119.24</td>\n",
       "      <td>1741</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225000</th>\n",
       "      <td>96</td>\n",
       "      <td>396</td>\n",
       "      <td>467</td>\n",
       "      <td>9047.58</td>\n",
       "      <td>1688</td>\n",
       "      <td>9025.74</td>\n",
       "      <td>20.60</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9123.05</td>\n",
       "      <td>1745</td>\n",
       "      <td>9102.59</td>\n",
       "      <td>20.36</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9119.24</td>\n",
       "      <td>1741</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        TRAIN:                               VALID:        \\\n",
       "       Time   Ep   Ct     LOSS   PPL      NLL     KL   REG     LOSS   PPL   \n",
       "5000    100    8  463  9175.77  1876  9157.67  17.82  0.27  9156.08  1791   \n",
       "10000    92   17  360  9131.72  1809  9112.23  19.30  0.22  9136.03  1764   \n",
       "15000    91   26  257  9111.77  1780  9091.72  19.87  0.19  9131.22  1756   \n",
       "20000    91   35  154  9099.94  1763  9079.66  20.15  0.18  9127.53  1751   \n",
       "25000    89   44   51  9091.84  1752  9071.42  20.31  0.17  9124.96  1748   \n",
       "30000    89   52  515  9086.05  1744  9065.49  20.40  0.16  9124.32  1747   \n",
       "35000    88   61  412  9081.68  1738  9061.18  20.46  0.15  9123.49  1745   \n",
       "40000    88   70  309  9078.19  1732  9057.69  20.51  0.15  9123.15  1745   \n",
       "45000    85   79  206  9075.20  1728  9054.66  20.54  0.14  9121.44  1743   \n",
       "50000    84   88  103  9072.60  1725  9052.08  20.57  0.14  9120.72  1743   \n",
       "55000    75   97    0  9070.25  1722  9049.79  20.59  0.14  9121.31  1744   \n",
       "60000    75  105  464  9068.35  1719  9047.88  20.61  0.13  9121.82  1743   \n",
       "65000    77  114  361  9066.80  1716  9046.26  20.63  0.13  9121.00  1743   \n",
       "70000    80  123  258  9065.33  1714  9044.70  20.64  0.13  9121.51  1744   \n",
       "75000    76  132  155  9063.95  1712  9043.28  20.65  0.13  9118.39  1740   \n",
       "80000    76  141   52  9062.66  1711  9041.94  20.66  0.13  9120.56  1742   \n",
       "85000    77  149  516  9061.53  1709  9040.78  20.66  0.12  9119.63  1740   \n",
       "90000    83  158  413  9060.53  1707  9039.76  20.67  0.12  9120.26  1742   \n",
       "95000    79  167  310  9059.63  1706  9038.84  20.67  0.12  9121.33  1743   \n",
       "100000   78  176  207  9058.73  1705  9037.93  20.67  0.12  9120.45  1742   \n",
       "105000   76  185  104  9057.89  1704  9037.09  20.67  0.12  9120.83  1743   \n",
       "110000   77  194    1  9057.06  1703  9036.25  20.67  0.12  9121.86  1744   \n",
       "115000   77  202  465  9056.35  1702  9035.53  20.67  0.12  9122.33  1745   \n",
       "120000   77  211  362  9055.74  1701  9034.89  20.67  0.12  9121.30  1744   \n",
       "125000   77  220  259  9055.14  1700  9034.20  20.67  0.12  9120.69  1743   \n",
       "130000   87  229  156  9054.56  1699  9033.53  20.67  0.12  9120.14  1742   \n",
       "135000   75  238   53  9053.98  1698  9032.86  20.67  0.11  9120.11  1742   \n",
       "140000   77  246  517  9053.46  1697  9032.27  20.66  0.11  9120.68  1743   \n",
       "145000   79  255  414  9053.00  1697  9031.74  20.66  0.11  9120.62  1742   \n",
       "150000   86  264  311  9052.58  1696  9031.25  20.65  0.11  9122.20  1744   \n",
       "155000   82  273  208  9052.14  1695  9030.76  20.65  0.11  9121.59  1745   \n",
       "160000   86  282  105  9051.72  1695  9030.29  20.64  0.11  9121.12  1743   \n",
       "165000   78  291    2  9051.28  1694  9029.79  20.64  0.11  9121.95  1745   \n",
       "170000   78  299  466  9050.89  1693  9029.36  20.64  0.11  9123.14  1746   \n",
       "175000   78  308  363  9050.56  1693  9028.99  20.63  0.11  9121.28  1744   \n",
       "180000   78  317  260  9050.23  1692  9028.61  20.63  0.11  9122.27  1745   \n",
       "185000   77  326  157  9049.88  1692  9028.23  20.62  0.11  9120.53  1743   \n",
       "190000   81  335   54  9049.53  1691  9027.84  20.62  0.11  9122.55  1745   \n",
       "195000   81  343  518  9049.21  1691  9027.49  20.62  0.11  9122.39  1745   \n",
       "200000   80  352  415  9048.92  1690  9027.18  20.62  0.11  9122.32  1744   \n",
       "205000   81  361  312  9048.66  1690  9026.90  20.61  0.11  9122.04  1745   \n",
       "210000   79  370  209  9048.39  1690  9026.60  20.61  0.11  9122.55  1746   \n",
       "215000   79  379  106  9048.12  1689  9026.31  20.61  0.11  9123.29  1747   \n",
       "220000   87  388    3  9047.83  1689  9026.00  20.61  0.11  9122.92  1746   \n",
       "225000   96  396  467  9047.58  1688  9025.74  20.60  0.11  9123.05  1745   \n",
       "\n",
       "                                TEST:       SPEC:             HIER:        \n",
       "            NLL     KL   REG     LOSS   PPL     1     2     3 CHILD OTHER  \n",
       "5000    9136.12  19.77  0.19  9156.00  1791  0.39  0.45  0.41  0.30  0.24  \n",
       "10000   9115.48  20.40  0.15  9135.53  1763  0.41  0.51  0.46  0.24  0.20  \n",
       "15000   9110.29  20.80  0.13  9130.96  1756  0.41  0.52  0.48  0.23  0.19  \n",
       "20000   9106.58  20.83  0.12  9127.03  1751  0.42  0.52  0.49  0.22  0.19  \n",
       "25000   9104.15  20.69  0.12  9125.23  1749  0.43  0.52  0.48  0.23  0.20  \n",
       "30000   9103.63  20.56  0.12  9123.97  1746  0.44  0.52  0.48  0.23  0.20  \n",
       "35000   9102.74  20.63  0.11  9123.47  1746  0.44  0.52  0.48  0.23  0.20  \n",
       "40000   9102.42  20.61  0.11  9123.47  1746  0.44  0.51  0.49  0.23  0.20  \n",
       "45000   9100.74  20.59  0.11  9121.59  1743  0.43  0.52  0.49  0.22  0.20  \n",
       "50000   9100.03  20.58  0.11  9121.20  1743  0.45  0.51  0.49  0.23  0.20  \n",
       "55000   9100.64  20.57  0.11  9121.20  1743  0.46  0.51  0.49  0.23  0.20  \n",
       "60000   9101.33  20.39  0.11  9121.20  1743  0.45  0.50  0.48  0.23  0.21  \n",
       "65000   9100.34  20.56  0.11  9121.20  1743  0.45  0.51  0.49  0.23  0.20  \n",
       "70000   9100.82  20.58  0.10  9121.20  1743  0.45  0.51  0.49  0.22  0.20  \n",
       "75000   9097.67  20.62  0.10  9119.24  1741  0.44  0.51  0.49  0.22  0.20  \n",
       "80000   9099.88  20.58  0.10  9119.24  1741  0.46  0.50  0.49  0.23  0.21  \n",
       "85000   9099.04  20.50  0.10  9119.24  1741  0.47  0.51  0.49  0.23  0.21  \n",
       "90000   9099.58  20.58  0.10  9119.24  1741  0.46  0.51  0.49  0.22  0.21  \n",
       "95000   9100.68  20.55  0.10  9119.24  1741  0.45  0.51  0.49  0.22  0.21  \n",
       "100000  9099.85  20.50  0.10  9119.24  1741  0.45  0.51  0.49  0.22  0.21  \n",
       "105000  9100.09  20.63  0.10  9119.24  1741  0.46  0.50  0.49  0.23  0.21  \n",
       "110000  9101.22  20.54  0.10  9119.24  1741  0.46  0.51  0.49  0.22  0.21  \n",
       "115000  9101.74  20.49  0.10  9119.24  1741  0.46  0.50  0.48  0.23  0.22  \n",
       "120000  9100.71  20.49  0.10  9119.24  1741  0.45  0.51  0.49  0.22  0.21  \n",
       "125000  9100.08  20.52  0.10  9119.24  1741  0.45  0.51  0.49  0.22  0.21  \n",
       "130000  9099.58  20.47  0.09  9119.24  1741  0.45  0.51  0.49  0.21  0.21  \n",
       "135000  9099.58  20.43  0.10  9119.24  1741  0.46  0.50  0.49  0.22  0.21  \n",
       "140000  9100.11  20.48  0.10  9119.24  1741  0.47  0.51  0.49  0.23  0.21  \n",
       "145000  9100.20  20.33  0.09  9119.24  1741  0.46  0.51  0.49  0.22  0.21  \n",
       "150000  9101.71  20.40  0.09  9119.24  1741  0.46  0.51  0.50  0.22  0.21  \n",
       "155000  9101.08  20.41  0.09  9119.24  1741  0.45  0.51  0.49  0.21  0.21  \n",
       "160000  9100.60  20.43  0.09  9119.24  1741  0.46  0.50  0.49  0.22  0.21  \n",
       "165000  9101.50  20.36  0.09  9119.24  1741  0.46  0.51  0.49  0.21  0.21  \n",
       "170000  9102.60  20.44  0.09  9119.24  1741  0.47  0.51  0.49  0.22  0.21  \n",
       "175000  9100.80  20.38  0.09  9119.24  1741  0.46  0.51  0.49  0.22  0.21  \n",
       "180000  9101.91  20.28  0.09  9119.24  1741  0.45  0.51  0.49  0.22  0.21  \n",
       "185000  9100.08  20.36  0.09  9119.24  1741  0.45  0.51  0.50  0.21  0.21  \n",
       "190000  9102.09  20.37  0.09  9119.24  1741  0.47  0.50  0.49  0.22  0.21  \n",
       "195000  9102.05  20.25  0.09  9119.24  1741  0.47  0.51  0.49  0.22  0.21  \n",
       "200000  9101.89  20.34  0.09  9119.24  1741  0.46  0.51  0.49  0.22  0.21  \n",
       "205000  9101.53  20.42  0.09  9119.24  1741  0.46  0.51  0.50  0.22  0.21  \n",
       "210000  9102.04  20.42  0.09  9119.24  1741  0.46  0.51  0.50  0.21  0.21  \n",
       "215000  9102.76  20.44  0.09  9119.24  1741  0.46  0.50  0.49  0.22  0.21  \n",
       "220000  9102.47  20.37  0.09  9119.24  1741  0.47  0.51  0.50  0.21  0.21  \n",
       "225000  9102.59  20.36  0.09  9119.24  1741  0.47  0.51  0.49  0.22  0.21  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 R: 1.000 P: 0.111 speech user dialogue human utterance utterances speaker systems recognition users\n",
      "   1 R: 0.234 P: 0.079 semantic verb structure syntactic lexical type subject noun representation meaning\n",
      "     11 R: 0.078 P: 0.078 tree parsing dependency parser grammar node rules trees parse rule\n",
      "     12 R: 0.077 P: 0.077 languages morphological errors english pos character error chinese tags tag\n",
      "   2 R: 0.191 P: 0.057 resources domain linguistic annotation ontology knowledge languages project concepts semantic\n",
      "     21 R: 0.062 P: 0.062 translation english source alignment target phrase pairs parallel translations sentences\n",
      "     22 R: 0.072 P: 0.072 sense semantic wordnet lexical senses noun verb context nouns terms\n",
      "   3 R: 0.177 P: 0.033 entity entities named mentions coreference mention extraction names features resolution\n",
      "     31 R: 0.091 P: 0.091 models probability algorithm training method distribution function parameters probabilities sequence\n",
      "     32 R: 0.053 P: 0.053 document documents topic sentences query terms method term topics web\n",
      "   4 R: 0.168 P: 0.036 event relations relation annotation semantic events discourse argument temporal annotated\n",
      "     41 R: 0.044 P: 0.044 similarity semantic graph vector vectors pairs space matrix models clustering\n",
      "     42 R: 0.088 P: 0.088 features feature training performance classification learning classifier domain accuracy test\n",
      "   5 R: 0.119 P: 0.028 question answer questions answers knowledge qa systems answering query sentences\n",
      "     51 R: 0.045 P: 0.045 models network neural embeddings training layer input embedding learning vector\n",
      "     52 R: 0.046 P: 0.046 sentiment tweets analysis negative positive polarity social opinion twitter emotion\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "while epoch < config.n_epochs:\n",
    "    # train\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = model.get_feed_dict(batch)\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, ppls_batch, global_step_log = \\\n",
    "        sess.run([model.opt, model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_reg, model.topic_ppls, tf.train.get_global_step()], feed_dict = feed_dict)\n",
    "\n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if global_step_log % config.log_period == 0:\n",
    "            # validate\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "\n",
    "            # test\n",
    "            if ppl_dev < ppl_min:\n",
    "                ppl_min = ppl_dev\n",
    "                loss_test, _, _, _, ppl_test, _ = validate(sess, test_batches, model)\n",
    "                saver.save(sess, config.path_model, global_step=global_step_log)\n",
    "                cPickle.dump(config, open(config.path_config % global_step_log, 'wb'))\n",
    "                update_checkpoint(config, checkpoint, global_step_log)\n",
    "            \n",
    "            # visualize topic\n",
    "            topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "            topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "            topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "            topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "            descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "            recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "            \n",
    "            depth_specs = get_topic_specialization(sess, model, instances_test)\n",
    "            hierarchical_affinities = get_hierarchical_affinity(sess, model)\n",
    "            \n",
    "            # log\n",
    "            clear_output()\n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_reg_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_reg_dev, \\\n",
    "                    '%.2f'%loss_test, '%.0f'%ppl_test, \\\n",
    "                    '%.2f'%depth_specs[1], '%.2f'%depth_specs[2], '%.2f'%depth_specs[3], \\\n",
    "                    '%.2f'%hierarchical_affinities[0], '%.2f'%hierarchical_affinities[1]],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "            cPickle.dump(log_df, open(os.path.join(config.path_log), 'wb'))\n",
    "            print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)\n",
    "\n",
    "            # update tree\n",
    "            if not config.static:\n",
    "                config.tree_idxs, update_tree_flg = model.update_tree(topic_prob_topic, recur_prob_topic)\n",
    "                if update_tree_flg:\n",
    "                    print(config.tree_idxs)\n",
    "                    name_variables = {tensor.name: variable for tensor, variable in zip(tf.global_variables(), sess.run(tf.global_variables()))} # store paremeters\n",
    "                    if 'sess' in globals(): sess.close()\n",
    "                    model = HierarchicalNeuralTopicModel(config)\n",
    "                    sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "                    name_tensors = {tensor.name: tensor for tensor in tf.global_variables()}\n",
    "                    sess.run([name_tensors[name].assign(variable) for name, variable in name_variables.items()]) # restore parameters\n",
    "                    saver = tf.train.Saver(max_to_keep=1)\n",
    "                \n",
    "            time_start = time.time()\n",
    "\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "    epoch += 1\n",
    "\n",
    "loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "display(log_df)\n",
    "print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_year():\n",
    "    probs_topics = []\n",
    "    years = []\n",
    "    for i, train_batch in train_batches:\n",
    "        probs_topics_batch = sess.run(model.prob_topic, feed_dict=model.get_feed_dict(train_batch, mode='test'))\n",
    "        years_batch = [instance.year for instance in train_batch]\n",
    "        probs_topics += [probs_topics_batch]\n",
    "        years += years_batch\n",
    "    probs_topics = np.concatenate(probs_topics)\n",
    "    years = np.array(years)\n",
    "\n",
    "    topic_years = years.dot(probs_topics) / np.sum(probs_topics, 0)\n",
    "    topic_year = {model.topic_idxs[i]: year for i, year in enumerate(topic_years)}\n",
    "    return topic_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36288, 16), (36288,))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_topics.shape, years.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 2006.1405287293783,\n",
       " 1: 2015.073065684627,\n",
       " 2: 2006.6358226095088,\n",
       " 3: 2010.2605308626469,\n",
       " 4: 2011.1153424999409,\n",
       " 5: 2010.2201559271937,\n",
       " 11: 2010.638231762466,\n",
       " 12: 2013.6412160983643,\n",
       " 21: 2009.3432210176409,\n",
       " 22: 2003.0377479786164,\n",
       " 31: 2010.7845425693845,\n",
       " 32: 2007.4959372377405,\n",
       " 41: 2008.9745451250799,\n",
       " 42: 2009.024079592822,\n",
       " 51: 2007.963976828227,\n",
       " 52: 2008.7701026307714}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topic_sample(sess, model, topic_prob_topic=None, recur_prob_topic=None, topic_freq_tokens=None, topic_year=None, parent_idx=0, depth=0):\n",
    "    if depth == 0: # print root\n",
    "        assert len(topic_prob_topic) == len(recur_prob_topic) == len(topic_freq_tokens) == len(topic_year)\n",
    "        freq_tokens = topic_freq_tokens[parent_idx]\n",
    "        recur_topic = recur_prob_topic[parent_idx]\n",
    "        prob_topic = topic_prob_topic[parent_idx]\n",
    "        year = topic_year[parent_idx]\n",
    "        print(parent_idx, 'Avg Year: %i' % year, ' '.join(freq_tokens))\n",
    "    \n",
    "    child_idxs = model.tree_idxs[parent_idx]\n",
    "    depth += 1\n",
    "    for child_idx in child_idxs:\n",
    "        freq_tokens = topic_freq_tokens[child_idx]\n",
    "        recur_topic = recur_prob_topic[child_idx]\n",
    "        prob_topic = topic_prob_topic[child_idx]\n",
    "        year = topic_year[child_idx]\n",
    "        print('  '*depth, child_idx, 'Avg Year: %i' % year, ' '.join(freq_tokens))\n",
    "        \n",
    "        if child_idx in model.tree_idxs: \n",
    "            print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens, topic_year=topic_year, parent_idx=child_idx, depth=depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Avg Year: 2006 user dialogue knowledge utterance generation domain action state systems human\n",
      "   1 Avg Year: 2015 models embeddings neural vector network training vectors representations embedding representation\n",
      "     11 Avg Year: 2010 document documents question topic query sentences questions answer method similarity\n",
      "     12 Avg Year: 2013 sentiment features tweets negative positive classification analysis social opinion polarity\n",
      "   2 Avg Year: 2006 tree parsing dependency parser grammar node trees parse rules nodes\n",
      "     21 Avg Year: 2009 event relations annotation relation discourse events argument semantic annotated temporal\n",
      "     22 Avg Year: 2003 verb semantic syntactic structure noun lexical verbs rules grammar phrase\n",
      "   3 Avg Year: 2010 training models algorithm probability features feature function learning performance distribution\n",
      "     31 Avg Year: 2010 features feature training performance entity learning classification classifier entities class\n",
      "     32 Avg Year: 2007 errors pos character chinese error segmentation tag tags tagging method\n",
      "   4 Avg Year: 2011 translation source target alignment phrase english pairs training parallel sentences\n",
      "     41 Avg Year: 2008 sense semantic similarity wordnet lexical senses terms context relations method\n",
      "     42 Avg Year: 2009 languages english morphological translation arabic lexicon dictionary forms form spanish\n",
      "   5 Avg Year: 2010 evaluation systems human sentences scores test quality score metrics mt\n",
      "     51 Avg Year: 2007 annotation web resources tools user research project tool linguistic processing\n",
      "     52 Avg Year: 2008 speech speakers speaker study features spoken recognition models native participants\n"
     ]
    }
   ],
   "source": [
    "print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens, topic_year=topic_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
