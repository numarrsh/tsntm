{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "# %matplotlib nbagg\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import hmean\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from data_structure import get_batches, get_test_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "\n",
    "from topic_beam_search_decoder import BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '1', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/bags/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/topic_vae', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'bags', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 1000, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 3000, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "# flags.DEFINE_string('opt', 'Adam', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.01, 'lr')\n",
    "flags.DEFINE_float('reg', 1., 'regularization term')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_bool('warmup', True, 'flg of warming up')\n",
    "flags.DEFINE_integer('epochs_cycle', 5, 'number of epochs within a cycle')\n",
    "flags.DEFINE_float('r_cycle', 0.5, 'proportion used to increase beta within a cycle')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 20, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_bow', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_hidden_topic', 512, 'dim_hidden_topic')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "flags.DEFINE_bool('bidirectional', True, 'flg of bidirectional encoding')\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_batches(instances, batch_size, iterator=False):\n",
    "    iter_instances = iter(instances)\n",
    "    n_batch = len(instances)//batch_size\n",
    "    \n",
    "    batches = [(i_batch, [next(iter_instances) for i_doc in range(batch_size)]) for i_batch in range(n_batch)]\n",
    "    \n",
    "    if iterator: batches = iter(batches)\n",
    "    return batches\n",
    "\n",
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "flags.DEFINE_integer('cycle_steps', len(train_batches)*config.epochs_cycle, 'number of steps for each cycle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     10,
     24,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables, model):\n",
    "    sample_batch = dev_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, model, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "def check_shape(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()\n",
    "    \n",
    "def check_value(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doubly rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoublyRNNCell:\n",
    "    def __init__(self, dim_hidden, output_layer=None):\n",
    "        self.dim_hidden = dim_hidden\n",
    "        \n",
    "        self.ancestral_layer=tf.layers.Dense(units=dim_hidden, activation=tf.nn.tanh, name='ancestral')\n",
    "        self.fraternal_layer=tf.layers.Dense(units=dim_hidden, activation=tf.nn.tanh, name='fraternal')\n",
    "        self.hidden_layer = tf.layers.Dense(units=dim_hidden, name='hidden')\n",
    "        \n",
    "        self.output_layer=output_layer\n",
    "        \n",
    "    def __call__(self, state_ancestral, state_fraternal, reuse=True):\n",
    "        with tf.variable_scope('input', reuse=reuse):\n",
    "            state_ancestral = self.ancestral_layer(state_ancestral)\n",
    "            state_fraternal = self.fraternal_layer(state_fraternal)\n",
    "\n",
    "        with tf.variable_scope('output', reuse=reuse):\n",
    "            state_hidden = self.hidden_layer(state_ancestral + state_fraternal)\n",
    "            if self.output_layer is not None: \n",
    "                output = self.output_layer(state_hidden)\n",
    "            else:\n",
    "                output = state_hidden\n",
    "            \n",
    "        return output, state_hidden\n",
    "    \n",
    "    def get_initial_state(self, name):\n",
    "        initial_state = tf.get_variable(name, [1, self.dim_hidden], dtype=tf.float32)\n",
    "        return initial_state\n",
    "    \n",
    "    def get_zero_state(self, name):\n",
    "        zero_state = tf.zeros([1, self.dim_hidden], dtype=tf.float32, name=name)\n",
    "        return zero_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubly_rnn(dim_hidden, tree_idxs, initial_state_parent=None, initial_state_sibling=None, output_layer=None, name=''):\n",
    "    outputs, states_parent = {}, {}\n",
    "    \n",
    "    with tf.variable_scope(name, reuse=False):\n",
    "        doubly_rnn_cell = DoublyRNNCell(dim_hidden, output_layer)\n",
    "\n",
    "        if initial_state_parent is None: \n",
    "            initial_state_parent = doubly_rnn_cell.get_initial_state('init_state_parent')\n",
    "#             initial_state_parent = doubly_rnn_cell.get_zero_state('init_state_parent')\n",
    "        if initial_state_sibling is None: \n",
    "#             initial_state_sibling = doubly_rnn_cell.get_initial_state('init_state_sibling')\n",
    "            initial_state_sibling = doubly_rnn_cell.get_zero_state('init_state_sibling')\n",
    "        output, state_sibling = doubly_rnn_cell(initial_state_parent, initial_state_sibling, reuse=False)\n",
    "        outputs[0], states_parent[0] = output, state_sibling\n",
    "\n",
    "        for parent_idx, child_idxs in tree_idxs.items():\n",
    "            state_parent = states_parent[parent_idx]\n",
    "            state_sibling = initial_state_sibling\n",
    "            for child_idx in child_idxs:\n",
    "                output, state_sibling = doubly_rnn_cell(state_parent, state_sibling)\n",
    "                outputs[child_idx], states_parent[child_idx] = output, state_sibling\n",
    "\n",
    "    return outputs, states_parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell:\n",
    "    def __init__(self, dim_hidden, output_layer=None):\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.hidden_layer = tf.layers.Dense(units=dim_hidden, activation=tf.nn.tanh, name='hidden')\n",
    "        self.output_layer=output_layer\n",
    "        \n",
    "    def __call__(self, state, reuse=True):\n",
    "        with tf.variable_scope('output', reuse=reuse):\n",
    "            state_hidden = self.hidden_layer(state)\n",
    "            if self.output_layer is not None: \n",
    "                output = self.output_layer(state_hidden)\n",
    "            else:\n",
    "                output = state_hidden\n",
    "            \n",
    "        return output, state_hidden\n",
    "    \n",
    "    def get_initial_state(self, name):\n",
    "        initial_state = tf.get_variable(name, [1, self.dim_hidden], dtype=tf.float32)\n",
    "        return initial_state\n",
    "    \n",
    "    def get_zero_state(self, name):\n",
    "        zero_state = tf.zeros([1, self.dim_hidden], dtype=tf.float32, name=name)\n",
    "        return zero_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(dim_hidden, max_depth, initial_state=None, output_layer=None, name=''):\n",
    "    outputs, states_hidden = [], []\n",
    "    \n",
    "    with tf.variable_scope(name, reuse=False):\n",
    "        rnn_cell = RNNCell(dim_hidden, output_layer)\n",
    "\n",
    "        if initial_state is not None: \n",
    "            state_hidden = initial_state\n",
    "        else:\n",
    "            state_hidden = rnn_cell.get_initial_state('init_state')\n",
    "#             state_hidden = rnn_cell.get_zero_state('init_state_parent')\n",
    "        \n",
    "        for depth in range(max_depth):\n",
    "            if depth == 0:                \n",
    "                output, state_hidden = rnn_cell(state_hidden, reuse=False)\n",
    "            else:\n",
    "                output, state_hidden = rnn_cell(state_hidden, reuse=True)\n",
    "            outputs.append(output)\n",
    "            states_hidden.append(state_hidden)\n",
    "\n",
    "    outputs = tf.concat(outputs, 1)\n",
    "    states_hidden = tf.concat(states_hidden, 0)\n",
    "    return outputs, states_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nCRP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "code_folding": [
     1,
     2,
     25,
     48,
     63,
     232
    ]
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, config, tree_idxs):\n",
    "        def get_depth(parent_idx=0, tree_depth=None, depth=1):\n",
    "            if tree_depth is None: tree_depth={0: depth}\n",
    "\n",
    "            child_idxs = tree_idxs[parent_idx]\n",
    "            depth +=1\n",
    "            for child_idx in child_idxs:\n",
    "                tree_depth[child_idx] = depth\n",
    "                if child_idx in tree_idxs: get_depth(child_idx, tree_depth, depth)\n",
    "            return tree_depth\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        self.t_variables = {}\n",
    "        \n",
    "        self.tree_idxs = tree_idxs\n",
    "        self.topic_idxs = [0] + [idx for child_idxs in tree_idxs.values() for idx in child_idxs]\n",
    "        self.child_to_parent_idxs = {child_idx: parent_idx for parent_idx, child_idxs in self.tree_idxs.items() for child_idx in child_idxs}\n",
    "        self.tree_depth = get_depth()\n",
    "        self.n_depth = max(self.tree_depth.values())\n",
    "        \n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        def nCRP(tree_sticks_topic):\n",
    "            tree_prob_topic = {}\n",
    "            tree_prob_leaf = {}\n",
    "            # calculate topic probability and save\n",
    "            tree_prob_topic[0] = 1.\n",
    "\n",
    "            for parent_idx, child_idxs in self.tree_idxs.items():\n",
    "                rest_prob_topic = tree_prob_topic[parent_idx]\n",
    "                for child_idx in child_idxs:\n",
    "                    stick_topic = tree_sticks_topic[child_idx]\n",
    "                    if child_idx == child_idxs[-1]:\n",
    "                        prob_topic = rest_prob_topic * 1.\n",
    "                    else:\n",
    "                        prob_topic = rest_prob_topic * stick_topic\n",
    "\n",
    "                    if not child_idx in self.tree_idxs: # leaf childs\n",
    "                        tree_prob_leaf[child_idx] = prob_topic\n",
    "                    else:\n",
    "                        tree_prob_topic[child_idx] = prob_topic\n",
    "\n",
    "                    rest_prob_topic -= prob_topic\n",
    "            return tree_prob_leaf\n",
    "\n",
    "        def sbp(sticks_depth, max_depth):\n",
    "            prob_depth_list = []\n",
    "            rest_prob_depth = 1.\n",
    "            for depth in range(max_depth):\n",
    "                stick_depth = tf.expand_dims(sticks_depth[:, depth], 1)\n",
    "                if depth == max_depth -1:\n",
    "                    prob_depth = rest_prob_depth * 1.\n",
    "                else:\n",
    "                    prob_depth = rest_prob_depth * stick_depth\n",
    "                prob_depth_list.append(prob_depth)\n",
    "                rest_prob_depth -= prob_depth\n",
    "\n",
    "            prob_depth = tf.concat(prob_depth_list, 1)\n",
    "            return prob_depth\n",
    "       \n",
    "        def get_prob_topic(tree_prob_leaf, prob_depth):\n",
    "            def get_ancestor_idxs(leaf_idx, ancestor_idxs = None):\n",
    "                if ancestor_idxs is None: ancestor_idxs = [leaf_idx]\n",
    "                parent_idx = self.child_to_parent_idxs[leaf_idx]\n",
    "                ancestor_idxs += [parent_idx]\n",
    "                if parent_idx in self.child_to_parent_idxs: get_ancestor_idxs(parent_idx, ancestor_idxs)\n",
    "                return ancestor_idxs[::-1]\n",
    "            \n",
    "            tree_prob_topic = defaultdict(float)\n",
    "            leaf_ancestor_idxs = {leaf_idx: get_ancestor_idxs(leaf_idx) for leaf_idx in tree_prob_leaf}\n",
    "            for leaf_idx, ancestor_idxs in leaf_ancestor_idxs.items():\n",
    "                prob_leaf = tree_prob_leaf[leaf_idx]\n",
    "                for i, ancestor_idx in enumerate(ancestor_idxs):\n",
    "                    prob_ancestor = prob_leaf * tf.expand_dims(prob_depth[:, i], -1)\n",
    "                    tree_prob_topic[ancestor_idx] += prob_ancestor\n",
    "            prob_topic = tf.concat([tree_prob_topic[topic_idx] for topic_idx in self.topic_idxs], -1)\n",
    "            return prob_topic\n",
    "        \n",
    "        def get_tree_topic_bow(tree_topic_embeddings):\n",
    "            def softmax_with_temperature(logits, axis=None, name=None, temperature=1.):\n",
    "                if axis is None:\n",
    "                    axis = -1\n",
    "                return tf.exp(logits / temperature) / tf.reduce_sum(tf.exp(logits / temperature), axis=axis)\n",
    "\n",
    "            tree_topic_bow = {}\n",
    "            for topic_idx, depth in self.tree_depth.items():\n",
    "                topic_embedding = tree_topic_embeddings[topic_idx]\n",
    "                temperature = tf.constant(1. ** (1./depth), dtype=tf.float32)\n",
    "                logits = tf.matmul(topic_embedding, self.bow_embeddings, transpose_b=True)\n",
    "                tree_topic_bow[topic_idx] = softmax_with_temperature(logits, axis=-1, temperature=temperature)\n",
    "            return tree_topic_bow\n",
    "        \n",
    "        def get_topic_loss_reg(tree_topic_embeddings):\n",
    "            def get_tree_mask_reg(all_child_idxs):        \n",
    "                tree_mask_reg = np.zeros([len(all_child_idxs), len(all_child_idxs)], dtype=np.float32)\n",
    "                for parent_idx, child_idxs in self.tree_idxs.items():\n",
    "#                     neighbor_idxs = child_idxs + [parent_idx] if parent_idx in all_child_idxs else child_idxs\n",
    "                    neighbor_idxs = child_idxs\n",
    "                    for neighbor_idx1 in neighbor_idxs:\n",
    "                        for neighbor_idx2 in neighbor_idxs:\n",
    "                            neighbor_index1 = all_child_idxs.index(neighbor_idx1)\n",
    "                            neighbor_index2 = all_child_idxs.index(neighbor_idx2)\n",
    "                            tree_mask_reg[neighbor_index1, neighbor_index2] = tree_mask_reg[neighbor_index2, neighbor_index1] = 1.\n",
    "                return tree_mask_reg\n",
    "            \n",
    "            all_child_idxs = list(self.child_to_parent_idxs.keys())\n",
    "            self.diff_topic_embeddings = tf.concat([tree_topic_embeddings[child_idx] - tree_topic_embeddings[self.child_to_parent_idxs[child_idx]] for child_idx in all_child_idxs], axis=0)\n",
    "            diff_topic_embeddings_norm = self.diff_topic_embeddings / tf.norm(self.diff_topic_embeddings, axis=1, keepdims=True)\n",
    "            self.topic_dots = tf.clip_by_value(tf.matmul(diff_topic_embeddings_norm, tf.transpose(diff_topic_embeddings_norm)), -1., 1.)        \n",
    "\n",
    "            self.tree_mask_reg = get_tree_mask_reg(all_child_idxs)\n",
    "            self.topic_losses_reg = tf.square(self.topic_dots - tf.eye(len(all_child_idxs))) * self.tree_mask_reg\n",
    "            self.topic_loss_reg = tf.reduce_sum(self.topic_losses_reg) / tf.reduce_sum(self.tree_mask_reg)\n",
    "            return self.topic_loss_reg\n",
    "        \n",
    "#         def get_topic_loss_reg(tree_topic_embeddings):\n",
    "#             def get_tree_mask_reg(all_child_idxs):        \n",
    "#                 tree_mask_reg = np.zeros([len(all_child_idxs), len(all_child_idxs)], dtype=np.float32)\n",
    "#                 for parent_idx, child_idxs in self.tree_idxs.items():\n",
    "#                     neighbor_idxs = child_idxs + [parent_idx] if parent_idx in all_child_idxs else child_idxs\n",
    "#                     for neighbor_idx1 in neighbor_idxs:\n",
    "#                         for neighbor_idx2 in neighbor_idxs:\n",
    "#                             neighbor_index1 = all_child_idxs.index(neighbor_idx1)\n",
    "#                             neighbor_index2 = all_child_idxs.index(neighbor_idx2)\n",
    "#                             tree_mask_reg[neighbor_index1, neighbor_index2] = tree_mask_reg[neighbor_index2, neighbor_index1] = 1.\n",
    "#                 return tree_mask_reg\n",
    "            \n",
    "#             self.topic_losses_reg = []\n",
    "#             for parent_idx, child_idxs in self.tree_idxs.items():\n",
    "#                 diff_topic_embeddings = tf.concat([tree_topic_embeddings[child_idx] - tree_topic_embeddings[parent_idx] for child_idx in child_idxs], axis=0)\n",
    "#                 diff_topic_embeddings_norm = diff_topic_embeddings / tf.norm(diff_topic_embeddings, axis=1, keepdims=True)\n",
    "#                 topic_dots = tf.clip_by_value(tf.matmul(diff_topic_embeddings_norm, tf.transpose(diff_topic_embeddings_norm)), -1., 1.)        \n",
    "\n",
    "#                 mean_angles = tf.asin(tf.sqrt(tf.clip_by_value(tf.linalg.det(topic_dots), 0, 1)))\n",
    "#                 var_angles = tf.square(tf.constant(np.pi/2., dtype=tf.float32)-mean_angles)\n",
    "\n",
    "#                 topic_loss_reg = var_angles - mean_angles\n",
    "#                 self.topic_losses_reg.append(topic_loss_reg)\n",
    "\n",
    "#             self.topic_loss_reg = tf.reduce_sum(self.topic_losses_reg)\n",
    "#             return self.topic_loss_reg\n",
    "           \n",
    "        # -------------- Build Model --------------\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.t_variables['bow'] = tf.placeholder(tf.float32, [None, self.config.dim_bow])\n",
    "        self.t_variables['keep_prob'] = tf.placeholder(tf.float32)\n",
    "        \n",
    "        # encode bow\n",
    "        with tf.variable_scope('topic/enc', reuse=False):\n",
    "            hidden_bow_ = tf.layers.Dense(units=self.config.dim_hidden_bow, activation=tf.nn.tanh, name='hidden_bow')(self.t_variables['bow'])\n",
    "            hidden_bow = tf.layers.Dropout(self.t_variables['keep_prob'])(hidden_bow_)\n",
    "            means_bow = tf.layers.Dense(units=self.config.dim_latent_bow, name='mean_bow')(hidden_bow)\n",
    "            logvars_bow = tf.layers.Dense(units=self.config.dim_latent_bow, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_bow')(hidden_bow)\n",
    "            latents_bow = sample_latents(means_bow, logvars_bow) # sample latent vectors\n",
    "            prob_layer = lambda h: tf.nn.sigmoid(tf.matmul(latents_bow, h, transpose_b=True))\n",
    "\n",
    "            tree_sticks_topic, tree_states_sticks_topic = doubly_rnn(self.config.dim_latent_bow, self.tree_idxs, output_layer=prob_layer, name='sticks_topic')\n",
    "            tree_prob_leaf = nCRP(tree_sticks_topic)\n",
    "            self.tree_prob_leaf = tree_prob_leaf\n",
    "#             prob_depth = tf.layers.Dense(units=self.n_depth, activation=tf.nn.softmax, name='prob_depth')(latents_bow) # inference of topic probabilities\n",
    "            sticks_depth, _ = rnn(config.dim_latent_bow, self.n_depth, output_layer=prob_layer, name='prob_depth')\n",
    "            prob_depth = sbp(sticks_depth, self.n_depth)\n",
    "            self.prob_depth = prob_depth\n",
    "\n",
    "            prob_topic = get_prob_topic(tree_prob_leaf, prob_depth)\n",
    "            self.prob_topic = prob_topic # n_batch x n_topic\n",
    "\n",
    "        # decode bow\n",
    "        with tf.variable_scope('shared', reuse=False):\n",
    "            self.bow_embeddings = tf.get_variable('emb', [self.config.dim_bow, self.config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "        with tf.variable_scope('topic/dec', reuse=False):\n",
    "        #     tree_topic_embeddings, tree_states_topic_embeddings = doubly_rnn(self.config.dim_emb, self.tree_idxs, name='emb_topic')\n",
    "            emb_layer = lambda h: tf.layers.Dense(units=self.config.dim_emb, name='output')(tf.nn.tanh(h))\n",
    "            tree_topic_embeddings, tree_states_topic_embeddings = doubly_rnn(self.config.dim_emb, self.tree_idxs, output_layer=emb_layer, name='emb_topic')\n",
    "#             topic_embeddings = tf.get_variable('topic_emb', [len(self.topic_idxs), self.config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of topics\n",
    "#             tree_topic_embeddings = {topic_idx: tf.expand_dims(topic_embeddings[self.topic_idxs.index(topic_idx)], 0) for topic_idx in self.topic_idxs}\n",
    "\n",
    "            self.tree_topic_bow = get_tree_topic_bow(tree_topic_embeddings) # bow vectors for each topic\n",
    "\n",
    "            topic_bow = tf.concat([self.tree_topic_bow[topic_idx] for topic_idx in self.topic_idxs], 0) # KxV\n",
    "            self.topic_bow = topic_bow\n",
    "            logits_bow = tf_log(tf.matmul(prob_topic, topic_bow)) # predicted bow distribution N_Batch x  V\n",
    "            self.logits_bow = logits_bow\n",
    "            \n",
    "        # define losses\n",
    "        self.topic_losses_recon = -tf.reduce_sum(tf.multiply(self.t_variables['bow'], logits_bow), 1)\n",
    "        self.topic_loss_recon = tf.reduce_mean(self.topic_losses_recon) # negative log likelihood of each words\n",
    "\n",
    "        self.topic_loss_kl = compute_kl_loss(means_bow, logvars_bow) # KL divergence b/w latent dist & gaussian std\n",
    "        \n",
    "        self.topic_embeddings = tf.concat([tree_topic_embeddings[topic_idx] for topic_idx in self.topic_idxs], 0) # temporary\n",
    "        self.topic_loss_reg = get_topic_loss_reg(tree_topic_embeddings)\n",
    "\n",
    "#         self.topic_embeddings = tf.concat([tree_topic_embeddings[topic_idx] for topic_idx in self.topic_idxs], 0)\n",
    "#         topic_embeddings_norm = self.topic_embeddings / tf.norm(self.topic_embeddings, axis=1, keepdims=True)\n",
    "#         self.topic_dots = tf.clip_by_value(tf.matmul(topic_embeddings_norm, tf.transpose(topic_embeddings_norm)), -1., 1.)\n",
    "#         self.mean_angles = tf.asin(tf.sqrt(tf.linalg.det(self.topic_dots)))\n",
    "#         self.var_angles = tf.square(tf.constant(np.pi/2., dtype=tf.float32)-self.mean_angles)\n",
    "#         self.topic_loss_reg = self.var_angles - self.mean_angles\n",
    "\n",
    "        self.global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "\n",
    "        self.loss = self.topic_loss_recon + self.topic_loss_kl + self.config.reg * self.topic_loss_reg\n",
    "\n",
    "        # define optimizer\n",
    "        if self.config.opt == 'Adam':\n",
    "            optimizer = tf.train.AdamOptimizer(self.config.lr)\n",
    "        elif self.config.opt == 'Adagrad':\n",
    "            optimizer = tf.train.AdagradOptimizer(self.config.lr)\n",
    "\n",
    "        self.grad_vars = optimizer.compute_gradients(self.loss)\n",
    "        self.clipped_grad_vars = [(tf.clip_by_value(grad, -self.config.grad_clip, self.config.grad_clip), var) for grad, var in self.grad_vars]\n",
    "        self.opt = optimizer.apply_gradients(self.clipped_grad_vars, global_step=self.global_step)\n",
    "\n",
    "        # monitor\n",
    "        self.n_bow = tf.reduce_sum(self.t_variables['bow'], 1)\n",
    "        self.topic_ppls = tf.divide(self.topic_losses_recon, tf.maximum(1e-5, self.n_bow))\n",
    "        self.topics_freq_bow_indices = tf.nn.top_k(topic_bow, 10, name='topic_freq_bow').indices\n",
    "    \n",
    "        # growth criteria\n",
    "#         self.dist_bow = -tf.matmul(self.t_variables['bow'], tf.log(topic_bow), transpose_b=True)\n",
    "#         self.rads_bow = tf.divide(tf.multiply(self.dist_bow, prob_topic), tf.expand_dims(self.n_bow, -1))\n",
    "        self.n_topics = tf.multiply(tf.expand_dims(self.n_bow, -1), prob_topic)\n",
    "        \n",
    "        self.arcs_bow = tf.acos(tf.matmul(tf.linalg.l2_normalize(self.bow_embeddings, axis=-1), tf.linalg.l2_normalize(self.topic_embeddings, axis=-1), transpose_b=True)) # n_vocab x n_topic\n",
    "        self.rads_bow = tf.multiply(tf.matmul(self.t_variables['bow'], self.arcs_bow), self.prob_topic) # n_batch x n_topic\n",
    "    \n",
    "    def get_feed_dict(self, batch, mode='train'):\n",
    "        bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "        keep_prob = self.config.keep_prob if mode == 'train' else 1.0\n",
    "        feed_dict = {\n",
    "                    self.t_variables['bow']: bow, \n",
    "                    self.t_variables['keep_prob']: keep_prob\n",
    "        }\n",
    "        return  feed_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_loss(sess, batches, model):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    rads_bow_list = []\n",
    "    prob_topic_list = []\n",
    "    n_bow_list = []\n",
    "    n_topics_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = model.get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, ppls_batch, rads_bow_batch, prob_topic_batch, n_bow_batch, n_topics_batch \\\n",
    "            = sess.run([model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_reg, model.topic_ppls, model.rads_bow, model.prob_topic, model.n_bow, model.n_topics], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "        rads_bow_list.append(rads_bow_batch)\n",
    "        prob_topic_list.append(prob_topic_batch)\n",
    "        n_bow_list.append(n_bow_batch)\n",
    "        n_topics_list.append(n_topics_batch)\n",
    "    loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    \n",
    "    probs_topic = np.concatenate(prob_topic_list, 0)\n",
    "    \n",
    "    n_bow = np.concatenate(n_bow_list, 0)\n",
    "    n_topics = np.concatenate(n_topics_list, 0)\n",
    "    probs_topic_mean = np.sum(n_topics, 0) / np.sum(n_bow)\n",
    "    \n",
    "    rads_bow = np.concatenate(rads_bow_list, 0)\n",
    "    rads_bow_mean = np.cos(np.sum(rads_bow, 0) / np.sum(n_topics, 0))\n",
    "    \n",
    "    return loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, ppl_mean, rads_bow_mean, probs_topic_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def print_topic_sample(tree_idxs, sess=None, model=None, topic_rad_bow=None, topic_prob_topic=None, parent_idx=0, topics_freq_bow_idxs=None, depth = 0):\n",
    "    if topics_freq_bow_idxs is None:\n",
    "        topics_freq_bow_idxs = bow_idxs[sess.run(model.topics_freq_bow_indices)]\n",
    "        topic_freq_bow_idxs = topics_freq_bow_idxs[model.topic_idxs.index(parent_idx)]\n",
    "        rad_bow = topic_rad_bow[parent_idx]\n",
    "        prob_topic = topic_prob_topic[parent_idx]\n",
    "        print(parent_idx, 'R: %.3f' % rad_bow, 'P: %.3f' % prob_topic, ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "    \n",
    "    child_idxs = tree_idxs[parent_idx]\n",
    "    depth += 1\n",
    "    for child_idx in child_idxs:\n",
    "        topic_freq_bow_idxs = topics_freq_bow_idxs[model.topic_idxs.index(child_idx)]\n",
    "        rad_bow = topic_rad_bow[child_idx]\n",
    "        prob_topic = topic_prob_topic[child_idx]\n",
    "        print('  '*depth, child_idx, 'R: %.2f' % rad_bow, 'P: %.3f' % prob_topic, ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        \n",
    "        if child_idx in tree_idxs: print_topic_sample(tree_idxs, model=model, topic_rad_bow=topic_rad_bow, topic_prob_topic=topic_prob_topic, parent_idx=child_idx, topics_freq_bow_idxs=topics_freq_bow_idxs, depth=depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topic_specialization(sess, model):\n",
    "    topics_bow = sess.run(model.topic_bow)\n",
    "    norm_bow = np.sum([instance.bow for instance in instances_test], 0)\n",
    "    topics_vec = topics_bow / np.linalg.norm(topics_bow, axis=1, keepdims=True)\n",
    "    norm_vec = norm_bow / np.linalg.norm(norm_bow)\n",
    "\n",
    "    topics_spec = 1 - topics_vec.dot(norm_vec)\n",
    "\n",
    "    depth_topic_idxs = defaultdict(list)\n",
    "    for topic_idx, depth in model.tree_depth.items():\n",
    "        depth_topic_idxs[depth].append(topic_idx)\n",
    "\n",
    "    for depth, topic_idxs in depth_topic_idxs.items():\n",
    "        topic_indices = np.array([model.topic_idxs.index(topic_idx) for topic_idx in topic_idxs])\n",
    "        depth_spec = np.mean(topics_spec[topic_indices])\n",
    "        print(depth, depth_spec)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_hierarchical_affinity(sess, model):\n",
    "    def get_cos_sim(parent_to_child_idxs):\n",
    "        parent_child_bows = {parent_idx: np.concatenate([normed_tree_topic_bow[child_idx] for child_idx in child_idxs], 0) for parent_idx, child_idxs in parent_to_child_idxs.items()}\n",
    "        cos_sim = np.mean([np.mean(normed_tree_topic_bow[parent_idx].dot(child_bows.T)) for parent_idx, child_bows in parent_child_bows.items()])\n",
    "        return cos_sim    \n",
    "    \n",
    "    tree_topic_bow = sess.run(model.tree_topic_bow)\n",
    "    normed_tree_topic_bow = {topic_idx: topic_bow/np.linalg.norm(topic_bow) for topic_idx, topic_bow in tree_topic_bow.items()}\n",
    "\n",
    "    third_child_idxs = [child_idx for child_idx, depth in model.tree_depth.items() if depth==3]\n",
    "    second_parent_to_child_idxs = {parent_idx:child_idxs for parent_idx, child_idxs in model.tree_idxs.items() if model.tree_depth[parent_idx] == 2}\n",
    "    second_parent_to_unchild_idxs = {parent_idx: [child_idx for child_idx in third_child_idxs if child_idx not in child_idxs] for parent_idx, child_idxs in second_parent_to_child_idxs.items()}\n",
    "    \n",
    "    child_cos_sim = get_cos_sim(second_parent_to_child_idxs)\n",
    "    unchild_cos_sim = get_cos_sim(second_parent_to_unchild_idxs)\n",
    "    print('child %.3f, not-child: %.3f' % (child_cos_sim, unchild_cos_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recur_topic_idxs(model, parent_idx, recur_topic_idxs = None):\n",
    "    if recur_topic_idxs is None: recur_topic_idxs = [parent_idx]\n",
    "\n",
    "    if parent_idx in model.tree_idxs:\n",
    "        child_idxs = model.tree_idxs[parent_idx]\n",
    "        recur_topic_idxs += child_idxs\n",
    "        for child_idx in child_idxs:\n",
    "            if child_idx in model.tree_idxs: get_recur_topic_idxs(model, child_idx, recur_topic_idxs)\n",
    "    return recur_topic_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def update_tree(recur_prob_topic, topic_prob_topic, model, add_threshold=0.3, remove_threshold=0.1):    \n",
    "    assert len(model.topic_idxs) == len(recur_prob_topic) == len(topic_prob_topic)\n",
    "    update_tree_flg = False\n",
    "    \n",
    "    def add_topic(topic_idx, tree_idxs):\n",
    "        if topic_idx in tree_idxs:\n",
    "            child_idx = min([10*topic_idx+i for i in range(1, 10) if 10*topic_idx+i not in tree_idxs[topic_idx]])\n",
    "            tree_idxs[topic_idx].append(child_idx)        \n",
    "        else:\n",
    "            child_idx = 10*topic_idx+1\n",
    "            tree_idxs[topic_idx] = [10*topic_idx+1]\n",
    "        return tree_idxs, child_idx\n",
    "    \n",
    "    added_tree_idxs = copy.deepcopy(model.tree_idxs)\n",
    "    for parent_idx, child_idxs in model.tree_idxs.items():\n",
    "#         rad_bow = topic_rad_bow[parent_idx]\n",
    "        rad_bow = topic_prob_topic[parent_idx]\n",
    "        if rad_bow > add_threshold:\n",
    "            update_tree_flg = True\n",
    "            for depth in range(model.tree_depth[parent_idx], model.n_depth):\n",
    "                added_tree_idxs, parent_idx = add_topic(parent_idx, added_tree_idxs)\n",
    "    \n",
    "    def remove_topic(parent_idx, child_idx, tree_idxs):\n",
    "        if parent_idx in tree_idxs:\n",
    "            tree_idxs[parent_idx].remove(child_idx)\n",
    "            if child_idx in tree_idxs:\n",
    "                tree_idxs.pop(child_idx)    \n",
    "        return tree_idxs\n",
    "    \n",
    "    removed_tree_idxs = copy.deepcopy(added_tree_idxs)\n",
    "    for parent_idx, child_idxs in model.tree_idxs.items():\n",
    "#         probs_child = np.array([topic_prob_topic[child_idx] for child_idx in child_idxs])\n",
    "        probs_child = np.array([recur_prob_topic[child_idx] for child_idx in child_idxs])\n",
    "#         prob_child = np.min(probs_child)\n",
    "#         child_idx = child_idxs[np.argmin(probs_child)]\n",
    "        for prob_child, child_idx in zip(probs_child, child_idxs):\n",
    "            if prob_child < remove_threshold:\n",
    "                update_tree_flg = True\n",
    "                removed_tree_idxs = remove_topic(parent_idx, child_idx, removed_tree_idxs)\n",
    "                if parent_idx in removed_tree_idxs:\n",
    "                    if len(removed_tree_idxs[parent_idx]) == 0:\n",
    "                        ancestor_idx = model.child_to_parent_idxs[parent_idx]\n",
    "                        removed_tree_idxs = remove_topic(ancestor_idx, parent_idx, removed_tree_idxs)\n",
    "    return removed_tree_idxs, update_tree_flg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','TM','','','','VALID:','TM','','',''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL','REG','LOSS','PPL','NLL','KL','REG']]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree_idxs = {0:[1, 2], \n",
    "#           1:[10, 11], 2:[20, 21]}\n",
    "\n",
    "tree_idxs = {0:[1, 2, 3], \n",
    "              1:[11, 12], 2:[21, 22], 3:[31, 32]}\n",
    "\n",
    "# tree_idxs = {0:[1, 2, 3], \n",
    "#               1:[10, 11, 12], 2:[20, 21, 22], 3:[30, 31, 32]}\n",
    "\n",
    "\n",
    "if 'sess' in globals(): sess.close()\n",
    "model = Model(config, tree_idxs)\n",
    "sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "name_variables = {tensor.name: variable for tensor, variable in zip(tf.global_variables(), sess.run(tf.global_variables()))}\n",
    "saver = tf.train.Saver(max_to_keep=10)\n",
    "update_tree_flg = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>VALID:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>49</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>111.50</td>\n",
       "      <td>493</td>\n",
       "      <td>110.52</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.98</td>\n",
       "      <td>465</td>\n",
       "      <td>102.77</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>53</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>111.09</td>\n",
       "      <td>474</td>\n",
       "      <td>109.84</td>\n",
       "      <td>1.24</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.86</td>\n",
       "      <td>452</td>\n",
       "      <td>102.38</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>49</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>110.86</td>\n",
       "      <td>463</td>\n",
       "      <td>109.44</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.53</td>\n",
       "      <td>438</td>\n",
       "      <td>101.93</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>54</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>110.71</td>\n",
       "      <td>456</td>\n",
       "      <td>109.18</td>\n",
       "      <td>1.53</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.62</td>\n",
       "      <td>440</td>\n",
       "      <td>101.89</td>\n",
       "      <td>1.72</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>54</td>\n",
       "      <td>50</td>\n",
       "      <td>49</td>\n",
       "      <td>110.60</td>\n",
       "      <td>450</td>\n",
       "      <td>108.97</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.44</td>\n",
       "      <td>430</td>\n",
       "      <td>101.60</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>57</td>\n",
       "      <td>60</td>\n",
       "      <td>59</td>\n",
       "      <td>110.51</td>\n",
       "      <td>446</td>\n",
       "      <td>108.81</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.45</td>\n",
       "      <td>427</td>\n",
       "      <td>101.54</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35000</th>\n",
       "      <td>56</td>\n",
       "      <td>70</td>\n",
       "      <td>69</td>\n",
       "      <td>110.43</td>\n",
       "      <td>442</td>\n",
       "      <td>108.67</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.44</td>\n",
       "      <td>426</td>\n",
       "      <td>101.46</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40000</th>\n",
       "      <td>64</td>\n",
       "      <td>80</td>\n",
       "      <td>79</td>\n",
       "      <td>110.36</td>\n",
       "      <td>439</td>\n",
       "      <td>108.55</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.53</td>\n",
       "      <td>424</td>\n",
       "      <td>101.50</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45000</th>\n",
       "      <td>59</td>\n",
       "      <td>90</td>\n",
       "      <td>89</td>\n",
       "      <td>110.31</td>\n",
       "      <td>436</td>\n",
       "      <td>108.46</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.30</td>\n",
       "      <td>422</td>\n",
       "      <td>101.32</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000</th>\n",
       "      <td>81</td>\n",
       "      <td>100</td>\n",
       "      <td>99</td>\n",
       "      <td>110.26</td>\n",
       "      <td>434</td>\n",
       "      <td>108.37</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.27</td>\n",
       "      <td>415</td>\n",
       "      <td>101.17</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55000</th>\n",
       "      <td>60</td>\n",
       "      <td>110</td>\n",
       "      <td>109</td>\n",
       "      <td>110.22</td>\n",
       "      <td>432</td>\n",
       "      <td>108.29</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.14</td>\n",
       "      <td>414</td>\n",
       "      <td>101.06</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60000</th>\n",
       "      <td>59</td>\n",
       "      <td>120</td>\n",
       "      <td>119</td>\n",
       "      <td>110.18</td>\n",
       "      <td>430</td>\n",
       "      <td>108.22</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.14</td>\n",
       "      <td>412</td>\n",
       "      <td>100.99</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65000</th>\n",
       "      <td>53</td>\n",
       "      <td>130</td>\n",
       "      <td>129</td>\n",
       "      <td>110.15</td>\n",
       "      <td>428</td>\n",
       "      <td>108.17</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.27</td>\n",
       "      <td>414</td>\n",
       "      <td>101.17</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70000</th>\n",
       "      <td>66</td>\n",
       "      <td>140</td>\n",
       "      <td>139</td>\n",
       "      <td>110.12</td>\n",
       "      <td>427</td>\n",
       "      <td>108.11</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.07</td>\n",
       "      <td>412</td>\n",
       "      <td>100.90</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75000</th>\n",
       "      <td>55</td>\n",
       "      <td>150</td>\n",
       "      <td>149</td>\n",
       "      <td>110.09</td>\n",
       "      <td>426</td>\n",
       "      <td>108.07</td>\n",
       "      <td>2.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.12</td>\n",
       "      <td>413</td>\n",
       "      <td>101.00</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80000</th>\n",
       "      <td>69</td>\n",
       "      <td>160</td>\n",
       "      <td>159</td>\n",
       "      <td>110.07</td>\n",
       "      <td>425</td>\n",
       "      <td>108.03</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.10</td>\n",
       "      <td>411</td>\n",
       "      <td>100.95</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85000</th>\n",
       "      <td>67</td>\n",
       "      <td>170</td>\n",
       "      <td>169</td>\n",
       "      <td>110.05</td>\n",
       "      <td>424</td>\n",
       "      <td>107.99</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>102.85</td>\n",
       "      <td>403</td>\n",
       "      <td>100.73</td>\n",
       "      <td>2.12</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90000</th>\n",
       "      <td>63</td>\n",
       "      <td>180</td>\n",
       "      <td>179</td>\n",
       "      <td>110.04</td>\n",
       "      <td>423</td>\n",
       "      <td>107.96</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.24</td>\n",
       "      <td>414</td>\n",
       "      <td>101.10</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95000</th>\n",
       "      <td>71</td>\n",
       "      <td>190</td>\n",
       "      <td>189</td>\n",
       "      <td>110.02</td>\n",
       "      <td>422</td>\n",
       "      <td>107.93</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.00</td>\n",
       "      <td>408</td>\n",
       "      <td>100.78</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>48</td>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>110.02</td>\n",
       "      <td>422</td>\n",
       "      <td>107.92</td>\n",
       "      <td>2.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.29</td>\n",
       "      <td>416</td>\n",
       "      <td>101.20</td>\n",
       "      <td>2.09</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105000</th>\n",
       "      <td>58</td>\n",
       "      <td>210</td>\n",
       "      <td>209</td>\n",
       "      <td>110.01</td>\n",
       "      <td>421</td>\n",
       "      <td>107.90</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>102.95</td>\n",
       "      <td>406</td>\n",
       "      <td>100.79</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110000</th>\n",
       "      <td>57</td>\n",
       "      <td>220</td>\n",
       "      <td>219</td>\n",
       "      <td>109.99</td>\n",
       "      <td>421</td>\n",
       "      <td>107.88</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.15</td>\n",
       "      <td>411</td>\n",
       "      <td>100.98</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115000</th>\n",
       "      <td>65</td>\n",
       "      <td>230</td>\n",
       "      <td>229</td>\n",
       "      <td>109.98</td>\n",
       "      <td>420</td>\n",
       "      <td>107.86</td>\n",
       "      <td>2.12</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.12</td>\n",
       "      <td>410</td>\n",
       "      <td>100.90</td>\n",
       "      <td>2.22</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120000</th>\n",
       "      <td>60</td>\n",
       "      <td>240</td>\n",
       "      <td>239</td>\n",
       "      <td>109.97</td>\n",
       "      <td>419</td>\n",
       "      <td>107.83</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.09</td>\n",
       "      <td>410</td>\n",
       "      <td>100.88</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125000</th>\n",
       "      <td>62</td>\n",
       "      <td>250</td>\n",
       "      <td>249</td>\n",
       "      <td>109.96</td>\n",
       "      <td>419</td>\n",
       "      <td>107.81</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.00</td>\n",
       "      <td>405</td>\n",
       "      <td>100.80</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130000</th>\n",
       "      <td>61</td>\n",
       "      <td>260</td>\n",
       "      <td>259</td>\n",
       "      <td>109.95</td>\n",
       "      <td>418</td>\n",
       "      <td>107.80</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.10</td>\n",
       "      <td>409</td>\n",
       "      <td>100.88</td>\n",
       "      <td>2.22</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135000</th>\n",
       "      <td>57</td>\n",
       "      <td>270</td>\n",
       "      <td>269</td>\n",
       "      <td>109.94</td>\n",
       "      <td>418</td>\n",
       "      <td>107.78</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.03</td>\n",
       "      <td>406</td>\n",
       "      <td>100.83</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140000</th>\n",
       "      <td>60</td>\n",
       "      <td>280</td>\n",
       "      <td>279</td>\n",
       "      <td>109.93</td>\n",
       "      <td>418</td>\n",
       "      <td>107.76</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.09</td>\n",
       "      <td>410</td>\n",
       "      <td>100.92</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145000</th>\n",
       "      <td>61</td>\n",
       "      <td>290</td>\n",
       "      <td>289</td>\n",
       "      <td>109.92</td>\n",
       "      <td>417</td>\n",
       "      <td>107.75</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.16</td>\n",
       "      <td>409</td>\n",
       "      <td>100.94</td>\n",
       "      <td>2.22</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150000</th>\n",
       "      <td>67</td>\n",
       "      <td>300</td>\n",
       "      <td>299</td>\n",
       "      <td>109.91</td>\n",
       "      <td>417</td>\n",
       "      <td>107.73</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.17</td>\n",
       "      <td>413</td>\n",
       "      <td>100.94</td>\n",
       "      <td>2.23</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155000</th>\n",
       "      <td>54</td>\n",
       "      <td>310</td>\n",
       "      <td>309</td>\n",
       "      <td>109.90</td>\n",
       "      <td>416</td>\n",
       "      <td>107.72</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.07</td>\n",
       "      <td>410</td>\n",
       "      <td>100.92</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160000</th>\n",
       "      <td>63</td>\n",
       "      <td>320</td>\n",
       "      <td>319</td>\n",
       "      <td>109.90</td>\n",
       "      <td>416</td>\n",
       "      <td>107.70</td>\n",
       "      <td>2.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>102.99</td>\n",
       "      <td>406</td>\n",
       "      <td>100.76</td>\n",
       "      <td>2.23</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       TRAIN:   TM                      VALID:   TM          \\\n",
       "       Time   Ep   Ct    LOSS  PPL     NLL    KL   REG    LOSS  PPL     NLL   \n",
       "5000     49   10    9  111.50  493  110.52  0.98  0.01  103.98  465  102.77   \n",
       "10000    53   20   19  111.09  474  109.84  1.24  0.01  103.86  452  102.38   \n",
       "15000    49   30   29  110.86  463  109.44  1.41  0.01  103.53  438  101.93   \n",
       "20000    54   40   39  110.71  456  109.18  1.53  0.01  103.62  440  101.89   \n",
       "25000    54   50   49  110.60  450  108.97  1.61  0.01  103.44  430  101.60   \n",
       "30000    57   60   59  110.51  446  108.81  1.69  0.01  103.45  427  101.54   \n",
       "35000    56   70   69  110.43  442  108.67  1.75  0.01  103.44  426  101.46   \n",
       "40000    64   80   79  110.36  439  108.55  1.80  0.01  103.53  424  101.50   \n",
       "45000    59   90   89  110.31  436  108.46  1.84  0.01  103.30  422  101.32   \n",
       "50000    81  100   99  110.26  434  108.37  1.88  0.01  103.27  415  101.17   \n",
       "55000    60  110  109  110.22  432  108.29  1.92  0.01  103.14  414  101.06   \n",
       "60000    59  120  119  110.18  430  108.22  1.95  0.01  103.14  412  100.99   \n",
       "65000    53  130  129  110.15  428  108.17  1.97  0.01  103.27  414  101.17   \n",
       "70000    66  140  139  110.12  427  108.11  2.00  0.01  103.07  412  100.90   \n",
       "75000    55  150  149  110.09  426  108.07  2.02  0.01  103.12  413  101.00   \n",
       "80000    69  160  159  110.07  425  108.03  2.03  0.01  103.10  411  100.95   \n",
       "85000    67  170  169  110.05  424  107.99  2.05  0.01  102.85  403  100.73   \n",
       "90000    63  180  179  110.04  423  107.96  2.07  0.01  103.24  414  101.10   \n",
       "95000    71  190  189  110.02  422  107.93  2.08  0.01  103.00  408  100.78   \n",
       "100000   48  200  199  110.02  422  107.92  2.09  0.01  103.29  416  101.20   \n",
       "105000   58  210  209  110.01  421  107.90  2.10  0.01  102.95  406  100.79   \n",
       "110000   57  220  219  109.99  421  107.88  2.11  0.01  103.15  411  100.98   \n",
       "115000   65  230  229  109.98  420  107.86  2.12  0.01  103.12  410  100.90   \n",
       "120000   60  240  239  109.97  419  107.83  2.13  0.01  103.09  410  100.88   \n",
       "125000   62  250  249  109.96  419  107.81  2.14  0.01  103.00  405  100.80   \n",
       "130000   61  260  259  109.95  418  107.80  2.15  0.00  103.10  409  100.88   \n",
       "135000   57  270  269  109.94  418  107.78  2.16  0.00  103.03  406  100.83   \n",
       "140000   60  280  279  109.93  418  107.76  2.16  0.00  103.09  410  100.92   \n",
       "145000   61  290  289  109.92  417  107.75  2.17  0.00  103.16  409  100.94   \n",
       "150000   67  300  299  109.91  417  107.73  2.18  0.00  103.17  413  100.94   \n",
       "155000   54  310  309  109.90  416  107.72  2.18  0.00  103.07  410  100.92   \n",
       "160000   63  320  319  109.90  416  107.70  2.19  0.00  102.99  406  100.76   \n",
       "\n",
       "                    \n",
       "          KL   REG  \n",
       "5000    1.20  0.00  \n",
       "10000   1.47  0.00  \n",
       "15000   1.60  0.00  \n",
       "20000   1.72  0.00  \n",
       "25000   1.84  0.01  \n",
       "30000   1.91  0.00  \n",
       "35000   1.98  0.00  \n",
       "40000   2.03  0.00  \n",
       "45000   1.98  0.00  \n",
       "50000   2.10  0.00  \n",
       "55000   2.08  0.00  \n",
       "60000   2.14  0.00  \n",
       "65000   2.10  0.00  \n",
       "70000   2.17  0.00  \n",
       "75000   2.13  0.00  \n",
       "80000   2.15  0.00  \n",
       "85000   2.12  0.00  \n",
       "90000   2.14  0.00  \n",
       "95000   2.21  0.00  \n",
       "100000  2.09  0.00  \n",
       "105000  2.16  0.00  \n",
       "110000  2.17  0.00  \n",
       "115000  2.22  0.00  \n",
       "120000  2.20  0.00  \n",
       "125000  2.20  0.00  \n",
       "130000  2.22  0.00  \n",
       "135000  2.20  0.00  \n",
       "140000  2.17  0.00  \n",
       "145000  2.22  0.00  \n",
       "150000  2.23  0.00  \n",
       "155000  2.15  0.00  \n",
       "160000  2.23  0.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 R: 1.000 P: 0.421 carry pockets room pocket small back nice perfect hold work\n",
      "   1 R: 0.21 P: 0.089 - inch : zipper sleeve quality nice price made bit\n",
      "     13 R: 0.04 P: 0.037 ; & pro tablet surface air hard protection drive inside\n",
      "     11 R: 0.09 P: 0.085 sleeve protection air protect inside pro perfectly soft ipad snug\n",
      "   2 R: 0.15 P: 0.060 price bought quality ... $ amazon purchase purchased buy !\n",
      "     22 R: 0.05 P: 0.054 color love ! perfectly mac pink picture perfect pro blue\n",
      "     21 R: 0.03 P: 0.033 ! love perfect awesome ... recommend absolutely buy compliments highly\n",
      "   3 R: 0.09 P: 0.039 months broke year weeks started month bought years zipper time\n",
      "     31 R: 0.05 P: 0.054 bottom top plastic corners piece cracked part rubber feet cover\n",
      "   5 R: 0.06 P: 0.021 smell bad days cheap air pay return reviews 'm item\n",
      "     51 R: 0.04 P: 0.039 cover keyboard pro mac hard retina scratches air color nice\n",
      "   4 R: 0.07 P: 0.027 item return received money seller amazon customer shipping ordered quality\n",
      "     41 R: 0.04 P: 0.040 cover keyboard protector screen color hard keys pro nice mac\n",
      "1 0.30214816\n",
      "2 0.6104678\n",
      "3 0.6209165\n",
      "child 0.180, not-child: 0.099\n",
      "{0: [1, 2, 3, 6], 1: [11, 12], 2: [22, 23], 3: [31], 6: [61]}\n"
     ]
    }
   ],
   "source": [
    "if len(log_df) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "while epoch < config.epochs:\n",
    "    # train\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = model.get_feed_dict(batch)\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, ppls_batch, global_step_log = \\\n",
    "        sess.run([model.opt, model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_reg, model.topic_ppls, tf.train.get_global_step()], feed_dict = feed_dict)\n",
    "\n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        # validate\n",
    "#         if global_step_log % config.log_period == 0:\n",
    "        if global_step_log % 5000 == 0:            \n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, rads_bow_dev, probs_topic_dev = get_loss(sess, dev_batches, model)\n",
    "\n",
    "            # log\n",
    "            clear_output()\n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_reg_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_reg_dev],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "\n",
    "            # visualize topic\n",
    "#             topic_rad_bow = {topic_idx: rad_bow for topic_idx, rad_bow in zip(model.topic_idxs, rads_bow_dev)}\n",
    "            topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "            \n",
    "            recur_topic_idxs = {parent_idx: get_recur_topic_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "            recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in recur_topic_idxs.items()}\n",
    "            \n",
    "            print_topic_sample(tree_idxs, sess, model, topic_prob_topic=topic_prob_topic, topic_rad_bow=recur_prob_topic)\n",
    "            print_topic_specialization(sess, model)\n",
    "            print_hierarchical_affinity(sess, model)\n",
    "            time_start = time.time()\n",
    "\n",
    "            # update tree\n",
    "#             tree_idxs, update_tree_flg = update_tree(topic_rad_bow, topic_prob_topic, model, add_threshold=0.2, remove_threshold=0.05)\n",
    "            tree_idxs, update_tree_flg = update_tree(recur_prob_topic, topic_prob_topic, model, add_threshold=0.05, remove_threshold=0.05)\n",
    "            if update_tree_flg:\n",
    "                print(tree_idxs)\n",
    "                name_variables = {tensor.name: variable for tensor, variable in zip(tf.global_variables(), sess.run(tf.global_variables()))} # store paremeters\n",
    "                if 'sess' in globals(): sess.close()\n",
    "                model = Model(config, tree_idxs)\n",
    "                sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "                name_tensors = {tensor.name: tensor for tensor in tf.global_variables()}\n",
    "                sess.run([name_tensors[name].assign(variable) for name, variable in name_variables.items()]) # restore parameters\n",
    "\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "    epoch += 1\n",
    "\n",
    "loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, rads_bow_dev, probs_topic_dev = get_loss(sess, dev_batches, model)\n",
    "topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "\n",
    "recur_topic_idxs = {parent_idx: get_recur_topic_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in recur_topic_idxs.items()}\n",
    "display(log_df)\n",
    "print_topic_sample(tree_idxs, sess, model, topic_prob_topic=topic_prob_topic, topic_rad_bow=recur_prob_topic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
