{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from data_structure import get_batches\n",
    "from hntm import HierarchicalNeuralTopicModel\n",
    "from nhdp import nestedHierarchicalNeuralTopicModel\n",
    "from tsgntm import TreeStructuredGaussianNeuralTopicModel\n",
    "from tree import get_descendant_idxs\n",
    "from evaluation import get_hierarchical_affinity, get_topic_specialization, print_topic_sample, print_topic_year\n",
    "from configure import get_config\n",
    "\n",
    "pd.set_option('display.max_columns', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(nb_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu\n",
    "np.random.seed(config.seed)\n",
    "random.seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.path_data,'rb'))\n",
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)\n",
    "config.dim_bow = len(bow_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     6
    ]
   },
   "outputs": [],
   "source": [
    "def debug(variable, sample_batch=None):\n",
    "    if sample_batch is None: sample_batch = test_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch, mode='test')\n",
    "    _variable = sess.run(variable, feed_dict=feed_dict)\n",
    "    return _variable\n",
    "\n",
    "def check(variable):\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sample_batch = test_batches[0]\n",
    "    feed_dict = model.get_feed_dict(sample_batch, mode='test', assertion=True)\n",
    "    _variable = sess.run(variable, feed_dict=feed_dict)\n",
    "    return _variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "TopicModels = {'hntm': HierarchicalNeuralTopicModel, 'nhdp': nestedHierarchicalNeuralTopicModel, 'tsgntm': TreeStructuredGaussianNeuralTopicModel}\n",
    "TopicModel = TopicModels[config.model]\n",
    "model = TopicModel(config)\n",
    "sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(max_to_keep=config.max_to_keep)\n",
    "update_tree_flg = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     24
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = []\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "ppl_min = np.inf\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','','','','','','VALID:','','','','','','TEST:','', 'SPEC:', '', '', 'HIER:', ''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL', 'GAUSS', 'REG','LOSS','PPL','NLL','KL', 'GAUSS','REG','LOSS','PPL', '1', '2', '3', 'CHILD', 'OTHER']]))))\n",
    "\n",
    "cmd_rm = 'rm -r %s' % config.dir_model\n",
    "res = subprocess.call(cmd_rm.split())\n",
    "cmd_mk = 'mkdir %s' % config.dir_model\n",
    "res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "def update_checkpoint(config, checkpoint, global_step):\n",
    "    checkpoint.append(config.path_model + '-%i' % global_step)\n",
    "    if len(checkpoint) > config.max_to_keep:\n",
    "        path_model = checkpoint.pop(0) + '.*'\n",
    "        for p in glob.glob(path_model):\n",
    "            os.remove(p)\n",
    "    cPickle.dump(checkpoint, open(config.path_checkpoint, 'wb'))\n",
    "    \n",
    "def validate(sess, batches, model):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    prob_topic_list = []\n",
    "    n_bow_list = []\n",
    "    n_topics_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = model.get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch, ppls_batch, prob_topic_batch, n_bow_batch, n_topics_batch \\\n",
    "            = sess.run([model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_gauss, model.topic_loss_reg, model.topic_ppls, model.prob_topic, model.n_bow, model.n_topics], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "        prob_topic_list.append(prob_topic_batch)\n",
    "        n_bow_list.append(n_bow_batch)\n",
    "        n_topics_list.append(n_topics_batch)\n",
    "    loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_gauss_mean, topic_loss_reg_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    \n",
    "    probs_topic = np.concatenate(prob_topic_list, 0)\n",
    "    \n",
    "    n_bow = np.concatenate(n_bow_list, 0)\n",
    "    n_topics = np.concatenate(n_topics_list, 0)\n",
    "    probs_topic_mean = np.sum(n_topics, 0) / np.sum(n_bow)\n",
    "    \n",
    "    return loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_gauss_mean, topic_loss_reg_mean, ppl_mean, probs_topic_mean    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train & validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th colspan=\"5\" halign=\"left\"></th>\n",
       "      <th>VALID:</th>\n",
       "      <th colspan=\"5\" halign=\"left\"></th>\n",
       "      <th>TEST:</th>\n",
       "      <th></th>\n",
       "      <th>SPEC:</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "      <th>HIER:</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>GAUSS</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>GAUSS</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>CHILD</th>\n",
       "      <th>OTHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>78</td>\n",
       "      <td>8</td>\n",
       "      <td>463</td>\n",
       "      <td>9281.17</td>\n",
       "      <td>2002</td>\n",
       "      <td>9240.27</td>\n",
       "      <td>15.17</td>\n",
       "      <td>25.57</td>\n",
       "      <td>0.18</td>\n",
       "      <td>9255.12</td>\n",
       "      <td>1889</td>\n",
       "      <td>9201.12</td>\n",
       "      <td>18.17</td>\n",
       "      <td>35.71</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9255.25</td>\n",
       "      <td>1888</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>76</td>\n",
       "      <td>17</td>\n",
       "      <td>360</td>\n",
       "      <td>9240.09</td>\n",
       "      <td>1922</td>\n",
       "      <td>9189.30</td>\n",
       "      <td>17.02</td>\n",
       "      <td>33.65</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9233.99</td>\n",
       "      <td>1847</td>\n",
       "      <td>9172.31</td>\n",
       "      <td>18.82</td>\n",
       "      <td>42.76</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9234.57</td>\n",
       "      <td>1846</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>74</td>\n",
       "      <td>26</td>\n",
       "      <td>257</td>\n",
       "      <td>9219.42</td>\n",
       "      <td>1885</td>\n",
       "      <td>9164.55</td>\n",
       "      <td>17.72</td>\n",
       "      <td>37.03</td>\n",
       "      <td>0.13</td>\n",
       "      <td>9220.10</td>\n",
       "      <td>1822</td>\n",
       "      <td>9156.47</td>\n",
       "      <td>18.95</td>\n",
       "      <td>44.59</td>\n",
       "      <td>0.08</td>\n",
       "      <td>9223.07</td>\n",
       "      <td>1826</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>76</td>\n",
       "      <td>35</td>\n",
       "      <td>154</td>\n",
       "      <td>9206.75</td>\n",
       "      <td>1862</td>\n",
       "      <td>9149.28</td>\n",
       "      <td>18.08</td>\n",
       "      <td>39.28</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9218.93</td>\n",
       "      <td>1816</td>\n",
       "      <td>9152.78</td>\n",
       "      <td>19.17</td>\n",
       "      <td>46.91</td>\n",
       "      <td>0.08</td>\n",
       "      <td>9217.94</td>\n",
       "      <td>1815</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>76</td>\n",
       "      <td>44</td>\n",
       "      <td>51</td>\n",
       "      <td>9197.84</td>\n",
       "      <td>1846</td>\n",
       "      <td>9138.52</td>\n",
       "      <td>18.30</td>\n",
       "      <td>40.89</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9216.13</td>\n",
       "      <td>1811</td>\n",
       "      <td>9149.92</td>\n",
       "      <td>19.07</td>\n",
       "      <td>47.05</td>\n",
       "      <td>0.08</td>\n",
       "      <td>9218.68</td>\n",
       "      <td>1816</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>75</td>\n",
       "      <td>52</td>\n",
       "      <td>515</td>\n",
       "      <td>9191.12</td>\n",
       "      <td>1834</td>\n",
       "      <td>9130.47</td>\n",
       "      <td>18.47</td>\n",
       "      <td>42.02</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9210.39</td>\n",
       "      <td>1803</td>\n",
       "      <td>9143.90</td>\n",
       "      <td>19.32</td>\n",
       "      <td>47.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9210.60</td>\n",
       "      <td>1803</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35000</th>\n",
       "      <td>75</td>\n",
       "      <td>61</td>\n",
       "      <td>412</td>\n",
       "      <td>9185.60</td>\n",
       "      <td>1825</td>\n",
       "      <td>9124.13</td>\n",
       "      <td>18.61</td>\n",
       "      <td>42.71</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9205.57</td>\n",
       "      <td>1796</td>\n",
       "      <td>9138.98</td>\n",
       "      <td>19.43</td>\n",
       "      <td>47.06</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9204.35</td>\n",
       "      <td>1795</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40000</th>\n",
       "      <td>74</td>\n",
       "      <td>70</td>\n",
       "      <td>309</td>\n",
       "      <td>9180.90</td>\n",
       "      <td>1816</td>\n",
       "      <td>9118.74</td>\n",
       "      <td>18.73</td>\n",
       "      <td>43.30</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9205.43</td>\n",
       "      <td>1796</td>\n",
       "      <td>9138.54</td>\n",
       "      <td>19.45</td>\n",
       "      <td>47.35</td>\n",
       "      <td>0.08</td>\n",
       "      <td>9203.39</td>\n",
       "      <td>1793</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45000</th>\n",
       "      <td>76</td>\n",
       "      <td>79</td>\n",
       "      <td>206</td>\n",
       "      <td>9176.77</td>\n",
       "      <td>1809</td>\n",
       "      <td>9113.97</td>\n",
       "      <td>18.83</td>\n",
       "      <td>43.84</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9201.43</td>\n",
       "      <td>1789</td>\n",
       "      <td>9132.81</td>\n",
       "      <td>19.71</td>\n",
       "      <td>48.83</td>\n",
       "      <td>0.08</td>\n",
       "      <td>9199.73</td>\n",
       "      <td>1786</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000</th>\n",
       "      <td>75</td>\n",
       "      <td>88</td>\n",
       "      <td>103</td>\n",
       "      <td>9173.19</td>\n",
       "      <td>1803</td>\n",
       "      <td>9109.81</td>\n",
       "      <td>18.92</td>\n",
       "      <td>44.33</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9200.83</td>\n",
       "      <td>1788</td>\n",
       "      <td>9131.90</td>\n",
       "      <td>19.69</td>\n",
       "      <td>49.16</td>\n",
       "      <td>0.08</td>\n",
       "      <td>9199.82</td>\n",
       "      <td>1785</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55000</th>\n",
       "      <td>75</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>9170.00</td>\n",
       "      <td>1798</td>\n",
       "      <td>9106.08</td>\n",
       "      <td>19.00</td>\n",
       "      <td>44.77</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9201.39</td>\n",
       "      <td>1788</td>\n",
       "      <td>9132.58</td>\n",
       "      <td>19.79</td>\n",
       "      <td>48.93</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9199.82</td>\n",
       "      <td>1785</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      TRAIN:                                      VALID:  \\\n",
       "      Time  Ep   Ct     LOSS   PPL      NLL     KL  GAUSS   REG     LOSS   \n",
       "5000    78   8  463  9281.17  2002  9240.27  15.17  25.57  0.18  9255.12   \n",
       "10000   76  17  360  9240.09  1922  9189.30  17.02  33.65  0.14  9233.99   \n",
       "15000   74  26  257  9219.42  1885  9164.55  17.72  37.03  0.13  9220.10   \n",
       "20000   76  35  154  9206.75  1862  9149.28  18.08  39.28  0.12  9218.93   \n",
       "25000   76  44   51  9197.84  1846  9138.52  18.30  40.89  0.11  9216.13   \n",
       "30000   75  52  515  9191.12  1834  9130.47  18.47  42.02  0.11  9210.39   \n",
       "35000   75  61  412  9185.60  1825  9124.13  18.61  42.71  0.10  9205.57   \n",
       "40000   74  70  309  9180.90  1816  9118.74  18.73  43.30  0.10  9205.43   \n",
       "45000   76  79  206  9176.77  1809  9113.97  18.83  43.84  0.10  9201.43   \n",
       "50000   75  88  103  9173.19  1803  9109.81  18.92  44.33  0.10  9200.83   \n",
       "55000   75  97    0  9170.00  1798  9106.08  19.00  44.77  0.10  9201.39   \n",
       "\n",
       "                                            TEST:       SPEC:              \\\n",
       "        PPL      NLL     KL  GAUSS   REG     LOSS   PPL     1     2     3   \n",
       "5000   1889  9201.12  18.17  35.71  0.12  9255.25  1888  0.32  0.39  0.34   \n",
       "10000  1847  9172.31  18.82  42.76  0.10  9234.57  1846  0.24  0.42  0.39   \n",
       "15000  1822  9156.47  18.95  44.59  0.08  9223.07  1826  0.17  0.36  0.39   \n",
       "20000  1816  9152.78  19.17  46.91  0.08  9217.94  1815  0.18  0.46  0.41   \n",
       "25000  1811  9149.92  19.07  47.05  0.08  9218.68  1816  0.19  0.47  0.42   \n",
       "30000  1803  9143.90  19.32  47.08  0.09  9210.60  1803  0.21  0.52  0.40   \n",
       "35000  1796  9138.98  19.43  47.06  0.09  9204.35  1795  0.22  0.54  0.41   \n",
       "40000  1796  9138.54  19.45  47.35  0.08  9203.39  1793  0.19  0.50  0.43   \n",
       "45000  1789  9132.81  19.71  48.83  0.08  9199.73  1786  0.24  0.44  0.44   \n",
       "50000  1788  9131.90  19.69  49.16  0.08  9199.82  1785  0.19  0.51  0.43   \n",
       "55000  1788  9132.58  19.79  48.93  0.09  9199.82  1785  0.21  0.51  0.43   \n",
       "\n",
       "      HIER:        \n",
       "      CHILD OTHER  \n",
       "5000   0.49  0.31  \n",
       "10000  0.40  0.24  \n",
       "15000  0.39  0.23  \n",
       "20000  0.42  0.23  \n",
       "25000  0.54  0.32  \n",
       "30000  0.45  0.25  \n",
       "35000  0.43  0.25  \n",
       "40000  0.45  0.27  \n",
       "45000  0.38  0.22  \n",
       "50000  0.38  0.26  \n",
       "55000  0.28  0.18  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 R: 1.000 P: 0.030 sentences features test performance method systems training models speech context\n",
      "   1 R: 0.239 P: 0.043 training method performance test models probability error segmentation errors character\n",
      "     11 R: 0.074 P: 0.074 translation alignment phrase source english translations mt target bleu parallel\n",
      "     12 R: 0.122 P: 0.122 models training learning features performance vector feature state network function\n",
      "   2 R: 0.241 P: 0.063 features classification feature sentiment classifier class negative positive tweets polarity\n",
      "     21 R: 0.072 P: 0.072 document sentences question evaluation documents terms topic method systems questions\n",
      "     22 R: 0.105 P: 0.105 similarity sense entity wordnet semantic relation pairs senses relations entities\n",
      "   3 R: 0.173 P: 0.034 annotation annotators agreement annotator annotations study students participants texts learners\n",
      "     31 R: 0.067 P: 0.067 annotation user project resources tools web tool database research processing\n",
      "     32 R: 0.072 P: 0.072 morphological pos languages tag tags tagging lexicon form forms english\n",
      "   4 R: 0.171 P: 0.029 user dialogue human systems utterance utterances users domain interaction conversation\n",
      "     41 R: 0.067 P: 0.067 knowledge semantic event domain relations events object meaning representation concepts\n",
      "     42 R: 0.075 P: 0.075 verb verbs semantic syntactic argument noun arguments sentences discourse subject\n",
      "   5 R: 0.146 P: 0.017 speech recognition speakers speaker acoustic transcription spoken prosodic boundaries signal\n",
      "     51 R: 0.067 P: 0.067 grammar tree rules algorithm rule node parsing nodes trees input\n",
      "     52 R: 0.062 P: 0.062 parsing dependency parser tree syntactic parse trees features head treebank\n",
      "[41.488     31.92668   21.17234   28.363297  33.057137  43.131264\n",
      "  6.1726832  5.572236   6.026255   4.4230986  9.329318   9.450958\n",
      "  6.7586207  5.111237   9.078522  10.246224 ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3c2a1a9b3f54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_recon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_kl_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_gauss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_reg_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppls_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_log\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_loss_recon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_loss_kl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_loss_gauss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_loss_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_ppls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tsntm/tsgntm.py\u001b[0m in \u001b[0;36mget_feed_dict\u001b[0;34m(self, batch, mode)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbow\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0mkeep_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_prob\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         feed_dict = {\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "while epoch < config.n_epochs:\n",
    "    # train\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = model.get_feed_dict(batch)\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch, ppls_batch, global_step_log = \\\n",
    "        sess.run([model.opt, model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_gauss, model.topic_loss_reg, model.topic_ppls, tf.train.get_global_step()], feed_dict = feed_dict)\n",
    "\n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if global_step_log % config.log_period == 0:\n",
    "            # validate\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_gauss_train, topic_loss_reg_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_gauss_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "\n",
    "            # test\n",
    "            if ppl_dev < ppl_min:\n",
    "                ppl_min = ppl_dev\n",
    "                loss_test, _, _, _, _, ppl_test, _ = validate(sess, test_batches, model)\n",
    "                saver.save(sess, config.path_model, global_step=global_step_log)\n",
    "                cPickle.dump(config, open(config.path_config % global_step_log, 'wb'))\n",
    "                update_checkpoint(config, checkpoint, global_step_log)\n",
    "            \n",
    "            # visualize topic\n",
    "            topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "            topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "            topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "            topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "            descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "            recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "            \n",
    "            depth_specs = get_topic_specialization(sess, model, instances_test)\n",
    "            hierarchical_affinities = get_hierarchical_affinity(sess, model)\n",
    "            \n",
    "            # log\n",
    "            clear_output()\n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_gauss_train, '%.2f'%topic_loss_reg_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_gauss_dev, '%.2f'%topic_loss_reg_dev, \\\n",
    "                    '%.2f'%loss_test, '%.0f'%ppl_test, \\\n",
    "                    '%.2f'%depth_specs[1], '%.2f'%depth_specs[2], '%.2f'%depth_specs[3], \\\n",
    "                    '%.2f'%hierarchical_affinities[0], '%.2f'%hierarchical_affinities[1]],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "            cPickle.dump(log_df, open(os.path.join(config.path_log), 'wb'))\n",
    "            print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)\n",
    "            print(np.sum(debug(model.topic_logvars), 1))\n",
    "            \n",
    "            # update tree\n",
    "            if not config.static:\n",
    "                config.tree_idxs, update_tree_flg = model.update_tree(topic_prob_topic, recur_prob_topic)\n",
    "                if update_tree_flg:\n",
    "                    print(config.tree_idxs)\n",
    "                    name_variables = {tensor.name: variable for tensor, variable in zip(tf.global_variables(), sess.run(tf.global_variables()))} # store paremeters\n",
    "                    if 'sess' in globals(): sess.close()\n",
    "                    model = HierarchicalNeuralTopicModel(config)\n",
    "                    sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "                    name_tensors = {tensor.name: tensor for tensor in tf.global_variables()}\n",
    "                    sess.run([name_tensors[name].assign(variable) for name, variable in name_variables.items()]) # restore parameters\n",
    "                    saver = tf.train.Saver(max_to_keep=1)\n",
    "                \n",
    "            time_start = time.time()\n",
    "\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "    epoch += 1\n",
    "\n",
    "loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "display(log_df)\n",
    "print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_year(sample_batches):\n",
    "    probs_topics = []\n",
    "    years = []\n",
    "    for i, sample_batch in sample_batches:\n",
    "        probs_topics_batch = sess.run(model.prob_topic, feed_dict=model.get_feed_dict(sample_batch, mode='test'))\n",
    "        years_batch = [instance.year for instance in sample_batch]\n",
    "        probs_topics += [probs_topics_batch]\n",
    "        years += years_batch\n",
    "    probs_topics = np.concatenate(probs_topics)\n",
    "    years = np.array(years)\n",
    "\n",
    "    topic_years = years.dot(probs_topics) / np.sum(probs_topics, 0)\n",
    "    topic_year = {model.topic_idxs[i]: year for i, year in enumerate(topic_years)}\n",
    "    return topic_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Avg Year: 2009 sentences features test performance method systems training models speech context\n",
      "   1 Avg Year: 2008 training method performance test models probability error segmentation errors character\n",
      "     11 Avg Year: 2010 translation alignment phrase source english translations mt target bleu parallel\n",
      "     12 Avg Year: 2013 models training learning features performance vector feature state network function\n",
      "   2 Avg Year: 2013 features classification feature sentiment classifier class negative positive tweets polarity\n",
      "     21 Avg Year: 2009 document sentences question evaluation documents terms topic method systems questions\n",
      "     22 Avg Year: 2010 similarity sense entity wordnet semantic relation pairs senses relations entities\n",
      "   3 Avg Year: 2011 annotation annotators agreement annotator annotations study students participants texts learners\n",
      "     31 Avg Year: 2007 annotation user project resources tools web tool database research processing\n",
      "     32 Avg Year: 2008 morphological pos languages tag tags tagging lexicon form forms english\n",
      "   4 Avg Year: 2008 user dialogue human systems utterance utterances users domain interaction conversation\n",
      "     41 Avg Year: 2003 knowledge semantic event domain relations events object meaning representation concepts\n",
      "     42 Avg Year: 2005 verb verbs semantic syntactic argument noun arguments sentences discourse subject\n",
      "   5 Avg Year: 2006 speech recognition speakers speaker acoustic transcription spoken prosodic boundaries signal\n",
      "     51 Avg Year: 2002 grammar tree rules algorithm rule node parsing nodes trees input\n",
      "     52 Avg Year: 2009 parsing dependency parser tree syntactic parse trees features head treebank\n"
     ]
    }
   ],
   "source": [
    "sample_batches = get_batches(instances_train, config.batch_size)\n",
    "topic_year = get_topic_year(sample_batches)\n",
    "print_topic_year(sess, model, topic_freq_tokens=topic_freq_tokens, topic_year=topic_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
