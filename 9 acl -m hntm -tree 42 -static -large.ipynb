{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from data_structure import get_batches\n",
    "from hntm import HierarchicalNeuralTopicModel\n",
    "from tree import get_descendant_idxs\n",
    "from evaluation import validate, get_hierarchical_affinity, get_topic_specialization, print_topic_sample\n",
    "from configure import get_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(nb_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu\n",
    "np.random.seed(config.seed)\n",
    "random.seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.path_data,'rb'))\n",
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)\n",
    "config.dim_bow = len(bow_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     10
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables, model):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, model, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    return _variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = []\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "ppl_min = np.inf\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','','','','','VALID:','','','','','TEST:','', 'SPEC:', '', '', 'HIER:', ''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL','REG','LOSS','PPL','NLL','KL','REG','LOSS','PPL', '1', '2', '3', 'CHILD', 'OTHER']]))))\n",
    "\n",
    "cmd_rm = 'rm -r %s' % config.dir_model\n",
    "res = subprocess.call(cmd_rm.split())\n",
    "cmd_mk = 'mkdir %s' % config.dir_model\n",
    "res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "def update_checkpoint(config, checkpoint, global_step):\n",
    "    checkpoint.append(config.path_model + '-%i' % global_step)\n",
    "    if len(checkpoint) > config.max_to_keep:\n",
    "        path_model = checkpoint.pop(0) + '.*'\n",
    "        for p in glob.glob(path_model):\n",
    "            os.remove(p)\n",
    "    cPickle.dump(checkpoint, open(config.path_checkpoint, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "model = HierarchicalNeuralTopicModel(config)\n",
    "sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(max_to_keep=config.max_to_keep)\n",
    "update_tree_flg = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train & validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th colspan=\"4\" halign=\"left\"></th>\n",
       "      <th>VALID:</th>\n",
       "      <th colspan=\"4\" halign=\"left\"></th>\n",
       "      <th>TEST:</th>\n",
       "      <th></th>\n",
       "      <th>SPEC:</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "      <th>HIER:</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>CHILD</th>\n",
       "      <th>OTHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>80</td>\n",
       "      <td>8</td>\n",
       "      <td>463</td>\n",
       "      <td>9155.95</td>\n",
       "      <td>1834</td>\n",
       "      <td>9134.10</td>\n",
       "      <td>21.71</td>\n",
       "      <td>0.16</td>\n",
       "      <td>8896.94</td>\n",
       "      <td>1752</td>\n",
       "      <td>8874.80</td>\n",
       "      <td>22.02</td>\n",
       "      <td>0.12</td>\n",
       "      <td>8897.10</td>\n",
       "      <td>1751</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>72</td>\n",
       "      <td>17</td>\n",
       "      <td>360</td>\n",
       "      <td>9130.44</td>\n",
       "      <td>1795</td>\n",
       "      <td>9108.77</td>\n",
       "      <td>21.54</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8890.81</td>\n",
       "      <td>1742</td>\n",
       "      <td>8869.95</td>\n",
       "      <td>20.74</td>\n",
       "      <td>0.12</td>\n",
       "      <td>8890.63</td>\n",
       "      <td>1743</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>69</td>\n",
       "      <td>26</td>\n",
       "      <td>257</td>\n",
       "      <td>9119.17</td>\n",
       "      <td>1779</td>\n",
       "      <td>9097.85</td>\n",
       "      <td>21.19</td>\n",
       "      <td>0.13</td>\n",
       "      <td>8887.29</td>\n",
       "      <td>1736</td>\n",
       "      <td>8866.96</td>\n",
       "      <td>20.22</td>\n",
       "      <td>0.11</td>\n",
       "      <td>8888.00</td>\n",
       "      <td>1737</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>154</td>\n",
       "      <td>9112.62</td>\n",
       "      <td>1769</td>\n",
       "      <td>9091.58</td>\n",
       "      <td>20.91</td>\n",
       "      <td>0.12</td>\n",
       "      <td>8885.76</td>\n",
       "      <td>1734</td>\n",
       "      <td>8865.75</td>\n",
       "      <td>19.91</td>\n",
       "      <td>0.10</td>\n",
       "      <td>8884.86</td>\n",
       "      <td>1734</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>69</td>\n",
       "      <td>44</td>\n",
       "      <td>51</td>\n",
       "      <td>9108.01</td>\n",
       "      <td>1763</td>\n",
       "      <td>9087.22</td>\n",
       "      <td>20.67</td>\n",
       "      <td>0.12</td>\n",
       "      <td>8883.87</td>\n",
       "      <td>1731</td>\n",
       "      <td>8864.23</td>\n",
       "      <td>19.54</td>\n",
       "      <td>0.10</td>\n",
       "      <td>8883.82</td>\n",
       "      <td>1731</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545000</th>\n",
       "      <td>73</td>\n",
       "      <td>961</td>\n",
       "      <td>112</td>\n",
       "      <td>9078.57</td>\n",
       "      <td>1719</td>\n",
       "      <td>9057.90</td>\n",
       "      <td>18.56</td>\n",
       "      <td>0.07</td>\n",
       "      <td>8882.88</td>\n",
       "      <td>1729</td>\n",
       "      <td>8864.73</td>\n",
       "      <td>18.09</td>\n",
       "      <td>0.06</td>\n",
       "      <td>8880.64</td>\n",
       "      <td>1726</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550000</th>\n",
       "      <td>73</td>\n",
       "      <td>970</td>\n",
       "      <td>9</td>\n",
       "      <td>9078.51</td>\n",
       "      <td>1719</td>\n",
       "      <td>9057.82</td>\n",
       "      <td>18.56</td>\n",
       "      <td>0.07</td>\n",
       "      <td>8883.78</td>\n",
       "      <td>1731</td>\n",
       "      <td>8865.66</td>\n",
       "      <td>18.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>8880.64</td>\n",
       "      <td>1726</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555000</th>\n",
       "      <td>73</td>\n",
       "      <td>978</td>\n",
       "      <td>473</td>\n",
       "      <td>9078.47</td>\n",
       "      <td>1719</td>\n",
       "      <td>9057.76</td>\n",
       "      <td>18.55</td>\n",
       "      <td>0.07</td>\n",
       "      <td>8882.62</td>\n",
       "      <td>1729</td>\n",
       "      <td>8864.54</td>\n",
       "      <td>18.03</td>\n",
       "      <td>0.06</td>\n",
       "      <td>8880.64</td>\n",
       "      <td>1726</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560000</th>\n",
       "      <td>73</td>\n",
       "      <td>987</td>\n",
       "      <td>370</td>\n",
       "      <td>9078.43</td>\n",
       "      <td>1719</td>\n",
       "      <td>9057.72</td>\n",
       "      <td>18.55</td>\n",
       "      <td>0.07</td>\n",
       "      <td>8883.93</td>\n",
       "      <td>1730</td>\n",
       "      <td>8865.82</td>\n",
       "      <td>18.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>8880.64</td>\n",
       "      <td>1726</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565000</th>\n",
       "      <td>73</td>\n",
       "      <td>996</td>\n",
       "      <td>267</td>\n",
       "      <td>9078.39</td>\n",
       "      <td>1719</td>\n",
       "      <td>9057.67</td>\n",
       "      <td>18.54</td>\n",
       "      <td>0.07</td>\n",
       "      <td>8883.26</td>\n",
       "      <td>1730</td>\n",
       "      <td>8865.14</td>\n",
       "      <td>18.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>8880.64</td>\n",
       "      <td>1726</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        TRAIN:                               VALID:        \\\n",
       "       Time   Ep   Ct     LOSS   PPL      NLL     KL   REG     LOSS   PPL   \n",
       "5000     80    8  463  9155.95  1834  9134.10  21.71  0.16  8896.94  1752   \n",
       "10000    72   17  360  9130.44  1795  9108.77  21.54  0.14  8890.81  1742   \n",
       "15000    69   26  257  9119.17  1779  9097.85  21.19  0.13  8887.29  1736   \n",
       "20000    72   35  154  9112.62  1769  9091.58  20.91  0.12  8885.76  1734   \n",
       "25000    69   44   51  9108.01  1763  9087.22  20.67  0.12  8883.87  1731   \n",
       "...     ...  ...  ...      ...   ...      ...    ...   ...      ...   ...   \n",
       "545000   73  961  112  9078.57  1719  9057.90  18.56  0.07  8882.88  1729   \n",
       "550000   73  970    9  9078.51  1719  9057.82  18.56  0.07  8883.78  1731   \n",
       "555000   73  978  473  9078.47  1719  9057.76  18.55  0.07  8882.62  1729   \n",
       "560000   73  987  370  9078.43  1719  9057.72  18.55  0.07  8883.93  1730   \n",
       "565000   73  996  267  9078.39  1719  9057.67  18.54  0.07  8883.26  1730   \n",
       "\n",
       "                                TEST:       SPEC:             HIER:        \n",
       "            NLL     KL   REG     LOSS   PPL     1     2     3 CHILD OTHER  \n",
       "5000    8874.80  22.02  0.12  8897.10  1751  0.43  0.46  0.41  0.32  0.23  \n",
       "10000   8869.95  20.74  0.12  8890.63  1743  0.44  0.48  0.42  0.28  0.21  \n",
       "15000   8866.96  20.22  0.11  8888.00  1737  0.44  0.47  0.43  0.29  0.21  \n",
       "20000   8865.75  19.91  0.10  8884.86  1734  0.44  0.48  0.43  0.27  0.21  \n",
       "25000   8864.23  19.54  0.10  8883.82  1731  0.46  0.47  0.44  0.28  0.21  \n",
       "...         ...    ...   ...      ...   ...   ...   ...   ...   ...   ...  \n",
       "545000  8864.73  18.09  0.06  8880.64  1726  0.46  0.49  0.45  0.26  0.21  \n",
       "550000  8865.66  18.06  0.06  8880.64  1726  0.46  0.49  0.45  0.25  0.21  \n",
       "555000  8864.54  18.03  0.06  8880.64  1726  0.46  0.49  0.44  0.26  0.21  \n",
       "560000  8865.82  18.05  0.06  8880.64  1726  0.46  0.50  0.45  0.25  0.21  \n",
       "565000  8865.14  18.06  0.06  8880.64  1726  0.46  0.49  0.44  0.25  0.21  \n",
       "\n",
       "[113 rows x 20 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 R: 1.000 P: 0.121 user dialogue speech users human research systems linguistic domain utterance\n",
      "   1 R: 0.231 P: 0.081 structure grammar rules semantic clause syntactic representation form structures type\n",
      "     11 R: 0.073 P: 0.073 translation english source alignment phrase target translations parallel sentences pairs\n",
      "     12 R: 0.077 P: 0.077 tree parsing dependency parser node rules algorithm grammar trees parse\n",
      "   2 R: 0.191 P: 0.041 pos morphological tags annotation tag languages tagging dependency treebank parsing\n",
      "     21 R: 0.077 P: 0.077 speech error errors character english chinese segmentation languages characters recognition\n",
      "     22 R: 0.072 P: 0.072 training models embeddings neural learning network vector performance trained input\n",
      "   3 R: 0.237 P: 0.057 verb semantic verbs noun syntactic lexical argument object role frame\n",
      "     31 R: 0.073 P: 0.073 semantic sense terms wordnet lexical relations senses similarity domain term\n",
      "     32 R: 0.108 P: 0.108 models probability algorithm performance training distribution similarity method function values\n",
      "   4 R: 0.220 P: 0.045 event annotation discourse relations relation entity events entities temporal annotated\n",
      "     41 R: 0.082 P: 0.082 document documents question topic query sentences questions answer search web\n",
      "     42 R: 0.093 P: 0.093 features feature classification training classifier sentiment performance learning positive negative\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th colspan=\"4\" halign=\"left\"></th>\n",
       "      <th>VALID:</th>\n",
       "      <th colspan=\"4\" halign=\"left\"></th>\n",
       "      <th>TEST:</th>\n",
       "      <th></th>\n",
       "      <th>SPEC:</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "      <th>HIER:</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>CHILD</th>\n",
       "      <th>OTHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>80</td>\n",
       "      <td>8</td>\n",
       "      <td>463</td>\n",
       "      <td>9155.95</td>\n",
       "      <td>1834</td>\n",
       "      <td>9134.10</td>\n",
       "      <td>21.71</td>\n",
       "      <td>0.16</td>\n",
       "      <td>8896.94</td>\n",
       "      <td>1752</td>\n",
       "      <td>8874.80</td>\n",
       "      <td>22.02</td>\n",
       "      <td>0.12</td>\n",
       "      <td>8897.10</td>\n",
       "      <td>1751</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>72</td>\n",
       "      <td>17</td>\n",
       "      <td>360</td>\n",
       "      <td>9130.44</td>\n",
       "      <td>1795</td>\n",
       "      <td>9108.77</td>\n",
       "      <td>21.54</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8890.81</td>\n",
       "      <td>1742</td>\n",
       "      <td>8869.95</td>\n",
       "      <td>20.74</td>\n",
       "      <td>0.12</td>\n",
       "      <td>8890.63</td>\n",
       "      <td>1743</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>69</td>\n",
       "      <td>26</td>\n",
       "      <td>257</td>\n",
       "      <td>9119.17</td>\n",
       "      <td>1779</td>\n",
       "      <td>9097.85</td>\n",
       "      <td>21.19</td>\n",
       "      <td>0.13</td>\n",
       "      <td>8887.29</td>\n",
       "      <td>1736</td>\n",
       "      <td>8866.96</td>\n",
       "      <td>20.22</td>\n",
       "      <td>0.11</td>\n",
       "      <td>8888.00</td>\n",
       "      <td>1737</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>154</td>\n",
       "      <td>9112.62</td>\n",
       "      <td>1769</td>\n",
       "      <td>9091.58</td>\n",
       "      <td>20.91</td>\n",
       "      <td>0.12</td>\n",
       "      <td>8885.76</td>\n",
       "      <td>1734</td>\n",
       "      <td>8865.75</td>\n",
       "      <td>19.91</td>\n",
       "      <td>0.10</td>\n",
       "      <td>8884.86</td>\n",
       "      <td>1734</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>69</td>\n",
       "      <td>44</td>\n",
       "      <td>51</td>\n",
       "      <td>9108.01</td>\n",
       "      <td>1763</td>\n",
       "      <td>9087.22</td>\n",
       "      <td>20.67</td>\n",
       "      <td>0.12</td>\n",
       "      <td>8883.87</td>\n",
       "      <td>1731</td>\n",
       "      <td>8864.23</td>\n",
       "      <td>19.54</td>\n",
       "      <td>0.10</td>\n",
       "      <td>8883.82</td>\n",
       "      <td>1731</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545000</th>\n",
       "      <td>73</td>\n",
       "      <td>961</td>\n",
       "      <td>112</td>\n",
       "      <td>9078.57</td>\n",
       "      <td>1719</td>\n",
       "      <td>9057.90</td>\n",
       "      <td>18.56</td>\n",
       "      <td>0.07</td>\n",
       "      <td>8882.88</td>\n",
       "      <td>1729</td>\n",
       "      <td>8864.73</td>\n",
       "      <td>18.09</td>\n",
       "      <td>0.06</td>\n",
       "      <td>8880.64</td>\n",
       "      <td>1726</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550000</th>\n",
       "      <td>73</td>\n",
       "      <td>970</td>\n",
       "      <td>9</td>\n",
       "      <td>9078.51</td>\n",
       "      <td>1719</td>\n",
       "      <td>9057.82</td>\n",
       "      <td>18.56</td>\n",
       "      <td>0.07</td>\n",
       "      <td>8883.78</td>\n",
       "      <td>1731</td>\n",
       "      <td>8865.66</td>\n",
       "      <td>18.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>8880.64</td>\n",
       "      <td>1726</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555000</th>\n",
       "      <td>73</td>\n",
       "      <td>978</td>\n",
       "      <td>473</td>\n",
       "      <td>9078.47</td>\n",
       "      <td>1719</td>\n",
       "      <td>9057.76</td>\n",
       "      <td>18.55</td>\n",
       "      <td>0.07</td>\n",
       "      <td>8882.62</td>\n",
       "      <td>1729</td>\n",
       "      <td>8864.54</td>\n",
       "      <td>18.03</td>\n",
       "      <td>0.06</td>\n",
       "      <td>8880.64</td>\n",
       "      <td>1726</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560000</th>\n",
       "      <td>73</td>\n",
       "      <td>987</td>\n",
       "      <td>370</td>\n",
       "      <td>9078.43</td>\n",
       "      <td>1719</td>\n",
       "      <td>9057.72</td>\n",
       "      <td>18.55</td>\n",
       "      <td>0.07</td>\n",
       "      <td>8883.93</td>\n",
       "      <td>1730</td>\n",
       "      <td>8865.82</td>\n",
       "      <td>18.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>8880.64</td>\n",
       "      <td>1726</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565000</th>\n",
       "      <td>73</td>\n",
       "      <td>996</td>\n",
       "      <td>267</td>\n",
       "      <td>9078.39</td>\n",
       "      <td>1719</td>\n",
       "      <td>9057.67</td>\n",
       "      <td>18.54</td>\n",
       "      <td>0.07</td>\n",
       "      <td>8883.26</td>\n",
       "      <td>1730</td>\n",
       "      <td>8865.14</td>\n",
       "      <td>18.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>8880.64</td>\n",
       "      <td>1726</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        TRAIN:                               VALID:        \\\n",
       "       Time   Ep   Ct     LOSS   PPL      NLL     KL   REG     LOSS   PPL   \n",
       "5000     80    8  463  9155.95  1834  9134.10  21.71  0.16  8896.94  1752   \n",
       "10000    72   17  360  9130.44  1795  9108.77  21.54  0.14  8890.81  1742   \n",
       "15000    69   26  257  9119.17  1779  9097.85  21.19  0.13  8887.29  1736   \n",
       "20000    72   35  154  9112.62  1769  9091.58  20.91  0.12  8885.76  1734   \n",
       "25000    69   44   51  9108.01  1763  9087.22  20.67  0.12  8883.87  1731   \n",
       "...     ...  ...  ...      ...   ...      ...    ...   ...      ...   ...   \n",
       "545000   73  961  112  9078.57  1719  9057.90  18.56  0.07  8882.88  1729   \n",
       "550000   73  970    9  9078.51  1719  9057.82  18.56  0.07  8883.78  1731   \n",
       "555000   73  978  473  9078.47  1719  9057.76  18.55  0.07  8882.62  1729   \n",
       "560000   73  987  370  9078.43  1719  9057.72  18.55  0.07  8883.93  1730   \n",
       "565000   73  996  267  9078.39  1719  9057.67  18.54  0.07  8883.26  1730   \n",
       "\n",
       "                                TEST:       SPEC:             HIER:        \n",
       "            NLL     KL   REG     LOSS   PPL     1     2     3 CHILD OTHER  \n",
       "5000    8874.80  22.02  0.12  8897.10  1751  0.43  0.46  0.41  0.32  0.23  \n",
       "10000   8869.95  20.74  0.12  8890.63  1743  0.44  0.48  0.42  0.28  0.21  \n",
       "15000   8866.96  20.22  0.11  8888.00  1737  0.44  0.47  0.43  0.29  0.21  \n",
       "20000   8865.75  19.91  0.10  8884.86  1734  0.44  0.48  0.43  0.27  0.21  \n",
       "25000   8864.23  19.54  0.10  8883.82  1731  0.46  0.47  0.44  0.28  0.21  \n",
       "...         ...    ...   ...      ...   ...   ...   ...   ...   ...   ...  \n",
       "545000  8864.73  18.09  0.06  8880.64  1726  0.46  0.49  0.45  0.26  0.21  \n",
       "550000  8865.66  18.06  0.06  8880.64  1726  0.46  0.49  0.45  0.25  0.21  \n",
       "555000  8864.54  18.03  0.06  8880.64  1726  0.46  0.49  0.44  0.26  0.21  \n",
       "560000  8865.82  18.05  0.06  8880.64  1726  0.46  0.50  0.45  0.25  0.21  \n",
       "565000  8865.14  18.06  0.06  8880.64  1726  0.46  0.49  0.44  0.25  0.21  \n",
       "\n",
       "[113 rows x 20 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 R: 1.000 P: 0.123 user dialogue users speech human research systems linguistic utterance utterances\n",
      "   1 R: 0.230 P: 0.081 structure grammar rules semantic syntactic clause representation type form structures\n",
      "     11 R: 0.073 P: 0.073 translation english source alignment phrase target translations sentences parallel pairs\n",
      "     12 R: 0.076 P: 0.076 tree parsing dependency parser node algorithm rules grammar trees parse\n",
      "   2 R: 0.188 P: 0.040 pos morphological tags tag annotation tagging languages dependency tagger treebank\n",
      "     21 R: 0.077 P: 0.077 speech error segmentation errors character chinese english recognition training languages\n",
      "     22 R: 0.071 P: 0.071 models training neural learning embeddings network performance trained vector input\n",
      "   3 R: 0.239 P: 0.059 verb semantic verbs noun syntactic lexical frame argument role object\n",
      "     31 R: 0.073 P: 0.073 semantic sense terms wordnet relations similarity lexical senses term concepts\n",
      "     32 R: 0.107 P: 0.107 models probability algorithm performance training distribution similarity method function values\n",
      "   4 R: 0.220 P: 0.045 event annotation relations discourse relation entity events entities coreference temporal\n",
      "     41 R: 0.082 P: 0.082 document documents topic question sentences query answer questions topics evaluation\n",
      "     42 R: 0.093 P: 0.093 features feature classification training classifier sentiment performance learning positive negative\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "while epoch < config.n_epochs:\n",
    "    # train\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = model.get_feed_dict(batch)\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, ppls_batch, global_step_log = \\\n",
    "        sess.run([model.opt, model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_reg, model.topic_ppls, tf.train.get_global_step()], feed_dict = feed_dict)\n",
    "\n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if global_step_log % config.log_period == 0:\n",
    "            # validate\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "\n",
    "            # test\n",
    "            if ppl_dev < ppl_min:\n",
    "                ppl_min = ppl_dev\n",
    "                loss_test, _, _, _, ppl_test, _ = validate(sess, test_batches, model)\n",
    "                saver.save(sess, config.path_model, global_step=global_step_log)\n",
    "                cPickle.dump(config, open(config.path_config % global_step_log, 'wb'))\n",
    "                update_checkpoint(config, checkpoint, global_step_log)\n",
    "            \n",
    "            # visualize topic\n",
    "            topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "            topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "            topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "            topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "            descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "            recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "            \n",
    "            depth_specs = get_topic_specialization(sess, model, instances_test)\n",
    "            hierarchical_affinities = get_hierarchical_affinity(sess, model)\n",
    "            \n",
    "            # log\n",
    "            clear_output()\n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_reg_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_reg_dev, \\\n",
    "                    '%.2f'%loss_test, '%.0f'%ppl_test, \\\n",
    "                    '%.2f'%depth_specs[1], '%.2f'%depth_specs[2], '%.2f'%depth_specs[3], \\\n",
    "                    '%.2f'%hierarchical_affinities[0], '%.2f'%hierarchical_affinities[1]],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "            cPickle.dump(log_df, open(os.path.join(config.path_log), 'wb'))\n",
    "            print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)\n",
    "\n",
    "            # update tree\n",
    "            if not config.static:\n",
    "                config.tree_idxs, update_tree_flg = model.update_tree(topic_prob_topic, recur_prob_topic)\n",
    "                if update_tree_flg:\n",
    "                    print(config.tree_idxs)\n",
    "                    name_variables = {tensor.name: variable for tensor, variable in zip(tf.global_variables(), sess.run(tf.global_variables()))} # store paremeters\n",
    "                    if 'sess' in globals(): sess.close()\n",
    "                    model = HierarchicalNeuralTopicModel(config)\n",
    "                    sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "                    name_tensors = {tensor.name: tensor for tensor in tf.global_variables()}\n",
    "                    sess.run([name_tensors[name].assign(variable) for name, variable in name_variables.items()]) # restore parameters\n",
    "                    saver = tf.train.Saver(max_to_keep=1)\n",
    "                \n",
    "            time_start = time.time()\n",
    "\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "    epoch += 1\n",
    "\n",
    "loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "display(log_df)\n",
    "print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
