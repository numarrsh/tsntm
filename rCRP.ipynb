{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import pdb\n",
    "import _pickle as cPickle\n",
    "import time\n",
    "import subprocess\n",
    "import glob\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from configure import get_config\n",
    "\n",
    "#coding: utf-8\n",
    "import os\n",
    "import pdb\n",
    "import time\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import gammaln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_name = '0 bags -m rcrp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(nb_name)\n",
    "np.random.seed(config.seed)\n",
    "random.seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.alp = 1\n",
    "config.gam = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.path_data,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31943, 1035)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.n_doc = len(instances_train)\n",
    "config.n_vocab = len(bow_idxs)\n",
    "config.n_doc, config.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.1, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.alp, config.gam, config.eta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Topic:\n",
    "    def __init__(self, idx, sibling_idx, parent, depth, config):\n",
    "        self.idx = idx\n",
    "        self.sibling_idx = sibling_idx\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.depth = depth\n",
    "        \n",
    "        self.cnt_doc_topic = 0\n",
    "        self.cum_doc_topic = 0\n",
    "        \n",
    "        self.cnt_words = np.zeros(config.n_vocab)\n",
    "        self.cum_words = np.zeros(config.n_vocab)\n",
    "        \n",
    "        self.config=config\n",
    "    \n",
    "    def sample_child(self, cnt_words=None, init=False, train=True):\n",
    "        if self.cnt_doc_topic == 0:\n",
    "            return self\n",
    "        else:\n",
    "            probs_child = self.get_probs_child(cnt_words, init=init)\n",
    "            child_index = np.random.multinomial(1, probs_child).argmax()\n",
    "\n",
    "            if child_index < len(self.children):\n",
    "                child = self.children[child_index]\n",
    "                return child.sample_child(init, train)\n",
    "            elif child_index == len(self.children):\n",
    "                return self\n",
    "            else:\n",
    "                assert child_index == len(self.children) + 1\n",
    "                child = self.get_new_child(train)\n",
    "                return child.sample_child(init, train)\n",
    "        \n",
    "    def get_probs_child(self, cnt_words, init=False, train=True):\n",
    "        if init:\n",
    "            logits_child = self.get_logits_child_prior()\n",
    "        else:\n",
    "            logits_child_prior = self.get_logits_child_prior()\n",
    "            logits_child_likelihood = self.get_logits_child_likelihood(cnt_words)\n",
    "            logits_child = logits_child_prior + logits_child_likelihood\n",
    "\n",
    "        logits_child -= np.max(logits_child)\n",
    "        s_child = np.exp(logits_child)\n",
    "        if np.sum(s_child) > 0:\n",
    "            probs_child = s_child/np.sum(s_child)\n",
    "            probs_child = probs_child.astype(np.float64)\n",
    "        else:\n",
    "            probs_child = np.zeros_like(logits_child, dtype=np.float64)\n",
    "            probs_child[np.argmax(s_child)] = 1.\n",
    "            \n",
    "        return probs_child\n",
    "        \n",
    "    def get_logits_child_prior(self):\n",
    "        s_child_prior = [child.cum_doc_topic for child in self.children]\n",
    "        s_child_prior += [self.cnt_doc_topic]\n",
    "        s_child_prior += [self.config.gam**(self.depth+1)]\n",
    "\n",
    "        logits_child_prior = np.log(s_child_prior)\n",
    "        return logits_child_prior\n",
    "    \n",
    "    def get_logits_child_likelihood(self, cnt_words_doc_topic):\n",
    "        if len(self.children) > 0:\n",
    "            children_cnt_words = np.array([child.cum_words for child in self.children]) # (Children) x Vocabulary\n",
    "            children_cnt_words = np.concatenate([children_cnt_words, self.cnt_words[None, :], np.zeros([1, self.config.n_vocab])], 0) # (Children+Self+NewChildren) x Vocabulary\n",
    "        else:\n",
    "            children_cnt_words = np.concatenate([self.cnt_words[None, :], np.zeros([1, self.config.n_vocab])], 0) # (Self+NewChildren) x Vocabulary\n",
    "\n",
    "        logits_child_likelihood = gammaln(np.sum(children_cnt_words, -1) + self.config.n_vocab*self.config.eta**(self.depth+1)) \\\n",
    "                            - np.sum(gammaln(children_cnt_words + self.config.eta**(self.depth+1)), -1) \\\n",
    "                            - gammaln(np.sum(children_cnt_words + cnt_words_doc_topic, -1) + self.config.n_vocab*self.config.eta**(self.depth+1)) \\\n",
    "                            + np.sum(gammaln(children_cnt_words + cnt_words_doc_topic + self.config.eta**(self.depth+1)), -1)\n",
    "        return logits_child_likelihood\n",
    "    \n",
    "    def get_new_child(self, train=True):\n",
    "        sibling_idx = max([child.sibling_idx for child in self.children]) + 1 if len(self.children) > 0 else 1\n",
    "        idx = self.idx + '-' + str(sibling_idx)\n",
    "        depth = self.depth+1\n",
    "        child = Topic(idx=idx, sibling_idx=sibling_idx, parent=self, depth=depth, config=self.config)        \n",
    "        if train: self.children += [child]\n",
    "        return child    \n",
    "    \n",
    "    def increment_cnt(self, cnt_words):\n",
    "        def increment_cum(topic, cnt_words):\n",
    "            topic.cum_doc_topic += 1\n",
    "            topic.cum_words += cnt_words\n",
    "            if topic.parent is not None: increment_cum(topic.parent, cnt_words=cnt_words)\n",
    "        \n",
    "        self.cnt_doc_topic += 1\n",
    "        self.cnt_words += cnt_words\n",
    "        increment_cum(self, cnt_words=cnt_words)\n",
    "            \n",
    "    def decrement_cnt(self, cnt_words):\n",
    "        def decrement_cum(topic, cnt_words):\n",
    "            topic.cum_doc_topic -= 1\n",
    "            topic.cum_words -= cnt_words\n",
    "            if topic.parent is not None: decrement_cum(topic.parent, cnt_words=cnt_words)\n",
    "                \n",
    "        self.cnt_doc_topic -= 1\n",
    "        self.cnt_words -= cnt_words\n",
    "        decrement_cum(self, cnt_words=cnt_words)\n",
    "            \n",
    "    def increment_cnt_words(self, word_idx):\n",
    "        def increment_cum_words(topic, word_idx):\n",
    "            topic.cum_words[word_idx] += 1\n",
    "            if topic.parent is not None: increment_cum_words(topic.parent, word_idx=word_idx)\n",
    "                \n",
    "        self.cnt_words[word_idx] += 1\n",
    "        increment_cum_words(self, word_idx=word_idx)\n",
    "        \n",
    "    def decrement_cnt_words(self, word_idx):\n",
    "        def decrement_cum_words(topic, word_idx):\n",
    "            topic.cum_words[word_idx] -= 1\n",
    "            if topic.parent is not None: decrement_cum_words(topic.parent, word_idx=word_idx)\n",
    "                \n",
    "        self.cnt_words[word_idx] -= 1\n",
    "        decrement_cum_words(self, word_idx=word_idx)\n",
    "        \n",
    "    def increment_cnt_doc(self):\n",
    "        def increment_cum_doc(topic):\n",
    "            topic.cum_doc_topic += 1\n",
    "            if topic.parent is not None: increment_cum_doc(topic.parent)\n",
    "        \n",
    "        self.cnt_doc_topic += 1\n",
    "        increment_cum_doc(self)\n",
    "        \n",
    "    def delete_topic(self):\n",
    "        self.parent.children.remove(self)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Doc:\n",
    "    def __init__(self, idx, words, bow, config):\n",
    "        self.idx = idx\n",
    "        self.words = words\n",
    "        self.cnt_words = bow\n",
    "        self.config = config\n",
    "        assert len(words) == np.sum(bow)\n",
    "        \n",
    "        self.index_cnt_words = [] # n_indices_topic x n_vocab\n",
    "        self.word_indices = [] # n_words\n",
    "        self.topics = [] # n_indices_topic\n",
    "\n",
    "    def sample_index(self, topic_root, init=False, train=True):\n",
    "        probs_index = self.get_probs_index(topic_root, init=init)\n",
    "        index = np.random.multinomial(1, probs_index).argmax()\n",
    "        return index\n",
    "    \n",
    "    def get_probs_index(self, topic_root, init=False):\n",
    "        if init:\n",
    "            logits_index = self.get_logits_index_prior()\n",
    "        else:\n",
    "            logits_index_prior = self.get_logits_index_prior()\n",
    "            logits_index_likelihood = self.get_logits_index_likelihood(topic_root)\n",
    "            logits_index = logits_index_prior + logits_index_likelihood\n",
    "        \n",
    "        logits_index -= np.max(logits_index)\n",
    "        s_index = np.exp(logits_index)\n",
    "        \n",
    "        if np.sum(s_index) > 0:\n",
    "            probs_index = s_index/np.sum(s_index)\n",
    "            probs_index = probs_index.astype(np.float64)\n",
    "        else:\n",
    "            probs_index = np.zeros_like(logits_index, dtype=np.float64)\n",
    "            probs_index[np.argmax(s_index)] = 1.\n",
    "        \n",
    "        return probs_index\n",
    "                \n",
    "    def get_logits_index_prior(self):\n",
    "        if len(self.index_cnt_words) == 0:\n",
    "            s_index_prior = [self.config.alp]\n",
    "        else:\n",
    "            s_index_prior = np.sum(self.index_cnt_words, 1)\n",
    "            s_index_prior = np.append(s_index_prior, self.config.alp)\n",
    "        logits_index_prior = np.log(s_index_prior)\n",
    "        return logits_index_prior\n",
    "    \n",
    "    def get_logits_index_likelihood(self, topic_root):\n",
    "        def get_logit_new_index_likelihood(topic_root):\n",
    "            def get_all_topics(topic):\n",
    "                topics = [topic]\n",
    "                for child in topic.children:\n",
    "                    topics += get_all_topics(child)\n",
    "                return topics\n",
    "\n",
    "            all_topics = get_all_topics(topic_root)\n",
    "\n",
    "            s_child_prior = [topic.cnt_doc_topic for topic in all_topics]\n",
    "            s_child_prior += [topic_root.config.gam]\n",
    "            p_child_prior = s_child_prior / np.sum(s_child_prior)\n",
    "\n",
    "            s_child_likelihood = np.array([topic.cum_words[word_idx] + topic.config.eta**(topic.depth) for topic in all_topics] + [0.])\n",
    "            z_child_likelihood = np.array([np.sum(topic.cum_words) + topic.config.n_vocab*(topic.config.eta**(topic.depth)) for topic in all_topics] + [0.])\n",
    "\n",
    "            p_child_likelihood = s_child_likelihood / z_child_likelihood\n",
    "            logit_new_index_likelihood = np.log(p_child_prior.dot(p_child_likelihood))\n",
    "\n",
    "            return logit_new_index_likelihood\n",
    "    \n",
    "        s_index_likelihood = np.array([topic.cum_words[word_idx] + topic.config.eta**(topic.depth) for topic in self.topics])\n",
    "        z_index_likelihood = np.array([np.sum(topic.cum_words) + topic.config.n_vocab*(topic.config.eta**(topic.depth)) for topic in self.topics])\n",
    "\n",
    "        logits_index_likelihood = np.log(s_index_likelihood/z_index_likelihood)\n",
    "        logit_new_index_likelihood = get_logit_new_index_likelihood(topic_root)\n",
    "        logits_index_likelihood = np.append(logits_index_likelihood, logit_new_index_likelihood)\n",
    "\n",
    "        return logits_index_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_child_idxs(topic):\n",
    "    topic_freq_words = [idx_to_word[bow_idxs[bow_index]] for bow_index in np.argsort(topic.cum_words)[::-1][:10]]\n",
    "    print('  '*topic.depth, topic.idx, ':', [child.idx for child in topic.children], topic.cnt_doc_topic, np.sum(topic.cnt_words), topic_freq_words)\n",
    "    for topic in topic.children:\n",
    "        print_child_idxs(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_cnt(docs):\n",
    "    topic_cnt_words = {}\n",
    "    topic_cnt_doc_topic = {}\n",
    "    for doc in docs:\n",
    "        for index, topic in enumerate(doc.topics):\n",
    "            if topic not in topic_cnt_words:\n",
    "                topic_cnt_words[topic] = np.zeros(config.n_vocab)\n",
    "                topic_cnt_doc_topic[topic] = 0\n",
    "            topic_cnt_words[topic] += doc.index_cnt_words[index]\n",
    "            topic_cnt_doc_topic[topic] += 1\n",
    "    return topic_cnt_words, topic_cnt_doc_topic\n",
    "\n",
    "def assert_cnt(topic, topic_cnt_words, topic_cnt_doc_topic):\n",
    "    assert all(topic_cnt_words[topic] == topic.cnt_words)\n",
    "    assert topic_cnt_doc_topic[topic] == topic.cnt_doc_topic\n",
    "    for child in topic.children:\n",
    "        assert_cnt(child, topic_cnt_words, topic_cnt_doc_topic)\n",
    "\n",
    "def get_cum_doc_topic(topic):\n",
    "    cum_doc_topic = np.sum(topic.cnt_doc_topic)\n",
    "    for child in topic.children:\n",
    "        cum_doc_topic += get_cum_doc_topic(child)\n",
    "    return cum_doc_topic\n",
    "\n",
    "def get_cum_words(topic):\n",
    "    cum_words = np.zeros_like(topic.cnt_words)\n",
    "    cum_words += topic.cnt_words\n",
    "    for child in topic.children:\n",
    "        cum_words += get_cum_words(child)\n",
    "    return cum_words\n",
    "\n",
    "def assert_cum(topic_root, cum_doc_topic, cum_words):\n",
    "    assert topic_root.cum_doc_topic == cum_doc_topic\n",
    "    assert all(topic_root.cum_words == cum_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs(instances, config):\n",
    "    docs_bow = [instance.bow for instance in instances]\n",
    "    docs_raw = [[[bow_index]*int(doc_bow[bow_index]) for bow_index in np.where(doc_bow > 0)[0]] for doc_bow in docs_bow]\n",
    "    docs_words = [[idx for idxs in doc for idx in idxs] for doc in docs_raw]\n",
    "    docs = [Doc(idx=doc_idx, words=doc_words, bow=doc_bow, config=config) for doc_idx, (doc_words, doc_bow) in enumerate(zip(docs_words, docs_bow)) if len(doc_words) > 0]\n",
    "    return docs\n",
    "\n",
    "train_docs = get_docs(instances_train, config)[:5000]\n",
    "topic_root = Topic(idx='0', sibling_idx=0, parent=None, depth=1, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init t (assign doc to doc_topic_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_doc in train_docs:\n",
    "    for word_index, word_idx in enumerate(train_doc.words):\n",
    "        new_index = train_doc.sample_index(topic_root, init=True, train=True)\n",
    "        \n",
    "        if new_index == len(train_doc.index_cnt_words):\n",
    "            if len(train_doc.index_cnt_words) == 0:\n",
    "                train_doc.index_cnt_words = np.zeros([1, train_doc.config.n_vocab])\n",
    "            else:\n",
    "                train_doc.index_cnt_words = np.concatenate([train_doc.index_cnt_words, np.zeros([1, train_doc.config.n_vocab])], 0)\n",
    "            \n",
    "        train_doc.index_cnt_words[new_index, word_idx] += 1\n",
    "        train_doc.word_indices.append(new_index)\n",
    "        \n",
    "    assert len(train_doc.words) == np.sum(train_doc.index_cnt_words) == len(train_doc.word_indices)\n",
    "    assert len(train_doc.index_cnt_words) == np.max(train_doc.word_indices) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init k (assign doc_topic_index to topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_doc in train_docs:\n",
    "    for index, cnt_words in enumerate(train_doc.index_cnt_words):\n",
    "        new_topic = topic_root.sample_child(init=True, train=True)\n",
    "        train_doc.topics.append(new_topic)\n",
    "        \n",
    "        new_topic.increment_cnt(cnt_words=cnt_words) # increment count of doc_topic & words\n",
    "            \n",
    "    assert len(train_doc.topics) == len(train_doc.index_cnt_words)\n",
    "assert np.sum([len(train_doc.index_cnt_words) for train_doc in train_docs]) == topic_root.cum_doc_topic == get_cum_doc_topic(topic_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_cnt_words, topic_cnt_doc_topic = get_topic_cnt(train_docs)\n",
    "assert_cnt(topic_root, topic_cnt_words, topic_cnt_doc_topic)\n",
    "cum_doc_topic = get_cum_doc_topic(topic_root)\n",
    "cum_words = get_cum_words(topic_root)\n",
    "assert_cum(topic_root, cum_doc_topic, cum_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 : [] 17111 92009.0 ['!', 'sleeve', 'carry', 'bought', 'nice', 'room', 'pockets', 'pocket', 'quality', 'price']\n"
     ]
    }
   ],
   "source": [
    "print_child_idxs(topic_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample t (assign doc to doc_topic_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:44: RuntimeWarning: divide by zero encountered in log\n",
      "/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:64: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "for train_doc in train_docs:\n",
    "    for word_index, word_idx in enumerate(train_doc.words):\n",
    "        # refer index of word\n",
    "        old_index = train_doc.word_indices[word_index]\n",
    "        old_topic = train_doc.topics[old_index]\n",
    "        \n",
    "        # decrement count of words\n",
    "        train_doc.index_cnt_words[old_index, word_idx] -= 1\n",
    "        old_topic.decrement_cnt_words(word_idx=word_idx)\n",
    "        assert train_doc.index_cnt_words[old_index, word_idx] >= 0\n",
    "        assert old_topic.cnt_words[word_idx] >= 0\n",
    "        \n",
    "        # sample topic_index of word\n",
    "        new_index = train_doc.sample_index(topic_root, init=False, train=True)\n",
    "        \n",
    "        if new_index == len(train_doc.index_cnt_words):\n",
    "            cnt_words = np.zeros([1, train_doc.config.n_vocab])\n",
    "            train_doc.index_cnt_words = np.concatenate([train_doc.index_cnt_words, cnt_words], 0)\n",
    "            new_topic = topic_root.sample_child(cnt_words, init=False, train=True)\n",
    "            new_topic.increment_cnt_doc()\n",
    "            train_doc.topics.append(new_topic)\n",
    "                \n",
    "        new_topic = train_doc.topics[new_index]\n",
    "        \n",
    "        # increment count of words\n",
    "        train_doc.index_cnt_words[new_index, word_idx] += 1\n",
    "        new_topic.increment_cnt_words(word_idx=word_idx)\n",
    "        train_doc.word_indices[word_index] = new_index\n",
    "        \n",
    "    assert len(train_doc.words) == np.sum(train_doc.index_cnt_words) == len(train_doc.word_indices)\n",
    "    assert np.sum(np.sum(train_doc.index_cnt_words, 1) > 0) == len(np.unique(train_doc.word_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_cnt_words, topic_cnt_doc_topic = get_topic_cnt(train_docs)\n",
    "assert_cnt(topic_root, topic_cnt_words, topic_cnt_doc_topic)\n",
    "cum_doc_topic = get_cum_doc_topic(topic_root)\n",
    "cum_words = get_cum_words(topic_root)\n",
    "assert_cum(topic_root, cum_doc_topic, cum_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 : ['0-1'] 17066 91226.0 ['!', 'sleeve', 'carry', 'bought', 'nice', 'room', 'pockets', 'pocket', 'quality', 'price']\n",
      "     0-1 : [] 45 783.0 ['usb', 'card', 'power', 'ports', 'external', 'slot', 'works', 'drive', 'port', 'needed']\n"
     ]
    }
   ],
   "source": [
    "print_child_idxs(topic_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample k (assign doc to doc_topic_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_doc in train_docs:\n",
    "    for index, cnt_words in enumerate(train_doc.index_cnt_words):\n",
    "        old_topic = train_doc.topics[index]\n",
    "        \n",
    "        old_topic.decrement_cnt(cnt_words)\n",
    "        assert old_topic.cnt_doc_topic >= 0\n",
    "        assert np.min(old_topic.cnt_words) >= 0\n",
    "        \n",
    "        if old_topic.cnt_doc_topic == 0:\n",
    "            assert np.sum(old_topic.cnt_words) == 0\n",
    "            old_topic.delete_topic()\n",
    "        \n",
    "        new_topic = topic_root.sample_child(cnt_words, init=False, train=True)\n",
    "        new_topic.increment_cnt(cnt_words) # increment count of doc_topic & words\n",
    "        train_doc.topics[index] = new_topic\n",
    "            \n",
    "    assert len(train_doc.topics) == len(train_doc.index_cnt_words)\n",
    "    \n",
    "assert np.sum([len(train_doc.index_cnt_words) for train_doc in train_docs]) == topic_root.cum_doc_topic == get_cum_doc_topic(topic_root)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_cnt_words, topic_cnt_doc_topic = get_topic_cnt(train_docs)\n",
    "assert_cnt(topic_root, topic_cnt_words, topic_cnt_doc_topic)\n",
    "cum_doc_topic = get_cum_doc_topic(topic_root)\n",
    "cum_words = get_cum_words(topic_root)\n",
    "assert_cum(topic_root, cum_doc_topic, cum_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 : ['0-1'] 16969 90351.0 ['!', 'sleeve', 'carry', 'bought', 'nice', 'room', 'pockets', 'pocket', 'quality', 'price']\n",
      "     0-1 : [] 142 1658.0 ['usb', 'card', 'drive', 'power', 'ports', 'works', 'drives', 'slot', 'external', 'work']\n"
     ]
    }
   ],
   "source": [
    "print_child_idxs(topic_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
