{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from data_structure import get_batches, get_test_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "from topic_model import TopicModel\n",
    "\n",
    "from topic_beam_search_decoder import BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '0', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/bags/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/tglm_vae_tmp0', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'bags', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 50, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 500, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.01, 'lr')\n",
    "flags.DEFINE_float('reg', 1., 'regularization term')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_bool('warmup', True, 'flg of warming up')\n",
    "flags.DEFINE_integer('epochs_cycle', 5, 'number of epochs within a cycle')\n",
    "flags.DEFINE_float('r_cycle', 0.5, 'proportion used to increase beta within a cycle')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 10, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_bow', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_hidden_topic', 512, 'dim_hidden_topic')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "flags.DEFINE_bool('bidirectional', True, 'flg of bidirectional encoding')\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_test_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('PAD_IDX', word_to_idx[PAD], 'PAD_IDX')\n",
    "flags.DEFINE_integer('UNK_IDX', word_to_idx[UNK], 'UNK_IDX')\n",
    "flags.DEFINE_integer('BOS_IDX', word_to_idx[BOS], 'BOS_IDX')\n",
    "flags.DEFINE_integer('EOS_IDX', word_to_idx[EOS], 'EOS_IDX')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "maximum_iterations = max([max([instance.max_sent_l for instance in batch]) for ct, batch in dev_batches])\n",
    "flags.DEFINE_integer('maximum_iterations', maximum_iterations, 'maximum_iterations')\n",
    "\n",
    "flags.DEFINE_integer('cycle_steps', len(train_batches)*config.epochs_cycle, 'number of steps for each cycle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build language model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['bow'] = tf.placeholder(tf.float32, [None, config.dim_bow], name='bow')\n",
    "t_variables['input_token_idxs'] = tf.placeholder(tf.int32, [None, None], name='input_token_idxs')\n",
    "t_variables['dec_input_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_input_idxs')\n",
    "t_variables['dec_target_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_target_idxs')\n",
    "t_variables['batch_l'] = tf.placeholder(tf.int32, name='batch_l')\n",
    "t_variables['doc_l'] = tf.placeholder(tf.int32, [None], name='doc_l')\n",
    "t_variables['sent_l'] = tf.placeholder(tf.int32, [None], name='sent_l')\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train', assertion=False):\n",
    "    def token_dropout(sent_idxs):\n",
    "        sent_idxs_dropout = np.asarray(sent_idxs)\n",
    "        sent_idxs_dropout[np.random.rand(len(sent_idxs)) > config.word_keep_prob] = config.UNK_IDX\n",
    "        return list(sent_idxs_dropout)\n",
    "\n",
    "    bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "    \n",
    "    doc_l = np.array([len(instance.token_idxs) for instance in batch])\n",
    "    \n",
    "    feed_input_token_idxs_list = [sent_idxs for instance in batch for sent_idxs in instance.token_idxs]\n",
    "    feed_dec_input_idxs_list = [[config.BOS_IDX] + token_dropout(sent_idxs) for sent_idxs in feed_input_token_idxs_list]\n",
    "    feed_dec_target_idxs_list = [sent_idxs + [config.EOS_IDX]  for sent_idxs in feed_input_token_idxs_list]\n",
    "        \n",
    "    sent_l = np.array([len(sent_idxs) for sent_idxs in feed_input_token_idxs_list], np.int32)\n",
    "    batch_l = len(sent_l)\n",
    "    \n",
    "    feed_input_token_idxs = pad_sequences(feed_input_token_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_input_idxs = pad_sequences(feed_dec_input_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_target_idxs = pad_sequences(feed_dec_target_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    \n",
    "    if assertion:\n",
    "        index = 0\n",
    "        for instance in batch:\n",
    "            for line_idxs in instance.token_idxs:\n",
    "                assert feed_input_token_idxs_list[index] == line_idxs\n",
    "                index += 1\n",
    "        assert feed_input_token_idxs.shape[1] == np.max(sent_l)\n",
    "        assert feed_dec_input_idxs.shape[1] == np.max(sent_l) + 1\n",
    "        assert feed_dec_target_idxs.shape[1] == np.max(sent_l) + 1\n",
    "    \n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "\n",
    "    feed_dict = {\n",
    "                t_variables['bow']: bow, \n",
    "                t_variables['batch_l']: batch_l, t_variables['doc_l']: doc_l, t_variables['sent_l']: sent_l, \n",
    "                t_variables['input_token_idxs']: feed_input_token_idxs, t_variables['dec_input_idxs']: feed_dec_input_idxs, t_variables['dec_target_idxs']: feed_dec_target_idxs, \n",
    "                t_variables['keep_prob']: keep_prob\n",
    "    }\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     10,
     24,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables):\n",
    "    sample_batch = dev_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "def check_shape(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()\n",
    "    \n",
    "def check_value(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()    \n",
    "    \n",
    "# sent_loss_kl_categ_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, tf_log(prob_topic_infer/prob_topic_sents)), 1))\n",
    "# debug_value([sent_loss_kl_categ, sent_loss_kl_categ_tmp])\n",
    "# sent_loss_kl_gauss_tmp = 0.5 * tf.reduce_sum(tf.exp(logvars_topic_infer-logvars_topic) + tf.square(means_topic - means_topic_infer) / tf.exp(logvars_topic) - 1 + (logvars_topic - logvars_topic_infer), -1)\n",
    "# sent_loss_kl_gmm_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss_tmp), -1))\n",
    "# debug_value([sent_loss_kl_gmm_tmp, sent_loss_kl_gmm])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encode bow\n",
    "with tf.variable_scope('topic/enc', reuse=False):\n",
    "    hidden_bow_ = tf.layers.Dense(units=config.dim_hidden_bow, activation=tf.nn.relu, name='hidden_bow')(t_variables['bow'])\n",
    "    hidden_bow = tf.layers.Dropout(t_variables['keep_prob'])(hidden_bow_)\n",
    "    means_bow = tf.layers.Dense(units=config.dim_latent_bow, name='mean_bow')(hidden_bow)\n",
    "    logvars_bow = tf.layers.Dense(units=config.dim_latent_bow, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_bow)\n",
    "    latents_bow = sample_latents(means_bow, logvars_bow) # sample latent vectors\n",
    "\n",
    "    prob_topic = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic')(latents_bow) # inference of topic probabilities\n",
    "\n",
    "# decode bow\n",
    "with tf.variable_scope('shared', reuse=False):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "bow_embeddings = tf.nn.embedding_lookup(embeddings, bow_idxs) # embeddings of each bow features\n",
    "\n",
    "with tf.variable_scope('topic/dec', reuse=False):\n",
    "    topic_embeddings = tf.get_variable('topic_emb', [config.n_topic, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of topics\n",
    "\n",
    "    topic_bow = tf.nn.softmax(tf.matmul(topic_embeddings, bow_embeddings, transpose_b=True), 1) # bow vectors for each topic\n",
    "    logits_bow = tf_log(tf.matmul(prob_topic, topic_bow)) # predicted bow distribution\n",
    "\n",
    "    # prior of each gaussian distribution (computed for each topic)\n",
    "    hidden_topic = tf.layers.Dense(units=config.dim_hidden_topic, activation=tf.nn.relu, name='hidden_topic')(topic_bow)\n",
    "    means_topic = tf.layers.Dense(units=config.dim_latent, name='mean_topic')(hidden_topic)\n",
    "    logvars_topic = tf.layers.Dense(units=config.dim_latent, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_topic)\n",
    "    sigma_topic = tf.exp(0.5 * logvars_topic)\n",
    "    gauss_topic = tfd.Normal(loc=means_topic, scale=sigma_topic)    \n",
    "    \n",
    "# define losses\n",
    "topic_losses_recon = -tf.reduce_sum(tf.multiply(t_variables['bow'], logits_bow), 1)\n",
    "topic_loss_recon = tf.reduce_mean(topic_losses_recon) # negative log likelihood of each words\n",
    "\n",
    "topic_loss_kl = compute_kl_loss(means_bow, logvars_bow) # KL divergence b/w latent dist & gaussian std\n",
    "\n",
    "topic_bow_norm = topic_bow / tf.norm(topic_bow, axis=1, keepdims=True)\n",
    "topic_dots = tf.clip_by_value(tf.matmul(topic_bow_norm, tf.transpose(topic_bow_norm)), -1., 1.)\n",
    "topic_loss_reg = tf.reduce_mean(tf.square(topic_dots - tf.eye(config.n_topic)))\n",
    "# topic_angles = tf.acos(topic_dots)\n",
    "# topic_angles_mean = tf.reduce_mean(topic_angles)\n",
    "# topic_angles_vars = tf.reduce_mean(tf.square(topic_angles - topic_angles_mean))\n",
    "# topic_loss_reg = tf.exp(topic_angles_vars - topic_angles_mean)\n",
    "\n",
    "# monitor\n",
    "n_bow = tf.reduce_sum(t_variables['bow'], 1)\n",
    "topic_ppls = tf.divide(topic_losses_recon, tf.maximum(1e-5, n_bow))\n",
    "topics_freq_bow_indices = tf.nn.top_k(topic_bow, 10, name='topic_freq_bow').indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "input_token_idxs = t_variables['input_token_idxs']\n",
    "batch_l = t_variables['batch_l']\n",
    "sent_l = t_variables['sent_l']\n",
    "max_sent_l = tf.reduce_max(sent_l)\n",
    "\n",
    "with tf.variable_scope('sent/enc', reuse=False):\n",
    "    # get word embedding\n",
    "    enc_input = tf.nn.embedding_lookup(embeddings, input_token_idxs)\n",
    "\n",
    "    # get sentence embedding\n",
    "    _, enc_state = dynamic_bi_rnn(enc_input, sent_l, config.dim_hidden, t_variables['keep_prob'])\n",
    "\n",
    "    # TODO House Holder flow\n",
    "    hidden_topic_infer =  tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='hidden_topic_infer')(enc_state)\n",
    "    prob_topic_infer = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic_infer')(hidden_topic_infer)\n",
    "\n",
    "    w_mean_topic_infer = tf.get_variable('mean_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32)\n",
    "    b_mean_topic_infer = tf.get_variable('mean_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32)\n",
    "    means_topic_infer = tf.tensordot(enc_state, w_mean_topic_infer, axes=[[1], [1]]) + b_mean_topic_infer\n",
    "    \n",
    "    w_logvar_topic_infer = tf.get_variable('logvar_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    b_logvar_topic_infer = tf.get_variable('logvar_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    logvars_topic_infer = tf.tensordot(enc_state, w_logvar_topic_infer, axes=[[1], [1]]) + b_logvar_topic_infer\n",
    "    sigma_topic_infer = tf.exp(0.5 * logvars_topic_infer)\n",
    "    gauss_topic_infer = tfd.Normal(loc=means_topic_infer, scale=sigma_topic_infer)\n",
    "    \n",
    "    # latent vectors from each gaussian dist.\n",
    "    latents_topic_infer = sample_latents(means_topic_infer, logvars_topic_infer) \n",
    "    # latent vector from gaussian mixture\n",
    "    latents_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), latents_topic_infer, transpose_a=True)\n",
    "    \n",
    "    # for beam search\n",
    "    means_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), means_topic_infer, transpose_a=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for decoding\n",
    "dec_sent_l = tf.add(sent_l, 1)\n",
    "dec_input_idxs = t_variables['dec_input_idxs']\n",
    "dec_input = tf.nn.embedding_lookup(embeddings, dec_input_idxs)\n",
    "\n",
    "dec_latents_input = tf.tile(latents_input, [1, tf.shape(dec_input)[1], 1])\n",
    "dec_concat_input = tf.concat([dec_input, dec_latents_input], -1)\n",
    "\n",
    "# decode for training\n",
    "with tf.variable_scope('sent/dec/rnn', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32, reuse=False):\n",
    "    dec_cell = tf.contrib.rnn.GRUCell(config.dim_hidden)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob = t_variables['keep_prob'])\n",
    "\n",
    "    dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(latents_input, 1))\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_concat_input, sequence_length=dec_sent_l)\n",
    "\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=dec_cell,\n",
    "        helper=helper,\n",
    "        initial_state=dec_initial_state)\n",
    "\n",
    "    dec_outputs, _, output_sent_l = tf.contrib.seq2seq.dynamic_decode(train_decoder)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(config.n_vocab, use_bias=False, name='out')\n",
    "    output_logits = output_layer(dec_outputs.rnn_output)\n",
    "    \n",
    "    output_token_idxs = tf.argmax(output_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tokens = tf.fill([batch_l], config.BOS_IDX)\n",
    "end_token = config.EOS_IDX\n",
    "\n",
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    infer_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(means_input, 1))\n",
    "    beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(infer_dec_initial_state, multiplier=config.beam_width)\n",
    "    beam_latents_input = tf.contrib.seq2seq.tile_batch(tf.squeeze(means_input, 1), multiplier=config.beam_width) # added\n",
    "    \n",
    "    beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=beam_latents_input)\n",
    "\n",
    "    beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    beam_output_token_idxs = beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    inter_means_input = tf.placeholder(tf.float32, [None, config.dim_latent])\n",
    "    \n",
    "    inter_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(inter_means_input)\n",
    "    inter_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(inter_dec_initial_state, multiplier=config.beam_width)\n",
    "    inter_beam_latents_input = tf.contrib.seq2seq.tile_batch(inter_means_input, multiplier=config.beam_width) # added\n",
    "    \n",
    "    inter_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=inter_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=inter_beam_latents_input)\n",
    "\n",
    "    inter_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inter_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    inter_beam_output_token_idxs = inter_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    topic_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic)\n",
    "    topic_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(topic_dec_initial_state, multiplier=config.beam_width)\n",
    "    topic_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic, multiplier=config.beam_width) # added\n",
    "    \n",
    "    topic_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=topic_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=topic_beam_latents_input)\n",
    "\n",
    "    topic_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        topic_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    topic_beam_output_token_idxs = topic_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    means_topic_summary = tf.reduce_mean(means_topic_infer, 0)\n",
    "    \n",
    "    summary_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic_summary)\n",
    "    summary_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(summary_dec_initial_state, multiplier=config.beam_width)\n",
    "    summary_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic_summary, multiplier=config.beam_width) # added\n",
    "    \n",
    "    summary_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=summary_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width,\n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=summary_beam_latents_input)\n",
    "\n",
    "    summary_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        summary_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    summary_beam_output_token_idxs = summary_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language modeling cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target and mask\n",
    "dec_target_idxs = t_variables['dec_target_idxs']\n",
    "dec_mask_tokens = tf.sequence_mask(dec_sent_l, maxlen=max_sent_l+1, dtype=tf.float32)\n",
    "\n",
    "# nll for each token (averaged over batch & sentence)\n",
    "sent_loss_recon = tf.contrib.seq2seq.sequence_loss(output_logits, dec_target_idxs, dec_mask_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_l = t_variables['doc_l']\n",
    "mask_sents = tf.sequence_mask(doc_l)\n",
    "mask_sents_flatten = tf.reshape(mask_sents, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1]])\n",
    "\n",
    "prob_topic_tiled = tf.tile(tf.expand_dims(prob_topic, 1), [1, tf.shape(mask_sents)[1], 1])\n",
    "prob_topic_flatten = tf.reshape(prob_topic_tiled, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1], config.n_topic])\n",
    "prob_topic_sents = tf.boolean_mask(prob_topic_flatten, mask_sents_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferred mixture probabilities (computed for each sentence)\n",
    "categ_topic_infer = tfd.Categorical(probs=prob_topic_infer)\n",
    "\n",
    "# prior of mixture probabilities (computed for each document, tiled for each sentence)\n",
    "categ_topic = tfd.Categorical(probs=prob_topic_sents)\n",
    "\n",
    "sent_loss_kl_categ = tf.reduce_mean(tfd.kl_divergence(categ_topic_infer, categ_topic))\n",
    "\n",
    "# inference of each gaussian gaussribution (computed for each sentence)\n",
    "\n",
    "sent_loss_kl_gauss = tf.reduce_sum(tfd.kl_divergence(gauss_topic_infer, gauss_topic), -1)\n",
    "sent_loss_kl_gmm = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss), -1))\n",
    "\n",
    "sent_loss_kl = sent_loss_kl_categ + sent_loss_kl_gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "tau = tf.cast(tf.divide(tf.mod(global_step, tf.constant(config.cycle_steps)), tf.constant(config.cycle_steps)), dtype=tf.float32)\n",
    "beta = tf.minimum(1., tau/config.r_cycle)\n",
    "\n",
    "sent_loss = sent_loss_recon + beta * sent_loss_kl\n",
    "\n",
    "topic_loss = topic_loss_recon + topic_loss_kl + config.reg * topic_loss_reg\n",
    "loss = topic_loss + sent_loss\n",
    "\n",
    "# define optimizer\n",
    "if config.opt == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif config.opt == 'Adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "    \n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_sents(token_idxs, config, idx_to_word):\n",
    "    sents = []\n",
    "    for sent_idxs in token_idxs:\n",
    "        tokens = []\n",
    "        for idx in sent_idxs:\n",
    "            if idx == config.EOS_IDX: break\n",
    "            tokens.append(idx_to_word[idx])\n",
    "        sent = ' '.join(tokens)\n",
    "        sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, ppls_batch \\\n",
    "            = sess.run([loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, topic_ppls], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "    loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, sent_loss_recon_mean, sent_loss_kl_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    return loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, sent_loss_recon_mean, sent_loss_kl_mean, ppl_mean\n",
    "\n",
    "def get_all_losses(sess, batches):\n",
    "    losses = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch = \\\n",
    "        sess.run([loss, topic_loss_recon, topic_loss_kl, sent_loss_recon, sent_loss_kl], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "    print('LOSS %.2f | TM NLL: %.2f, KL: %.4f | LM NLL: %.2f, KL: %.4f' %  np.mean(losses, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_batch):\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    pred_token_idxs = sess.run(output_token_idxs, feed_dict = feed_dict)\n",
    "    true_token_idxs = [sent_idxs for instance in sample_batch for sent_idxs in instance.token_idxs]\n",
    "    \n",
    "    assert len(pred_token_idxs) == len(true_token_idxs)\n",
    "    \n",
    "    pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "    true_sents = idxs_to_sents(true_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    for i, (true_sent, pred_sent) in enumerate(zip(true_sents, pred_sents)):        \n",
    "        print(i, 'TRUE: %s' % true_sent)\n",
    "        print(i, 'PRED: %s' % pred_sent)\n",
    "\n",
    "def print_topic_sample():\n",
    "    pred_topics_freq_bow_indices, pred_topic_token_idxs = sess.run([topics_freq_bow_indices, topic_beam_output_token_idxs], \n",
    "                                                                                                           feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "    pred_topic_sents = idxs_to_sents(pred_topic_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Topic Samples-----------')\n",
    "    for i, (topic_freq_bow_idxs, pred_topic_sent) in enumerate(zip(topics_freq_bow_idxs, pred_topic_sents)):\n",
    "        print(i, ' BOW:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' SENTENCE:', pred_topic_sent)\n",
    "        \n",
    "def print_summary(test_batch):\n",
    "    feed_dict = get_feed_dict(test_batch)\n",
    "    feed_dict[t_variables['batch_l']] = config.n_topic\n",
    "    feed_dict[t_variables['keep_prob']] = 1.\n",
    "    pred_topics_freq_bow_indices, pred_summary_token_idxs = sess.run([topics_freq_bow_indices, summary_beam_output_token_idxs], feed_dict=feed_dict)\n",
    "    pred_summary_sents = idxs_to_sents(pred_summary_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Output sentences for each topic-----------')\n",
    "    print('Item idx:', test_batch[0].item_idx)\n",
    "    for i, (topic_freq_bow_idxs, pred_summary_sent) in enumerate(zip(topics_freq_bow_idxs, pred_summary_sents)):\n",
    "        print(i, ' BOW:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' SENTENCE:', pred_summary_sent)\n",
    "        \n",
    "    print('-----------Summaries-----------')\n",
    "    for i, summary in enumerate(test_batch[0].summaries):\n",
    "        print('SUMMARY %i :'%i, '\\n', summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "saver = tf.train.Saver(max_to_keep=10)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','TM','','','','LM','','VALID:','TM','','','','LM','', ''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL','REG','NLL','KL','LOSS','PPL','NLL','KL','REG','NLL','KL', 'Beta']]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>LM</th>\n",
       "      <th></th>\n",
       "      <th>VALID:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>LM</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>137.42</td>\n",
       "      <td>1034</td>\n",
       "      <td>126.92</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.90</td>\n",
       "      <td>9.13</td>\n",
       "      <td>1.46</td>\n",
       "      <td>126.64</td>\n",
       "      <td>1035</td>\n",
       "      <td>116.21</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.90</td>\n",
       "      <td>9.13</td>\n",
       "      <td>1.42</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>128.30</td>\n",
       "      <td>776</td>\n",
       "      <td>118.94</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.84</td>\n",
       "      <td>8.28</td>\n",
       "      <td>1.38</td>\n",
       "      <td>113.58</td>\n",
       "      <td>550</td>\n",
       "      <td>105.81</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.78</td>\n",
       "      <td>6.77</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>124.32</td>\n",
       "      <td>660</td>\n",
       "      <td>115.97</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.80</td>\n",
       "      <td>7.31</td>\n",
       "      <td>1.19</td>\n",
       "      <td>112.11</td>\n",
       "      <td>530</td>\n",
       "      <td>105.19</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.73</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>1500</td>\n",
       "      <td>122.93</td>\n",
       "      <td>622</td>\n",
       "      <td>115.06</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.77</td>\n",
       "      <td>6.87</td>\n",
       "      <td>1.01</td>\n",
       "      <td>111.86</td>\n",
       "      <td>526</td>\n",
       "      <td>105.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.68</td>\n",
       "      <td>5.83</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>121.81</td>\n",
       "      <td>603</td>\n",
       "      <td>114.21</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.74</td>\n",
       "      <td>6.62</td>\n",
       "      <td>0.89</td>\n",
       "      <td>111.71</td>\n",
       "      <td>525</td>\n",
       "      <td>105.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.62</td>\n",
       "      <td>5.77</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>121.55</td>\n",
       "      <td>596</td>\n",
       "      <td>114.05</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.72</td>\n",
       "      <td>6.52</td>\n",
       "      <td>0.84</td>\n",
       "      <td>111.68</td>\n",
       "      <td>525</td>\n",
       "      <td>105.06</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.60</td>\n",
       "      <td>5.75</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2776</th>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>121.01</td>\n",
       "      <td>588</td>\n",
       "      <td>113.66</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.69</td>\n",
       "      <td>6.40</td>\n",
       "      <td>0.75</td>\n",
       "      <td>111.65</td>\n",
       "      <td>528</td>\n",
       "      <td>105.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.54</td>\n",
       "      <td>5.73</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3276</th>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>120.85</td>\n",
       "      <td>581</td>\n",
       "      <td>113.61</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.67</td>\n",
       "      <td>6.31</td>\n",
       "      <td>0.68</td>\n",
       "      <td>111.57</td>\n",
       "      <td>525</td>\n",
       "      <td>105.07</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.50</td>\n",
       "      <td>5.72</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3776</th>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>120.59</td>\n",
       "      <td>576</td>\n",
       "      <td>113.44</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.64</td>\n",
       "      <td>6.24</td>\n",
       "      <td>0.63</td>\n",
       "      <td>111.39</td>\n",
       "      <td>522</td>\n",
       "      <td>104.96</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.46</td>\n",
       "      <td>5.71</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4276</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>120.42</td>\n",
       "      <td>573</td>\n",
       "      <td>113.34</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.62</td>\n",
       "      <td>6.18</td>\n",
       "      <td>0.58</td>\n",
       "      <td>111.32</td>\n",
       "      <td>521</td>\n",
       "      <td>104.91</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.43</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4551</th>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>120.35</td>\n",
       "      <td>571</td>\n",
       "      <td>113.31</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.61</td>\n",
       "      <td>6.16</td>\n",
       "      <td>0.55</td>\n",
       "      <td>111.33</td>\n",
       "      <td>522</td>\n",
       "      <td>104.93</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.41</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5051</th>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>120.12</td>\n",
       "      <td>569</td>\n",
       "      <td>113.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.59</td>\n",
       "      <td>6.12</td>\n",
       "      <td>0.51</td>\n",
       "      <td>111.25</td>\n",
       "      <td>520</td>\n",
       "      <td>104.87</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.40</td>\n",
       "      <td>5.69</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5551</th>\n",
       "      <td>54</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>120.07</td>\n",
       "      <td>566</td>\n",
       "      <td>113.13</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.57</td>\n",
       "      <td>6.09</td>\n",
       "      <td>0.48</td>\n",
       "      <td>111.26</td>\n",
       "      <td>521</td>\n",
       "      <td>104.91</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.37</td>\n",
       "      <td>5.69</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6051</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>1500</td>\n",
       "      <td>119.95</td>\n",
       "      <td>564</td>\n",
       "      <td>113.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.55</td>\n",
       "      <td>6.06</td>\n",
       "      <td>0.45</td>\n",
       "      <td>111.20</td>\n",
       "      <td>520</td>\n",
       "      <td>104.89</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.35</td>\n",
       "      <td>5.68</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6551</th>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>119.91</td>\n",
       "      <td>563</td>\n",
       "      <td>113.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.53</td>\n",
       "      <td>6.04</td>\n",
       "      <td>0.43</td>\n",
       "      <td>111.11</td>\n",
       "      <td>519</td>\n",
       "      <td>104.83</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.31</td>\n",
       "      <td>5.68</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6826</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>119.85</td>\n",
       "      <td>562</td>\n",
       "      <td>113.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.53</td>\n",
       "      <td>6.03</td>\n",
       "      <td>0.41</td>\n",
       "      <td>111.08</td>\n",
       "      <td>519</td>\n",
       "      <td>104.81</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.30</td>\n",
       "      <td>5.68</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7326</th>\n",
       "      <td>54</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>119.73</td>\n",
       "      <td>560</td>\n",
       "      <td>112.91</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.51</td>\n",
       "      <td>6.01</td>\n",
       "      <td>0.39</td>\n",
       "      <td>111.08</td>\n",
       "      <td>520</td>\n",
       "      <td>104.83</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.28</td>\n",
       "      <td>5.68</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7826</th>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>119.65</td>\n",
       "      <td>559</td>\n",
       "      <td>112.86</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.49</td>\n",
       "      <td>5.99</td>\n",
       "      <td>0.37</td>\n",
       "      <td>111.04</td>\n",
       "      <td>517</td>\n",
       "      <td>104.79</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.27</td>\n",
       "      <td>5.67</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8326</th>\n",
       "      <td>54</td>\n",
       "      <td>3</td>\n",
       "      <td>1500</td>\n",
       "      <td>119.61</td>\n",
       "      <td>557</td>\n",
       "      <td>112.84</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.48</td>\n",
       "      <td>5.98</td>\n",
       "      <td>0.36</td>\n",
       "      <td>111.04</td>\n",
       "      <td>518</td>\n",
       "      <td>104.81</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.26</td>\n",
       "      <td>5.67</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8826</th>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>119.55</td>\n",
       "      <td>556</td>\n",
       "      <td>112.80</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.47</td>\n",
       "      <td>5.96</td>\n",
       "      <td>0.34</td>\n",
       "      <td>110.98</td>\n",
       "      <td>517</td>\n",
       "      <td>104.75</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.25</td>\n",
       "      <td>5.67</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9101</th>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>119.54</td>\n",
       "      <td>556</td>\n",
       "      <td>112.80</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.46</td>\n",
       "      <td>5.96</td>\n",
       "      <td>0.34</td>\n",
       "      <td>111.07</td>\n",
       "      <td>518</td>\n",
       "      <td>104.83</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.24</td>\n",
       "      <td>5.67</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9601</th>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "      <td>119.44</td>\n",
       "      <td>554</td>\n",
       "      <td>112.73</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.45</td>\n",
       "      <td>5.94</td>\n",
       "      <td>0.32</td>\n",
       "      <td>110.82</td>\n",
       "      <td>512</td>\n",
       "      <td>104.59</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.22</td>\n",
       "      <td>5.66</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10101</th>\n",
       "      <td>55</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>119.46</td>\n",
       "      <td>553</td>\n",
       "      <td>112.76</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.44</td>\n",
       "      <td>5.93</td>\n",
       "      <td>0.31</td>\n",
       "      <td>110.91</td>\n",
       "      <td>514</td>\n",
       "      <td>104.68</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.20</td>\n",
       "      <td>5.66</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10601</th>\n",
       "      <td>54</td>\n",
       "      <td>4</td>\n",
       "      <td>1500</td>\n",
       "      <td>119.37</td>\n",
       "      <td>552</td>\n",
       "      <td>112.68</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.43</td>\n",
       "      <td>5.92</td>\n",
       "      <td>0.30</td>\n",
       "      <td>110.90</td>\n",
       "      <td>513</td>\n",
       "      <td>104.67</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.19</td>\n",
       "      <td>5.66</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11101</th>\n",
       "      <td>55</td>\n",
       "      <td>4</td>\n",
       "      <td>2000</td>\n",
       "      <td>119.33</td>\n",
       "      <td>551</td>\n",
       "      <td>112.65</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.42</td>\n",
       "      <td>5.92</td>\n",
       "      <td>0.30</td>\n",
       "      <td>110.85</td>\n",
       "      <td>513</td>\n",
       "      <td>104.60</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.18</td>\n",
       "      <td>5.66</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11376</th>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>119.30</td>\n",
       "      <td>551</td>\n",
       "      <td>112.63</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.41</td>\n",
       "      <td>5.91</td>\n",
       "      <td>0.29</td>\n",
       "      <td>110.72</td>\n",
       "      <td>510</td>\n",
       "      <td>104.58</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.66</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11876</th>\n",
       "      <td>63</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>119.24</td>\n",
       "      <td>549</td>\n",
       "      <td>112.59</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.40</td>\n",
       "      <td>5.90</td>\n",
       "      <td>0.28</td>\n",
       "      <td>110.67</td>\n",
       "      <td>508</td>\n",
       "      <td>104.48</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.16</td>\n",
       "      <td>5.65</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12376</th>\n",
       "      <td>63</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>119.22</td>\n",
       "      <td>548</td>\n",
       "      <td>112.57</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.39</td>\n",
       "      <td>5.90</td>\n",
       "      <td>0.28</td>\n",
       "      <td>110.64</td>\n",
       "      <td>508</td>\n",
       "      <td>104.43</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.15</td>\n",
       "      <td>5.65</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12876</th>\n",
       "      <td>54</td>\n",
       "      <td>5</td>\n",
       "      <td>1500</td>\n",
       "      <td>119.17</td>\n",
       "      <td>547</td>\n",
       "      <td>112.53</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>5.89</td>\n",
       "      <td>0.27</td>\n",
       "      <td>110.76</td>\n",
       "      <td>511</td>\n",
       "      <td>104.52</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.15</td>\n",
       "      <td>5.65</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13376</th>\n",
       "      <td>54</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>119.13</td>\n",
       "      <td>546</td>\n",
       "      <td>112.50</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.37</td>\n",
       "      <td>5.88</td>\n",
       "      <td>0.27</td>\n",
       "      <td>110.68</td>\n",
       "      <td>507</td>\n",
       "      <td>104.42</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.15</td>\n",
       "      <td>5.65</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13651</th>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>119.09</td>\n",
       "      <td>545</td>\n",
       "      <td>112.46</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.37</td>\n",
       "      <td>5.88</td>\n",
       "      <td>0.27</td>\n",
       "      <td>110.67</td>\n",
       "      <td>508</td>\n",
       "      <td>104.41</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.14</td>\n",
       "      <td>5.65</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14151</th>\n",
       "      <td>66</td>\n",
       "      <td>6</td>\n",
       "      <td>500</td>\n",
       "      <td>119.05</td>\n",
       "      <td>545</td>\n",
       "      <td>112.43</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.36</td>\n",
       "      <td>5.87</td>\n",
       "      <td>0.27</td>\n",
       "      <td>110.64</td>\n",
       "      <td>505</td>\n",
       "      <td>104.35</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.14</td>\n",
       "      <td>5.65</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14651</th>\n",
       "      <td>53</td>\n",
       "      <td>6</td>\n",
       "      <td>1000</td>\n",
       "      <td>119.01</td>\n",
       "      <td>543</td>\n",
       "      <td>112.39</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.35</td>\n",
       "      <td>5.87</td>\n",
       "      <td>0.26</td>\n",
       "      <td>110.65</td>\n",
       "      <td>506</td>\n",
       "      <td>104.33</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.13</td>\n",
       "      <td>5.65</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     TRAIN:    TM                        LM        VALID:  \\\n",
       "      Time Ep    Ct    LOSS   PPL     NLL    KL   REG   NLL    KL    LOSS   \n",
       "1       20  0     0  137.42  1034  126.92  0.47  0.90  9.13  1.46  126.64   \n",
       "501     70  0   500  128.30   776  118.94  0.19  0.84  8.28  1.38  113.58   \n",
       "1001    67  0  1000  124.32   660  115.97  0.15  0.80  7.31  1.19  112.11   \n",
       "1501    65  0  1500  122.93   622  115.06  0.13  0.77  6.87  1.01  111.86   \n",
       "2001    63  0  2000  121.81   603  114.21  0.12  0.74  6.62  0.89  111.71   \n",
       "2276    41  1     0  121.55   596  114.05  0.12  0.72  6.52  0.84  111.68   \n",
       "2776    65  1   500  121.01   588  113.66  0.12  0.69  6.40  0.75  111.65   \n",
       "3276    65  1  1000  120.85   581  113.61  0.13  0.67  6.31  0.68  111.57   \n",
       "3776    64  1  1500  120.59   576  113.44  0.13  0.64  6.24  0.63  111.39   \n",
       "4276    70  1  2000  120.42   573  113.34  0.13  0.62  6.18  0.58  111.32   \n",
       "4551    30  2     0  120.35   571  113.31  0.13  0.61  6.16  0.55  111.33   \n",
       "5051    69  2   500  120.12   569  113.14  0.14  0.59  6.12  0.51  111.25   \n",
       "5551    54  2  1000  120.07   566  113.13  0.14  0.57  6.09  0.48  111.26   \n",
       "6051    64  2  1500  119.95   564  113.05  0.15  0.55  6.06  0.45  111.20   \n",
       "6551    74  2  2000  119.91   563  113.05  0.15  0.53  6.04  0.43  111.11   \n",
       "6826    42  3     0  119.85   562  113.00  0.16  0.53  6.03  0.41  111.08   \n",
       "7326    54  3   500  119.73   560  112.91  0.16  0.51  6.01  0.39  111.08   \n",
       "7826    64  3  1000  119.65   559  112.86  0.17  0.49  5.99  0.37  111.04   \n",
       "8326    54  3  1500  119.61   557  112.84  0.18  0.48  5.98  0.36  111.04   \n",
       "8826    82  3  2000  119.55   556  112.80  0.18  0.47  5.96  0.34  110.98   \n",
       "9101    30  4     0  119.54   556  112.80  0.19  0.46  5.96  0.34  111.07   \n",
       "9601    64  4   500  119.44   554  112.73  0.20  0.45  5.94  0.32  110.82   \n",
       "10101   55  4  1000  119.46   553  112.76  0.20  0.44  5.93  0.31  110.91   \n",
       "10601   54  4  1500  119.37   552  112.68  0.21  0.43  5.92  0.30  110.90   \n",
       "11101   55  4  2000  119.33   551  112.65  0.22  0.42  5.92  0.30  110.85   \n",
       "11376   40  5     0  119.30   551  112.63  0.23  0.41  5.91  0.29  110.72   \n",
       "11876   63  5   500  119.24   549  112.59  0.24  0.40  5.90  0.28  110.67   \n",
       "12376   63  5  1000  119.22   548  112.57  0.25  0.39  5.90  0.28  110.64   \n",
       "12876   54  5  1500  119.17   547  112.53  0.26  0.38  5.89  0.27  110.76   \n",
       "13376   54  5  2000  119.13   546  112.50  0.27  0.37  5.88  0.27  110.68   \n",
       "13651   30  6     0  119.09   545  112.46  0.27  0.37  5.88  0.27  110.67   \n",
       "14151   66  6   500  119.05   545  112.43  0.28  0.36  5.87  0.27  110.64   \n",
       "14651   53  6  1000  119.01   543  112.39  0.29  0.35  5.87  0.26  110.65   \n",
       "\n",
       "         TM                        LM               \n",
       "        PPL     NLL    KL   REG   NLL    KL   Beta  \n",
       "1      1035  116.21  0.40  0.90  9.13  1.42  0.000  \n",
       "501     550  105.81  0.11  0.78  6.77  1.31  0.088  \n",
       "1001    530  105.19  0.07  0.73  6.00  0.73  0.176  \n",
       "1501    526  105.09  0.09  0.68  5.83  0.61  0.264  \n",
       "2001    525  105.07  0.08  0.62  5.77  0.44  0.352  \n",
       "2276    525  105.06  0.11  0.60  5.75  0.39  0.400  \n",
       "2776    528  105.11  0.11  0.54  5.73  0.32  0.488  \n",
       "3276    525  105.07  0.12  0.50  5.72  0.28  0.576  \n",
       "3776    522  104.96  0.12  0.46  5.71  0.22  0.664  \n",
       "4276    521  104.91  0.14  0.43  5.70  0.18  0.752  \n",
       "4551    522  104.93  0.15  0.41  5.70  0.17  0.800  \n",
       "5051    520  104.87  0.16  0.40  5.69  0.14  0.888  \n",
       "5551    521  104.91  0.16  0.37  5.69  0.13  0.976  \n",
       "6051    520  104.89  0.16  0.35  5.68  0.11  1.000  \n",
       "6551    519  104.83  0.18  0.31  5.68  0.10  1.000  \n",
       "6826    519  104.81  0.18  0.30  5.68  0.11  1.000  \n",
       "7326    520  104.83  0.20  0.28  5.68  0.09  1.000  \n",
       "7826    517  104.79  0.20  0.27  5.67  0.10  1.000  \n",
       "8326    518  104.81  0.20  0.26  5.67  0.10  1.000  \n",
       "8826    517  104.75  0.22  0.25  5.67  0.10  1.000  \n",
       "9101    518  104.83  0.23  0.24  5.67  0.09  1.000  \n",
       "9601    512  104.59  0.26  0.22  5.66  0.09  1.000  \n",
       "10101   514  104.68  0.27  0.20  5.66  0.09  1.000  \n",
       "10601   513  104.67  0.28  0.19  5.66  0.11  1.000  \n",
       "11101   513  104.60  0.30  0.18  5.66  0.11  1.000  \n",
       "11376   510  104.58  0.31  0.17  5.66  0.12  0.000  \n",
       "11876   508  104.48  0.36  0.16  5.65  0.14  0.088  \n",
       "12376   508  104.43  0.38  0.15  5.65  0.15  0.176  \n",
       "12876   511  104.52  0.39  0.15  5.65  0.16  0.264  \n",
       "13376   507  104.42  0.40  0.15  5.65  0.18  0.352  \n",
       "13651   508  104.41  0.40  0.14  5.65  0.17  0.400  \n",
       "14151   505  104.35  0.43  0.14  5.65  0.15  0.488  \n",
       "14651   506  104.33  0.44  0.13  5.65  0.18  0.576  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Output sentences for each topic-----------\n",
      "Item idx: B000VB7EFW\n",
      "0  BOW: ! love cover color perfectly air pro ; & hard\n",
      "0  SENTENCE: i is is the the\n",
      "1  BOW: center average insert batteries organize removable mp essentials airplane tall\n",
      "1  SENTENCE: i is is the the\n",
      "2  BOW: cover & ; color ! keyboard pro love mac perfectly\n",
      "2  SENTENCE: i is is the the\n",
      "3  BOW: cable foam everyday purpose attractive daily storage holding expect usb\n",
      "3  SENTENCE: i is is the the\n",
      "4  BOW: bought price nice quality perfect love recommend protection perfectly time\n",
      "4  SENTENCE: i is is the the\n",
      "5  BOW: back made inside - pockets pocket small lot big put\n",
      "5  SENTENCE: i is is the the\n",
      "6  BOW: sleeve quality nice perfect price 'm protection recommend bought ...\n",
      "6  SENTENCE: i is is the the\n",
      "7  BOW: ripped load plane sd shoulders textbooks headphones cable bill middle\n",
      "7  SENTENCE: i is is the the\n",
      "8  BOW: carry nice pocket pockets zipper strap hold - work handle\n",
      "8  SENTENCE: i is is the the\n",
      "9  BOW: room strap pockets small pocket hold compartment back inside shoulder\n",
      "9  SENTENCE: i is is the the\n",
      "-----------Summaries-----------\n",
      "SUMMARY 0 : \n",
      " This is a very well made bag, nice construction, lots of pockets.\n",
      "the straps are very comfortable.\n",
      "and protects everything inside.\n",
      "It says\n",
      "it fits a 17inch notebook,\n",
      "however it did not.\n",
      "after using the pack for less than a month,\n",
      "it is ripping out already.\n",
      "SUMMARY 1 : \n",
      " This is a very well made bag, nice construction, lots of pockets.\n",
      "The quality is excellent\n",
      "and it is very durable.\n",
      "the straps are very comfortable.\n",
      "and protects everything inside.\n",
      "The color is a true red\n",
      "and it fits nicely.\n",
      "it is ripping out already.\n",
      "and it doesnt fit.\n",
      "It's just not the lightest backpack\n",
      "because some zipper teeth were not aligned.\n",
      "SUMMARY 2 : \n",
      " The quality is excellent\n",
      "and I love all the pockets and compartments.\n",
      "and it is very durable.\n",
      "and can't beleive the price\n",
      "and protects everything inside.\n",
      "The laptop\n",
      "doesn't fit in it.\n",
      "it is ripping out already.\n",
      "It's just not the lightest backpack\n",
      "0 TRUE: so , my wife -lrb- love ya <unk> . . . -rrb- accidentally dropped my mbp onto the <unk>\n",
      "0 PRED: i is the the the the the\n",
      "1 TRUE: it was in the marware sportfolio sleeve at the time , so things could have been much worse\n",
      "1 PRED: i is is a the the\n",
      "2 TRUE: but despite being in the sleeve , the front corner was bent up pretty badly - a $ # + repair , at minimum\n",
      "2 PRED: i is is the the the the\n",
      "3 TRUE: which is the biggest flaw of this case\n",
      "3 PRED: i is the the the\n",
      "4 TRUE: if it had fallen on one of the back corners , it may have <unk> better as it has heavy rubber padding around the back and sides\n",
      "4 PRED: i is is a\n",
      "5 TRUE: but the front corners are not nearly as protected due to the zipper\n",
      "5 PRED: i is the the the\n",
      "6 TRUE: i do n't know if this could be designed better , but the lack of protection on the front end is disappointing\n",
      "6 PRED: i is the is the the\n",
      "7 TRUE: -lrb- sidenote : the little pocket is virtually worthless\n",
      "7 PRED: i is is the the\n",
      "8 TRUE: the case is adorable\n",
      "8 PRED: i is is the the ,\n",
      "9 TRUE: the pockets are fantastic\n",
      "9 PRED: i is a the the the\n",
      "10 TRUE: it 's very bright though ... much brighter then it appears on the computer\n",
      "10 PRED: i is the a the , ,\n",
      "11 TRUE: however , it stinks\n",
      "11 PRED: i is is a the the the\n",
      "12 TRUE: has this strange chemical smell to it\n",
      "12 PRED: i is is a the the the\n",
      "13 TRUE: i guess a little febreeze may help\n",
      "13 PRED: i is is the the , the the the\n",
      "14 TRUE: i thought if i just let it sit out in the air the smell would go away\n",
      "14 PRED: i is the the the the\n",
      "15 TRUE: but # weeks later and it 's still there\n",
      "15 PRED: i is the a\n",
      "16 TRUE: it 's <unk>\n",
      "16 PRED: i is is the the the the the\n",
      "17 TRUE: very well made\n",
      "17 PRED: i is is the the\n",
      "18 TRUE: like the other buyer said\n",
      "18 PRED: i is is a the\n",
      "19 TRUE: it can fit multiple laptops with room to spare\n",
      "19 PRED: i is is the the the\n",
      "20 TRUE: i 'm # ft # in\n",
      "20 PRED: i is the the the the\n",
      "21 TRUE: and , i still find it big\n",
      "21 PRED: i is is the\n",
      "22 TRUE: but it was the only bag that will fit my dell m # . # inch work laptop\n",
      "22 PRED: i is is the\n",
      "23 TRUE: the case looks much more expensive than it is\n",
      "23 PRED: i is is the\n",
      "24 TRUE: the material where your wrists set is soft and has not picked up dirt like i thought it would\n",
      "24 PRED: i is the a the\n",
      "25 TRUE: yes it hides that i am using a mac when i am at starbucks and i am great with that , one of the reasons that i got this case\n",
      "25 PRED: i is the the the\n",
      "26 TRUE: hopefully someone wo n't just grab it and run because they see the apple logo\n",
      "26 PRED: i is is a the the the the\n",
      "27 TRUE: the only reason this did n't get # stars is the elastic straps that hold the top of the case to the screen cover up the corners just a touch\n",
      "27 PRED: i is is the the the\n",
      "28 TRUE: overall i am very pleased with this case for my ridiculously expensive mac , seems to provide good protection for it\n",
      "28 PRED: i is is the the\n",
      "29 TRUE: i bought this for all my gear and it fits\n",
      "29 PRED: i is is the the\n",
      "30 TRUE: i was able to put in my : sony camcorder , canon sd # , canon g # , <unk> housing for the two cameras , and # lenses for the g #\n",
      "30 PRED: i is is the the\n",
      "31 TRUE: i thought the bag was comfortable , but my husband thought it was too small for him\n",
      "31 PRED: i is the the the the the\n",
      "32 TRUE: i 'm # ' # `` / average and he 's # ' # / # lbs , with broad shoulders\n",
      "32 PRED: i is is the the the the\n",
      "33 TRUE: i would suggest taking off the canon logo on the bag just to deter theft\n",
      "33 PRED: i is is a the\n",
      "34 TRUE: i was n't aware it was used .\n",
      "34 PRED: i is a a the the the\n",
      "35 TRUE: it has dents , and there is a stain on the keyboard\n",
      "35 PRED: i is the the the\n",
      "36 TRUE: that was kind of a bummer .\n",
      "36 PRED: i is is the the the the\n",
      "37 TRUE: i wo n't order through this person again , but the company that makes the cases are excellent\n",
      "37 PRED: i is is the the the the the\n",
      "38 TRUE: i 'll be going through the actual manufacturer next time i want a new color .\n",
      "38 PRED: i is is a\n",
      "39 TRUE: i originally wrote a negative review because i got the wrong item in the mail\n",
      "39 PRED: i is is the the the\n",
      "40 TRUE: after writing the native review i got contacted by the seller and he shipped me the correct item at no additional charge\n",
      "40 PRED: i is is a the the\n",
      "41 TRUE: the case itself fits great and i love the color\n",
      "41 PRED: i is is a the\n",
      "42 TRUE: the rubber keyboard cover however does not fit correctly\n",
      "42 PRED: i is is the the the ,\n",
      "43 TRUE: other than that i really love the case\n",
      "43 PRED: i is is the the\n",
      "44 TRUE: this is a very good looking case that fits the computer very very well\n",
      "44 PRED: i is is a\n",
      "45 TRUE: but the bottom portion latches to the computer right where you wrists rest when you type and the plastic pieces scratch and cut the wrist when typing\n",
      "45 PRED: i is is the\n",
      "46 TRUE: it is so incredibly uncomfortable\n",
      "46 PRED: i is a the the\n",
      "47 TRUE: so , i 'm going to have to send this one back :\n",
      "47 PRED: i is the the , the\n",
      "48 TRUE: case is nicely built , and has helped me organize my many sd cards\n",
      "48 PRED: i is is the the\n",
      "49 TRUE: holds about # cards\n",
      "49 PRED: i is is the a the\n",
      "50 TRUE: may be ordering a second case to use when traveling\n",
      "50 PRED: i is is the , the the the\n",
      "51 TRUE: would help me from misplacing my cards\n",
      "51 PRED: i is is a the the\n",
      "52 TRUE: advertised for its spacious interior space - not true\n",
      "52 PRED: i is the a the\n",
      "53 TRUE: wheels unbalanced and difficult to maneuver on all four ... find myself pulling it on two\n",
      "53 PRED: i is is the the the\n",
      "54 TRUE: a great disappointment and poor materials for the construction\n",
      "54 PRED: i is is the the and the\n",
      "55 TRUE: if you want a slim , tight fitting , durable , and sleek <unk> is it\n",
      "55 PRED: i is is the ,\n",
      "56 TRUE: strong , yet adds virtually no weight -- fits the # `` macbook pro perfectly and provides very good protection\n",
      "56 PRED: i is is a the\n",
      "57 TRUE: i think the perforations are important to heat <unk> <unk> <unk> other covers and add a <unk> `` <unk> `` look\n",
      "57 PRED: i is is the the , the\n",
      "58 TRUE: was worried the red leather tab would be too red , and it is , but love it\n",
      "58 PRED: i is the the the the\n",
      "59 TRUE: the pocket is too tight for a charger but fine for earbuds\n",
      "59 PRED: i is is the the the the\n",
      "60 TRUE: feels much nicer to walk out the door with this in hand than with a bigger case on a shoulder strap\n",
      "60 PRED: i is is the the\n",
      "61 TRUE: this year , my daughter really wanted a messenger style bag for school , once we ordered and received this item , she was really happy\n",
      "61 PRED: i is is a the the the the\n",
      "62 TRUE: the style and design of the back is quite fashionable , and the bag works just right for her needs\n",
      "62 PRED: i is is the\n",
      "63 TRUE: the greatest thing i bought for my daughters computer\n",
      "63 PRED: i is is the the\n",
      "64 TRUE: kids lover stickers on their laptops and this keeps it clean and she was able to dress up the cover instead\n",
      "64 PRED: i is is the the ,\n"
     ]
    }
   ],
   "source": [
    "if len(log_df) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "while epoch < config.epochs:\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = get_feed_dict(batch)\n",
    "\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, sent_loss_kl_categ_batch, sent_loss_kl_gmm_batch, ppls_batch = \\\n",
    "        sess.run([opt, loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, sent_loss_kl_categ, sent_loss_kl_gmm, topic_ppls], feed_dict = feed_dict)\n",
    "   \n",
    "        if sent_loss_kl_batch == np.inf:\n",
    "            print('Nan occured')\n",
    "            ckpt = tf.train.get_checkpoint_state(config.modeldir)\n",
    "            model_checkpoint_path = ckpt.all_model_checkpoint_paths[-1]\n",
    "            saver.restore(sess, model_checkpoint_path)            \n",
    "            break\n",
    "            \n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if ct%config.log_period==0:\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train, sent_loss_recon_train, sent_loss_kl_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, sent_loss_recon_dev, sent_loss_kl_dev, ppl_dev = get_loss(sess, dev_batches)\n",
    "            global_step_log, beta_eval = sess.run([tf.train.get_global_step(), beta])\n",
    "            \n",
    "            if loss_dev < loss_min:\n",
    "                loss_min = loss_dev\n",
    "                saver.save(sess, config.modelpath, global_step=global_step_log)\n",
    "\n",
    "            clear_output()\n",
    "    \n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_reg_train, '%.2f'%sent_loss_recon_train, '%.2f'%sent_loss_kl_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_reg_dev, '%.2f'%sent_loss_recon_dev, '%.2f'%sent_loss_kl_dev,  '%.3f'%beta_eval],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "\n",
    "            print_summary(test_batches[1][1])\n",
    "            print_sample(batch)\n",
    "            \n",
    "            time_start = time.time()\n",
    "            \n",
    "    epoch += 1\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_prob_topic, _prob_topic_sents, _prob_topic_infer, _means_topic_infer = debug_value([prob_topic, prob_topic_sents, prob_topic_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_i = 4\n",
    "_prob_topic_sents[batch_i], _prob_topic_infer[batch_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_means_topic_infer[0][:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_means_topic, b_means_topic = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"topic/dec/mean_topic\")\n",
    "\n",
    "pred_topic_embeddings, pred_topic_bow, pred_means_topic, pred_logvars_topic, pred_token_idxs, _w_means_topic, _b_means_topic, _w_mean_topic_infer = \\\n",
    "                                sess.run([topic_embeddings, topic_bow, means_topic, logvars_topic, topic_beam_output_token_idxs, w_means_topic, b_means_topic, w_mean_topic_infer], \n",
    "                                         feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "\n",
    "pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "\n",
    "pred_topics_freq_bow_indices = np.argsort(pred_topic_bow, 1)[:, ::-1][:, :10]\n",
    "pred_topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idxs in pred_topics_freq_bow_idxs:\n",
    "    print([idx_to_word[idx] for idx in idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_topic_embeddings[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_topic_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_w_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_w_mean_topic_infer[:, :10, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_enc_state_infer, _means_topic_infer = debug_value([enc_state_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_enc_state_infer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_means_topic_infer[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
