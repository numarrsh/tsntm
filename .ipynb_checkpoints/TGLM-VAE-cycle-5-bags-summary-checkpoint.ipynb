{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from data_structure import get_batches, get_test_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "from topic_model import TopicModel\n",
    "\n",
    "from topic_beam_search_decoder import BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '5', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/bags/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/tglm_vae_tmp', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'bags', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 50, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 500, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.1, 'lr')\n",
    "flags.DEFINE_float('reg', 1., 'regularization term')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_bool('warmup', True, 'flg of warming up')\n",
    "flags.DEFINE_integer('epochs_cycle', 5, 'number of epochs within a cycle')\n",
    "flags.DEFINE_float('r_cycle', 0.5, 'proportion used to increase beta within a cycle')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 10, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_bow', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_hidden_topic', 512, 'dim_hidden_topic')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "flags.DEFINE_bool('bidirectional', True, 'flg of bidirectional encoding')\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_test_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('PAD_IDX', word_to_idx[PAD], 'PAD_IDX')\n",
    "flags.DEFINE_integer('UNK_IDX', word_to_idx[UNK], 'UNK_IDX')\n",
    "flags.DEFINE_integer('BOS_IDX', word_to_idx[BOS], 'BOS_IDX')\n",
    "flags.DEFINE_integer('EOS_IDX', word_to_idx[EOS], 'EOS_IDX')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "maximum_iterations = max([max([instance.max_sent_l for instance in batch]) for ct, batch in dev_batches])\n",
    "flags.DEFINE_integer('maximum_iterations', maximum_iterations, 'maximum_iterations')\n",
    "\n",
    "flags.DEFINE_integer('cycle_steps', len(train_batches)*config.epochs_cycle, 'number of steps for each cycle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build language model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['bow'] = tf.placeholder(tf.float32, [None, config.dim_bow], name='bow')\n",
    "t_variables['input_token_idxs'] = tf.placeholder(tf.int32, [None, None], name='input_token_idxs')\n",
    "t_variables['dec_input_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_input_idxs')\n",
    "t_variables['dec_target_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_target_idxs')\n",
    "t_variables['batch_l'] = tf.placeholder(tf.int32, name='batch_l')\n",
    "t_variables['doc_l'] = tf.placeholder(tf.int32, [None], name='doc_l')\n",
    "t_variables['sent_l'] = tf.placeholder(tf.int32, [None], name='sent_l')\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train', assertion=False):\n",
    "    def token_dropout(sent_idxs):\n",
    "        sent_idxs_dropout = np.asarray(sent_idxs)\n",
    "        sent_idxs_dropout[np.random.rand(len(sent_idxs)) > config.word_keep_prob] = config.UNK_IDX\n",
    "        return list(sent_idxs_dropout)\n",
    "\n",
    "    bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "    \n",
    "    doc_l = np.array([len(instance.token_idxs) for instance in batch])\n",
    "    \n",
    "    feed_input_token_idxs_list = [sent_idxs for instance in batch for sent_idxs in instance.token_idxs]\n",
    "    feed_dec_input_idxs_list = [[config.BOS_IDX] + token_dropout(sent_idxs) for sent_idxs in feed_input_token_idxs_list]\n",
    "    feed_dec_target_idxs_list = [sent_idxs + [config.EOS_IDX]  for sent_idxs in feed_input_token_idxs_list]\n",
    "        \n",
    "    sent_l = np.array([len(sent_idxs) for sent_idxs in feed_input_token_idxs_list], np.int32)\n",
    "    batch_l = len(sent_l)\n",
    "    \n",
    "    feed_input_token_idxs = pad_sequences(feed_input_token_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_input_idxs = pad_sequences(feed_dec_input_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_target_idxs = pad_sequences(feed_dec_target_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    \n",
    "    if assertion:\n",
    "        index = 0\n",
    "        for instance in batch:\n",
    "            for line_idxs in instance.token_idxs:\n",
    "                assert feed_input_token_idxs_list[index] == line_idxs\n",
    "                index += 1\n",
    "        assert feed_input_token_idxs.shape[1] == np.max(sent_l)\n",
    "        assert feed_dec_input_idxs.shape[1] == np.max(sent_l) + 1\n",
    "        assert feed_dec_target_idxs.shape[1] == np.max(sent_l) + 1\n",
    "    \n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "\n",
    "    feed_dict = {\n",
    "                t_variables['bow']: bow, \n",
    "                t_variables['batch_l']: batch_l, t_variables['doc_l']: doc_l, t_variables['sent_l']: sent_l, \n",
    "                t_variables['input_token_idxs']: feed_input_token_idxs, t_variables['dec_input_idxs']: feed_dec_input_idxs, t_variables['dec_target_idxs']: feed_dec_target_idxs, \n",
    "                t_variables['keep_prob']: keep_prob\n",
    "    }\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     10,
     24,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables):\n",
    "    sample_batch = dev_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "def check_shape(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()\n",
    "    \n",
    "def check_value(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()    \n",
    "    \n",
    "# sent_loss_kl_categ_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, tf_log(prob_topic_infer/prob_topic_sents)), 1))\n",
    "# debug_value([sent_loss_kl_categ, sent_loss_kl_categ_tmp])\n",
    "# sent_loss_kl_gauss_tmp = 0.5 * tf.reduce_sum(tf.exp(logvars_topic_infer-logvars_topic) + tf.square(means_topic - means_topic_infer) / tf.exp(logvars_topic) - 1 + (logvars_topic - logvars_topic_infer), -1)\n",
    "# sent_loss_kl_gmm_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss_tmp), -1))\n",
    "# debug_value([sent_loss_kl_gmm_tmp, sent_loss_kl_gmm])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encode bow\n",
    "with tf.variable_scope('topic/enc', reuse=False):\n",
    "    hidden_bow_ = tf.layers.Dense(units=config.dim_hidden_bow, activation=tf.nn.relu, name='hidden_bow')(t_variables['bow'])\n",
    "    hidden_bow = tf.layers.Dropout(t_variables['keep_prob'])(hidden_bow_)\n",
    "    means_bow = tf.layers.Dense(units=config.dim_latent_bow, name='mean_bow')(hidden_bow)\n",
    "    logvars_bow = tf.layers.Dense(units=config.dim_latent_bow, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_bow)\n",
    "    latents_bow = sample_latents(means_bow, logvars_bow) # sample latent vectors\n",
    "\n",
    "    prob_topic = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic')(latents_bow) # inference of topic probabilities\n",
    "\n",
    "# decode bow\n",
    "with tf.variable_scope('shared', reuse=False):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "bow_embeddings = tf.nn.embedding_lookup(embeddings, bow_idxs) # embeddings of each bow features\n",
    "\n",
    "with tf.variable_scope('topic/dec', reuse=False):\n",
    "    topic_embeddings = tf.get_variable('topic_emb', [config.n_topic, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of topics\n",
    "\n",
    "    topic_bow = tf.nn.softmax(tf.matmul(topic_embeddings, bow_embeddings, transpose_b=True), 1) # bow vectors for each topic\n",
    "    logits_bow = tf_log(tf.matmul(prob_topic, topic_bow)) # predicted bow distribution\n",
    "\n",
    "    # prior of each gaussian distribution (computed for each topic)\n",
    "    hidden_topic = tf.layers.Dense(units=config.dim_hidden_topic, activation=tf.nn.relu, name='hidden_topic')(topic_bow)\n",
    "    means_topic = tf.layers.Dense(units=config.dim_latent, name='mean_topic')(hidden_topic)\n",
    "    logvars_topic = tf.layers.Dense(units=config.dim_latent, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_topic)\n",
    "    sigma_topic = tf.exp(0.5 * logvars_topic)\n",
    "    gauss_topic = tfd.Normal(loc=means_topic, scale=sigma_topic)    \n",
    "    \n",
    "# define losses\n",
    "topic_losses_recon = -tf.reduce_sum(tf.multiply(t_variables['bow'], logits_bow), 1)\n",
    "topic_loss_recon = tf.reduce_mean(topic_losses_recon) # negative log likelihood of each words\n",
    "\n",
    "topic_loss_kl = compute_kl_loss(means_bow, logvars_bow) # KL divergence b/w latent dist & gaussian std\n",
    "\n",
    "topic_bow_norm = topic_bow / tf.norm(topic_bow, axis=1, keepdims=True)\n",
    "topic_dots = tf.clip_by_value(tf.matmul(topic_bow_norm, tf.transpose(topic_bow_norm)), -1., 1.)\n",
    "topic_loss_reg = tf.reduce_mean(tf.square(topic_dots - tf.eye(config.n_topic)))\n",
    "# topic_angles = tf.acos(topic_dots)\n",
    "# topic_angles_mean = tf.reduce_mean(topic_angles)\n",
    "# topic_angles_vars = tf.reduce_mean(tf.square(topic_angles - topic_angles_mean))\n",
    "# topic_loss_reg = tf.exp(topic_angles_vars - topic_angles_mean)\n",
    "\n",
    "# monitor\n",
    "n_bow = tf.reduce_sum(t_variables['bow'], 1)\n",
    "topic_ppls = tf.divide(topic_losses_recon, tf.maximum(1e-5, n_bow))\n",
    "topics_freq_bow_indices = tf.nn.top_k(topic_bow, 10, name='topic_freq_bow').indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "input_token_idxs = t_variables['input_token_idxs']\n",
    "batch_l = t_variables['batch_l']\n",
    "sent_l = t_variables['sent_l']\n",
    "max_sent_l = tf.reduce_max(sent_l)\n",
    "\n",
    "with tf.variable_scope('sent/enc', reuse=False):\n",
    "    # get word embedding\n",
    "    enc_input = tf.nn.embedding_lookup(embeddings, input_token_idxs)\n",
    "\n",
    "    # get sentence embedding\n",
    "    _, enc_state = dynamic_bi_rnn(enc_input, sent_l, config.dim_hidden, t_variables['keep_prob'])\n",
    "\n",
    "    # TODO House Holder flow\n",
    "    hidden_topic_infer =  tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='hidden_topic_infer')(enc_state)\n",
    "    prob_topic_infer = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic_infer')(hidden_topic_infer)\n",
    "\n",
    "    w_mean_topic_infer = tf.get_variable('mean_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32)\n",
    "    b_mean_topic_infer = tf.get_variable('mean_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32)\n",
    "    means_topic_infer = tf.tensordot(enc_state, w_mean_topic_infer, axes=[[1], [1]]) + b_mean_topic_infer\n",
    "    \n",
    "    w_logvar_topic_infer = tf.get_variable('logvar_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    b_logvar_topic_infer = tf.get_variable('logvar_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    logvars_topic_infer = tf.tensordot(enc_state, w_logvar_topic_infer, axes=[[1], [1]]) + b_logvar_topic_infer\n",
    "    sigma_topic_infer = tf.exp(0.5 * logvars_topic_infer)\n",
    "    gauss_topic_infer = tfd.Normal(loc=means_topic_infer, scale=sigma_topic_infer)\n",
    "    \n",
    "    # latent vectors from each gaussian dist.\n",
    "    latents_topic_infer = sample_latents(means_topic_infer, logvars_topic_infer) \n",
    "    # latent vector from gaussian mixture\n",
    "    latents_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), latents_topic_infer, transpose_a=True)\n",
    "    \n",
    "    # for beam search\n",
    "    means_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), means_topic_infer, transpose_a=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for decoding\n",
    "dec_sent_l = tf.add(sent_l, 1)\n",
    "dec_input_idxs = t_variables['dec_input_idxs']\n",
    "dec_input = tf.nn.embedding_lookup(embeddings, dec_input_idxs)\n",
    "\n",
    "dec_latents_input = tf.tile(latents_input, [1, tf.shape(dec_input)[1], 1])\n",
    "dec_concat_input = tf.concat([dec_input, dec_latents_input], -1)\n",
    "\n",
    "# decode for training\n",
    "with tf.variable_scope('sent/dec/rnn', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32, reuse=False):\n",
    "    dec_cell = tf.contrib.rnn.GRUCell(config.dim_hidden)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob = t_variables['keep_prob'])\n",
    "\n",
    "    dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(latents_input, 1))\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_concat_input, sequence_length=dec_sent_l)\n",
    "\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=dec_cell,\n",
    "        helper=helper,\n",
    "        initial_state=dec_initial_state)\n",
    "\n",
    "    dec_outputs, _, output_sent_l = tf.contrib.seq2seq.dynamic_decode(train_decoder)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(config.n_vocab, use_bias=False, name='out')\n",
    "    output_logits = output_layer(dec_outputs.rnn_output)\n",
    "    \n",
    "    output_token_idxs = tf.argmax(output_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tokens = tf.fill([batch_l], config.BOS_IDX)\n",
    "end_token = config.EOS_IDX\n",
    "\n",
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    infer_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(means_input, 1))\n",
    "    beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(infer_dec_initial_state, multiplier=config.beam_width)\n",
    "    beam_latents_input = tf.contrib.seq2seq.tile_batch(tf.squeeze(means_input, 1), multiplier=config.beam_width) # added\n",
    "    \n",
    "    beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=beam_latents_input)\n",
    "\n",
    "    beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    beam_output_token_idxs = beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    inter_means_input = tf.placeholder(tf.float32, [None, config.dim_latent])\n",
    "    \n",
    "    inter_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(inter_means_input)\n",
    "    inter_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(inter_dec_initial_state, multiplier=config.beam_width)\n",
    "    inter_beam_latents_input = tf.contrib.seq2seq.tile_batch(inter_means_input, multiplier=config.beam_width) # added\n",
    "    \n",
    "    inter_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=inter_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=inter_beam_latents_input)\n",
    "\n",
    "    inter_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inter_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    inter_beam_output_token_idxs = inter_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    topic_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic)\n",
    "    topic_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(topic_dec_initial_state, multiplier=config.beam_width)\n",
    "    topic_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic, multiplier=config.beam_width) # added\n",
    "    \n",
    "    topic_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=topic_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=topic_beam_latents_input)\n",
    "\n",
    "    topic_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        topic_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    topic_beam_output_token_idxs = topic_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    means_topic_summary = tf.reduce_mean(means_topic_infer, 0)\n",
    "    \n",
    "    summary_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic_summary)\n",
    "    summary_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(summary_dec_initial_state, multiplier=config.beam_width)\n",
    "    summary_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic_summary, multiplier=config.beam_width) # added\n",
    "    \n",
    "    summary_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=summary_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width,\n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=summary_beam_latents_input)\n",
    "\n",
    "    summary_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        summary_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    summary_beam_output_token_idxs = summary_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language modeling cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target and mask\n",
    "dec_target_idxs = t_variables['dec_target_idxs']\n",
    "dec_mask_tokens = tf.sequence_mask(dec_sent_l, maxlen=max_sent_l+1, dtype=tf.float32)\n",
    "\n",
    "# nll for each token (averaged over batch & sentence)\n",
    "sent_loss_recon = tf.contrib.seq2seq.sequence_loss(output_logits, dec_target_idxs, dec_mask_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_l = t_variables['doc_l']\n",
    "mask_sents = tf.sequence_mask(doc_l)\n",
    "mask_sents_flatten = tf.reshape(mask_sents, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1]])\n",
    "\n",
    "prob_topic_tiled = tf.tile(tf.expand_dims(prob_topic, 1), [1, tf.shape(mask_sents)[1], 1])\n",
    "prob_topic_flatten = tf.reshape(prob_topic_tiled, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1], config.n_topic])\n",
    "prob_topic_sents = tf.boolean_mask(prob_topic_flatten, mask_sents_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferred mixture probabilities (computed for each sentence)\n",
    "categ_topic_infer = tfd.Categorical(probs=prob_topic_infer)\n",
    "\n",
    "# prior of mixture probabilities (computed for each document, tiled for each sentence)\n",
    "categ_topic = tfd.Categorical(probs=prob_topic_sents)\n",
    "\n",
    "sent_loss_kl_categ = tf.reduce_mean(tfd.kl_divergence(categ_topic_infer, categ_topic))\n",
    "\n",
    "# inference of each gaussian gaussribution (computed for each sentence)\n",
    "\n",
    "sent_loss_kl_gauss = tf.reduce_sum(tfd.kl_divergence(gauss_topic_infer, gauss_topic), -1)\n",
    "sent_loss_kl_gmm = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss), -1))\n",
    "\n",
    "sent_loss_kl = sent_loss_kl_categ + sent_loss_kl_gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "tau = tf.cast(tf.divide(tf.mod(global_step, tf.constant(config.cycle_steps)), tf.constant(config.cycle_steps)), dtype=tf.float32)\n",
    "beta = tf.minimum(1., tau/config.r_cycle)\n",
    "\n",
    "sent_loss = sent_loss_recon + beta * sent_loss_kl\n",
    "\n",
    "topic_loss = topic_loss_recon + topic_loss_kl + config.reg * topic_loss_reg\n",
    "loss = topic_loss + sent_loss\n",
    "\n",
    "# define optimizer\n",
    "if config.opt == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif config.opt == 'Adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "    \n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_sents(token_idxs, config, idx_to_word):\n",
    "    sents = []\n",
    "    for sent_idxs in token_idxs:\n",
    "        tokens = []\n",
    "        for idx in sent_idxs:\n",
    "            if idx == config.EOS_IDX: break\n",
    "            tokens.append(idx_to_word[idx])\n",
    "        sent = ' '.join(tokens)\n",
    "        sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, ppls_batch \\\n",
    "            = sess.run([loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, topic_ppls], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "    loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, sent_loss_recon_mean, sent_loss_kl_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    return loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, sent_loss_recon_mean, sent_loss_kl_mean, ppl_mean\n",
    "\n",
    "def get_all_losses(sess, batches):\n",
    "    losses = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch = \\\n",
    "        sess.run([loss, topic_loss_recon, topic_loss_kl, sent_loss_recon, sent_loss_kl], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "    print('LOSS %.2f | TM NLL: %.2f, KL: %.4f | LM NLL: %.2f, KL: %.4f' %  np.mean(losses, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_batch):\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    pred_token_idxs = sess.run(output_token_idxs, feed_dict = feed_dict)\n",
    "    true_token_idxs = [sent_idxs for instance in sample_batch for sent_idxs in instance.token_idxs]\n",
    "    \n",
    "    assert len(pred_token_idxs) == len(true_token_idxs)\n",
    "    \n",
    "    pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "    true_sents = idxs_to_sents(true_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    for i, (true_sent, pred_sent) in enumerate(zip(true_sents, pred_sents)):        \n",
    "        print(i, 'TRUE: %s' % true_sent)\n",
    "        print(i, 'PRED: %s' % pred_sent)\n",
    "\n",
    "def print_topic_sample():\n",
    "    pred_topics_freq_bow_indices, pred_topic_token_idxs = sess.run([topics_freq_bow_indices, topic_beam_output_token_idxs], \n",
    "                                                                                                           feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "    pred_topic_sents = idxs_to_sents(pred_topic_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Topic Samples-----------')\n",
    "    for i, (topic_freq_bow_idxs, pred_topic_sent) in enumerate(zip(topics_freq_bow_idxs, pred_topic_sents)):\n",
    "        print(i, ' BOW:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' SENTENCE:', pred_topic_sent)\n",
    "        \n",
    "def print_summary(test_batch):\n",
    "    feed_dict = get_feed_dict(test_batch)\n",
    "    feed_dict[t_variables['batch_l']] = config.n_topic\n",
    "    feed_dict[t_variables['keep_prob']] = 1.\n",
    "    pred_topics_freq_bow_indices, pred_summary_token_idxs = sess.run([topics_freq_bow_indices, summary_beam_output_token_idxs], feed_dict=feed_dict)\n",
    "    pred_summary_sents = idxs_to_sents(pred_summary_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Output sentences for each topic-----------')\n",
    "    print('Item idx:', test_batch[0].item_idx)\n",
    "    for i, (topic_freq_bow_idxs, pred_summary_sent) in enumerate(zip(topics_freq_bow_idxs, pred_summary_sents)):\n",
    "        print(i, ' BOW:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' SENTENCE:', pred_summary_sent)\n",
    "        \n",
    "    print('-----------Summaries-----------')\n",
    "    for i, summary in enumerate(test_batch[0].summaries):\n",
    "        print('SUMMARY %i :'%i, '\\n', summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "saver = tf.train.Saver(max_to_keep=10)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','TM','','','','LM','','VALID:','TM','','','','LM','', ''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL','REG','NLL','KL','LOSS','PPL','NLL','KL','REG','NLL','KL', 'Beta']]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>LM</th>\n",
       "      <th></th>\n",
       "      <th>VALID:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>LM</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>134.57</td>\n",
       "      <td>1036</td>\n",
       "      <td>123.99</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.90</td>\n",
       "      <td>9.13</td>\n",
       "      <td>1.41</td>\n",
       "      <td>126.47</td>\n",
       "      <td>1033</td>\n",
       "      <td>116.10</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.90</td>\n",
       "      <td>9.12</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>121.15</td>\n",
       "      <td>579</td>\n",
       "      <td>114.19</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.42</td>\n",
       "      <td>6.21</td>\n",
       "      <td>1.39</td>\n",
       "      <td>111.25</td>\n",
       "      <td>519</td>\n",
       "      <td>104.75</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.72</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>119.81</td>\n",
       "      <td>550</td>\n",
       "      <td>112.97</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.27</td>\n",
       "      <td>5.99</td>\n",
       "      <td>1.10</td>\n",
       "      <td>110.67</td>\n",
       "      <td>499</td>\n",
       "      <td>104.05</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.68</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>1500</td>\n",
       "      <td>119.03</td>\n",
       "      <td>535</td>\n",
       "      <td>112.18</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.21</td>\n",
       "      <td>5.90</td>\n",
       "      <td>0.92</td>\n",
       "      <td>110.16</td>\n",
       "      <td>484</td>\n",
       "      <td>103.51</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.62</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>118.52</td>\n",
       "      <td>523</td>\n",
       "      <td>111.67</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.84</td>\n",
       "      <td>0.80</td>\n",
       "      <td>109.78</td>\n",
       "      <td>471</td>\n",
       "      <td>103.02</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.49</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>118.30</td>\n",
       "      <td>518</td>\n",
       "      <td>111.44</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.16</td>\n",
       "      <td>5.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>109.79</td>\n",
       "      <td>472</td>\n",
       "      <td>103.11</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.03</td>\n",
       "      <td>5.41</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2776</th>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>118.03</td>\n",
       "      <td>510</td>\n",
       "      <td>111.17</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.13</td>\n",
       "      <td>5.73</td>\n",
       "      <td>0.68</td>\n",
       "      <td>109.33</td>\n",
       "      <td>466</td>\n",
       "      <td>102.84</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.25</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3276</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>117.77</td>\n",
       "      <td>504</td>\n",
       "      <td>110.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.12</td>\n",
       "      <td>5.67</td>\n",
       "      <td>0.64</td>\n",
       "      <td>109.43</td>\n",
       "      <td>468</td>\n",
       "      <td>102.92</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.16</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3776</th>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>117.66</td>\n",
       "      <td>499</td>\n",
       "      <td>110.83</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>109.23</td>\n",
       "      <td>463</td>\n",
       "      <td>102.71</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.07</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4276</th>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>117.49</td>\n",
       "      <td>494</td>\n",
       "      <td>110.67</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>5.55</td>\n",
       "      <td>0.57</td>\n",
       "      <td>109.13</td>\n",
       "      <td>461</td>\n",
       "      <td>102.68</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4551</th>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>117.39</td>\n",
       "      <td>492</td>\n",
       "      <td>110.57</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>5.52</td>\n",
       "      <td>0.56</td>\n",
       "      <td>109.05</td>\n",
       "      <td>458</td>\n",
       "      <td>102.54</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.97</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5051</th>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>117.19</td>\n",
       "      <td>489</td>\n",
       "      <td>110.38</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.48</td>\n",
       "      <td>0.54</td>\n",
       "      <td>108.90</td>\n",
       "      <td>453</td>\n",
       "      <td>102.37</td>\n",
       "      <td>1.27</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.92</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5551</th>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>117.08</td>\n",
       "      <td>485</td>\n",
       "      <td>110.27</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.43</td>\n",
       "      <td>0.52</td>\n",
       "      <td>108.95</td>\n",
       "      <td>452</td>\n",
       "      <td>102.36</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.87</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6051</th>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>1500</td>\n",
       "      <td>116.98</td>\n",
       "      <td>483</td>\n",
       "      <td>110.17</td>\n",
       "      <td>1.15</td>\n",
       "      <td>0.07</td>\n",
       "      <td>5.39</td>\n",
       "      <td>0.50</td>\n",
       "      <td>108.74</td>\n",
       "      <td>445</td>\n",
       "      <td>102.19</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.82</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6551</th>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>116.96</td>\n",
       "      <td>480</td>\n",
       "      <td>110.14</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.07</td>\n",
       "      <td>5.36</td>\n",
       "      <td>0.49</td>\n",
       "      <td>108.72</td>\n",
       "      <td>448</td>\n",
       "      <td>102.23</td>\n",
       "      <td>1.37</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.78</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6826</th>\n",
       "      <td>43</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>116.91</td>\n",
       "      <td>479</td>\n",
       "      <td>110.10</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.07</td>\n",
       "      <td>5.34</td>\n",
       "      <td>0.48</td>\n",
       "      <td>108.61</td>\n",
       "      <td>447</td>\n",
       "      <td>102.12</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.75</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7326</th>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>116.85</td>\n",
       "      <td>477</td>\n",
       "      <td>110.03</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.06</td>\n",
       "      <td>5.31</td>\n",
       "      <td>0.47</td>\n",
       "      <td>108.67</td>\n",
       "      <td>440</td>\n",
       "      <td>101.95</td>\n",
       "      <td>1.63</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.74</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7826</th>\n",
       "      <td>69</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>116.84</td>\n",
       "      <td>475</td>\n",
       "      <td>110.02</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.06</td>\n",
       "      <td>5.27</td>\n",
       "      <td>0.46</td>\n",
       "      <td>108.36</td>\n",
       "      <td>434</td>\n",
       "      <td>101.77</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.69</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8326</th>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>1500</td>\n",
       "      <td>116.74</td>\n",
       "      <td>472</td>\n",
       "      <td>109.92</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.06</td>\n",
       "      <td>5.25</td>\n",
       "      <td>0.45</td>\n",
       "      <td>108.45</td>\n",
       "      <td>438</td>\n",
       "      <td>101.88</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.67</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8826</th>\n",
       "      <td>59</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>116.61</td>\n",
       "      <td>470</td>\n",
       "      <td>109.79</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.22</td>\n",
       "      <td>0.45</td>\n",
       "      <td>108.36</td>\n",
       "      <td>437</td>\n",
       "      <td>101.82</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.65</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9101</th>\n",
       "      <td>43</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>116.57</td>\n",
       "      <td>469</td>\n",
       "      <td>109.75</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.21</td>\n",
       "      <td>0.45</td>\n",
       "      <td>108.33</td>\n",
       "      <td>434</td>\n",
       "      <td>101.71</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.64</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9601</th>\n",
       "      <td>59</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "      <td>116.50</td>\n",
       "      <td>467</td>\n",
       "      <td>109.68</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.18</td>\n",
       "      <td>0.44</td>\n",
       "      <td>108.44</td>\n",
       "      <td>439</td>\n",
       "      <td>101.84</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.61</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10101</th>\n",
       "      <td>74</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>116.40</td>\n",
       "      <td>466</td>\n",
       "      <td>109.58</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.16</td>\n",
       "      <td>0.44</td>\n",
       "      <td>108.31</td>\n",
       "      <td>438</td>\n",
       "      <td>101.78</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.59</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10601</th>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>1500</td>\n",
       "      <td>116.39</td>\n",
       "      <td>464</td>\n",
       "      <td>109.56</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.14</td>\n",
       "      <td>0.43</td>\n",
       "      <td>108.40</td>\n",
       "      <td>438</td>\n",
       "      <td>101.87</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.57</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11101</th>\n",
       "      <td>63</td>\n",
       "      <td>4</td>\n",
       "      <td>2000</td>\n",
       "      <td>116.35</td>\n",
       "      <td>463</td>\n",
       "      <td>109.52</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.12</td>\n",
       "      <td>0.43</td>\n",
       "      <td>108.33</td>\n",
       "      <td>435</td>\n",
       "      <td>101.77</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.55</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11376</th>\n",
       "      <td>46</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>116.31</td>\n",
       "      <td>462</td>\n",
       "      <td>109.48</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.10</td>\n",
       "      <td>0.43</td>\n",
       "      <td>107.76</td>\n",
       "      <td>430</td>\n",
       "      <td>101.57</td>\n",
       "      <td>1.63</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.55</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11876</th>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>116.26</td>\n",
       "      <td>460</td>\n",
       "      <td>109.44</td>\n",
       "      <td>1.43</td>\n",
       "      <td>0.04</td>\n",
       "      <td>5.08</td>\n",
       "      <td>0.43</td>\n",
       "      <td>107.78</td>\n",
       "      <td>433</td>\n",
       "      <td>101.62</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.50</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12376</th>\n",
       "      <td>73</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>116.21</td>\n",
       "      <td>459</td>\n",
       "      <td>109.40</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.04</td>\n",
       "      <td>5.07</td>\n",
       "      <td>0.43</td>\n",
       "      <td>107.67</td>\n",
       "      <td>423</td>\n",
       "      <td>101.32</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.49</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12876</th>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>1500</td>\n",
       "      <td>116.17</td>\n",
       "      <td>458</td>\n",
       "      <td>109.37</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.04</td>\n",
       "      <td>5.05</td>\n",
       "      <td>0.43</td>\n",
       "      <td>107.93</td>\n",
       "      <td>430</td>\n",
       "      <td>101.55</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.49</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13376</th>\n",
       "      <td>63</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>116.09</td>\n",
       "      <td>457</td>\n",
       "      <td>109.29</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.04</td>\n",
       "      <td>5.03</td>\n",
       "      <td>0.43</td>\n",
       "      <td>107.75</td>\n",
       "      <td>425</td>\n",
       "      <td>101.39</td>\n",
       "      <td>1.72</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.46</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78378</th>\n",
       "      <td>16</td>\n",
       "      <td>33</td>\n",
       "      <td>500</td>\n",
       "      <td>114.43</td>\n",
       "      <td>418</td>\n",
       "      <td>107.79</td>\n",
       "      <td>2.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.28</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.88</td>\n",
       "      <td>404</td>\n",
       "      <td>100.66</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.76</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78878</th>\n",
       "      <td>53</td>\n",
       "      <td>33</td>\n",
       "      <td>1000</td>\n",
       "      <td>114.43</td>\n",
       "      <td>417</td>\n",
       "      <td>107.79</td>\n",
       "      <td>2.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.28</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.72</td>\n",
       "      <td>400</td>\n",
       "      <td>100.48</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.77</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79378</th>\n",
       "      <td>53</td>\n",
       "      <td>33</td>\n",
       "      <td>1500</td>\n",
       "      <td>114.43</td>\n",
       "      <td>417</td>\n",
       "      <td>107.79</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.28</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.92</td>\n",
       "      <td>406</td>\n",
       "      <td>100.61</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79878</th>\n",
       "      <td>64</td>\n",
       "      <td>33</td>\n",
       "      <td>2000</td>\n",
       "      <td>114.42</td>\n",
       "      <td>417</td>\n",
       "      <td>107.78</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.28</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.47</td>\n",
       "      <td>408</td>\n",
       "      <td>100.71</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80153</th>\n",
       "      <td>49</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>114.41</td>\n",
       "      <td>417</td>\n",
       "      <td>107.78</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.28</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.41</td>\n",
       "      <td>404</td>\n",
       "      <td>100.55</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80653</th>\n",
       "      <td>55</td>\n",
       "      <td>34</td>\n",
       "      <td>500</td>\n",
       "      <td>114.41</td>\n",
       "      <td>417</td>\n",
       "      <td>107.78</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.27</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.48</td>\n",
       "      <td>403</td>\n",
       "      <td>100.60</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81153</th>\n",
       "      <td>53</td>\n",
       "      <td>34</td>\n",
       "      <td>1000</td>\n",
       "      <td>114.41</td>\n",
       "      <td>417</td>\n",
       "      <td>107.78</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.27</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.55</td>\n",
       "      <td>403</td>\n",
       "      <td>100.57</td>\n",
       "      <td>2.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.74</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81653</th>\n",
       "      <td>53</td>\n",
       "      <td>34</td>\n",
       "      <td>1500</td>\n",
       "      <td>114.40</td>\n",
       "      <td>417</td>\n",
       "      <td>107.77</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.27</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.61</td>\n",
       "      <td>403</td>\n",
       "      <td>100.63</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82153</th>\n",
       "      <td>52</td>\n",
       "      <td>34</td>\n",
       "      <td>2000</td>\n",
       "      <td>114.39</td>\n",
       "      <td>417</td>\n",
       "      <td>107.77</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.26</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.67</td>\n",
       "      <td>405</td>\n",
       "      <td>100.65</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82428</th>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>114.39</td>\n",
       "      <td>417</td>\n",
       "      <td>107.76</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.26</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.60</td>\n",
       "      <td>403</td>\n",
       "      <td>100.55</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.71</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82928</th>\n",
       "      <td>53</td>\n",
       "      <td>35</td>\n",
       "      <td>500</td>\n",
       "      <td>114.39</td>\n",
       "      <td>417</td>\n",
       "      <td>107.76</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.26</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.69</td>\n",
       "      <td>404</td>\n",
       "      <td>100.59</td>\n",
       "      <td>2.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83428</th>\n",
       "      <td>53</td>\n",
       "      <td>35</td>\n",
       "      <td>1000</td>\n",
       "      <td>114.38</td>\n",
       "      <td>417</td>\n",
       "      <td>107.76</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.26</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.62</td>\n",
       "      <td>403</td>\n",
       "      <td>100.53</td>\n",
       "      <td>2.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83928</th>\n",
       "      <td>53</td>\n",
       "      <td>35</td>\n",
       "      <td>1500</td>\n",
       "      <td>114.38</td>\n",
       "      <td>417</td>\n",
       "      <td>107.75</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.26</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.79</td>\n",
       "      <td>404</td>\n",
       "      <td>100.65</td>\n",
       "      <td>2.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.74</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84428</th>\n",
       "      <td>53</td>\n",
       "      <td>35</td>\n",
       "      <td>2000</td>\n",
       "      <td>114.37</td>\n",
       "      <td>417</td>\n",
       "      <td>107.75</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.25</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.75</td>\n",
       "      <td>400</td>\n",
       "      <td>100.54</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84703</th>\n",
       "      <td>30</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>114.37</td>\n",
       "      <td>417</td>\n",
       "      <td>107.75</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.25</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.73</td>\n",
       "      <td>403</td>\n",
       "      <td>100.52</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85203</th>\n",
       "      <td>53</td>\n",
       "      <td>36</td>\n",
       "      <td>500</td>\n",
       "      <td>114.36</td>\n",
       "      <td>416</td>\n",
       "      <td>107.75</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.25</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.87</td>\n",
       "      <td>404</td>\n",
       "      <td>100.64</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85703</th>\n",
       "      <td>52</td>\n",
       "      <td>36</td>\n",
       "      <td>1000</td>\n",
       "      <td>114.36</td>\n",
       "      <td>416</td>\n",
       "      <td>107.75</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.25</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.78</td>\n",
       "      <td>401</td>\n",
       "      <td>100.51</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.71</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86203</th>\n",
       "      <td>55</td>\n",
       "      <td>36</td>\n",
       "      <td>1500</td>\n",
       "      <td>114.36</td>\n",
       "      <td>416</td>\n",
       "      <td>107.74</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.24</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.86</td>\n",
       "      <td>405</td>\n",
       "      <td>100.66</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86703</th>\n",
       "      <td>54</td>\n",
       "      <td>36</td>\n",
       "      <td>2000</td>\n",
       "      <td>114.35</td>\n",
       "      <td>416</td>\n",
       "      <td>107.74</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.24</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.93</td>\n",
       "      <td>408</td>\n",
       "      <td>100.68</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86978</th>\n",
       "      <td>31</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>114.35</td>\n",
       "      <td>416</td>\n",
       "      <td>107.74</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.24</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.87</td>\n",
       "      <td>404</td>\n",
       "      <td>100.61</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87478</th>\n",
       "      <td>55</td>\n",
       "      <td>37</td>\n",
       "      <td>500</td>\n",
       "      <td>114.35</td>\n",
       "      <td>416</td>\n",
       "      <td>107.73</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.24</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.83</td>\n",
       "      <td>403</td>\n",
       "      <td>100.57</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.71</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87978</th>\n",
       "      <td>53</td>\n",
       "      <td>37</td>\n",
       "      <td>1000</td>\n",
       "      <td>114.34</td>\n",
       "      <td>416</td>\n",
       "      <td>107.73</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.24</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.70</td>\n",
       "      <td>400</td>\n",
       "      <td>100.45</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88478</th>\n",
       "      <td>53</td>\n",
       "      <td>37</td>\n",
       "      <td>1500</td>\n",
       "      <td>114.34</td>\n",
       "      <td>416</td>\n",
       "      <td>107.72</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.23</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.79</td>\n",
       "      <td>402</td>\n",
       "      <td>100.56</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88978</th>\n",
       "      <td>53</td>\n",
       "      <td>37</td>\n",
       "      <td>2000</td>\n",
       "      <td>114.34</td>\n",
       "      <td>416</td>\n",
       "      <td>107.72</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.23</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.86</td>\n",
       "      <td>406</td>\n",
       "      <td>100.68</td>\n",
       "      <td>2.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.71</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89253</th>\n",
       "      <td>30</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>114.34</td>\n",
       "      <td>416</td>\n",
       "      <td>107.72</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.23</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.80</td>\n",
       "      <td>402</td>\n",
       "      <td>100.56</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.71</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89753</th>\n",
       "      <td>53</td>\n",
       "      <td>38</td>\n",
       "      <td>500</td>\n",
       "      <td>114.33</td>\n",
       "      <td>416</td>\n",
       "      <td>107.72</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.23</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.78</td>\n",
       "      <td>404</td>\n",
       "      <td>100.58</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.71</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90253</th>\n",
       "      <td>53</td>\n",
       "      <td>38</td>\n",
       "      <td>1000</td>\n",
       "      <td>114.33</td>\n",
       "      <td>416</td>\n",
       "      <td>107.72</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.23</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.69</td>\n",
       "      <td>403</td>\n",
       "      <td>100.50</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.71</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90753</th>\n",
       "      <td>53</td>\n",
       "      <td>38</td>\n",
       "      <td>1500</td>\n",
       "      <td>114.32</td>\n",
       "      <td>416</td>\n",
       "      <td>107.71</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.22</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.77</td>\n",
       "      <td>401</td>\n",
       "      <td>100.56</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.70</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91253</th>\n",
       "      <td>62</td>\n",
       "      <td>38</td>\n",
       "      <td>2000</td>\n",
       "      <td>114.32</td>\n",
       "      <td>416</td>\n",
       "      <td>107.71</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.22</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.32</td>\n",
       "      <td>402</td>\n",
       "      <td>100.54</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.68</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91528</th>\n",
       "      <td>29</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>114.32</td>\n",
       "      <td>416</td>\n",
       "      <td>107.71</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.22</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.36</td>\n",
       "      <td>401</td>\n",
       "      <td>100.55</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.68</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      TRAIN:    TM                        LM        VALID:  \\\n",
       "      Time  Ep    Ct    LOSS   PPL     NLL    KL   REG   NLL    KL    LOSS   \n",
       "1       13   0     0  134.57  1036  123.99  0.56  0.90  9.13  1.41  126.47   \n",
       "501     70   0   500  121.15   579  114.19  0.27  0.42  6.21  1.39  111.25   \n",
       "1001    70   0  1000  119.81   550  112.97  0.50  0.27  5.99  1.10  110.67   \n",
       "1501    68   0  1500  119.03   535  112.18  0.65  0.21  5.90  0.92  110.16   \n",
       "2001    70   0  2000  118.52   523  111.67  0.75  0.17  5.84  0.80  109.78   \n",
       "2276    33   1     0  118.30   518  111.44  0.79  0.16  5.80  0.75  109.79   \n",
       "2776    69   1   500  118.03   510  111.17  0.87  0.13  5.73  0.68  109.33   \n",
       "3276    61   1  1000  117.77   504  110.93  0.92  0.12  5.67  0.64  109.43   \n",
       "3776    74   1  1500  117.66   499  110.83  0.98  0.10  5.60  0.60  109.23   \n",
       "4276    69   1  2000  117.49   494  110.67  1.02  0.09  5.55  0.57  109.13   \n",
       "4551    43   2     0  117.39   492  110.57  1.04  0.09  5.52  0.56  109.05   \n",
       "5051    69   2   500  117.19   489  110.38  1.08  0.08  5.48  0.54  108.90   \n",
       "5551    60   2  1000  117.08   485  110.27  1.11  0.08  5.43  0.52  108.95   \n",
       "6051    69   2  1500  116.98   483  110.17  1.15  0.07  5.39  0.50  108.74   \n",
       "6551    70   2  2000  116.96   480  110.14  1.18  0.07  5.36  0.49  108.72   \n",
       "6826    43   3     0  116.91   479  110.10  1.20  0.07  5.34  0.48  108.61   \n",
       "7326    60   3   500  116.85   477  110.03  1.23  0.06  5.31  0.47  108.67   \n",
       "7826    69   3  1000  116.84   475  110.02  1.26  0.06  5.27  0.46  108.36   \n",
       "8326    60   3  1500  116.74   472  109.92  1.28  0.06  5.25  0.45  108.45   \n",
       "8826    59   3  2000  116.61   470  109.79  1.31  0.05  5.22  0.45  108.36   \n",
       "9101    43   4     0  116.57   469  109.75  1.32  0.05  5.21  0.45  108.33   \n",
       "9601    59   4   500  116.50   467  109.68  1.34  0.05  5.18  0.44  108.44   \n",
       "10101   74   4  1000  116.40   466  109.58  1.36  0.05  5.16  0.44  108.31   \n",
       "10601   64   4  1500  116.39   464  109.56  1.38  0.05  5.14  0.43  108.40   \n",
       "11101   63   4  2000  116.35   463  109.52  1.40  0.05  5.12  0.43  108.33   \n",
       "11376   46   5     0  116.31   462  109.48  1.41  0.05  5.10  0.43  107.76   \n",
       "11876   64   5   500  116.26   460  109.44  1.43  0.04  5.08  0.43  107.78   \n",
       "12376   73   5  1000  116.21   459  109.40  1.45  0.04  5.07  0.43  107.67   \n",
       "12876   64   5  1500  116.17   458  109.37  1.47  0.04  5.05  0.43  107.93   \n",
       "13376   63   5  2000  116.09   457  109.29  1.49  0.04  5.03  0.43  107.75   \n",
       "...    ...  ..   ...     ...   ...     ...   ...   ...   ...   ...     ...   \n",
       "78378   16  33   500  114.43   418  107.79  2.04  0.02  4.28  0.42  106.88   \n",
       "78878   53  33  1000  114.43   417  107.79  2.04  0.02  4.28  0.42  106.72   \n",
       "79378   53  33  1500  114.43   417  107.79  2.05  0.02  4.28  0.42  106.92   \n",
       "79878   64  33  2000  114.42   417  107.78  2.05  0.02  4.28  0.42  106.47   \n",
       "80153   49  34     0  114.41   417  107.78  2.05  0.02  4.28  0.42  106.41   \n",
       "80653   55  34   500  114.41   417  107.78  2.05  0.02  4.27  0.42  106.48   \n",
       "81153   53  34  1000  114.41   417  107.78  2.05  0.02  4.27  0.42  106.55   \n",
       "81653   53  34  1500  114.40   417  107.77  2.05  0.02  4.27  0.42  106.61   \n",
       "82153   52  34  2000  114.39   417  107.77  2.05  0.02  4.26  0.42  106.67   \n",
       "82428   30  35     0  114.39   417  107.76  2.05  0.02  4.26  0.42  106.60   \n",
       "82928   53  35   500  114.39   417  107.76  2.05  0.02  4.26  0.42  106.69   \n",
       "83428   53  35  1000  114.38   417  107.76  2.06  0.02  4.26  0.42  106.62   \n",
       "83928   53  35  1500  114.38   417  107.75  2.06  0.02  4.26  0.42  106.79   \n",
       "84428   53  35  2000  114.37   417  107.75  2.06  0.02  4.25  0.42  106.75   \n",
       "84703   30  36     0  114.37   417  107.75  2.06  0.02  4.25  0.42  106.73   \n",
       "85203   53  36   500  114.36   416  107.75  2.06  0.02  4.25  0.42  106.87   \n",
       "85703   52  36  1000  114.36   416  107.75  2.06  0.02  4.25  0.42  106.78   \n",
       "86203   55  36  1500  114.36   416  107.74  2.06  0.02  4.24  0.42  106.86   \n",
       "86703   54  36  2000  114.35   416  107.74  2.06  0.02  4.24  0.42  106.93   \n",
       "86978   31  37     0  114.35   416  107.74  2.06  0.02  4.24  0.42  106.87   \n",
       "87478   55  37   500  114.35   416  107.73  2.06  0.02  4.24  0.42  106.83   \n",
       "87978   53  37  1000  114.34   416  107.73  2.07  0.02  4.24  0.42  106.70   \n",
       "88478   53  37  1500  114.34   416  107.72  2.07  0.02  4.23  0.42  106.79   \n",
       "88978   53  37  2000  114.34   416  107.72  2.07  0.02  4.23  0.42  106.86   \n",
       "89253   30  38     0  114.34   416  107.72  2.07  0.02  4.23  0.42  106.80   \n",
       "89753   53  38   500  114.33   416  107.72  2.07  0.02  4.23  0.42  106.78   \n",
       "90253   53  38  1000  114.33   416  107.72  2.07  0.02  4.23  0.42  106.69   \n",
       "90753   53  38  1500  114.32   416  107.71  2.07  0.02  4.22  0.42  106.77   \n",
       "91253   62  38  2000  114.32   416  107.71  2.07  0.02  4.22  0.42  106.32   \n",
       "91528   29  39     0  114.32   416  107.71  2.07  0.02  4.22  0.42  106.36   \n",
       "\n",
       "         TM                        LM               \n",
       "        PPL     NLL    KL   REG   NLL    KL   Beta  \n",
       "1      1033  116.10  0.34  0.90  9.12  1.41  0.000  \n",
       "501     519  104.75  0.52  0.17  5.72  0.96  0.088  \n",
       "1001    499  104.05  0.74  0.10  5.68  0.57  0.176  \n",
       "1501    484  103.51  0.80  0.08  5.62  0.59  0.264  \n",
       "2001    471  103.02  1.08  0.05  5.49  0.41  0.352  \n",
       "2276    472  103.11  1.08  0.03  5.41  0.43  0.400  \n",
       "2776    466  102.84  1.04  0.02  5.25  0.37  0.488  \n",
       "3276    468  102.92  1.12  0.02  5.16  0.37  0.576  \n",
       "3776    463  102.71  1.20  0.02  5.07  0.34  0.664  \n",
       "4276    461  102.68  1.19  0.02  5.00  0.33  0.752  \n",
       "4551    458  102.54  1.23  0.02  4.97  0.36  0.800  \n",
       "5051    453  102.37  1.27  0.02  4.92  0.37  0.888  \n",
       "5551    452  102.36  1.39  0.02  4.87  0.33  0.976  \n",
       "6051    445  102.19  1.35  0.02  4.82  0.36  1.000  \n",
       "6551    448  102.23  1.37  0.02  4.78  0.33  1.000  \n",
       "6826    447  102.12  1.40  0.02  4.75  0.33  1.000  \n",
       "7326    440  101.95  1.63  0.02  4.74  0.33  1.000  \n",
       "7826    434  101.77  1.54  0.02  4.69  0.35  1.000  \n",
       "8326    438  101.88  1.51  0.02  4.67  0.37  1.000  \n",
       "8826    437  101.82  1.54  0.01  4.65  0.34  1.000  \n",
       "9101    434  101.71  1.61  0.02  4.64  0.36  1.000  \n",
       "9601    439  101.84  1.61  0.02  4.61  0.36  1.000  \n",
       "10101   438  101.78  1.57  0.01  4.59  0.35  1.000  \n",
       "10601   438  101.87  1.59  0.02  4.57  0.36  1.000  \n",
       "11101   435  101.77  1.61  0.01  4.55  0.39  1.000  \n",
       "11376   430  101.57  1.63  0.01  4.55  0.36  0.000  \n",
       "11876   433  101.62  1.59  0.02  4.50  0.52  0.088  \n",
       "12376   423  101.32  1.76  0.02  4.49  0.47  0.176  \n",
       "12876   430  101.55  1.75  0.02  4.49  0.47  0.264  \n",
       "13376   425  101.39  1.72  0.02  4.46  0.46  0.352  \n",
       "...     ...     ...   ...   ...   ...   ...    ...  \n",
       "78378   404  100.66  2.05  0.01  3.76  0.40  1.000  \n",
       "78878   400  100.48  2.06  0.01  3.77  0.40  1.000  \n",
       "79378   406  100.61  2.06  0.01  3.75  0.48  1.000  \n",
       "79878   408  100.71  2.00  0.01  3.73  0.49  0.044  \n",
       "80153   404  100.55  2.08  0.01  3.72  0.49  0.093  \n",
       "80653   403  100.60  2.05  0.01  3.73  0.47  0.181  \n",
       "81153   403  100.57  2.09  0.01  3.74  0.50  0.269  \n",
       "81653   403  100.63  2.08  0.01  3.72  0.48  0.357  \n",
       "82153   405  100.65  2.07  0.01  3.73  0.46  0.444  \n",
       "82428   403  100.55  2.08  0.01  3.71  0.49  0.493  \n",
       "82928   404  100.59  2.09  0.01  3.73  0.46  0.581  \n",
       "83428   403  100.53  2.04  0.01  3.72  0.46  0.669  \n",
       "83928   404  100.65  2.04  0.01  3.74  0.45  0.757  \n",
       "84428   400  100.54  2.10  0.01  3.72  0.44  0.844  \n",
       "84703   403  100.52  2.10  0.01  3.72  0.43  0.893  \n",
       "85203   404  100.64  2.08  0.01  3.72  0.42  0.981  \n",
       "85703   401  100.51  2.11  0.01  3.71  0.43  1.000  \n",
       "86203   405  100.66  2.06  0.01  3.73  0.40  1.000  \n",
       "86703   408  100.68  2.08  0.01  3.72  0.44  1.000  \n",
       "86978   404  100.61  2.07  0.01  3.73  0.45  1.000  \n",
       "87478   403  100.57  2.11  0.01  3.71  0.42  1.000  \n",
       "87978   400  100.45  2.10  0.01  3.72  0.42  1.000  \n",
       "88478   402  100.56  2.06  0.01  3.72  0.42  1.000  \n",
       "88978   406  100.68  2.04  0.01  3.71  0.41  1.000  \n",
       "89253   402  100.56  2.08  0.01  3.71  0.43  1.000  \n",
       "89753   404  100.58  2.07  0.01  3.71  0.41  1.000  \n",
       "90253   403  100.50  2.05  0.01  3.71  0.42  1.000  \n",
       "90753   401  100.56  2.10  0.01  3.70  0.39  1.000  \n",
       "91253   402  100.54  2.07  0.01  3.68  0.54  0.044  \n",
       "91528   401  100.55  2.07  0.01  3.68  0.50  0.093  \n",
       "\n",
       "[203 rows x 18 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Output sentences for each topic-----------\n",
      "Item idx: B000VB7EFW\n",
      "0  BOW: quality 'm price bought ... made - 've time $\n",
      "0  SENTENCE: i 've had this case for about # months now and it has already cracked\n",
      "1  BOW: strap carry shoulder back pockets straps compartments handle comfortable pack\n",
      "1  SENTENCE: the shoulder strap is comfortable and the straps are comfortable\n",
      "2  BOW: room small carry pocket ipad nice inside perfect pockets space\n",
      "2  SENTENCE: plenty of room for my # inch laptop , power cord , mouse , power cord , pens , pens , etc .\n",
      "3  BOW: power mouse pocket cord charger netbook cards usb drive cords\n",
      "3  SENTENCE: there is plenty of room for my # & # # ; laptop and the power cord and the power cord\n",
      "4  BOW: cover bottom keyboard color top hard pro mac nice apple\n",
      "4  SENTENCE: the keyboard cover does n't fit correctly\n",
      "5  BOW: sleeve protection air inch pro inside protect snug nice perfectly\n",
      "5  SENTENCE: this sleeve fits my # inch macbook pro perfectly\n",
      "6  BOW: months broke years year started back return weeks ago zipper\n",
      "6  SENTENCE: i 've had this case for about # months now and it has held up well\n",
      "7  BOW: ! love color recommend perfect perfectly price ordered bought received\n",
      "7  SENTENCE: the color is perfect ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "8  BOW: ; & pro big air size laptops perfectly hp retina\n",
      "8  SENTENCE: it fits my # . # inch laptop perfectly\n",
      "9  BOW: junk trust test spend dollars based cheaply paid positive wait\n",
      "9  SENTENCE: i would have given it # stars because it is a bit heavy\n",
      "-----------Summaries-----------\n",
      "SUMMARY 0 : \n",
      " This is a very well made bag, nice construction, lots of pockets.\n",
      "the straps are very comfortable.\n",
      "and protects everything inside.\n",
      "It says\n",
      "it fits a 17inch notebook,\n",
      "however it did not.\n",
      "after using the pack for less than a month,\n",
      "it is ripping out already.\n",
      "SUMMARY 1 : \n",
      " This is a very well made bag, nice construction, lots of pockets.\n",
      "The quality is excellent\n",
      "and it is very durable.\n",
      "the straps are very comfortable.\n",
      "and protects everything inside.\n",
      "The color is a true red\n",
      "and it fits nicely.\n",
      "it is ripping out already.\n",
      "and it doesnt fit.\n",
      "It's just not the lightest backpack\n",
      "because some zipper teeth were not aligned.\n",
      "SUMMARY 2 : \n",
      " The quality is excellent\n",
      "and I love all the pockets and compartments.\n",
      "and it is very durable.\n",
      "and can't beleive the price\n",
      "and protects everything inside.\n",
      "The laptop\n",
      "doesn't fit in it.\n",
      "it is ripping out already.\n",
      "It's just not the lightest backpack\n",
      "0 TRUE: it 's beautiful but it does n't hold much without looking too bulky\n",
      "0 PRED: i 's a and it is n't have a much any at\n",
      "1 TRUE: do n't get me wrong , it is able to hold enough\n",
      "1 PRED: i , expect it to , but is a to be up laptop\n",
      "2 TRUE: i can put my laptop in there and couple of notebooks\n",
      "2 PRED: it is not a laptop in the and it of of\n",
      "3 TRUE: the front pockets are good if you put minimal stuff however , if you put in too much or to big an item , the bag will look bulky ... and it 's not an even sort of bulky\n",
      "3 PRED: it bag pocket are great for you need it things in\n",
      "4 TRUE: the strap is nice and has a solid grip so it wo n't slide off . the drawstring ... can get stuck\n",
      "4 PRED: i bag is a and the , lot material\n",
      "5 TRUE: i 've had it for a couple of weeks now and some of the drawstring gets caught when you <unk> it , pulling some of those pieces off\n",
      "5 PRED: i 've the the the the couple of months , , i of the zipper on stuck off it put it\n",
      "6 TRUE: other than that : -rrb- it 's pretty decent for day to day\n",
      "6 PRED: i than that , it it i worth good\n",
      "7 TRUE: if you need to pack lots of stuff though , it 'll prolly be in your best <unk> to choose something more <unk>\n",
      "7 PRED: the shoulder are a carry your of compartments , , you is be a a the\n",
      "8 TRUE: i have bought # of these covers\n",
      "8 PRED: i was to this of these and and and and and and and and and and\n",
      "9 TRUE: looks ok , although clearly a plastic look\n",
      "9 PRED: i like , but it not the ,\n",
      "10 TRUE: the small plastic tabs on the top cover that hold it onto the top broke on the first two after a few months\n",
      "10 PRED: the bottom issue feet on the bottom broke broke came the on the laptop broke off the first time months the month months\n",
      "11 TRUE: even knowing this i bought two more -lrb- one to use , one a spare -rrb-\n",
      "11 PRED: i though that is would it for than bag , carry\n",
      "12 TRUE: basically , the top of the z # is paper thin and the repair center tells me they break often\n",
      "12 PRED: i , i i of the bag is # the , the the zipper one is me to are the\n",
      "13 TRUE: i carry my toshiba everywhere and do n't have the luxury of babying it\n",
      "13 PRED: i is it laptop in and it n't have to case of the it\n",
      "14 TRUE: this product is , as far as i know , only decent option to protect it properly , so i live with the issues\n",
      "14 PRED: i case is not but i as a have it i was case , protect the\n",
      "15 TRUE: nice , thin protection for the macbook air\n",
      "15 PRED: it case snug and for my macbook air\n",
      "16 TRUE: not the sturdiest material , but it was reasonably priced -lrb- and i bought it to replace a much more expensive & # # ; name brand & # # ; cover that cracked . .\n",
      "16 PRED: i sure best of , but i is n't to\n",
      "17 TRUE: i like the flip up legs in the back\n",
      "17 PRED: i have the fact on the the the bottom and\n",
      "18 TRUE: they are a little flimsy feeling , but so far no problems\n",
      "18 PRED: i was not little case , but but i far it complaints\n",
      "19 TRUE: great cover at a good price\n",
      "19 PRED: i product , the great price , and ,\n",
      "20 TRUE: it is a tad slippery , but that might be that i am used to my former cover , which had a slight rubbery feel\n",
      "20 PRED: i , a little difficult , but it does have a if would not to take laptop\n",
      "21 TRUE: this one is smooth plastic\n",
      "21 PRED: it case is not and the ,\n",
      "22 TRUE: i was looking for a computer sleeve with handles that does not look boring\n",
      "22 PRED: i is a for a sleeve case for a and are n't fit the\n",
      "23 TRUE: this certainly fit the bill\n",
      "23 PRED: it is is my bill perfectly\n",
      "24 TRUE: it 's nice , colorful & distinctive\n",
      "24 PRED: this 's a\n",
      "25 TRUE: it is exactly as pictured\n",
      "25 PRED: it is a what described\n",
      "26 TRUE: fitted my new # . # `` macpro perfectly\n",
      "26 PRED: it my # my # # `` laptop pro\n",
      "27 TRUE: i am happy with my purchase\n",
      "27 PRED: i am very with this purchase\n",
      "28 TRUE: it is ok\n",
      "28 PRED: i is a\n",
      "29 TRUE: it fits my laptop , but i do n't feel that it offers very much protection\n",
      "29 PRED: it is a # perfectly and it have n't think it it is protection snug protection\n",
      "30 TRUE: the padding is thin\n",
      "30 PRED: i is is a and and , , ,\n",
      "31 TRUE: it came with some foam in it as part of the packaging to help it keep it 's shape\n",
      "31 PRED: i was a a of , i it it it the case\n",
      "32 TRUE: i actually keep it in there with my laptop for extra protection since the sleeve is not very padded\n",
      "32 PRED: the have have my in my 's a laptop in my\n",
      "33 TRUE: i really do like this bag\n",
      "33 PRED: i love love this this bag\n",
      "34 TRUE: there 's enough padding for my dell , and it just looks pretty cool\n",
      "34 PRED: it is a room for my laptop and but it 's does like good\n",
      "35 TRUE: the only thing -lrb- and it 's a minor thing -rrb- that i wish it had was more pockets\n",
      "35 PRED: i only is the i the is not -rrb- -rrb- -rrb-\n",
      "36 TRUE: i 'm a pretty big fan of pockets , tho , so i just might be partial\n",
      "36 PRED: i have this to to for of the , but i and i can want to able to\n",
      "37 TRUE: it is a good bag , tho\n",
      "37 PRED: it is a well and\n",
      "38 TRUE: it is highly qualified\n",
      "38 PRED: i is a recommended\n",
      "39 TRUE: i like the color and the quality\n",
      "39 PRED: i love the color and the price\n",
      "40 TRUE: i bought it for my boyfriend\n",
      "40 PRED: i is this for my wife\n",
      "41 TRUE: he was quite satisfied with this item\n",
      "41 PRED: i loves very pleased\n",
      "42 TRUE: and it is cheap , good for students\n",
      "42 PRED: i i is a\n",
      "43 TRUE: bought this backpack for my husband\n",
      "43 PRED: i this bag for my husband\n",
      "44 TRUE: he was looking for a scansmart backback because of his frequent business travels\n",
      "44 PRED: i uses looking for a backpack backpack that carry my laptop and his\n",
      "45 TRUE: it is heavy duty and can easily hold two laptops and files\n",
      "45 PRED: the has comfortable enough and and easily hold all lenses and books , and , ,\n",
      "46 TRUE: nice compartments to keep organized\n",
      "46 PRED: it has are great and\n",
      "47 TRUE: purchased this bag in place of my old roller bag\n",
      "47 PRED: i this bag for my for my for # bag\n",
      "48 TRUE: this is nice and light to carry , has a great strap for hanging it off the shoulder and it also has nice storage capability\n",
      "48 PRED: the is a and has weight carry\n",
      "49 TRUE: i would order another of these if i ever needed one\n",
      "49 PRED: i have n't this one these\n",
      "50 TRUE: this one appears to be wearing like iron and most likely will last for years\n",
      "50 PRED: i is is to be well of the and the of have last a years\n",
      "51 TRUE: i bought this case to use for my iconia tablet or my # . # `` netbook depending on which i needed to take\n",
      "51 PRED: it bought this for for store my my new # and # # `` # inch ; laptop in it i the\n",
      "52 TRUE: both fit perfectly !\n",
      "52 PRED: it color ! and !\n",
      "53 TRUE: good build quality at a great price !\n",
      "53 PRED: i is is of the\n",
      "54 TRUE: if you are looking for a larger gadget bag that is easy to take on the road this one if for you\n",
      "54 PRED: i is 're looking for a bag bag , , this this this this it your road\n",
      "55 TRUE: it has the room for a strobe , two camera bodies and several lenses plus accessories\n",
      "55 PRED: it has plenty laptop for my laptop , lenses , , , a lenses\n",
      "56 TRUE: there are plenty of straps and pockets for you to load down with other gadgets or stuff for a hike\n",
      "56 PRED: the are plenty of compartments to a for everything\n",
      "57 TRUE: i have had no problems going through the <unk> with this bag , so far\n",
      "57 PRED: i have had this problems with to security airport and the bag\n",
      "58 TRUE: it 's been screened # times and each time , no questions asked\n",
      "58 PRED: i 's been a a years so i time i i it\n",
      "59 TRUE: great purchase if you travel alot\n",
      "59 PRED: i is this you are looking of and and\n",
      "60 TRUE: - tons of room for carrying extra <unk> strap/handle combo gives good <unk> quality construction than i 've found in other messenger bag style laptop <unk> enough to not even look like a laptop <unk> bad\n",
      "60 PRED: it the of pockets\n",
      "61 TRUE: - fold over flap is the only top closure and does n't even cover the width of the notebook compartment , leaving electronics and other items in the bag little protection from the\n",
      "61 PRED: i the in for on a right one of\n",
      "62 TRUE: not any complaints , have been using it for months at college and it does its job very well with no issues\n",
      "62 PRED: i i of , i been using it for for\n",
      "63 TRUE: would recommend to anyone with a # inch laptop\n",
      "63 PRED: i is is be my a # & #\n"
     ]
    }
   ],
   "source": [
    "if len(log_df) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "while epoch < config.epochs:\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = get_feed_dict(batch)\n",
    "\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, sent_loss_kl_categ_batch, sent_loss_kl_gmm_batch, ppls_batch = \\\n",
    "        sess.run([opt, loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, sent_loss_kl_categ, sent_loss_kl_gmm, topic_ppls], feed_dict = feed_dict)\n",
    "   \n",
    "        if sent_loss_kl_batch == np.inf:\n",
    "            print('Nan occured')\n",
    "            ckpt = tf.train.get_checkpoint_state(config.modeldir)\n",
    "            model_checkpoint_path = ckpt.all_model_checkpoint_paths[-1]\n",
    "            saver.restore(sess, model_checkpoint_path)            \n",
    "            break\n",
    "            \n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if ct%config.log_period==0:\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train, sent_loss_recon_train, sent_loss_kl_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, sent_loss_recon_dev, sent_loss_kl_dev, ppl_dev = get_loss(sess, dev_batches)\n",
    "            global_step_log, beta_eval = sess.run([tf.train.get_global_step(), beta])\n",
    "            \n",
    "            if loss_dev < loss_min:\n",
    "                loss_min = loss_dev\n",
    "                saver.save(sess, config.modelpath, global_step=global_step_log)\n",
    "\n",
    "            clear_output()\n",
    "    \n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_reg_train, '%.2f'%sent_loss_recon_train, '%.2f'%sent_loss_kl_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_reg_dev, '%.2f'%sent_loss_recon_dev, '%.2f'%sent_loss_kl_dev,  '%.3f'%beta_eval],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "\n",
    "            print_summary(test_batches[1][1])\n",
    "            print_sample(batch)\n",
    "            \n",
    "            time_start = time.time()\n",
    "            \n",
    "    epoch += 1\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_prob_topic, _prob_topic_sents, _prob_topic_infer, _means_topic_infer = debug_value([prob_topic, prob_topic_sents, prob_topic_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_i = 4\n",
    "_prob_topic_sents[batch_i], _prob_topic_infer[batch_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_means_topic_infer[0][:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_means_topic, b_means_topic = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"topic/dec/mean_topic\")\n",
    "\n",
    "pred_topic_embeddings, pred_topic_bow, pred_means_topic, pred_logvars_topic, pred_token_idxs, _w_means_topic, _b_means_topic, _w_mean_topic_infer = \\\n",
    "                                sess.run([topic_embeddings, topic_bow, means_topic, logvars_topic, topic_beam_output_token_idxs, w_means_topic, b_means_topic, w_mean_topic_infer], \n",
    "                                         feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "\n",
    "pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "\n",
    "pred_topics_freq_bow_indices = np.argsort(pred_topic_bow, 1)[:, ::-1][:, :10]\n",
    "pred_topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idxs in pred_topics_freq_bow_idxs:\n",
    "    print([idx_to_word[idx] for idx in idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_topic_embeddings[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_topic_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_w_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_w_mean_topic_infer[:, :10, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_enc_state_infer, _means_topic_infer = debug_value([enc_state_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_enc_state_infer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_means_topic_infer[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
