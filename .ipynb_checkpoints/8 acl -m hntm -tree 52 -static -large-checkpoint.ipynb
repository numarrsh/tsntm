{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from data_structure import get_batches\n",
    "from hntm import HierarchicalNeuralTopicModel\n",
    "from tree import get_descendant_idxs\n",
    "from evaluation import validate, get_hierarchical_affinity, get_topic_specialization, print_topic_sample\n",
    "from configure import get_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(nb_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu\n",
    "np.random.seed(config.seed)\n",
    "random.seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.path_data,'rb'))\n",
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)\n",
    "config.dim_bow = len(bow_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0,
     10
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables, model):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, model, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    return _variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = []\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "ppl_min = np.inf\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','','','','','VALID:','','','','','TEST:','', 'SPEC:', '', '', 'HIER:', ''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL','REG','LOSS','PPL','NLL','KL','REG','LOSS','PPL', '1', '2', '3', 'CHILD', 'OTHER']]))))\n",
    "\n",
    "cmd_rm = 'rm -r %s' % config.dir_model\n",
    "res = subprocess.call(cmd_rm.split())\n",
    "cmd_mk = 'mkdir %s' % config.dir_model\n",
    "res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "def update_checkpoint(config, checkpoint, global_step):\n",
    "    checkpoint.append(config.path_model + '-%i' % global_step)\n",
    "    if len(checkpoint) > config.max_to_keep:\n",
    "        path_model = checkpoint.pop(0) + '.*'\n",
    "        for p in glob.glob(path_model):\n",
    "            os.remove(p)\n",
    "    cPickle.dump(checkpoint, open(config.path_checkpoint, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "model = HierarchicalNeuralTopicModel(config)\n",
    "sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(max_to_keep=config.max_to_keep)\n",
    "update_tree_flg = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train & validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th colspan=\"4\" halign=\"left\"></th>\n",
       "      <th>VALID:</th>\n",
       "      <th colspan=\"4\" halign=\"left\"></th>\n",
       "      <th>TEST:</th>\n",
       "      <th></th>\n",
       "      <th>SPEC:</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "      <th>HIER:</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>CHILD</th>\n",
       "      <th>OTHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>74</td>\n",
       "      <td>8</td>\n",
       "      <td>463</td>\n",
       "      <td>9533.87</td>\n",
       "      <td>1781</td>\n",
       "      <td>9511.69</td>\n",
       "      <td>22.00</td>\n",
       "      <td>0.19</td>\n",
       "      <td>9373.25</td>\n",
       "      <td>1750</td>\n",
       "      <td>9350.33</td>\n",
       "      <td>22.76</td>\n",
       "      <td>0.15</td>\n",
       "      <td>9373.19</td>\n",
       "      <td>1750</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>65</td>\n",
       "      <td>17</td>\n",
       "      <td>360</td>\n",
       "      <td>9505.93</td>\n",
       "      <td>1742</td>\n",
       "      <td>9483.42</td>\n",
       "      <td>22.32</td>\n",
       "      <td>0.17</td>\n",
       "      <td>9363.04</td>\n",
       "      <td>1736</td>\n",
       "      <td>9340.63</td>\n",
       "      <td>22.26</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9363.26</td>\n",
       "      <td>1736</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>68</td>\n",
       "      <td>26</td>\n",
       "      <td>257</td>\n",
       "      <td>9493.16</td>\n",
       "      <td>1724</td>\n",
       "      <td>9470.73</td>\n",
       "      <td>22.24</td>\n",
       "      <td>0.16</td>\n",
       "      <td>9359.11</td>\n",
       "      <td>1729</td>\n",
       "      <td>9336.83</td>\n",
       "      <td>22.15</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9359.06</td>\n",
       "      <td>1729</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>68</td>\n",
       "      <td>35</td>\n",
       "      <td>154</td>\n",
       "      <td>9485.31</td>\n",
       "      <td>1714</td>\n",
       "      <td>9462.93</td>\n",
       "      <td>22.14</td>\n",
       "      <td>0.15</td>\n",
       "      <td>9357.38</td>\n",
       "      <td>1727</td>\n",
       "      <td>9335.55</td>\n",
       "      <td>21.70</td>\n",
       "      <td>0.13</td>\n",
       "      <td>9356.75</td>\n",
       "      <td>1727</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>67</td>\n",
       "      <td>44</td>\n",
       "      <td>51</td>\n",
       "      <td>9479.77</td>\n",
       "      <td>1707</td>\n",
       "      <td>9457.43</td>\n",
       "      <td>22.03</td>\n",
       "      <td>0.15</td>\n",
       "      <td>9355.67</td>\n",
       "      <td>1724</td>\n",
       "      <td>9334.09</td>\n",
       "      <td>21.46</td>\n",
       "      <td>0.13</td>\n",
       "      <td>9355.44</td>\n",
       "      <td>1724</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>71</td>\n",
       "      <td>52</td>\n",
       "      <td>515</td>\n",
       "      <td>9475.86</td>\n",
       "      <td>1701</td>\n",
       "      <td>9453.62</td>\n",
       "      <td>21.93</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9354.64</td>\n",
       "      <td>1723</td>\n",
       "      <td>9333.33</td>\n",
       "      <td>21.20</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9355.96</td>\n",
       "      <td>1725</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35000</th>\n",
       "      <td>89</td>\n",
       "      <td>61</td>\n",
       "      <td>412</td>\n",
       "      <td>9473.04</td>\n",
       "      <td>1697</td>\n",
       "      <td>9450.82</td>\n",
       "      <td>21.84</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9355.26</td>\n",
       "      <td>1723</td>\n",
       "      <td>9333.97</td>\n",
       "      <td>21.17</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9355.96</td>\n",
       "      <td>1725</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40000</th>\n",
       "      <td>78</td>\n",
       "      <td>70</td>\n",
       "      <td>309</td>\n",
       "      <td>9470.78</td>\n",
       "      <td>1694</td>\n",
       "      <td>9448.55</td>\n",
       "      <td>21.76</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9355.38</td>\n",
       "      <td>1724</td>\n",
       "      <td>9334.13</td>\n",
       "      <td>21.13</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9355.96</td>\n",
       "      <td>1725</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      TRAIN:                               VALID:        \\\n",
       "      Time  Ep   Ct     LOSS   PPL      NLL     KL   REG     LOSS   PPL   \n",
       "5000    74   8  463  9533.87  1781  9511.69  22.00  0.19  9373.25  1750   \n",
       "10000   65  17  360  9505.93  1742  9483.42  22.32  0.17  9363.04  1736   \n",
       "15000   68  26  257  9493.16  1724  9470.73  22.24  0.16  9359.11  1729   \n",
       "20000   68  35  154  9485.31  1714  9462.93  22.14  0.15  9357.38  1727   \n",
       "25000   67  44   51  9479.77  1707  9457.43  22.03  0.15  9355.67  1724   \n",
       "30000   71  52  515  9475.86  1701  9453.62  21.93  0.14  9354.64  1723   \n",
       "35000   89  61  412  9473.04  1697  9450.82  21.84  0.14  9355.26  1723   \n",
       "40000   78  70  309  9470.78  1694  9448.55  21.76  0.14  9355.38  1724   \n",
       "\n",
       "                               TEST:       SPEC:             HIER:        \n",
       "           NLL     KL   REG     LOSS   PPL     1     2     3 CHILD OTHER  \n",
       "5000   9350.33  22.76  0.15  9373.19  1750  0.37  0.34  0.42  0.38  0.31  \n",
       "10000  9340.63  22.26  0.14  9363.26  1736  0.39  0.37  0.42  0.36  0.29  \n",
       "15000  9336.83  22.15  0.14  9359.06  1729  0.39  0.39  0.43  0.34  0.28  \n",
       "20000  9335.55  21.70  0.13  9356.75  1727  0.39  0.40  0.44  0.32  0.27  \n",
       "25000  9334.09  21.46  0.13  9355.44  1724  0.40  0.40  0.43  0.33  0.28  \n",
       "30000  9333.33  21.20  0.12  9355.96  1725  0.41  0.38  0.43  0.34  0.29  \n",
       "35000  9333.97  21.17  0.12  9355.96  1725  0.40  0.39  0.43  0.33  0.29  \n",
       "40000  9334.13  21.13  0.12  9355.96  1725  0.41  0.39  0.44  0.33  0.28  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 R: 1.000 P: 0.113 speech user dialogue human utterance utterances speaker task time figure\n",
      "   1 R: 0.300 P: 0.059 models training embeddings network neural vector task dataset learning vectors\n",
      "     11 R: 0.072 P: 0.072 translation english source alignment target phrase translations pairs parallel systems\n",
      "     12 R: 0.096 P: 0.096 training models probability algorithm feature learning performance number test distribution\n",
      "     13 R: 0.072 P: 0.072 tree grammar rules node rule parsing trees figure nodes algorithm\n",
      "   2 R: 0.270 P: 0.068 sentiment classification feature positive task classifier negative analysis text tweets\n",
      "     21 R: 0.068 P: 0.068 languages english character morphological dictionary errors chinese segmentation arabic lexicon\n",
      "     22 R: 0.077 P: 0.077 document similarity documents topic method terms text sentences number term\n",
      "     23 R: 0.057 P: 0.057 pos dependency annotation syntactic parser parsing sentences tags annotated treebank\n",
      "   3 R: 0.317 P: 0.073 entity event question entities relation relations answer questions extraction events\n",
      "     31 R: 0.090 P: 0.090 text annotation web resources user tools project linguistic search texts\n",
      "     32 R: 0.067 P: 0.067 semantic sense lexical wordnet senses noun relations verb nouns verbs\n",
      "     33 R: 0.087 P: 0.087 verb discourse semantic structure syntactic case argument verbs relations type\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "while epoch < config.n_epochs:\n",
    "    # train\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = model.get_feed_dict(batch)\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, ppls_batch, global_step_log = \\\n",
    "        sess.run([model.opt, model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_reg, model.topic_ppls, tf.train.get_global_step()], feed_dict = feed_dict)\n",
    "\n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if global_step_log % config.log_period == 0:\n",
    "            # validate\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "\n",
    "            # test\n",
    "            if ppl_dev < ppl_min:\n",
    "                ppl_min = ppl_dev\n",
    "                loss_test, _, _, _, ppl_test, _ = validate(sess, test_batches, model)\n",
    "                saver.save(sess, config.path_model, global_step=global_step_log)\n",
    "                cPickle.dump(config, open(config.path_config % global_step_log, 'wb'))\n",
    "                update_checkpoint(config, checkpoint, global_step_log)\n",
    "            \n",
    "            # visualize topic\n",
    "            topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "            topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "            topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "            topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "            descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "            recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "            \n",
    "            depth_specs = get_topic_specialization(sess, model, instances_test)\n",
    "            hierarchical_affinities = get_hierarchical_affinity(sess, model)\n",
    "            \n",
    "            # log\n",
    "            clear_output()\n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_reg_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_reg_dev, \\\n",
    "                    '%.2f'%loss_test, '%.0f'%ppl_test, \\\n",
    "                    '%.2f'%depth_specs[1], '%.2f'%depth_specs[2], '%.2f'%depth_specs[3], \\\n",
    "                    '%.2f'%hierarchical_affinities[0], '%.2f'%hierarchical_affinities[1]],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "            cPickle.dump(log_df, open(os.path.join(config.path_log), 'wb'))\n",
    "            print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)\n",
    "\n",
    "            # update tree\n",
    "            if not config.static:\n",
    "                config.tree_idxs, update_tree_flg = model.update_tree(topic_prob_topic, recur_prob_topic)\n",
    "                if update_tree_flg:\n",
    "                    print(config.tree_idxs)\n",
    "                    name_variables = {tensor.name: variable for tensor, variable in zip(tf.global_variables(), sess.run(tf.global_variables()))} # store paremeters\n",
    "                    if 'sess' in globals(): sess.close()\n",
    "                    model = HierarchicalNeuralTopicModel(config)\n",
    "                    sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "                    name_tensors = {tensor.name: tensor for tensor in tf.global_variables()}\n",
    "                    sess.run([name_tensors[name].assign(variable) for name, variable in name_variables.items()]) # restore parameters\n",
    "                    saver = tf.train.Saver(max_to_keep=1)\n",
    "                \n",
    "            time_start = time.time()\n",
    "\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "    epoch += 1\n",
    "\n",
    "loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "display(log_df)\n",
    "print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
