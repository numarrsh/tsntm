{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from data_structure import get_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "from topic_model import TopicModel\n",
    "\n",
    "from topic_beam_search_decoder import BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '5', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/apnews/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/nvdm_vae', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'apnews', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 1000, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 500, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.1, 'lr')\n",
    "flags.DEFINE_float('reg', 1., 'regularization term')\n",
    "flags.DEFINE_float('beta', 0.001, 'initial value of beta')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_integer('warmup', 5000, 'warmup period for KL')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 10, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_bow', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_hidden_topic', 512, 'dim_hidden_topic')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "flags.DEFINE_bool('bidirectional', True, 'flg of bidirectional encoding')\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('PAD_IDX', word_to_idx[PAD], 'PAD_IDX')\n",
    "flags.DEFINE_integer('UNK_IDX', word_to_idx[UNK], 'UNK_IDX')\n",
    "flags.DEFINE_integer('BOS_IDX', word_to_idx[BOS], 'BOS_IDX')\n",
    "flags.DEFINE_integer('EOS_IDX', word_to_idx[EOS], 'EOS_IDX')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "maximum_iterations = max([max([instance.max_sent_l for instance in batch]) for ct, batch in dev_batches])\n",
    "flags.DEFINE_integer('maximum_iterations', maximum_iterations, 'maximum_iterations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build language model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['bow'] = tf.placeholder(tf.float32, [None, config.dim_bow], name='bow')\n",
    "t_variables['input_token_idxs'] = tf.placeholder(tf.int32, [None, None], name='input_token_idxs')\n",
    "t_variables['dec_input_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_input_idxs')\n",
    "t_variables['dec_target_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_target_idxs')\n",
    "t_variables['batch_l'] = tf.placeholder(tf.int32, name='batch_l')\n",
    "t_variables['doc_l'] = tf.placeholder(tf.int32, [None], name='doc_l')\n",
    "t_variables['sent_l'] = tf.placeholder(tf.int32, [None], name='sent_l')\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train', assertion=False):\n",
    "    def token_dropout(sent_idxs):\n",
    "        sent_idxs_dropout = np.asarray(sent_idxs)\n",
    "        sent_idxs_dropout[np.random.rand(len(sent_idxs)) > config.word_keep_prob] = config.UNK_IDX\n",
    "        return list(sent_idxs_dropout)\n",
    "\n",
    "    bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "    \n",
    "    doc_l = np.array([len(instance.token_idxs) for instance in batch])\n",
    "    \n",
    "    feed_input_token_idxs_list = [sent_idxs for instance in batch for sent_idxs in instance.token_idxs]\n",
    "    feed_dec_input_idxs_list = [[config.BOS_IDX] + token_dropout(sent_idxs) for sent_idxs in feed_input_token_idxs_list]\n",
    "    feed_dec_target_idxs_list = [sent_idxs + [config.EOS_IDX]  for sent_idxs in feed_input_token_idxs_list]\n",
    "        \n",
    "    sent_l = np.array([len(sent_idxs) for sent_idxs in feed_input_token_idxs_list], np.int32)\n",
    "    batch_l = len(sent_l)\n",
    "    \n",
    "    feed_input_token_idxs = pad_sequences(feed_input_token_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_input_idxs = pad_sequences(feed_dec_input_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_target_idxs = pad_sequences(feed_dec_target_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    \n",
    "    if assertion:\n",
    "        index = 0\n",
    "        for instance in batch:\n",
    "            for line_idxs in instance.token_idxs:\n",
    "                assert feed_input_token_idxs_list[index] == line_idxs\n",
    "                index += 1\n",
    "        assert feed_input_token_idxs.shape[1] == np.max(sent_l)\n",
    "        assert feed_dec_input_idxs.shape[1] == np.max(sent_l) + 1\n",
    "        assert feed_dec_target_idxs.shape[1] == np.max(sent_l) + 1\n",
    "    \n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "\n",
    "    feed_dict = {\n",
    "                t_variables['bow']: bow, \n",
    "                t_variables['batch_l']: batch_l, t_variables['doc_l']: doc_l, t_variables['sent_l']: sent_l, \n",
    "                t_variables['input_token_idxs']: feed_input_token_idxs, t_variables['dec_input_idxs']: feed_dec_input_idxs, t_variables['dec_target_idxs']: feed_dec_target_idxs, \n",
    "                t_variables['keep_prob']: keep_prob\n",
    "    }\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     10,
     24,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "def check_shape(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()\n",
    "    \n",
    "def check_value(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()    \n",
    "    \n",
    "# sent_loss_kl_categ_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, tf_log(prob_topic_infer/prob_topic_sents)), 1))\n",
    "# debug_value([sent_loss_kl_categ, sent_loss_kl_categ_tmp])\n",
    "# sent_loss_kl_gauss_tmp = 0.5 * tf.reduce_sum(tf.exp(logvars_topic_infer-logvars_topic) + tf.square(means_topic - means_topic_infer) / tf.exp(logvars_topic) - 1 + (logvars_topic - logvars_topic_infer), -1)\n",
    "# sent_loss_kl_gmm_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss_tmp), -1))\n",
    "# debug_value([sent_loss_kl_gmm_tmp, sent_loss_kl_gmm])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encode bow\n",
    "with tf.variable_scope('topic/enc', reuse=False):\n",
    "    hidden_bow_ = tf.layers.Dense(units=config.dim_hidden_bow, activation=tf.nn.relu, name='hidden_bow')(t_variables['bow'])\n",
    "    hidden_bow = tf.layers.Dropout(t_variables['keep_prob'])(hidden_bow_)\n",
    "    means_bow = tf.layers.Dense(units=config.dim_latent_bow, name='mean_bow')(hidden_bow)\n",
    "    logvars_bow = tf.layers.Dense(units=config.dim_latent_bow, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_bow)\n",
    "    latents_bow = sample_latents(means_bow, logvars_bow) # sample latent vectors\n",
    "\n",
    "    prob_topic = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic')(latents_bow) # inference of topic probabilities\n",
    "\n",
    "# decode bow\n",
    "with tf.variable_scope('shared', reuse=False):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "bow_embeddings = tf.nn.embedding_lookup(embeddings, bow_idxs) # embeddings of each bow features\n",
    "\n",
    "with tf.variable_scope('topic/dec', reuse=False):\n",
    "    topic_bow = tf.get_variable('topic_bow', [config.n_topic, config.dim_bow], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of topics\n",
    "    logits_bow = tf_log(tf.nn.softmax(tf.tensordot(prob_topic, topic_bow, axes=[[1], [0]]), 1))\n",
    "\n",
    "    # prior of each gaussian distribution (computed for each topic)\n",
    "    hidden_topic = tf.layers.Dense(units=config.dim_hidden_topic, activation=tf.nn.relu, name='hidden_topic')(topic_bow)\n",
    "    means_topic = tf.layers.Dense(units=config.dim_latent, name='mean_topic')(hidden_topic)\n",
    "    logvars_topic = tf.layers.Dense(units=config.dim_latent, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_topic)\n",
    "    sigma_topic = tf.exp(0.5 * logvars_topic)\n",
    "    gauss_topic = tfd.Normal(loc=means_topic, scale=sigma_topic)    \n",
    "    \n",
    "# define losses\n",
    "topic_losses_recon = -tf.reduce_sum(tf.multiply(t_variables['bow'], logits_bow), 1)\n",
    "topic_loss_recon = tf.reduce_mean(topic_losses_recon) # negative log likelihood of each words\n",
    "\n",
    "topic_loss_kl = compute_kl_loss(means_bow, logvars_bow) # KL divergence b/w latent dist & gaussian std\n",
    "\n",
    "topic_bow_norm = topic_bow / tf.norm(topic_bow, axis=1, keepdims=True)\n",
    "topic_dots = tf.clip_by_value(tf.matmul(topic_bow_norm, tf.transpose(topic_bow_norm)), -1., 1.)\n",
    "topic_loss_reg = tf.reduce_mean(tf.square(topic_dots - tf.eye(config.n_topic)))\n",
    "# topic_angles = tf.acos(topic_dots)\n",
    "# topic_angles_mean = tf.reduce_mean(topic_angles)\n",
    "# topic_angles_vars = tf.reduce_mean(tf.square(topic_angles - topic_angles_mean))\n",
    "# topic_loss_reg = tf.exp(topic_angles_vars - topic_angles_mean)\n",
    "\n",
    "# monitor\n",
    "n_bow = tf.reduce_sum(t_variables['bow'], 1)\n",
    "topic_ppls = tf.divide(topic_losses_recon, n_bow)\n",
    "topics_freq_bow_indices = tf.nn.top_k(topic_bow, 10, name='topic_freq_bow').indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "input_token_idxs = t_variables['input_token_idxs']\n",
    "batch_l = t_variables['batch_l']\n",
    "sent_l = t_variables['sent_l']\n",
    "max_sent_l = tf.reduce_max(sent_l)\n",
    "\n",
    "with tf.variable_scope('sent/enc', reuse=False):\n",
    "    # get word embedding\n",
    "    enc_input = tf.nn.embedding_lookup(embeddings, input_token_idxs)\n",
    "\n",
    "    # get sentence embedding\n",
    "    _, enc_state = dynamic_bi_rnn(enc_input, sent_l, config.dim_hidden, t_variables['keep_prob'])\n",
    "\n",
    "    # TODO House Holder flow\n",
    "    hidden_topic_infer =  tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='hidden_topic_infer')(enc_state)\n",
    "    prob_topic_infer = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic_infer')(hidden_topic_infer)\n",
    "\n",
    "    w_mean_topic_infer = tf.get_variable('mean_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32)\n",
    "    b_mean_topic_infer = tf.get_variable('mean_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32)\n",
    "    means_topic_infer = tf.tensordot(enc_state, w_mean_topic_infer, axes=[[1], [1]]) + b_mean_topic_infer\n",
    "    \n",
    "    w_logvar_topic_infer = tf.get_variable('logvar_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    b_logvar_topic_infer = tf.get_variable('logvar_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    logvars_topic_infer = tf.tensordot(enc_state, w_logvar_topic_infer, axes=[[1], [1]]) + b_logvar_topic_infer\n",
    "    sigma_topic_infer = tf.exp(0.5 * logvars_topic_infer)\n",
    "    gauss_topic_infer = tfd.Normal(loc=means_topic_infer, scale=sigma_topic_infer)\n",
    "    \n",
    "    # latent vectors from each gaussian dist.\n",
    "    latents_topic_infer = sample_latents(means_topic_infer, logvars_topic_infer) \n",
    "    # latent vector from gaussian mixture\n",
    "    latents_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), latents_topic_infer, transpose_a=True)\n",
    "    \n",
    "    # for beam search\n",
    "    means_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), means_topic_infer, transpose_a=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for decoding\n",
    "dec_sent_l = tf.add(sent_l, 1)\n",
    "dec_input_idxs = t_variables['dec_input_idxs']\n",
    "dec_input = tf.nn.embedding_lookup(embeddings, dec_input_idxs)\n",
    "\n",
    "dec_latents_input = tf.tile(latents_input, [1, tf.shape(dec_input)[1], 1])\n",
    "dec_concat_input = tf.concat([dec_input, dec_latents_input], -1)\n",
    "\n",
    "# decode for training\n",
    "with tf.variable_scope('sent/dec/rnn', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32, reuse=False):\n",
    "    dec_cell = tf.contrib.rnn.GRUCell(config.dim_hidden)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob = t_variables['keep_prob'])\n",
    "\n",
    "    dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(latents_input, 1))\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_concat_input, sequence_length=dec_sent_l)\n",
    "\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=dec_cell,\n",
    "        helper=helper,\n",
    "        initial_state=dec_initial_state)\n",
    "\n",
    "    dec_outputs, _, output_sent_l = tf.contrib.seq2seq.dynamic_decode(train_decoder)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(config.n_vocab, use_bias=False, name='out')\n",
    "    output_logits = output_layer(dec_outputs.rnn_output)\n",
    "    \n",
    "    output_token_idxs = tf.argmax(output_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tokens = tf.fill([batch_l], config.BOS_IDX)\n",
    "end_token = config.EOS_IDX\n",
    "\n",
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    infer_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(means_input, 1))\n",
    "    beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(infer_dec_initial_state, multiplier=config.beam_width)\n",
    "    beam_latents_input = tf.contrib.seq2seq.tile_batch(tf.squeeze(means_input, 1), multiplier=config.beam_width) # added\n",
    "    \n",
    "    beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=beam_latents_input)\n",
    "\n",
    "    beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    beam_output_token_idxs = beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    inter_means_input = tf.placeholder(tf.float32, [None, config.dim_latent])\n",
    "    \n",
    "    inter_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(inter_means_input)\n",
    "    inter_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(inter_dec_initial_state, multiplier=config.beam_width)\n",
    "    inter_beam_latents_input = tf.contrib.seq2seq.tile_batch(inter_means_input, multiplier=config.beam_width) # added\n",
    "    \n",
    "    inter_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=inter_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=inter_beam_latents_input)\n",
    "\n",
    "    inter_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inter_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    inter_beam_output_token_idxs = inter_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    topic_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic)\n",
    "    topic_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(topic_dec_initial_state, multiplier=config.beam_width)\n",
    "    topic_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic, multiplier=config.beam_width) # added\n",
    "    \n",
    "    topic_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=topic_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=topic_beam_latents_input)\n",
    "\n",
    "    topic_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        topic_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    topic_beam_output_token_idxs = topic_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language modeling cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target and mask\n",
    "dec_target_idxs = t_variables['dec_target_idxs']\n",
    "dec_mask_tokens = tf.sequence_mask(dec_sent_l, maxlen=max_sent_l+1, dtype=tf.float32)\n",
    "\n",
    "# nll for each token (averaged over batch & sentence)\n",
    "sent_loss_recon = tf.contrib.seq2seq.sequence_loss(output_logits, dec_target_idxs, dec_mask_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_l = t_variables['doc_l']\n",
    "mask_sents = tf.sequence_mask(doc_l)\n",
    "mask_sents_flatten = tf.reshape(mask_sents, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1]])\n",
    "\n",
    "prob_topic_tiled = tf.tile(tf.expand_dims(prob_topic, 1), [1, tf.shape(mask_sents)[1], 1])\n",
    "prob_topic_flatten = tf.reshape(prob_topic_tiled, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1], config.n_topic])\n",
    "prob_topic_sents = tf.boolean_mask(prob_topic_flatten, mask_sents_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferred mixture probabilities (computed for each sentence)\n",
    "categ_topic_infer = tfd.Categorical(probs=prob_topic_infer)\n",
    "\n",
    "# prior of mixture probabilities (computed for each document, tiled for each sentence)\n",
    "categ_topic = tfd.Categorical(probs=prob_topic_sents)\n",
    "\n",
    "sent_loss_kl_categ = tf.reduce_mean(tfd.kl_divergence(categ_topic_infer, categ_topic))\n",
    "\n",
    "# inference of each gaussian gaussribution (computed for each sentence)\n",
    "\n",
    "sent_loss_kl_gauss = tf.reduce_sum(tfd.kl_divergence(gauss_topic_infer, gauss_topic), -1)\n",
    "sent_loss_kl_gmm = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss), -1))\n",
    "\n",
    "sent_loss_kl = sent_loss_kl_categ + sent_loss_kl_gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "beta = tf.Variable(config.beta, name='beta', trainable=False) if config.warmup > 0 else tf.constant(1., name='beta')\n",
    "update_beta = tf.assign_add(beta, 1./(config.warmup*len(train_batches)))\n",
    "sent_loss = sent_loss_recon + beta * sent_loss_kl\n",
    "\n",
    "topic_loss = topic_loss_recon + topic_loss_kl + config.reg * topic_loss_reg\n",
    "loss = topic_loss + sent_loss\n",
    "\n",
    "# define optimizer\n",
    "if config.opt == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif config.opt == 'Adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "    \n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_sents(token_idxs, config, idx_to_word):\n",
    "    sents = []\n",
    "    for sent_idxs in token_idxs:\n",
    "        tokens = []\n",
    "        for idx in sent_idxs:\n",
    "            if idx == config.EOS_IDX: break\n",
    "            tokens.append(idx_to_word[idx])\n",
    "        sent = ' '.join(tokens)\n",
    "        sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_batch, sent_loss_batch, ppls_batch = sess.run([loss, topic_loss, sent_loss, topic_ppls], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_batch, sent_loss_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "    loss_mean, topic_loss_mean, sent_loss_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    return loss_mean, topic_loss_mean, sent_loss_mean, ppl_mean\n",
    "\n",
    "def get_all_losses(sess, batches):\n",
    "    losses = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch = \\\n",
    "        sess.run([loss, topic_loss_recon, topic_loss_kl, sent_loss_recon, sent_loss_kl], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "    print('LOSS %.2f | TM NLL: %.2f, KL: %.4f | LM NLL: %.2f, KL: %.4f' %  np.mean(losses, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_batch):\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    pred_token_idxs = sess.run(output_token_idxs, feed_dict = feed_dict)\n",
    "    true_token_idxs = [sent_idxs for instance in sample_batch for sent_idxs in instance.token_idxs]\n",
    "    \n",
    "    assert len(pred_token_idxs) == len(true_token_idxs)\n",
    "    \n",
    "    pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "    true_sents = idxs_to_sents(true_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    for i, (true_sent, pred_sent) in enumerate(zip(true_sents, pred_sents)):        \n",
    "        print(i, 'True: %s' % true_sent)\n",
    "        print(i, 'Pred: %s' % pred_sent)\n",
    "\n",
    "def print_topic_sample():\n",
    "    pred_topics_freq_bow_indices, pred_topic_token_idxs = sess.run([topics_freq_bow_indices, topic_beam_output_token_idxs], \n",
    "                                                                                                           feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "    pred_topic_sents = idxs_to_sents(pred_topic_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Topic Samples-----------')\n",
    "    for i, (topic_freq_bow_idxs, pred_topic_sent) in enumerate(zip(topics_freq_bow_idxs, pred_topic_sents)):\n",
    "        print(i, ' bow:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' sent:', pred_topic_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "logs = []\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "saver = tf.train.Saver(max_to_keep=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "033[s], 30[s], Ep: 00, Ct: 00000|TR LOSS: 347, PPL: 2661|TM NLL: 336, KL: 0.49, REG:0.00 | LM NLL: 10.34, KL: 1.51|DE LOSS: 347, PPL: 2659, TM: 337, LM: 10.34|BETA: 0.001000\n",
      "0 True: loews hotels & resorts said tuesday that it will buy the back bay hotel in boston from the doyle collection for an undisclosed price\n",
      "0 Pred: emitted wilmington buffington hives buffington finnish wilmington naturalized wilmington thoreson dieterle demolishing demolishing naturalized torrington establishes kitten kitten speight premieres cavity torrington die cactus gender partnered craps biblical helburn gender premieres courts courts courts gender diplomat nosack hurricanes\n",
      "1 True: the company , which is a subsidiary of loews corp. , said the historic hotel is a unique addition to its portfolio\n",
      "1 Pred: ellspermann lucrative fitness fitness fitness klain rustic keith keith panu mobsters shelve shelve keith satisfactory baines keith filed mobsters keith panu increasingly boland rustic filed pioneering users rustic filed abstaining panu shelve mobsters ut keith filed filed filed\n",
      "2 True: the building was constructed in the # s as the boston police department headquarters and was used as such until the late # s\n",
      "2 Pred: girard viacom cropland signals nora douglass unorganized legalizing scrub cavity unorganized ladner ladner logistics stamping wilmington logistics pioneer signals runyon ladner explicitly tualatin precaution leaning nate cavity unorganized ladner signals logistics derderian housekeeping logistics ladner nate unorganized tualatin\n",
      "3 True: it was redeveloped in # and has # <unk>\n",
      "3 Pred: quentin shari normandy cesarean cesarean cesarean cesarean cesarean nba shakers cesarean cesarean cesarean pedaling unreasonable purses burney cesarean chose assault cesarean chose burney endo ing abolitionist cesarean cesarean cesarean neck rapids giveaway cesarean gq deficit sixty finding burney\n",
      "4 True: the deal is expected to close in february\n",
      "4 Pred: industrial frickel dedicating choice choice slogan choice slogan choice choice striped slogan rwandan proceed proceed logistics undressed choice slogan slogan slogan striped figoski randy logistics welles striped slogan slogan striped forfeited welles forfeited slogan choice slogan randy randy\n",
      "5 True: loews , based in new york , will hold # hotels following this deal , including two under construction\n",
      "5 Pred: glenn hilario flaherty perchlorate urging welles perchlorate maneuvers plunging urging maneuvers maneuvers scuffled elected rwanda maneuvers welles welles articles maneuvers dashcam maneuvers welles dashcam vests aetna carley articles welles welles maneuvers dispatched welles welles welles goodrich die maneuvers\n",
      "6 True: it follows several other recent acquisitions by loews , including the madison hotel in washington , d.c.\n",
      "6 Pred: philippines drone drone drone bollea landowner joyride tongue babb horns stables tongue tongue tongue stables tongue stables smackover tongue stables starnews stables werdesheim metrohealth deep desegregation democker stables constables midmichigan reed montano montano juneau relation desegregation electricians baisden\n",
      "7 True: the company said it will continue to look at acquisitions in gateway cities and resort destinations as part of its plan to add hotels over the next five years\n",
      "7 Pred: ua packers neutralized dormer packers via branches vie signs britain via britain vie wolfson signs borman britain britain briefing signs vie signs signs britain briefing signs palestinian britain stated britain britain stated signs stated britain britain vie britain\n",
      "8 True: shares of loews corp. gained # cents to $ # by early afternoon\n",
      "8 Pred: runs princeton northera niger dashcam defibrillators bayonne eog plunging cabral sperling dedicating bayonne gering tallies flooding cabral cabral an cabral cabral sunken eog tracking dedicating gering cabral dedicating cabral cabral cabral boy monetary cabral confused dedicating cabral monetary\n",
      "9 True: its shares are near the top end of its # week trading range of $ # and $ #\n",
      "9 Pred: surplus atria bc hollenbeck keith poorest amerigas vinson equipped jada sarver projection tailgate sarver sarver sarver sarver vinson sarver furtherance sarver lorie vinson boland vinson furtherance sarver clues siddiqui sarver sarver keith vinson sarver sarver outlaw sarver vinson\n",
      "10 True: an ohio state lawmaker facing drunken driving charges in neighboring indiana tells his hometown newspaper he has pleaded not guilty to the charge\n",
      "10 Pred: harvison everson humphreys pistols milne sweeten motorola rhino spacey lawmaking pistols coffers linger petting finishing spacey motorola configured motorola finishing dailey claiming whitestown mongolia motorola minimize excused finishing mongolia mongolia coffers mongolia mongolia dells coffers finishing typhoon finishing\n",
      "11 True: republican state rep. robert mecklenborg tells the cincinnati enquirer on thursday that he represents his constituents well and has `` every presumption of innocence at this time .\n",
      "11 Pred: moncure ap wyatt demetrios hilkey wyatt incinerator wyatt implement wyatt aware wyatt wyatt wyatt wyatt wyatt aware wyatt kittens bendelladj wyatt perdock drawer siddiqui wyatt siddiqui drawer wyatt aware wyatt wyatt redistribution disgruntled figueredo rsu schoelkopf figueredo figueredo\n",
      "12 True: mecklenborg was pulled over for a burned out headlight at # a.m. on april # in dearborn county , ind . , bordering the western part of hamilton county , which he represents\n",
      "12 Pred: kelso goncalves landslide implement biopharmaceutical biopharmaceutical bryson idea valid schaumburg outcomes determine idea idea idea yearbook schaumburg schaumburg shelters yegutkin wftv schaumburg schaumburg idea stimulate schaumburg idea determine byproducts libertarians overflowing idea wftv variable directed schaumburg poizner libertarians\n",
      "13 True: arrest records show the # year old mecklenborg failed several sobriety tests\n",
      "13 Pred: snuffed pinnacle pinnacle miles bluestone miles provisions manistee dedicating constrictor tour defunct miles tour festivals tour molokai tour moped rude trust tex monetary tulip veteran monetary fakes monetary sentinel weaving tour searles tex fakes molokai fakes fakes fakes\n",
      "14 True: a toxicology report registered his blood alcohol content at # percent\n",
      "14 Pred: iola l. backdoor squeeze profiling wilmington naumann impeachment supreme graphic mondale graphic naturalized harpootlian naumann highland graphic mondale impeachment photovoltaic impeachment impeachment graphic graphic impeachment impeachment busing mondale graphic bengal impeachment askew busing mondale graphic mondale mondale palos\n",
      "15 True: <unk> and another pharmaceutical drug were also picked up in his system\n",
      "15 Pred: porn issued rim farbest happening nonunion printers farewell rim accompany whereabouts derailment accompany happening farewell productive exercised derailment transparency gustavus happening kay transparency kay kay orioles accompany medvedev rim farewell factored altschuler productive conquest kay whereabouts depositary happening\n",
      "16 True: he had a female passenger in the car\n",
      "16 Pred: pertaining pertaining digit territorial cormier cormier cheers wallingford authority albania fairer wallingford cardiopulmonary wallingford outhouse fairer cardiopulmonary cardiopulmonary rum cardiopulmonary cardiopulmonary cardiopulmonary pena cardiopulmonary wire projection nap outhouse cardiopulmonary rum projection cardiopulmonary albania rum cardiopulmonary projection albania sanctuary\n",
      "17 True: he 's due in court on july #\n",
      "17 Pred: armentrout cato cato director lakeview delegates realignment untold dusseau divergent dusseau dusseau dusseau kpho ppg dusseau masters curator cardiopulmonary disappearance ppg difficulty midyear regulatory self promised bentonville curator dusseau jackets says turner tsunami hemorrhaging masters ye dusseau cardiopulmonary\n",
      "18 True: an amtrak train bound for new orleans struck a flatbed truck at a railway crossing in amite , killing the truck 's driver and injuring two people on the train\n",
      "18 Pred: yang willingly melgren parliamentary squeeze rummaging dormer choice kellogg melgren sliwoski rowe melgren melgren melgren rashad buffs window gathers melgren melgren melgren melgren enable melgren joann melgren melgren kellogg window joann continue rashad melgren melgren enable melgren melgren\n",
      "19 True: multiple news outlets report the crash happened sunday afternoon about # miles north of new orleans\n",
      "19 Pred: theme bludgeoning sangamon does gilmer stilwell tilley smoker paddled smoker britons smoker sargent whmi ending smoker smoker dusting bother smoker stalemate smoker smoker smoker ending kn smoker smoker brubaker britons pot smoker pot greening raymond biologist ending smoker\n",
      "20 True: amite fire chief bruce cutrer says the train sustained significant engine damage in the collision and was disabled\n",
      "20 Pred: embezzlement elmhurst unknowingly condemned jeannie immelt establishment immelt payday kyle pattern plantations billiton suffocate gupta bradley photographed abstaining immelt kelvin plantations newborn bubacz kelvin profile semitrailer adm clustered folklife bradley clustered immelt belonged clustered clustered bradley plus bradley\n",
      "21 True: the driver of the flatbed truck has not been identified\n",
      "21 Pred: avista stow avista airlifted cutie manhole disallow dusting keute plus limon manhole logging elon manhole display manhole caffeine keeseville manhole stella oakwood firearm erramilli manhole corral manhole hurricanes corral disallow voided ana manhole manhole dusting corral streisand focused\n",
      "22 True: two people on the train suffered minor injuries and were treated at a hospital\n",
      "22 Pred: pretrial manner formalizing zumiez such teleconference setback manner teleconference therapeutic lesko patients therapeutic hilkey contamination patients outages manner contamination contamination contamination steadman sold sold fortress sold contamination rwandan sold patients sold fortress forfeited sold sold accumulate sold contamination\n",
      "23 True: cutrer said passengers were taken to a nearby church to await buses sent by amtrak to take them to their destination it was unclear where the train originated\n",
      "23 Pred: schweitzer moonshine spanaway bevins spanaway kleefisch encrypt encrypt charlestown spanaway vivian burton promote vivian prone piazza vivian vivian vivian promote promote vivian burton kleefisch powerful genre burton capsize vivian vivian casino vivian vivian vivian promote promote burton vivian\n",
      "24 True: cutrer said authorities had not determined the reason for the crash\n",
      "24 Pred: fourqurean luger dardis strangely condor eradicate elsinore braved ariel humala rebels bunda oils platform winthrop oils oils undermining humala kaukani row oils hillside engagement nautical bunda dardis kaukani humala humala uses clocks jobs amie maliciously kaukani absentia humala\n",
      "25 True: meridian mayor percy bland declared a state of emergency that will allow the city to remove the vehicles and to hire engineers to assess the collapse\n",
      "25 Pred: sections tsunami mueller mueller repurchased mueller mueller nadia nadia desperately barboursville deaguero mueller nadia nadia reprimand greatly reprimand nadia resolute nadia amityville amounts standing reprimand crazy shiloh efrain repurchased crazy commander reprimand cheese nadia tenn. zappa resolute commander\n",
      "26 True: ten cars , three sport utility vehicles and a pickup truck are visible , bland said in a news release\n",
      "26 Pred: kerri kunduz credentials scholar credentials moines credentials purdue brittle dole wreck converge pretenses cavity ge insubordination transcript ancestry darted parkway crain cavity nilas chevrolet err cavity hardman cavity redesigned veon springsnail redesigned ahlquist divest nutritional springsnail cavity rochford\n",
      "27 True: no one was hurt when the parking lot collapsed saturday night at the <unk> restaurant that had opened four days earlier on an interstate # frontage road\n",
      "27 Pred: lamar louisville canfor penis doj penis louisville pummeled mcfarland sequoyah shuttered pummeled mahmoud clinton penis penis cordoned mahmoud civilly mahmoud shelve outright penis flammable penis mcchrystal mahmoud compel clinton regulation spoil llach rises mahmoud penis breakaway outright cooks\n",
      "28 True: the collapse opened a hole # feet ( nine meters ) wide and # feet ( # meters ) long\n",
      "28 Pred: armistead crippling creativity gun consolidating laughlin consolidating immigrated srp kayla pommier gang mizell papayas tsa peacefully augustin immigrated schipke cabral laughlin pommier laughlin upholds pommier pommier bylaw laughlin tsa laughlin vest laughlin rammed dansby cabral cabral cabral boone\n",
      "29 True: it follows a culvert that channels water to a creek .\n",
      "29 Pred: n't willingly payout inviting lakewood lakewood nadia wearing nadia nadia brabson u.k. nadia schneller nadia moderated nadia nadia adequate nadia nadia nadia nadia nadia nadia nadia nadia molested listened alger nadia nadia nadia nadia bayne nesmith srp nadia\n",
      "30 True: we are moving as fast as we can to access the situation and determine the cause of the drainage structure collapse , `` bland said .\n",
      "30 Pred: henri puetz poisonous hull furnishing obryan obryan obryan obryan dhallan obryan obryan peg consolidate obryan obryan obryan obryan reconditioning obryan obryan peg obryan dhallan obryan obryan umoja obryan pahrump consolidate obryan overall stew sultan obryan obryan obryan pappas\n",
      "31 True: our biggest concern right now is the safety of our citizens .\n",
      "31 Pred: architectural rollover level contacts irrigate classified blending frontrunner smackover midyear blending blending elkridge frontrunner elkridge appalachia frontrunner dilley ventures loosely appalachia razed reinstating blending valenta nordic valenta banes razed turner valenta banes valenta willie valenta blending reinstating valenta\n",
      "32 True: carnival 's conquest cruise ship departed mobile about two hours late on a shortened cruise to the bahamas\n",
      "32 Pred: armentrout emitted emitted evaluated argentine levee gowing stalsby levee dream schock dream dream dream chadwick trainer ceulen chilean dream bakken dream diaper stalsby diaper hilkey lembo jeffreys region dream dream stalsby eviction dream dream gowing diaper dream helburn\n",
      "33 True: the new orleans based ship docked at mobile on sunday because the mississippi river was closed to ships due to a tugboat sinking\n",
      "33 Pred: ferris ferris mud nemazee mud mud fatal mud mud bancshares mud mud mud constrictor mud burton molokai mud mud molokai mud mud guillermo mud matching fatal burton burton constrictor lizard supervises mud mud mud mud constrictor tumble modest\n",
      "34 True: it was supposed to depart about # p.m. monday , but it got away about # p.m. because a bus carrying baggage arrived late from new orleans\n",
      "34 Pred: bunny trustmark reconciliation hantavirus semiconductor kindergarten sync sanction bylaws morehouse thwarting grafton terpening dobry grafton dobry dobry dobry sanction dobry dobry dobry bylaws dobry dobry grafton dobry pelosi trips vanderbilt whoever sanction terpening bylaws grafton procedural dobry dobry\n",
      "35 True: the general manager of mobile 's cruise terminal , sheila <unk> , told al.com ( http : //bit.ly/ # <unk> ) that two workers suffered minor injuries when heavy luggage fell from a forklift\n",
      "35 Pred: tanker digit onetime difusco ktiv pioneer mock cabral ortho sorento cabral rohny pioneer jeopardized conduit pedaling undeclared boggs jeopardized cabral special plunged plowing carrot protect labrake transmitted pie supportive musgrave seat ortho pie pie pie wilder pie special\n",
      "36 True: carnival shortened the conquest 's route from seven days to six days after having to shift to mobile\n",
      "36 Pred: mckean responses thing vas drowning vas vas vas vas vas enactment enactment marin vas marin marin marin signal vas broadened bonneville marin hnulik broadened marin marin marin marin marin marin marin marin marin vas marin broadened loosely marin\n",
      "37 True: mobile 's terminal used to be home to carnival 's <unk> cruise ship , but carnival moved the ship to new orleans in october #\n",
      "37 Pred: shoes erica muscle translational impacting midvale hyoo hyoo tour midvale morganza tour springtime midvale diffusion tour tour tour tour diffusion tour tour powerful tour celina tour tour tour tour tour diffusion tour tour tour tour tour tour tour\n",
      "38 True: police say they used social media to arrest nine men in an online prostitution investigation over the weekend in storm lake\n",
      "38 Pred: parties annarbor.com jeep miracle miracle miracle wish schneller schneller schneller marty schneller franz schneller marty cabins shots cabins schneller cabins wish schneller shortage schneller wish schneller schneller schneller cabins schneller cabins kingstown cabins schneller schneller schneller schneller schneller\n",
      "39 True: kcau tv reports ( http : //bit.ly/ # el # amc ) that the storm lake police department conducted its investigation throughout the day friday\n",
      "39 Pred: runs reuniting reuniting reuniting indemnity moratorium palisi homebuyer fudge moratorium regrettable biehl dells homebuyer dells dells dells socks pikeminnow dells enforce cordoned dells fathers socks sanctuary socks homebuyer dells dells socks palisi dells fireplaces jammeh socks palisi fathers\n",
      "40 True: of the nine men arrested , eight are from iowa and one is from minnesota\n",
      "40 Pred: yamaha tafanelli bombay auditor persists alexander proceed persists montgomery mueller machete truth mueller machete machete arbitrary destabilize truth machete disputing graduating truth proceed truth pageant easter mohammad calculating graduating calculating audition swap calculating manternach sherman disputing hut shipholding\n",
      "41 True: police say the investigation was geared toward those soliciting prostitution\n",
      "41 Pred: westboro crossing ctfastrak ctfastrak lenor calories stave tycoon fluff incurred calories raping calories calories outpaced calories calories snowflakes engraved legislature fluff loaded hunka snowflakes fluff minivan snowflakes onalaska legislature schubert onalaska fluff outpaced calories legislature hunka calories cut\n",
      "42 True: they say the men connected with an undercover officer through social media and later met with the officer at a storm lake motel , where they were arrested\n",
      "42 Pred: teenage smead sonar callahan cursed turcotte poorest nelms limon but pornographic bremerton camarena venezuela sihk wester bremerton finan venezuela sightings bremerton sightings sightings nelms sightings venezuela mitchel nelms unfortunate finan mitchel el finan mcinally venezuela venezuela reassigned dawid\n",
      "43 True: all men face charges of solicitation of prostitution\n",
      "43 Pred: sarkozy undetermined importing importing shelved options hennigan logged ot thal thal kay cobra cobra cutbacks rushdi stockholm myran stockholm folklife myran sunflower raked townsend hennigan wtok stockholm myran stockholm myran deepening balloonists diffusion stockholm decided stockholm sestak balloonists\n",
      "44 True: eighth graders at a northeast ohio school are getting a lesson in the horror of the sept. # , # , terrorist attacks\n",
      "44 Pred: civics abusive retroactive girlfriend vovnenko waylon girlfriend counties waylon opposing spoiled waylon rawlins melgren witnesses medium waylon melgren netanyahu patuxent wyatt deisler melgren dead netanyahu menendez spoiled tracey deisler melgren regime witnesses medium merchison dead ladders metropolis waylon\n",
      "45 True: the akron beacon journal ( http : <unk> # ) reports that a class at st. paul of akron elementary school has written the names of every one of the sept. # victims on individual note cards\n",
      "45 Pred: saytex nidal phea phea fluids duplex converting drumming lane pence drumming converting drumming drumming drumming drumming drumming drumming drumming clip drumming targeted moses crack drumming obregon duplex devlin moses drumming herlihy moratorium drumming devlin drumming devlin equality parolees\n",
      "46 True: all # of them\n",
      "46 Pred: bourg utilities reformer truss raping bloated raping forecaster baumbach //wdhne.ws/ //wdhne.ws/ poaching //wdhne.ws/ umg admiral genesis bloated daniel raping smead roaming\n",
      "47 True: the idea is to pass out the cards at a local sept. # memorial on tuesday\n",
      "47 Pred: defended defended deb destroy destroy destroy adler slavery destroy adler fade schmoll dedicating rodrigues adler chevrolet wedding root humiliated chevrolet commendation destroy adler carrot tsa wilbert adler die albany demonstrates destroy humiliated adler defending chevrolet dedicating piano adler\n",
      "48 True: that 's the # th anniversary of the attacks on the world trade center and pentagon and the crash of united airlines flight # into a field in <unk> , pa.\n",
      "48 Pred: unrolled oversee investigative albans stephenson albans lisi dahl relentless burials mcadenville mcadenville wrapped mcqueen wrapped mcqueen veteran veteran rebel rebel whitefield bianchi wrapped rebel mcadenville flights veteran saturdays wrapped veteran wrapped wrapped wrapped bianchi primus sullivan tremblay wrapped\n",
      "49 True: the project was the idea of retired akron firefighter john <unk> , who put up a local # memorial display in #\n",
      "49 Pred: quentin luger glands parliamentary commotion buttes parliamentary trey shame lowry buttes shame buttes buttes chilean stearns redirecting quilts distributors bluffton wride mustapha kingstown klentschy steamboat kerfoot dhec cosmetics shourd meakens shourd bluffton zuberi zuberi redirecting baa eurostat arnam\n",
      "50 True: a bronze sculpture of purdue university 's # th century founder has been dedicated on the west lafayette campus not far from his final resting place\n",
      "50 Pred: marquette fruit probable opposite berks berks kankakee southern southern pcbs geneske geneske routt southern dannel scripture saltz barnaby slots editorial mistreated inflammation inflammation endymion washburn washburn intercepted collinsville doubled inflammation editorial washburn routt southern counseled inflammation inflammation inflammation\n",
      "51 True: the statue of john purdue seated with a cane in one hand and an open book in the other was unveiled sunday morning near the school 's memorial fountain and his campus gravesite\n",
      "51 Pred: ebix ebix coke jams saved schlitter wayne statute statute assurance comair autopsy hearts credentials credentials autopsy steep recess alexandria hearts wayne discriminated self kenton lutz self arraigned lutz assurance schlitter hearts photo recess courant self discriminated hearts hearts\n",
      "52 True: the journal & courier reports ( http : //on.jconline.com/ # <unk> ) that john purdue was a prominent lafayette businessman who died in #\n",
      "52 Pred: sarkozy bernadette smith melgren gering proposing peristere peristere ballet proposing peristere ballet proposing oktibbeha proposing ballet proposing blanton ballet cavender regrettable dream mitchem ballet bhp proposing proposing ballet frequently gilley tpc rapman proposing proposing cavender //is.gd/e regrettable regrettable\n",
      "53 True: he was the university 's primary early benefactor and was known for taking special interest in students , even paying for the education of those he thought held promise\n",
      "53 Pred: bystander gondolas godfrey helburn studebaker sylvia bain helburn impeachment helburn instinct helburn suffer impeachment britto suffer suffer suffer suffer suffer helburn impact helburn suffer impeachment bain helburn suffer adrian suffer photovoltaic helburn ingrid pupo suffer suffer britto helburn\n",
      "54 True: his statue now stands at the spot where he 's believed to have sat watching construction of purdue 's university hall in # , the year the school opened\n",
      "54 Pred: bolstered cote hattiesburg zynga kucinich bengals structuring mcgee mcgee difficulty regis artifacts difficulty mcgee leg awards tate reorganized ahmadinejad ahmadinejad perfume ahmadinejad leg severed difficulty reorganized tate mcgee difficulty abbeville difficulty mcrath branden drier difficulty hanel ahmadinejad difficulty\n",
      "55 True: authorities in eastern iowa say a # year old mushroom hunter who was reported missing has been located and is safe\n",
      "55 Pred: albans incentives incentives spanaway roiled pleading disgruntled augment abatement pleading bv reuniting disgruntled disgruntled disgruntled reuniting disgruntled disgruntled disgruntled bv jam impacting blagojevich disgruntled disgruntled leaking disgruntled disgruntled disgruntled disgruntled chimp disgruntled disgruntled disgruntled disgruntled disgruntled disgruntled cullison\n",
      "56 True: linn county officials say david stoneking disappeared on saturday in rural springville\n",
      "56 Pred: heidt heidt utilities herlihy magnum bloodshed herlihy somerville somerville durchslag consisting isaah consisting herlihy herlihy conover durchslag durchslag durchslag durchslag durchslag durchslag herlihy durchslag herlihy herlihy magnum biennium lewiston durchslag neurodegenerative herlihy herlihy isaah durchslag durchslag kleefisch durchslag\n",
      "57 True: emergency workers , family and friends searched for stoneking\n",
      "57 Pred: willingly katie kaukani dol spc spc modernize romania unscheduled bonnie practices californian ernaut bonnie freeman californian salas umass bonnie bonnie punched objecting bonnie darin rodent spc romania inquiries punched freeman punched umass list umass salas inquiries severed umass\n",
      "58 True: linn county deputies and a k # unit located stoneking early sunday in a field southwest of his home\n",
      "58 Pred: caledonia tusks tusks belew orioles tusks zais rhoads assess sonangol nebraskans ladder refuges refuges disarmament bobby gamette locke gamette refuges rawls thumb tsongas orioles branden branden branden refuges orioles branden gamette branden gamette branden tsongas gamette branden orioles\n",
      "59 True: he was in good condition and returned home\n",
      "59 Pred: scholar noon sagging ) scholar krause rsv ) pitch cpl nye molinaro cpl molinaro feather leonards nye natalia nye nye ushered nye whmi pitch nye strangled strangled bluffton tenants strangled nye nye nye strangled nye natalia amendments nye\n",
      "60 True: the superintendent of the craig school district is taking a job in oregon\n",
      "60 Pred: alarmed snow mejia exploration letters fuel sheen incapable valuables fuel fuel vacations jeffreys ridiculed 's 's exclusion demoted valuables jeffreys colony clergy speed fuel 's 's colony cebull 's seuvaai fuel hilkey fuel sylvia 's 's cebull fuel\n",
      "61 True: the ketchikan daily news reports ( http : //bit.ly/j # <unk> ) jim thomas is leaving to work as the superintendent of the <unk> school district in oregon\n",
      "61 Pred: earl these embraces purdue boosted hsien newsome procedural purdue jenison newsome articles teva modest boosted articles boosted burton jeffrey articles jeffrey polka articles articles jeffrey sydow jeffrey jeffrey jeffrey jeffrey teva jeffrey jeffrey jeffrey jeffrey jeffrey jeffrey jeffrey\n",
      "62 True: thomas has been with the craig district for three years\n",
      "62 Pred: schakowsky schakowsky sandbar stalling suites sedans wampler recommends suites tech sedans suites sedans ari sedans piazza recommends sedans ari sedans aetna wampler aetna wampler sedans decreases ari power narcotics piazza piazza arfmann sedans ari wampler sedans benefits wampler\n",
      "63 True: he said while he enjoyed his time in southeast alaska , he missed his family in oregon\n",
      "63 Pred: retry frias politics fredericks rech waits jc offender offender joins wages jc jc joins offender joins waits mrozek waits denouncing denouncing joins offender wibc offender players offender midterm skyline traders offender lindsey waits joins skyline joins durchslag monetary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Topic Samples-----------\n",
      "0  bow: mayor hire early concern good saturday family men truck april\n",
      "0  sent: dialysis cc cc authoritarian whale whale whale esso whale esso esso baileyville adjust adjust whale popular esso adjust popular adjust popular adjust popular adjust popular adjust popular adjust popular adjust popular adjust popular adjust popular adjust popular adjust popular adjust\n",
      "1  bow: orleans mobile october southwest rural minor arrived returned restaurant http\n",
      "1  sent: dialysis devries produces produces produces produces registered devries repealing repealing devries repealing repealing sullivan devries repealing bretz bretz bretz bretz bretz leanne exterminator exterminator exterminator exterminator lunar exterminator exterminator lunar lunar lunar repealing killdeer killdeer killdeer killdeer killdeer killdeer killdeer\n",
      "2  bow: sunday university arrived monday resort based place workers orleans told\n",
      "2  sent: marmolejo marmolejo arabia resort interrupting pick pick pick pick pick pick leopards leopards projection javelinas javelinas javelinas javelinas javelinas rosol rosol rosol rosol rosol rosol rosol rosol rosol rosol rosol rosol rosol rosol rosol projection projection projection javelinas javelinas javelinas\n",
      "3  bow: orleans http weekend located unit cities alcohol power florida senate\n",
      "3  sent: framework schakowsky deet deet deet geraldine deet axelrod stars stars stars stars janelle luge luge interrogation defends stars stars stars stars maps luge luge interrogation defends defends mcdowell cliatt cliatt cliatt cliatt cliatt cliatt cliatt cliatt cliatt alcorn alcorn specialty\n",
      "4  bow: train picked drunken passenger trade matt drive relations pleaded saturday\n",
      "4  sent: desertion fibers fibers pull stomps pulse pulse pulse pulse lisle pulse watching o'leary da pulse pulse pulse watching watching o'leary o'leary deep deep deep deep deep sewell deep shirts deep deep deep shirts deep deep deep shirts kobe kobe ebert\n",
      "5  bow: storm media district shares charges ship department social construction arrest\n",
      "5  sent: bliss bliss refusal obtaining obtaining refusal refusal treadwell schoonmaker schoonmaker schoonmaker treadwell schoonmaker schoonmaker treadwell schoonmaker treadwell schoonmaker treadwell schoonmaker treadwell schoonmaker treadwell schoonmaker treadwell schoonmaker treadwell schoonmaker treadwell schoonmaker treadwell desecration collaborative alligators schoonmaker schoonmaker schoonmaker treadwell treadwell treadwell\n",
      "6  bow: driver reports special suffered john sunday road d.c. moving lake\n",
      "6  sent: midafternoon snared snared radiation curious drama restructure restructure restructure snowboarders restructure captivity captivity criticized criticized criticized criticized criticized criticized criticized criticized criticized criticized criticized criticized criticized criticized criticized criticized criticized criticized criticized criticized criticized rory rory martini martini martini services\n",
      "7  bow: idea school airlines written http flight john district taking disabled\n",
      "7  sent: wexford gorder arranged torres torres bbcn municipality forums forums forums forums kukaniloko forums kukaniloko forums kukaniloko forums kukaniloko forums cardinals forums forums cardinals forums forums forums vassalboro forums forums vassalboro forums forums heredia forums forums vassalboro forums forums heredia forums\n",
      "8  bow: driving alcohol records lawmaker hamilton western tests drug show registered\n",
      "8  sent: dominoes chloe policy canvass canvass drugging drugging drugging trappers trappers trappers baldiviez mayodan mayodan wisn looms deane deane drugging trappers baldiviez trappers baldiviez mayodan wisn deane baldiviez oreo baldiviez baldiviez bar baldiviez oreo baldiviez bar baldiviez oreo baldiviez baldiviez baldiviez\n",
      "9  bow: john memorial spot truck daily vehicles died saturday education collapsed\n",
      "9  sent: unknowingly box box tearing tearing tearing tearing tearing tearing tearing tearing tearing tearing tearing biltmore biltmore biltmore biltmore biltmore biltmore broadcast broadcast biltmore broadcast broadcast broadcast biltmore broadcast broadcast biltmore broadcast broadcast biltmore broadcast biltmore broadcast biltmore broadcast broadcast broadcast\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-997319395edd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_recon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_kl_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_reg_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_recon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_kl_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_kl_categ_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_kl_gmm_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppls_batch\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_recon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_kl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_recon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_kl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_kl_categ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_kl_gmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_ppls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msent_loss_kl_batch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if len(logs) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "while epoch < config.epochs:\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = get_feed_dict(batch)\n",
    "        if config.warmup > 0 and beta_eval < 1.0: sess.run(update_beta)\n",
    "\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, sent_loss_kl_categ_batch, sent_loss_kl_gmm_batch, ppls_batch = \\\n",
    "        sess.run([opt, loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, sent_loss_kl_categ, sent_loss_kl_gmm, topic_ppls], feed_dict = feed_dict)\n",
    "        \n",
    "        if sent_loss_kl_batch == np.inf:\n",
    "            print('Nan occured')\n",
    "#             pdb.set_trace()\n",
    "            ckpt = tf.train.get_checkpoint_state(config.modeldir)\n",
    "            model_checkpoint_path = ckpt.all_model_checkpoint_paths[-1]\n",
    "            saver.restore(sess, model_checkpoint_path)            \n",
    "            break\n",
    "            \n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if ct%config.log_period==0:\n",
    "            time_dev = time.time()\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train, sent_loss_recon_train, sent_loss_kl_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_dev, sent_loss_dev, ppl_dev = get_loss(sess, dev_batches)\n",
    "            \n",
    "            if loss_dev < loss_min:\n",
    "                loss_min = loss_dev\n",
    "                saver.save(sess, config.modelpath, global_step=epoch*10000+ct)\n",
    "\n",
    "            if config.warmup > 0: beta_eval = beta.eval(session=sess)\n",
    "\n",
    "            clear_output()\n",
    "            time_finish = time.time()\n",
    "            time_log = int(time_finish - time_start)\n",
    "            time_log_dev = int(time_finish - time_dev)\n",
    "            logs += [(time_log, time_log_dev, epoch, ct, loss_train, ppl_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train, sent_loss_recon_train, sent_loss_kl_train, loss_dev, ppl_dev, topic_loss_dev, sent_loss_dev, beta_eval)]\n",
    "            for log in logs:\n",
    "                print('%03d[s], %02d[s], Ep: %02d, Ct: %05d|TR LOSS: %.0f, PPL: %.0f|TM NLL: %.0f, KL: %.2f, REG:%.2f | LM NLL: %.2f, KL: %.2f|DE LOSS: %.0f, PPL: %.0f, TM: %.0f, LM: %.2f|BETA: %.6f' %  log)\n",
    "\n",
    "            print_sample(batch)\n",
    "\n",
    "            time_start = time.time()\n",
    "            \n",
    "            print_topic_sample()\n",
    "                \n",
    "    epoch += 1\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "_prob_topic, _prob_topic_sents, _prob_topic_infer, _means_topic_infer = debug_value([prob_topic, prob_topic_sents, prob_topic_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.2281889e-02, 6.7835855e-03, 2.0132679e-01, 1.1674699e-02,\n",
       "        6.5264981e-03, 1.1411838e-02, 6.9189700e-03, 2.8612482e-04,\n",
       "        7.4042326e-01, 2.3663496e-03], dtype=float32),\n",
       " array([0.09889801, 0.11335436, 0.12141278, 0.10027827, 0.13508864,\n",
       "        0.09455997, 0.0860846 , 0.08042429, 0.09635323, 0.07354581],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_i = 4\n",
    "_prob_topic_sents[batch_i], _prob_topic_infer[batch_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02824139, -0.22793637,  0.3788033 , -0.02814816],\n",
       "       [ 0.06249218, -0.08085692,  0.06756439,  0.2259818 ],\n",
       "       [-0.1377574 ,  0.28185233,  0.23656711, -0.3201025 ],\n",
       "       [-0.03711405,  0.16953555,  0.16389738, -0.16461828],\n",
       "       [ 0.27302447, -0.22462857,  0.37200606, -0.00549014],\n",
       "       [-0.16755986,  0.11964113,  0.17052333,  0.18083367],\n",
       "       [ 0.26351342, -0.33828875,  0.3920058 , -0.17029142],\n",
       "       [ 0.21033517, -0.1955087 ,  0.20963705, -0.24809986],\n",
       "       [ 0.01922028, -0.03185754,  0.28022933, -0.13928899],\n",
       "       [-0.26817918, -0.3364467 ,  0.36924574,  0.15818927]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_means_topic_infer[0][:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_means_topic, b_means_topic = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"topic/dec/mean_topic\")\n",
    "\n",
    "pred_topic_embeddings, pred_topic_bow, pred_means_topic, pred_logvars_topic, pred_token_idxs, _w_means_topic, _b_means_topic, _w_mean_topic_infer = \\\n",
    "                                sess.run([topic_embeddings, topic_bow, means_topic, logvars_topic, topic_beam_output_token_idxs, w_means_topic, b_means_topic, w_mean_topic_infer], \n",
    "                                         feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "\n",
    "pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "\n",
    "pred_topics_freq_bow_indices = np.argsort(pred_topic_bow, 1)[:, ::-1][:, :10]\n",
    "pred_topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['million', 'http', 'years', 'tuesday', 'federal', 'u.s.', 'month', 'office', 'people', 'monday']\n",
      "['reports', 'department', 'thursday', 'court', 'time', 'u.s.', 'years', 'monday', 'tuesday', \"'\"]\n",
      "['school', 'public', 'u.s.', 'board', 'university', 'federal', 'president', 'program', 'health', 'students']\n",
      "['fire', 'found', 'authorities', 'hospital', 'home', 'a.m.', 'dead', 'investigation', 'chief', 'morning']\n",
      "['service', 'national', 'department', 'http', 'water', 'park', 'area', 'center', 'wednesday', 'weather']\n",
      "['percent', 'company', 'million', 'billion', 'shares', 'inc.', 'revenue', 'average', 'cents', 'price']\n",
      "['design', 'researchers', 'donations', 'prize', 'encourage', 'peak', 'awards', 'feature', 'chapter', 'affect']\n",
      "['court', 'arrested', 'charged', 'attorney', 'death', 'authorities', 'found', 'prison', 'shooting', 'woman']\n",
      "['bill', 'senate', 'house', 'republican', 'committee', 'public', 'board', 'approved', 'u.s.', 'law']\n",
      "['todd', 'rear', 'drowned', 'confronted', 'foul', 'transported', 'assaulted', 'loaded', 'defender', 'unidentified']\n"
     ]
    }
   ],
   "source": [
    "for idxs in pred_topics_freq_bow_idxs:\n",
    "    print([idx_to_word[idx] for idx in idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09207956,  0.09077395,  0.06120839, -0.22176096,  0.19921999,\n",
       "        -0.05485373, -0.23069587,  0.05650459,  0.142239  , -0.17410392],\n",
       "       [ 0.29974467, -0.03281972,  0.25064805, -0.2893645 , -0.1312135 ,\n",
       "        -0.11136644,  0.02060958, -0.13598587,  0.24592783, -0.18463418],\n",
       "       [ 0.22819577,  0.14566512,  0.05597903, -0.1109414 , -0.00879419,\n",
       "        -0.1605561 ,  0.11445126,  0.00916692, -0.04847115, -0.0603605 ],\n",
       "       [ 0.3365141 , -0.06439152,  0.3631938 ,  0.14203475,  0.10052318,\n",
       "        -0.09119357, -0.08768138, -0.28562906,  0.3186355 , -0.08452626],\n",
       "       [ 0.26727033,  0.03540339,  0.28315246, -0.1286456 , -0.02444943,\n",
       "        -0.163507  , -0.02146725, -0.21729966,  0.26542294, -0.1589102 ],\n",
       "       [ 0.28826687, -0.18438952,  0.2760168 , -0.12692557,  0.05485712,\n",
       "         0.17983724,  0.14970489, -0.36168167,  0.30563408, -0.13128197],\n",
       "       [-0.32988435, -0.11005852, -0.27013943,  0.26166642, -0.3013388 ,\n",
       "        -0.2234488 ,  0.02287463,  0.20320751, -0.10510623,  0.28636736],\n",
       "       [ 0.37211177, -0.00585993,  0.2853141 , -0.05771174,  0.15170154,\n",
       "         0.05411884, -0.24091715, -0.01475959, -0.0408047 , -0.17167823],\n",
       "       [ 0.20512433,  0.18697639, -0.08326819,  0.03792854,  0.03214281,\n",
       "         0.05618311,  0.09544107,  0.03913466, -0.06514693, -0.04000437],\n",
       "       [-0.3745221 , -0.06385167, -0.29924905,  0.08158813, -0.28344202,\n",
       "        -0.01799835,  0.02471407,  0.408938  , -0.169172  ,  0.37273586]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_topic_embeddings[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0327697e-03, 4.6811462e-03, 1.7024261e-04, ..., 3.3795869e-04,\n",
       "        1.5871278e-04, 6.6570101e-05],\n",
       "       [5.9443718e-04, 7.4376534e-03, 2.0111131e-04, ..., 7.4048399e-04,\n",
       "        1.5028552e-04, 8.9953668e-05],\n",
       "       [8.5580017e-04, 2.8901210e-03, 1.6464434e-04, ..., 3.1770146e-04,\n",
       "        1.9349743e-04, 3.0012103e-04],\n",
       "       ...,\n",
       "       [1.3188391e-04, 2.6697975e-03, 2.3009101e-04, ..., 8.1575691e-04,\n",
       "        1.5104198e-04, 3.2140684e-05],\n",
       "       [3.8558390e-04, 4.0810513e-03, 1.4347673e-04, ..., 1.4157601e-04,\n",
       "        1.8342295e-04, 4.7484271e-05],\n",
       "       [7.1679087e-06, 2.9757638e-07, 1.8662996e-04, ..., 5.9841594e-05,\n",
       "        1.7972868e-04, 5.0798093e-04]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_topic_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09637171, -0.09069463,  0.09532599, ..., -0.02989542,\n",
       "        -0.02514938,  0.01415634],\n",
       "       [-0.02764424,  0.01254364,  0.03703775, ..., -0.01512817,\n",
       "         0.02590305,  0.01404281],\n",
       "       [-0.08408152, -0.00924051, -0.08365072, ...,  0.05623364,\n",
       "        -0.05008389,  0.0209127 ],\n",
       "       ...,\n",
       "       [-0.00629492, -0.05131948, -0.05206006, ..., -0.0955721 ,\n",
       "        -0.01980347, -0.02646225],\n",
       "       [-0.00839109, -0.05062511, -0.05731497, ...,  0.07618995,\n",
       "         0.03893617,  0.06835916],\n",
       "       [ 0.07883421, -0.02804748,  0.09326512, ..., -0.0734218 ,\n",
       "         0.0045918 , -0.10043538]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_w_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03207342, -0.08436021,  0.36105958, -0.02925379, -0.26779142,\n",
       "       -0.4809294 , -0.04713235, -0.6384421 ,  0.49655244, -0.18943405,\n",
       "        0.32139724,  0.00376519, -0.02733823,  0.06968141,  0.12723993,\n",
       "        0.17086434,  0.4321125 ,  0.2930171 ,  0.43750855, -0.32728267,\n",
       "       -0.29320472,  0.46633536,  0.03467208, -0.11661331,  0.02106199,\n",
       "        0.10828511,  0.14413024,  0.00801178, -0.16259423,  0.31461757,\n",
       "       -0.25328413,  0.11320477], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03563995, -0.3379975 ,  1.6953684 , -0.3394102 , -1.3313038 ,\n",
       "        -1.9561913 , -0.34439525, -3.1041431 ,  2.302101  , -0.80340064,\n",
       "         1.374892  ,  0.10505254,  0.13142577,  0.36230466,  0.5036737 ,\n",
       "         0.6154421 ,  1.8879794 ,  1.263777  ,  2.0281937 , -1.4289141 ,\n",
       "        -1.3110695 ,  2.0390706 ,  0.32288784, -0.39520237,  0.10320839,\n",
       "         0.35608286,  0.61754334, -0.1438482 , -0.74413097,  1.4731948 ,\n",
       "        -1.1249324 ,  0.33894244],\n",
       "       [ 0.03510073, -0.3374708 ,  1.6888778 , -0.3363807 , -1.3308815 ,\n",
       "        -1.9608293 , -0.34732953, -3.1081154 ,  2.3005512 , -0.80602133,\n",
       "         1.3695524 ,  0.10256523,  0.13283278,  0.36182955,  0.49822897,\n",
       "         0.6213894 ,  1.8883276 ,  1.266403  ,  2.023539  , -1.426166  ,\n",
       "        -1.3108103 ,  2.0448976 ,  0.32430837, -0.38903922,  0.10560946,\n",
       "         0.3596735 ,  0.6160856 , -0.14840832, -0.74532497,  1.4738762 ,\n",
       "        -1.1249926 ,  0.33331037],\n",
       "       [ 0.03490497, -0.33676392,  1.6840277 , -0.335459  , -1.3295547 ,\n",
       "        -1.95664   , -0.3462094 , -3.1003447 ,  2.296549  , -0.80408335,\n",
       "         1.367739  ,  0.10281808,  0.13425133,  0.36240244,  0.4972452 ,\n",
       "         0.6206323 ,  1.885982  ,  1.2645416 ,  2.0215812 , -1.4212072 ,\n",
       "        -1.3081043 ,  2.0386996 ,  0.32136053, -0.39039296,  0.1050597 ,\n",
       "         0.36023104,  0.6137113 , -0.15039542, -0.7433981 ,  1.4694595 ,\n",
       "        -1.1245916 ,  0.3360805 ],\n",
       "       [ 0.03728028, -0.33768234,  1.6832604 , -0.3353232 , -1.325423  ,\n",
       "        -1.9593027 , -0.34599763, -3.1006794 ,  2.2962196 , -0.80693865,\n",
       "         1.3678529 ,  0.10114851,  0.13371727,  0.36420822,  0.49783063,\n",
       "         0.62049437,  1.8832903 ,  1.2625886 ,  2.0200832 , -1.421174  ,\n",
       "        -1.3108637 ,  2.0388248 ,  0.32396334, -0.38813725,  0.10552703,\n",
       "         0.3586173 ,  0.6122677 , -0.15264365, -0.74335086,  1.468328  ,\n",
       "        -1.1214948 ,  0.337562  ],\n",
       "       [ 0.0355639 , -0.33673748,  1.6850188 , -0.334822  , -1.3297123 ,\n",
       "        -1.9579692 , -0.34517902, -3.0999303 ,  2.296747  , -0.8048171 ,\n",
       "         1.3683693 ,  0.10311563,  0.13507861,  0.36239594,  0.4968614 ,\n",
       "         0.61993766,  1.8848993 ,  1.2634866 ,  2.0213115 , -1.4220002 ,\n",
       "        -1.3081868 ,  2.0382977 ,  0.32180363, -0.39071375,  0.10513556,\n",
       "         0.35903966,  0.6132862 , -0.15032193, -0.74355686,  1.4703372 ,\n",
       "        -1.1240762 ,  0.3361809 ],\n",
       "       [ 0.03776056, -0.33906677,  1.6886759 , -0.33578348, -1.3277831 ,\n",
       "        -1.9574624 , -0.3511446 , -3.1008887 ,  2.298576  , -0.80058026,\n",
       "         1.3695161 ,  0.10116885,  0.13310117,  0.36374724,  0.49994904,\n",
       "         0.62002206,  1.8863395 ,  1.2639412 ,  2.0218256 , -1.4257591 ,\n",
       "        -1.3068638 ,  2.038707  ,  0.3251842 , -0.38946715,  0.10602362,\n",
       "         0.3615039 ,  0.6127277 , -0.15162012, -0.739843  ,  1.4696462 ,\n",
       "        -1.1235677 ,  0.34125343],\n",
       "       [ 0.03675649, -0.33748797,  1.6818695 , -0.3343631 , -1.3270382 ,\n",
       "        -1.9566132 , -0.34484407, -3.0962553 ,  2.2958052 , -0.8025343 ,\n",
       "         1.365838  ,  0.10129849,  0.13410446,  0.36187994,  0.49527365,\n",
       "         0.62073433,  1.8827194 ,  1.264018  ,  2.019868  , -1.4191431 ,\n",
       "        -1.3083122 ,  2.036423  ,  0.32277   , -0.38972613,  0.10532874,\n",
       "         0.36029965,  0.6137044 , -0.14923215, -0.7424054 ,  1.4679198 ,\n",
       "        -1.1228634 ,  0.33640632],\n",
       "       [ 0.03632384, -0.33631054,  1.6842166 , -0.33446336, -1.3275865 ,\n",
       "        -1.959459  , -0.34500512, -3.1014085 ,  2.2958436 , -0.804686  ,\n",
       "         1.3688817 ,  0.10245646,  0.13472772,  0.3634492 ,  0.49877107,\n",
       "         0.61891234,  1.8855013 ,  1.263787  ,  2.0223496 , -1.4211094 ,\n",
       "        -1.309351  ,  2.0364125 ,  0.32266062, -0.3891954 ,  0.10458721,\n",
       "         0.35922444,  0.61319816, -0.14912874, -0.7429457 ,  1.469351  ,\n",
       "        -1.1232082 ,  0.3378038 ],\n",
       "       [ 0.03574873, -0.3368752 ,  1.6819855 , -0.33521634, -1.3290966 ,\n",
       "        -1.9556532 , -0.34607896, -3.0987115 ,  2.2963123 , -0.80486095,\n",
       "         1.3669474 ,  0.10132913,  0.13473387,  0.36283588,  0.49758494,\n",
       "         0.6216788 ,  1.8858868 ,  1.2627757 ,  2.0219636 , -1.4208897 ,\n",
       "        -1.3082187 ,  2.0389626 ,  0.32159996, -0.39116064,  0.1062526 ,\n",
       "         0.35957128,  0.6122661 , -0.14942539, -0.74453473,  1.468337  ,\n",
       "        -1.123489  ,  0.33671775],\n",
       "       [ 0.03600421, -0.33810538,  1.6826388 , -0.33433414, -1.3267329 ,\n",
       "        -1.9560882 , -0.34396413, -3.096003  ,  2.2952938 , -0.8032101 ,\n",
       "         1.366725  ,  0.10274363,  0.13443008,  0.36315334,  0.4956261 ,\n",
       "         0.6213716 ,  1.882305  ,  1.2623034 ,  2.0204127 , -1.4200761 ,\n",
       "        -1.3082595 ,  2.034914  ,  0.32189733, -0.38998067,  0.10444504,\n",
       "         0.35928738,  0.6142209 , -0.1489011 , -0.74237704,  1.4684169 ,\n",
       "        -1.1216142 ,  0.3379858 ]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02438005, -0.02036242, -0.01039507,  0.00277665,  0.02350294,\n",
       "         0.01473409, -0.02157389,  0.02251465,  0.00814133,  0.01785252],\n",
       "       [ 0.00074128, -0.001453  , -0.01162554, -0.0014644 , -0.02097618,\n",
       "         0.00570552,  0.00674119, -0.01879225,  0.02161046,  0.01941887],\n",
       "       [ 0.01402811, -0.00613734, -0.0027943 , -0.01052572, -0.00041091,\n",
       "         0.0045091 , -0.01547336,  0.0194314 ,  0.00784631,  0.00505763],\n",
       "       [ 0.01655601, -0.02403533,  0.01607199, -0.01643238,  0.00686195,\n",
       "        -0.01788632,  0.01109112,  0.00087933,  0.00448566, -0.00823843],\n",
       "       [-0.00695892,  0.01631877,  0.01657149,  0.01772625,  0.02026216,\n",
       "         0.01811663,  0.02153757, -0.00020743, -0.02066918, -0.00025119],\n",
       "       [ 0.02195978,  0.00867016, -0.02278856,  0.0122384 , -0.02183604,\n",
       "         0.00798479,  0.00545774,  0.00923075,  0.01814621,  0.02018008],\n",
       "       [ 0.02242495, -0.00673729,  0.01447002,  0.02163557,  0.02312511,\n",
       "        -0.00920323, -0.01116639,  0.01708085, -0.01181559, -0.00763743],\n",
       "       [-0.01978129,  0.00916991, -0.00304236,  0.00106503,  0.00690803,\n",
       "        -0.01416942,  0.0063372 ,  0.00018093,  0.00393429, -0.00307162],\n",
       "       [-0.01821756,  0.01890057,  0.00228031, -0.01240117, -0.01108933,\n",
       "        -0.01674686, -0.01161283, -0.01693372,  0.00370204, -0.00271433],\n",
       "       [-0.0158878 ,  0.01951969, -0.01606018, -0.00457847,  0.00044555,\n",
       "        -0.0050344 , -0.0013686 ,  0.00539651,  0.02015766, -0.00510509]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_w_mean_topic_infer[:, :10, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03922943,  0.46112132, -0.08166478, -0.06917454, -0.01617851,\n",
       "        0.18225916, -0.28025913, -0.06620797,  0.03231472, -0.02211766,\n",
       "       -0.38082743,  0.11382294,  0.39363608, -0.4168986 ,  1.1677127 ,\n",
       "        0.08979508,  0.03593519, -0.07863234,  0.12493958,  0.03175303,\n",
       "        0.13351525,  0.8176425 , -0.28750482,  0.37927672, -0.01720515,\n",
       "       -0.21228473, -0.05377329, -0.10451841, -0.24399954, -0.25936088,\n",
       "        0.0845396 ,  0.33591354], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enc_state_infer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-ee7c3cd147b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_enc_state_infer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_means_topic_infer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdebug_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menc_state_infer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans_topic_infer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'enc_state_infer' is not defined"
     ]
    }
   ],
   "source": [
    "_enc_state_infer, _means_topic_infer = debug_value([enc_state_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67, 10, 1024)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_enc_state_infer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_means_topic_infer[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
