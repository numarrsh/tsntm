{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from data_structure import get_batches, get_test_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "\n",
    "from topic_beam_search_decoder import BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '0', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/bags/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/topic_vae', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'bags', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 50, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 1000, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.1, 'lr')\n",
    "flags.DEFINE_float('reg', 1., 'regularization term')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_bool('warmup', True, 'flg of warming up')\n",
    "flags.DEFINE_integer('epochs_cycle', 5, 'number of epochs within a cycle')\n",
    "flags.DEFINE_float('r_cycle', 0.5, 'proportion used to increase beta within a cycle')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 20, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_bow', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_hidden_topic', 512, 'dim_hidden_topic')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "flags.DEFINE_bool('bidirectional', True, 'flg of bidirectional encoding')\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_test_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('PAD_IDX', word_to_idx[PAD], 'PAD_IDX')\n",
    "flags.DEFINE_integer('UNK_IDX', word_to_idx[UNK], 'UNK_IDX')\n",
    "flags.DEFINE_integer('BOS_IDX', word_to_idx[BOS], 'BOS_IDX')\n",
    "flags.DEFINE_integer('EOS_IDX', word_to_idx[EOS], 'EOS_IDX')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "maximum_iterations = max([max([instance.max_sent_l for instance in batch]) for ct, batch in dev_batches])\n",
    "flags.DEFINE_integer('maximum_iterations', maximum_iterations, 'maximum_iterations')\n",
    "\n",
    "flags.DEFINE_integer('cycle_steps', len(train_batches)*config.epochs_cycle, 'number of steps for each cycle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0,
     10,
     18,
     24,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables):\n",
    "    sample_batch = dev_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "def check_shape(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()\n",
    "    \n",
    "def check_value(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train'):\n",
    "    bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "    feed_dict = {\n",
    "                t_variables['bow']: bow, \n",
    "                t_variables['keep_prob']: keep_prob\n",
    "    }\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['bow'] = tf.placeholder(tf.float32, [None, config.dim_bow])\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32)\n",
    "\n",
    "# tree_idxs = {0:[1, 2], \n",
    "#              1:[10, 11], 2:[20, 21, 22], \n",
    "#              10: [100, 101], 11: [110, 111, 112], 20: [200, 201], 21: [210, 211], 22:[220, 221, 222]\n",
    "#              }\n",
    "\n",
    "tree_idxs = {0:[1, 2, 3], \n",
    "                      1:[10, 11], 2:[20, 21], 3:[30, 31]}\n",
    "\n",
    "tree_idxs = {0:[1, 2, 3], \n",
    "                      1:[10, 11], 2:[20, 21], 3:[30, 31],\n",
    "                      10: [100, 101], 11: [110, 111], 20: [200, 201], 21: [210, 211], 30:[300, 301], 31:[310, 311]}\n",
    "\n",
    "topic_idxs = [0] + [idx for child_idxs in tree_idxs.values() for idx in child_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doubly rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoublyRNNCell:\n",
    "    def __init__(self, dim_hidden, output_layer=None):\n",
    "        self.dim_hidden = dim_hidden\n",
    "        \n",
    "        self.ancestral_layer=tf.layers.Dense(units=dim_hidden, activation=tf.nn.tanh, name='ancestral')\n",
    "        self.fraternal_layer=tf.layers.Dense(units=dim_hidden, activation=tf.nn.tanh, name='fraternal')\n",
    "        self.hidden_layer = tf.layers.Dense(units=dim_hidden, activation=tf.nn.tanh, name='hidden')\n",
    "        \n",
    "        self.output_layer=output_layer\n",
    "        \n",
    "    def __call__(self, state_ancestral, state_fraternal, reuse=True):\n",
    "        with tf.variable_scope('input', reuse=reuse):\n",
    "            state_ancestral = self.ancestral_layer(state_ancestral)\n",
    "            state_fraternal = self.fraternal_layer(state_fraternal)\n",
    "\n",
    "        with tf.variable_scope('output', reuse=reuse):\n",
    "            state_hidden = self.hidden_layer(state_ancestral + state_fraternal)\n",
    "            if self.output_layer is not None: \n",
    "                output = self.output_layer(state_hidden)\n",
    "            else:\n",
    "                output = state_hidden\n",
    "            \n",
    "        return output, state_hidden\n",
    "    \n",
    "    def get_initial_state(self, name):\n",
    "        initial_state = tf.get_variable(name, [1, self.dim_hidden], dtype=tf.float32)\n",
    "        return initial_state\n",
    "    \n",
    "    def get_zero_state(self, name):\n",
    "        zero_state = tf.zeros([1, self.dim_hidden], dtype=tf.float32, name=name)\n",
    "        return zero_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubly_rnn(dim_hidden, tree_idxs, initial_state_parent=None, initial_state_sibling=None, output_layer=None, name=''):\n",
    "    outputs, states_parent = {}, {}\n",
    "    \n",
    "    with tf.variable_scope(name, reuse=False):\n",
    "        doubly_rnn_cell = DoublyRNNCell(dim_hidden, output_layer)\n",
    "\n",
    "        if initial_state_parent is None: initial_state_parent = doubly_rnn_cell.get_initial_state('init_state_parent')\n",
    "        if initial_state_sibling is None: \n",
    "#             initial_state_sibling = doubly_rnn_cell.get_initial_state('init_state_sibling')\n",
    "            initial_state_sibling = doubly_rnn_cell.get_zero_state('init_state_sibling')\n",
    "        output, state_sibling = doubly_rnn_cell(initial_state_parent, initial_state_sibling, reuse=False)\n",
    "        outputs[0], states_parent[0] = output, state_sibling\n",
    "\n",
    "        for parent_idx, child_idxs in tree_idxs.items():\n",
    "            state_parent = states_parent[parent_idx]\n",
    "            state_sibling = initial_state_sibling\n",
    "            for child_idx in child_idxs:\n",
    "                output, state_sibling = doubly_rnn_cell(state_parent, state_sibling)\n",
    "                outputs[child_idx], states_parent[child_idx] = output, state_sibling\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stick break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_sbp(tree_sticks_topic, tree_sticks_branch):\n",
    "    tree_prob_topic = {}\n",
    "    rest_topics = {}\n",
    "\n",
    "    # calculate topic probability and save\n",
    "    stick_topic = tree_sticks_topic[0]\n",
    "    tree_prob_topic[0] = stick_topic\n",
    "    rest_topics[0] = 1.-stick_topic\n",
    "    for parent_idx, child_idxs in tree_idxs.items():\n",
    "        rest_topic = rest_topics[parent_idx]\n",
    "        rest_branch = 1.\n",
    "        for child_idx in child_idxs:\n",
    "            # calculate topic probability\n",
    "            if child_idx == child_idxs[-1]: # last child\n",
    "                prob_branch = rest_branch # phi\n",
    "            else:\n",
    "                stick_branch = tree_sticks_branch[child_idx] # psi\n",
    "                prob_branch = stick_branch * rest_branch # phi\n",
    "\n",
    "            if not child_idx in tree_idxs: # leaf childs\n",
    "                prob_topic = prob_branch * rest_topic # pi\n",
    "            else:\n",
    "                stick_topic = tree_sticks_topic[child_idx] # upsilon\n",
    "                prob_topic = stick_topic * prob_branch * rest_topic # pi\n",
    "\n",
    "            # save topic probability and update rest stick length\n",
    "            tree_prob_topic[child_idx] = prob_topic\n",
    "            rest_branch = (1.- stick_branch) * rest_branch\n",
    "            rest_topics[child_idx] = (1.-stick_topic)*prob_branch * rest_topic\n",
    "            \n",
    "    return tree_prob_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode bow\n",
    "with tf.variable_scope('topic/enc', reuse=False):\n",
    "    hidden_bow_ = tf.layers.Dense(units=config.dim_hidden_bow, activation=tf.nn.relu, name='hidden_bow')(t_variables['bow'])\n",
    "    hidden_bow = tf.layers.Dropout(t_variables['keep_prob'])(hidden_bow_)\n",
    "    means_bow = tf.layers.Dense(units=config.dim_latent_bow, name='mean_bow')(hidden_bow)\n",
    "    logvars_bow = tf.layers.Dense(units=config.dim_latent_bow, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_bow')(hidden_bow)\n",
    "    latents_bow = sample_latents(means_bow, logvars_bow) # sample latent vectors\n",
    "    prob_layer = lambda h: tf.nn.sigmoid(tf.matmul(latents_bow, h, transpose_b=True))\n",
    "    \n",
    "    tree_sticks_topic = doubly_rnn(config.dim_latent_bow, tree_idxs, output_layer=prob_layer, name='sticks_topic')\n",
    "    tree_sticks_branch = doubly_rnn(config.dim_latent_bow, tree_idxs, output_layer=prob_layer, name='sticks_branch')\n",
    "\n",
    "    tree_prob_topic = hierarchical_sbp(tree_sticks_topic, tree_sticks_branch)\n",
    "    prob_topic = tf.concat(list(tree_prob_topic.values()), 1)\n",
    "\n",
    "# decode bow\n",
    "with tf.variable_scope('shared', reuse=False):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "bow_embeddings = tf.nn.embedding_lookup(embeddings, bow_idxs) # embeddings of each bow features\n",
    "\n",
    "with tf.variable_scope('topic/dec', reuse=False):\n",
    "    tree_topic_embeddings = doubly_rnn(config.dim_emb, tree_idxs, name='emb_topic')\n",
    "    topic_embeddings = tf.concat(list(tree_topic_embeddings.values()), 0)\n",
    "    \n",
    "    topic_bow = tf.nn.softmax(tf.matmul(topic_embeddings, bow_embeddings, transpose_b=True), 1) # bow vectors for each topic\n",
    "    logits_bow = tf_log(tf.matmul(prob_topic, topic_bow)) # predicted bow distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree_mask_reg(tree_idxs):\n",
    "    tree_mask_reg = np.ones([len(topic_idxs), len(topic_idxs)], dtype=np.float32)\n",
    "    parent_to_descendant_idxs = {parent_idx: get_descendant_idxs(parent_idx) for parent_idx in tree_idxs}\n",
    "    \n",
    "    for parent_idx, descendant_idxs in parent_to_descendant_idxs.items():\n",
    "        for descendant_idx in descendant_idxs:\n",
    "            tree_mask_reg[topic_idxs.index(parent_idx), topic_idxs.index(descendant_idx)] = tree_mask_reg[topic_idxs.index(descendant_idx), topic_idxs.index(parent_idx)] = 0.\n",
    "            \n",
    "    return tree_mask_reg\n",
    "\n",
    "def get_descendant_idxs(parent_idx, descendant_idxs = None):\n",
    "    if descendant_idxs is None: descendant_idxs = []\n",
    "    \n",
    "    child_idxs = tree_idxs[parent_idx]\n",
    "    descendant_idxs += child_idxs\n",
    "    for child_idx in child_idxs:\n",
    "        if child_idx in tree_idxs: get_descendant_idxs(child_idx, descendant_idxs)\n",
    "    return descendant_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define losses\n",
    "topic_losses_recon = -tf.reduce_sum(tf.multiply(t_variables['bow'], logits_bow), 1)\n",
    "topic_loss_recon = tf.reduce_mean(topic_losses_recon) # negative log likelihood of each words\n",
    "\n",
    "topic_loss_kl = compute_kl_loss(means_bow, logvars_bow) # KL divergence b/w latent dist & gaussian std\n",
    "\n",
    "topic_bow_norm = topic_bow / tf.norm(topic_bow, axis=1, keepdims=True)\n",
    "topic_dots = tf.clip_by_value(tf.matmul(topic_bow_norm, tf.transpose(topic_bow_norm)), -1., 1.)\n",
    "# topic_loss_reg = tf.reduce_mean(tf.square(topic_dots - tf.eye(len(topic_idxs))))\n",
    "tree_mask_reg = get_tree_mask_reg(tree_idxs)\n",
    "topic_loss_reg = tf.reduce_mean(tf.square(topic_dots - tf.eye(len(topic_idxs))) * tree_mask_reg)\n",
    "\n",
    "\n",
    "global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "\n",
    "loss = topic_loss_recon + topic_loss_kl + config.reg * topic_loss_reg\n",
    "\n",
    "# define optimizer\n",
    "if config.opt == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif config.opt == 'Adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "\n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars, global_step=global_step)\n",
    "\n",
    "# monitor\n",
    "n_bow = tf.reduce_sum(t_variables['bow'], 1)\n",
    "topic_ppls = tf.divide(topic_losses_recon, tf.maximum(1e-5, n_bow))\n",
    "topics_freq_bow_indices = tf.nn.top_k(topic_bow, 10, name='topic_freq_bow').indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, ppls_batch \\\n",
    "            = sess.run([loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, topic_ppls], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "    loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    return loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, ppl_mean\n",
    "\n",
    "def print_topic_sample():\n",
    "    topics_freq_bow_idxs = bow_idxs[sess.run(topics_freq_bow_indices)]\n",
    "    assert len(topics_freq_bow_idxs) == len(topic_idxs)\n",
    "    for topic_idx, topic_freq_bow_idxs in zip(topic_idxs, topics_freq_bow_idxs):\n",
    "        print(topic_idx, ' BOW:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "saver = tf.train.Saver(max_to_keep=10)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','TM','','','','VALID:','TM','','',''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL','REG','LOSS','PPL','NLL','KL','REG']]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>VALID:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>123.78</td>\n",
       "      <td>1035</td>\n",
       "      <td>122.49</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.74</td>\n",
       "      <td>117.03</td>\n",
       "      <td>1023</td>\n",
       "      <td>116.00</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>123.48</td>\n",
       "      <td>951</td>\n",
       "      <td>122.02</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.14</td>\n",
       "      <td>108.67</td>\n",
       "      <td>611</td>\n",
       "      <td>107.55</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>119.77</td>\n",
       "      <td>767</td>\n",
       "      <td>118.52</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.14</td>\n",
       "      <td>107.67</td>\n",
       "      <td>575</td>\n",
       "      <td>106.70</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119.23</td>\n",
       "      <td>745</td>\n",
       "      <td>118.01</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.13</td>\n",
       "      <td>107.74</td>\n",
       "      <td>586</td>\n",
       "      <td>106.79</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3276</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>117.88</td>\n",
       "      <td>690</td>\n",
       "      <td>116.63</td>\n",
       "      <td>1.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>106.98</td>\n",
       "      <td>553</td>\n",
       "      <td>105.72</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4276</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>117.03</td>\n",
       "      <td>655</td>\n",
       "      <td>115.72</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>106.31</td>\n",
       "      <td>529</td>\n",
       "      <td>104.92</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4551</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>116.79</td>\n",
       "      <td>648</td>\n",
       "      <td>115.46</td>\n",
       "      <td>1.21</td>\n",
       "      <td>0.12</td>\n",
       "      <td>106.39</td>\n",
       "      <td>530</td>\n",
       "      <td>105.06</td>\n",
       "      <td>1.24</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5551</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>116.18</td>\n",
       "      <td>626</td>\n",
       "      <td>114.81</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.11</td>\n",
       "      <td>105.98</td>\n",
       "      <td>514</td>\n",
       "      <td>104.56</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6551</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>115.80</td>\n",
       "      <td>611</td>\n",
       "      <td>114.41</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.11</td>\n",
       "      <td>105.87</td>\n",
       "      <td>514</td>\n",
       "      <td>104.49</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6826</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>115.69</td>\n",
       "      <td>607</td>\n",
       "      <td>114.29</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.11</td>\n",
       "      <td>105.95</td>\n",
       "      <td>515</td>\n",
       "      <td>104.52</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7826</th>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>115.36</td>\n",
       "      <td>595</td>\n",
       "      <td>113.94</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.10</td>\n",
       "      <td>105.43</td>\n",
       "      <td>498</td>\n",
       "      <td>104.03</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8826</th>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>115.07</td>\n",
       "      <td>584</td>\n",
       "      <td>113.61</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.10</td>\n",
       "      <td>105.40</td>\n",
       "      <td>497</td>\n",
       "      <td>103.89</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9101</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>114.99</td>\n",
       "      <td>582</td>\n",
       "      <td>113.53</td>\n",
       "      <td>1.37</td>\n",
       "      <td>0.09</td>\n",
       "      <td>105.31</td>\n",
       "      <td>497</td>\n",
       "      <td>103.81</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10101</th>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>114.74</td>\n",
       "      <td>573</td>\n",
       "      <td>113.25</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.09</td>\n",
       "      <td>105.18</td>\n",
       "      <td>487</td>\n",
       "      <td>103.60</td>\n",
       "      <td>1.53</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11101</th>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>2000</td>\n",
       "      <td>114.52</td>\n",
       "      <td>565</td>\n",
       "      <td>113.00</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.08</td>\n",
       "      <td>104.86</td>\n",
       "      <td>478</td>\n",
       "      <td>103.27</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11376</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>114.48</td>\n",
       "      <td>563</td>\n",
       "      <td>112.95</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.08</td>\n",
       "      <td>104.81</td>\n",
       "      <td>474</td>\n",
       "      <td>103.17</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12376</th>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>114.27</td>\n",
       "      <td>556</td>\n",
       "      <td>112.71</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.08</td>\n",
       "      <td>104.90</td>\n",
       "      <td>475</td>\n",
       "      <td>103.26</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13376</th>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>114.12</td>\n",
       "      <td>550</td>\n",
       "      <td>112.54</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.08</td>\n",
       "      <td>104.81</td>\n",
       "      <td>472</td>\n",
       "      <td>103.12</td>\n",
       "      <td>1.66</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13651</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>114.09</td>\n",
       "      <td>549</td>\n",
       "      <td>112.51</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.07</td>\n",
       "      <td>104.87</td>\n",
       "      <td>475</td>\n",
       "      <td>103.20</td>\n",
       "      <td>1.64</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14651</th>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>1000</td>\n",
       "      <td>113.97</td>\n",
       "      <td>544</td>\n",
       "      <td>112.36</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.07</td>\n",
       "      <td>104.82</td>\n",
       "      <td>470</td>\n",
       "      <td>103.12</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15651</th>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>2000</td>\n",
       "      <td>113.83</td>\n",
       "      <td>539</td>\n",
       "      <td>112.20</td>\n",
       "      <td>1.56</td>\n",
       "      <td>0.07</td>\n",
       "      <td>104.69</td>\n",
       "      <td>466</td>\n",
       "      <td>103.00</td>\n",
       "      <td>1.67</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15926</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>113.80</td>\n",
       "      <td>538</td>\n",
       "      <td>112.16</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0.07</td>\n",
       "      <td>104.72</td>\n",
       "      <td>466</td>\n",
       "      <td>103.01</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16926</th>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>1000</td>\n",
       "      <td>113.70</td>\n",
       "      <td>534</td>\n",
       "      <td>112.05</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.06</td>\n",
       "      <td>104.73</td>\n",
       "      <td>467</td>\n",
       "      <td>102.99</td>\n",
       "      <td>1.72</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17926</th>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>2000</td>\n",
       "      <td>113.58</td>\n",
       "      <td>530</td>\n",
       "      <td>111.91</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.06</td>\n",
       "      <td>104.58</td>\n",
       "      <td>463</td>\n",
       "      <td>102.88</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18201</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>113.56</td>\n",
       "      <td>530</td>\n",
       "      <td>111.89</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.06</td>\n",
       "      <td>104.50</td>\n",
       "      <td>462</td>\n",
       "      <td>102.77</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19201</th>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>1000</td>\n",
       "      <td>113.47</td>\n",
       "      <td>527</td>\n",
       "      <td>111.79</td>\n",
       "      <td>1.63</td>\n",
       "      <td>0.06</td>\n",
       "      <td>104.62</td>\n",
       "      <td>463</td>\n",
       "      <td>102.92</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201</th>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>2000</td>\n",
       "      <td>113.39</td>\n",
       "      <td>524</td>\n",
       "      <td>111.69</td>\n",
       "      <td>1.64</td>\n",
       "      <td>0.06</td>\n",
       "      <td>104.53</td>\n",
       "      <td>465</td>\n",
       "      <td>102.80</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20476</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>113.37</td>\n",
       "      <td>523</td>\n",
       "      <td>111.66</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.06</td>\n",
       "      <td>104.69</td>\n",
       "      <td>465</td>\n",
       "      <td>102.95</td>\n",
       "      <td>1.72</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21476</th>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>1000</td>\n",
       "      <td>113.29</td>\n",
       "      <td>520</td>\n",
       "      <td>111.57</td>\n",
       "      <td>1.66</td>\n",
       "      <td>0.05</td>\n",
       "      <td>104.57</td>\n",
       "      <td>462</td>\n",
       "      <td>102.85</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22476</th>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>2000</td>\n",
       "      <td>113.22</td>\n",
       "      <td>518</td>\n",
       "      <td>111.50</td>\n",
       "      <td>1.67</td>\n",
       "      <td>0.05</td>\n",
       "      <td>104.37</td>\n",
       "      <td>457</td>\n",
       "      <td>102.64</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91001</th>\n",
       "      <td>7</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>111.64</td>\n",
       "      <td>465</td>\n",
       "      <td>109.62</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>103.34</td>\n",
       "      <td>428</td>\n",
       "      <td>101.55</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92001</th>\n",
       "      <td>25</td>\n",
       "      <td>40</td>\n",
       "      <td>1000</td>\n",
       "      <td>111.63</td>\n",
       "      <td>465</td>\n",
       "      <td>109.60</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>103.21</td>\n",
       "      <td>423</td>\n",
       "      <td>101.42</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93001</th>\n",
       "      <td>25</td>\n",
       "      <td>40</td>\n",
       "      <td>2000</td>\n",
       "      <td>111.62</td>\n",
       "      <td>464</td>\n",
       "      <td>109.59</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>103.29</td>\n",
       "      <td>426</td>\n",
       "      <td>101.50</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93276</th>\n",
       "      <td>7</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>111.61</td>\n",
       "      <td>464</td>\n",
       "      <td>109.59</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>103.28</td>\n",
       "      <td>426</td>\n",
       "      <td>101.48</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94276</th>\n",
       "      <td>24</td>\n",
       "      <td>41</td>\n",
       "      <td>1000</td>\n",
       "      <td>111.60</td>\n",
       "      <td>464</td>\n",
       "      <td>109.57</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>103.39</td>\n",
       "      <td>430</td>\n",
       "      <td>101.62</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95276</th>\n",
       "      <td>25</td>\n",
       "      <td>41</td>\n",
       "      <td>2000</td>\n",
       "      <td>111.59</td>\n",
       "      <td>464</td>\n",
       "      <td>109.56</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>103.12</td>\n",
       "      <td>421</td>\n",
       "      <td>101.36</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95551</th>\n",
       "      <td>7</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>111.58</td>\n",
       "      <td>464</td>\n",
       "      <td>109.55</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>103.32</td>\n",
       "      <td>429</td>\n",
       "      <td>101.56</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96551</th>\n",
       "      <td>24</td>\n",
       "      <td>42</td>\n",
       "      <td>1000</td>\n",
       "      <td>111.57</td>\n",
       "      <td>463</td>\n",
       "      <td>109.54</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>103.23</td>\n",
       "      <td>426</td>\n",
       "      <td>101.44</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97551</th>\n",
       "      <td>24</td>\n",
       "      <td>42</td>\n",
       "      <td>2000</td>\n",
       "      <td>111.56</td>\n",
       "      <td>463</td>\n",
       "      <td>109.53</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>103.26</td>\n",
       "      <td>425</td>\n",
       "      <td>101.48</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97826</th>\n",
       "      <td>7</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>111.55</td>\n",
       "      <td>463</td>\n",
       "      <td>109.52</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>103.35</td>\n",
       "      <td>427</td>\n",
       "      <td>101.56</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98826</th>\n",
       "      <td>24</td>\n",
       "      <td>43</td>\n",
       "      <td>1000</td>\n",
       "      <td>111.54</td>\n",
       "      <td>462</td>\n",
       "      <td>109.51</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>103.19</td>\n",
       "      <td>424</td>\n",
       "      <td>101.42</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99826</th>\n",
       "      <td>25</td>\n",
       "      <td>43</td>\n",
       "      <td>2000</td>\n",
       "      <td>111.53</td>\n",
       "      <td>462</td>\n",
       "      <td>109.50</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>103.19</td>\n",
       "      <td>423</td>\n",
       "      <td>101.40</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100101</th>\n",
       "      <td>7</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>111.53</td>\n",
       "      <td>462</td>\n",
       "      <td>109.49</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>103.37</td>\n",
       "      <td>427</td>\n",
       "      <td>101.55</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101101</th>\n",
       "      <td>25</td>\n",
       "      <td>44</td>\n",
       "      <td>1000</td>\n",
       "      <td>111.51</td>\n",
       "      <td>462</td>\n",
       "      <td>109.48</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>103.15</td>\n",
       "      <td>424</td>\n",
       "      <td>101.34</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102101</th>\n",
       "      <td>25</td>\n",
       "      <td>44</td>\n",
       "      <td>2000</td>\n",
       "      <td>111.50</td>\n",
       "      <td>461</td>\n",
       "      <td>109.47</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>103.07</td>\n",
       "      <td>420</td>\n",
       "      <td>101.27</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102376</th>\n",
       "      <td>7</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>111.50</td>\n",
       "      <td>461</td>\n",
       "      <td>109.47</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>103.26</td>\n",
       "      <td>426</td>\n",
       "      <td>101.48</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103376</th>\n",
       "      <td>25</td>\n",
       "      <td>45</td>\n",
       "      <td>1000</td>\n",
       "      <td>111.49</td>\n",
       "      <td>461</td>\n",
       "      <td>109.45</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>103.26</td>\n",
       "      <td>424</td>\n",
       "      <td>101.46</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104376</th>\n",
       "      <td>25</td>\n",
       "      <td>45</td>\n",
       "      <td>2000</td>\n",
       "      <td>111.48</td>\n",
       "      <td>461</td>\n",
       "      <td>109.44</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>103.25</td>\n",
       "      <td>427</td>\n",
       "      <td>101.48</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104651</th>\n",
       "      <td>7</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>111.47</td>\n",
       "      <td>460</td>\n",
       "      <td>109.44</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>103.36</td>\n",
       "      <td>429</td>\n",
       "      <td>101.56</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105651</th>\n",
       "      <td>25</td>\n",
       "      <td>46</td>\n",
       "      <td>1000</td>\n",
       "      <td>111.46</td>\n",
       "      <td>460</td>\n",
       "      <td>109.43</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>103.14</td>\n",
       "      <td>421</td>\n",
       "      <td>101.32</td>\n",
       "      <td>1.82</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106651</th>\n",
       "      <td>25</td>\n",
       "      <td>46</td>\n",
       "      <td>2000</td>\n",
       "      <td>111.45</td>\n",
       "      <td>460</td>\n",
       "      <td>109.42</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>103.21</td>\n",
       "      <td>426</td>\n",
       "      <td>101.41</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106926</th>\n",
       "      <td>7</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>111.45</td>\n",
       "      <td>460</td>\n",
       "      <td>109.41</td>\n",
       "      <td>2.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>103.07</td>\n",
       "      <td>421</td>\n",
       "      <td>101.26</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107926</th>\n",
       "      <td>25</td>\n",
       "      <td>47</td>\n",
       "      <td>1000</td>\n",
       "      <td>111.44</td>\n",
       "      <td>459</td>\n",
       "      <td>109.40</td>\n",
       "      <td>2.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>103.30</td>\n",
       "      <td>424</td>\n",
       "      <td>101.47</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108926</th>\n",
       "      <td>25</td>\n",
       "      <td>47</td>\n",
       "      <td>2000</td>\n",
       "      <td>111.43</td>\n",
       "      <td>459</td>\n",
       "      <td>109.39</td>\n",
       "      <td>2.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>103.11</td>\n",
       "      <td>421</td>\n",
       "      <td>101.30</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109201</th>\n",
       "      <td>7</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>111.42</td>\n",
       "      <td>459</td>\n",
       "      <td>109.39</td>\n",
       "      <td>2.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>103.19</td>\n",
       "      <td>425</td>\n",
       "      <td>101.37</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110201</th>\n",
       "      <td>24</td>\n",
       "      <td>48</td>\n",
       "      <td>1000</td>\n",
       "      <td>111.41</td>\n",
       "      <td>459</td>\n",
       "      <td>109.37</td>\n",
       "      <td>2.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>103.35</td>\n",
       "      <td>426</td>\n",
       "      <td>101.52</td>\n",
       "      <td>1.83</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111201</th>\n",
       "      <td>25</td>\n",
       "      <td>48</td>\n",
       "      <td>2000</td>\n",
       "      <td>111.40</td>\n",
       "      <td>458</td>\n",
       "      <td>109.36</td>\n",
       "      <td>2.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>103.17</td>\n",
       "      <td>424</td>\n",
       "      <td>101.38</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111476</th>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>111.40</td>\n",
       "      <td>458</td>\n",
       "      <td>109.36</td>\n",
       "      <td>2.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>103.10</td>\n",
       "      <td>422</td>\n",
       "      <td>101.28</td>\n",
       "      <td>1.82</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112476</th>\n",
       "      <td>25</td>\n",
       "      <td>49</td>\n",
       "      <td>1000</td>\n",
       "      <td>111.39</td>\n",
       "      <td>458</td>\n",
       "      <td>109.35</td>\n",
       "      <td>2.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>103.20</td>\n",
       "      <td>422</td>\n",
       "      <td>101.36</td>\n",
       "      <td>1.83</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113476</th>\n",
       "      <td>26</td>\n",
       "      <td>49</td>\n",
       "      <td>2000</td>\n",
       "      <td>111.38</td>\n",
       "      <td>458</td>\n",
       "      <td>109.34</td>\n",
       "      <td>2.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>103.23</td>\n",
       "      <td>423</td>\n",
       "      <td>101.37</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       TRAIN:    TM                      VALID:    TM          \\\n",
       "       Time  Ep    Ct    LOSS   PPL     NLL    KL   REG    LOSS   PPL     NLL   \n",
       "1         2   0     0  123.78  1035  122.49  0.56  0.74  117.03  1023  116.00   \n",
       "1001     25   0  1000  123.48   951  122.02  1.33  0.14  108.67   611  107.55   \n",
       "2001     25   0  2000  119.77   767  118.52  1.12  0.14  107.67   575  106.70   \n",
       "2276      7   1     0  119.23   745  118.01  1.09  0.13  107.74   586  106.79   \n",
       "3276     24   1  1000  117.88   690  116.63  1.13  0.13  106.98   553  105.72   \n",
       "4276     24   1  2000  117.03   655  115.72  1.19  0.12  106.31   529  104.92   \n",
       "4551      7   2     0  116.79   648  115.46  1.21  0.12  106.39   530  105.06   \n",
       "5551     24   2  1000  116.18   626  114.81  1.25  0.11  105.98   514  104.56   \n",
       "6551     24   2  2000  115.80   611  114.41  1.28  0.11  105.87   514  104.49   \n",
       "6826      7   3     0  115.69   607  114.29  1.29  0.11  105.95   515  104.52   \n",
       "7826     24   3  1000  115.36   595  113.94  1.32  0.10  105.43   498  104.03   \n",
       "8826     24   3  2000  115.07   584  113.61  1.36  0.10  105.40   497  103.89   \n",
       "9101      7   4     0  114.99   582  113.53  1.37  0.09  105.31   497  103.81   \n",
       "10101    24   4  1000  114.74   573  113.25  1.40  0.09  105.18   487  103.60   \n",
       "11101    24   4  2000  114.52   565  113.00  1.44  0.08  104.86   478  103.27   \n",
       "11376     7   5     0  114.48   563  112.95  1.44  0.08  104.81   474  103.17   \n",
       "12376    24   5  1000  114.27   556  112.71  1.48  0.08  104.90   475  103.26   \n",
       "13376    24   5  2000  114.12   550  112.54  1.50  0.08  104.81   472  103.12   \n",
       "13651     7   6     0  114.09   549  112.51  1.51  0.07  104.87   475  103.20   \n",
       "14651    24   6  1000  113.97   544  112.36  1.54  0.07  104.82   470  103.12   \n",
       "15651    24   6  2000  113.83   539  112.20  1.56  0.07  104.69   466  103.00   \n",
       "15926     6   7     0  113.80   538  112.16  1.57  0.07  104.72   466  103.01   \n",
       "16926    24   7  1000  113.70   534  112.05  1.59  0.06  104.73   467  102.99   \n",
       "17926    24   7  2000  113.58   530  111.91  1.61  0.06  104.58   463  102.88   \n",
       "18201     6   8     0  113.56   530  111.89  1.61  0.06  104.50   462  102.77   \n",
       "19201    25   8  1000  113.47   527  111.79  1.63  0.06  104.62   463  102.92   \n",
       "20201    24   8  2000  113.39   524  111.69  1.64  0.06  104.53   465  102.80   \n",
       "20476     7   9     0  113.37   523  111.66  1.65  0.06  104.69   465  102.95   \n",
       "21476    24   9  1000  113.29   520  111.57  1.66  0.05  104.57   462  102.85   \n",
       "22476    24   9  2000  113.22   518  111.50  1.67  0.05  104.37   457  102.64   \n",
       "...     ...  ..   ...     ...   ...     ...   ...   ...     ...   ...     ...   \n",
       "91001     7  40     0  111.64   465  109.62  2.01  0.02  103.34   428  101.55   \n",
       "92001    25  40  1000  111.63   465  109.60  2.01  0.02  103.21   423  101.42   \n",
       "93001    25  40  2000  111.62   464  109.59  2.01  0.02  103.29   426  101.50   \n",
       "93276     7  41     0  111.61   464  109.59  2.01  0.02  103.28   426  101.48   \n",
       "94276    24  41  1000  111.60   464  109.57  2.01  0.02  103.39   430  101.62   \n",
       "95276    25  41  2000  111.59   464  109.56  2.01  0.02  103.12   421  101.36   \n",
       "95551     7  42     0  111.58   464  109.55  2.01  0.02  103.32   429  101.56   \n",
       "96551    24  42  1000  111.57   463  109.54  2.01  0.02  103.23   426  101.44   \n",
       "97551    24  42  2000  111.56   463  109.53  2.01  0.02  103.26   425  101.48   \n",
       "97826     7  43     0  111.55   463  109.52  2.01  0.02  103.35   427  101.56   \n",
       "98826    24  43  1000  111.54   462  109.51  2.01  0.02  103.19   424  101.42   \n",
       "99826    25  43  2000  111.53   462  109.50  2.01  0.02  103.19   423  101.40   \n",
       "100101    7  44     0  111.53   462  109.49  2.01  0.02  103.37   427  101.55   \n",
       "101101   25  44  1000  111.51   462  109.48  2.01  0.02  103.15   424  101.34   \n",
       "102101   25  44  2000  111.50   461  109.47  2.01  0.02  103.07   420  101.27   \n",
       "102376    7  45     0  111.50   461  109.47  2.01  0.02  103.26   426  101.48   \n",
       "103376   25  45  1000  111.49   461  109.45  2.01  0.02  103.26   424  101.46   \n",
       "104376   25  45  2000  111.48   461  109.44  2.01  0.02  103.25   427  101.48   \n",
       "104651    7  46     0  111.47   460  109.44  2.01  0.02  103.36   429  101.56   \n",
       "105651   25  46  1000  111.46   460  109.43  2.01  0.02  103.14   421  101.32   \n",
       "106651   25  46  2000  111.45   460  109.42  2.01  0.02  103.21   426  101.41   \n",
       "106926    7  47     0  111.45   460  109.41  2.02  0.02  103.07   421  101.26   \n",
       "107926   25  47  1000  111.44   459  109.40  2.02  0.02  103.30   424  101.47   \n",
       "108926   25  47  2000  111.43   459  109.39  2.02  0.02  103.11   421  101.30   \n",
       "109201    7  48     0  111.42   459  109.39  2.02  0.02  103.19   425  101.37   \n",
       "110201   24  48  1000  111.41   459  109.37  2.02  0.02  103.35   426  101.52   \n",
       "111201   25  48  2000  111.40   458  109.36  2.02  0.02  103.17   424  101.38   \n",
       "111476    7  49     0  111.40   458  109.36  2.02  0.02  103.10   422  101.28   \n",
       "112476   25  49  1000  111.39   458  109.35  2.02  0.02  103.20   422  101.36   \n",
       "113476   26  49  2000  111.38   458  109.34  2.02  0.02  103.23   423  101.37   \n",
       "\n",
       "                    \n",
       "          KL   REG  \n",
       "1       0.29  0.74  \n",
       "1001    0.97  0.15  \n",
       "2001    0.83  0.14  \n",
       "2276    0.85  0.10  \n",
       "3276    1.16  0.10  \n",
       "4276    1.31  0.09  \n",
       "4551    1.24  0.09  \n",
       "5551    1.34  0.08  \n",
       "6551    1.30  0.08  \n",
       "6826    1.35  0.08  \n",
       "7826    1.35  0.06  \n",
       "8826    1.46  0.05  \n",
       "9101    1.45  0.05  \n",
       "10101   1.53  0.04  \n",
       "11101   1.57  0.03  \n",
       "11376   1.60  0.03  \n",
       "12376   1.61  0.03  \n",
       "13376   1.66  0.03  \n",
       "13651   1.64  0.02  \n",
       "14651   1.68  0.02  \n",
       "15651   1.67  0.02  \n",
       "15926   1.69  0.02  \n",
       "16926   1.72  0.02  \n",
       "17926   1.68  0.02  \n",
       "18201   1.71  0.02  \n",
       "19201   1.68  0.02  \n",
       "20201   1.71  0.02  \n",
       "20476   1.72  0.02  \n",
       "21476   1.70  0.02  \n",
       "22476   1.71  0.02  \n",
       "...      ...   ...  \n",
       "91001   1.78  0.01  \n",
       "92001   1.79  0.01  \n",
       "93001   1.79  0.01  \n",
       "93276   1.78  0.01  \n",
       "94276   1.76  0.01  \n",
       "95276   1.75  0.01  \n",
       "95551   1.75  0.01  \n",
       "96551   1.78  0.01  \n",
       "97551   1.77  0.01  \n",
       "97826   1.77  0.01  \n",
       "98826   1.76  0.01  \n",
       "99826   1.78  0.01  \n",
       "100101  1.81  0.01  \n",
       "101101  1.80  0.01  \n",
       "102101  1.79  0.01  \n",
       "102376  1.77  0.01  \n",
       "103376  1.79  0.01  \n",
       "104376  1.77  0.01  \n",
       "104651  1.79  0.01  \n",
       "105651  1.82  0.01  \n",
       "106651  1.79  0.01  \n",
       "106926  1.80  0.01  \n",
       "107926  1.81  0.01  \n",
       "108926  1.80  0.01  \n",
       "109201  1.81  0.01  \n",
       "110201  1.83  0.01  \n",
       "111201  1.79  0.01  \n",
       "111476  1.82  0.01  \n",
       "112476  1.83  0.01  \n",
       "113476  1.85  0.01  \n",
       "\n",
       "[150 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  BOW: ! sleeve love & ; bought nice recommend perfectly protection\n",
      "1  BOW: ... $ price made make 've bad stars work problem\n",
      "2  BOW: room ipad pockets pocket hold extra size perfect made carrying\n",
      "3  BOW: inch zipper extra material snug notebook protect tight samsung side\n",
      "10  BOW: quality leather worth item paid durable money ? reviews found\n",
      "11  BOW: cover keyboard color apple scratches mac hard protector snap shell\n",
      "20  BOW: carry small large plenty space holds travel stuff accessories compartments\n",
      "21  BOW: strap shoulder hold back pack pretty zippers : security front\n",
      "30  BOW: power netbook mouse charger cord adapter padding usb tablet mini\n",
      "31  BOW: soft snug air hard neoprene bit scratches chromebook loose scratch\n",
      "100  BOW: handle years bought back zipper year time straps stitching quality\n",
      "101  BOW: months customer broke weeks return service started pay month time\n",
      "110  BOW: ... ? cheap longer reason issue couple buy guess --\n",
      "111  BOW: bottom top piece nice plastic easily screen speck put rubber\n",
      "200  BOW: carry pack compartments lots camera easy comfortable books lot back\n",
      "201  BOW: pockets straps shoulder compartment handle large compartments padding back top\n",
      "210  BOW: bags work college school tote carrying size sturdy husband comfortable\n",
      "211  BOW: leather review 're 'll strong sturdy lap work build huge\n",
      "300  BOW: pocket inside front small nice main compartment padded bit put\n",
      "301  BOW: zipper - side nice main flap top thick velcro put\n",
      "310  BOW: padding velcro design flap closed fabric open material thing pull\n",
      "311  BOW: protection : protect open side screen tab easy pretty edge\n"
     ]
    }
   ],
   "source": [
    "if len(log_df) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "while epoch < config.epochs:\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = get_feed_dict(batch)\n",
    "\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, ppls_batch = \\\n",
    "        sess.run([opt, loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, topic_ppls], feed_dict = feed_dict)\n",
    "            \n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if ct%config.log_period==0:\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev = get_loss(sess, dev_batches)\n",
    "            global_step_log = sess.run(tf.train.get_global_step())\n",
    "            \n",
    "#             if loss_dev < loss_min:\n",
    "#                 loss_min = loss_dev\n",
    "#                 saver.save(sess, config.modelpath, global_step=global_step_log)\n",
    "\n",
    "            clear_output()\n",
    "    \n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_reg_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_reg_dev],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "            \n",
    "            # visualize topic\n",
    "            print_topic_sample()\n",
    "\n",
    "            time_start = time.time()\n",
    "            \n",
    "    epoch += 1\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strided_slice:0 : [[-0.795473   -0.9123729  -0.62665796  0.92393917 -0.0193283 ]\n",
      " [-0.9999835  -0.9878848  -0.5380099  -0.9997342   0.9999889 ]\n",
      " [ 0.9969462   0.99856764  1.          0.9986652   1.        ]\n",
      " [-1.         -1.         -1.         -1.         -0.97203815]\n",
      " [ 0.9845589   0.99035585  0.99999624 -0.98644197 -0.5144615 ]\n",
      " [-0.9999875  -0.9936736   0.2344933  -0.9869338   0.98554975]\n",
      " [ 1.         -0.9827667  -0.9273552  -0.999996    0.77007425]\n",
      " [ 0.9850305   0.9958252  -1.         -0.9916928  -0.9999942 ]\n",
      " [-0.99998236  0.9999998   0.99999857  0.99999374 -0.99999946]\n",
      " [-1.         -0.99744195  0.99333245  0.9999743  -1.        ]]\n"
     ]
    }
   ],
   "source": [
    "debug_value([topic_embeddings[:, :5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "_topic_bow, = debug_value([topic_bow], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 1035 artists>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(bow_idxs, topic_bow[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 1035 artists>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(bow_idxs, topic_bow[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 1035 artists>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(bow_idxs, topic_bow[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 1035 artists>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(bow_idxs, topic_bow[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1035"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bow_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strided_slice_2:0 : [0.1489713  0.0706474  0.07743409 0.03776893 0.14249046 0.13351493\n",
      " 0.18252574 0.168819   0.02399866 0.01382949]\n"
     ]
    }
   ],
   "source": [
    "debug_value([prob_topic[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_value([tf.exp(-tf.divide(topic_losses_recon, n_bow))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder:0 : (32, 4022)\n",
      "topic/enc/dropout/cond/Merge:0 : (32, 256)\n",
      "topic/enc/add:0 : (32, 32)\n",
      "topic/enc/prob/Softmax:0 : (32, 50)\n",
      "embedding_lookup:0 : (4022, 256)\n",
      "topic/dec/topic_emb:0 : (50, 256)\n",
      "topic/dec/Softmax:0 : (50, 4022)\n",
      "topic/dec/Log:0 : (32, 4022)\n"
     ]
    }
   ],
   "source": [
    "debug_shape([bow, hidden_bow, latents_bow, prob_topic, bow_embeddings, topic_embeddings, topic_bow, prob_bow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum:0 : (32,)\n",
      "Neg:0 : ()\n",
      "Sum_2:0 : (32,)\n",
      "Neg_1:0 : (32,)\n",
      "truediv_1:0 : (50, 256)\n",
      "ExpandDims_1:0 : (1,)\n",
      "Mean_3:0 : ()\n"
     ]
    }
   ],
   "source": [
    "debug_shape([topic_losses_recon, topic_loss_recon, n_bow, ppls, topic_embeddings_norm, tf.expand_dims(topic_angles_mean, -1), topic_angles_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.99999994, 0.9999999 , 1.        , 1.        ,\n",
       "       1.0000001 , 0.9999999 , 1.0000001 , 1.0000001 , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.9999999 , 0.9999999 , 0.99999994, 1.        ,\n",
       "       1.        , 0.9999999 , 1.0000001 , 1.        , 1.        ,\n",
       "       1.        , 0.99999994, 1.        , 0.99999994, 0.99999994,\n",
       "       1.0000001 , 0.9999999 , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 0.9999999 , 1.        ,\n",
       "       0.9999999 , 1.        , 1.        , 1.        , 0.99999994,\n",
       "       1.0000001 , 1.        , 1.        , 0.99999994, 1.        ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_value([tf.reduce_sum(tf.square(topic_embeddings_norm), 1)], return_value=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum_4:0 : [1.0000001  1.         1.0000001  0.99999994 1.         1.\n",
      " 1.         0.99999994 0.99999994 1.         1.         1.\n",
      " 0.99999994 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.9999999\n",
      " 1.         0.99999994 1.         1.0000001  0.99999994 1.\n",
      " 1.         1.        ]\n",
      "Sum_5:0 : [1.         0.99999994 1.         0.99999994 0.99999994 1.\n",
      " 1.         0.9999998  1.         1.         1.         1.\n",
      " 1.         1.         1.         1.0000001  0.99999994 1.\n",
      " 1.         0.99999994 0.9999999  0.99999994 1.0000001  1.\n",
      " 1.         1.         1.         0.99999994 0.9999999  1.\n",
      " 1.         1.         0.99999994 1.         0.99999994 1.\n",
      " 1.         0.9999999  1.         1.         1.         1.\n",
      " 0.99999994 1.         0.99999994 0.99999994 0.99999994 0.99999994\n",
      " 1.         1.        ]\n",
      "Sum_6:0 : [1.         1.         1.0000001  0.99999994 1.         1.\n",
      " 0.9999999  0.99999994 0.9999999  1.         1.0000001  1.\n",
      " 0.9999999  1.         1.         0.99999994 1.         0.9999999\n",
      " 0.9999999  0.9999999  1.0000001  1.         0.9999999  0.9999998\n",
      " 1.         0.99999994 0.99999994 0.9999999  0.99999994 0.9999999\n",
      " 1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "debug_value([tf.reduce_sum(prob_topic, -1), tf.reduce_sum(topic_bow, -1), tf.reduce_sum(tf.exp(prob_bow), 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_bow = tf.exp(0.5 * logvars_bow)\n",
    "dist_bow = tfd.Normal(means_bow, sigma_bow)\n",
    "dist_std = tfd.Normal(0., 1.)\n",
    "topic_loss_kl_tmp = tf.reduce_mean(tf.reduce_sum(tfd.kl_divergence(dist_bow, dist_std), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg:0 : 405.38312\n",
      "Mean_1:0 : 0.32056683\n",
      "Mean_4:0 : 0.32056683\n"
     ]
    }
   ],
   "source": [
    "debug_value([topic_loss_recon, topic_loss_kl, topic_loss_kl_tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "_logvars, _means, _kl_losses, _latents, _output_logits = sess.run([logvars, means, kl_losses, latents, output_logits], feed_dict=feed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 32), (32, 32), (32,), (32, 32))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_logvars.shape, _means.shape, _kl_losses.shape, _latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_output_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dec_target_idxs_do' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-7de59bc2cc54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_output_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dec_target_idxs_do\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dec_mask_tokens_do\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_recon_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_kl_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_target_idxs_do\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_mask_tokens_do\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dec_target_idxs_do' is not defined"
     ]
    }
   ],
   "source": [
    "_output_logits, _dec_target_idxs_do, _dec_mask_tokens_do, _recon_loss, _kl_losses, _ = sess.run([output_logits, dec_target_idxs_do, dec_mask_tokens_do, recon_loss, kl_losses, opt], feed_dict=feed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 46)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_max(output_logits, 2).eval(session=sess, feed_dict=feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 46, 20000), (120, 46), (120, 46))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_output_logits.shape, _dec_target_idxs_do.shape, _dec_mask_tokens_do.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "_logits = np.exp(_output_logits) / np.sum(np.exp(_output_logits), 2)[:, :, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "_idxs = _dec_target_idxs_do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "_losses = np.array([[-np.log(_logits[i, j, _idxs[i, j]]) for j in range(_idxs.shape[1])] for i in range(_idxs.shape[0])]) * _dec_mask_tokens_do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.903732"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(_losses)/np.sum(_dec_mask_tokens_do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.903732"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_recon_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_kl_losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
