{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import sys\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from data_structure import get_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "from topic_model import TopicModel\n",
    "\n",
    "# from absl import logging\n",
    "# logging.warning('Worrying Stuff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '2', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/apnews/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'sports', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 1000, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 128, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 500, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.1, 'lr')\n",
    "flags.DEFINE_float('reg', 0.1, 'regularization term')\n",
    "flags.DEFINE_float('beta', 0.0, 'initial value of beta')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_integer('warmup', 10, 'warmup period for KL')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 10, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_topic', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('PAD_IDX', word_to_idx[PAD], 'PAD_IDX')\n",
    "flags.DEFINE_integer('UNK_IDX', word_to_idx[UNK], 'UNK_IDX')\n",
    "flags.DEFINE_integer('BOS_IDX', word_to_idx[BOS], 'BOS_IDX')\n",
    "flags.DEFINE_integer('EOS_IDX', word_to_idx[EOS], 'EOS_IDX')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "maximum_iterations = max([max([instance.max_sent_l for instance in batch]) for ct, batch in dev_batches])\n",
    "flags.DEFINE_integer('maximum_iterations', maximum_iterations, 'maximum_iterations')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build language model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['bow'] = tf.placeholder(tf.float32, [None, config.dim_bow])\n",
    "t_variables['input_token_idxs'] = tf.placeholder(tf.int32, [None, None])\n",
    "t_variables['dec_input_idxs'] = tf.placeholder(tf.int32, [None, None])\n",
    "t_variables['dec_target_idxs'] = tf.placeholder(tf.int32, [None, None])\n",
    "t_variables['doc_l'] = tf.placeholder(tf.int32, [None])\n",
    "t_variables['sent_l'] = tf.placeholder(tf.int32, [None])\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train', assertion=False):\n",
    "    def token_dropout(sent_idxs):\n",
    "        sent_idxs_dropout = np.asarray(sent_idxs)\n",
    "        sent_idxs_dropout[np.random.rand(len(sent_idxs)) > config.word_keep_prob] = config.UNK_IDX\n",
    "        return list(sent_idxs_dropout)\n",
    "\n",
    "    bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "    \n",
    "    doc_l = np.array([len(instance.token_idxs) for instance in batch])\n",
    "    \n",
    "    feed_input_token_idxs_list = [sent_idxs for instance in batch for sent_idxs in instance.token_idxs]\n",
    "    feed_dec_input_idxs_list = [[config.BOS_IDX] + token_dropout(sent_idxs) for sent_idxs in feed_input_token_idxs_list]\n",
    "    feed_dec_target_idxs_list = [sent_idxs + [config.EOS_IDX]  for sent_idxs in feed_input_token_idxs_list]\n",
    "        \n",
    "    sent_l = np.array([len(sent_idxs) for sent_idxs in feed_input_token_idxs_list], np.int32)\n",
    "    \n",
    "    feed_input_token_idxs = pad_sequences(feed_input_token_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_input_idxs = pad_sequences(feed_dec_input_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_target_idxs = pad_sequences(feed_dec_target_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    \n",
    "    if assertion:\n",
    "        index = 0\n",
    "        for instance in batch:\n",
    "            for line_idxs in instance.token_idxs:\n",
    "                assert feed_input_token_idxs_list[index] == line_idxs\n",
    "                index += 1\n",
    "        assert feed_input_token_idxs.shape[1] == np.max(sent_l)\n",
    "        assert feed_dec_input_idxs.shape[1] == np.max(sent_l) + 1\n",
    "        assert feed_dec_target_idxs.shape[1] == np.max(sent_l) + 1\n",
    "    \n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "\n",
    "    feed_dict = {\n",
    "                t_variables['bow']: bow, \n",
    "                t_variables['doc_l']: doc_l, t_variables['sent_l']: sent_l, \n",
    "                t_variables['input_token_idxs']: feed_input_token_idxs, t_variables['dec_input_idxs']: feed_dec_input_idxs, t_variables['dec_target_idxs']: feed_dec_target_idxs, \n",
    "                t_variables['keep_prob']: keep_prob\n",
    "    }\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = test_batches[0][1]\n",
    "get_feed_dict(batch);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "code_folding": [
     0,
     18
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables, sess_init=None):\n",
    "    if sess_init is None:\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    else:\n",
    "        sess = sess_init\n",
    "        \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    if sess_init is None: sess.close()\n",
    "\n",
    "def debug_value(variables, return_value=False, sess_init=None):\n",
    "    if sess_init is None:\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    else:\n",
    "        sess = sess_init\n",
    "\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "    if sess_init is None: sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encode bow\n",
    "with tf.variable_scope('topic/enc', reuse=False):\n",
    "    hidden_bow_ = tf.keras.layers.Dense(units=config.dim_hidden_bow, activation=tf.nn.relu, name='hidden')(t_variables['bow'])\n",
    "    hidden_bow = tf.keras.layers.Dropout(t_variables['keep_prob'])(hidden_bow_)\n",
    "    means_bow = tf.keras.layers.Dense(units=config.dim_latent_topic)(hidden_bow)\n",
    "    logvars_bow = tf.keras.layers.Dense(units=config.dim_latent_topic, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0))(hidden_bow)\n",
    "    latents_bow = sample_latents(means_bow, logvars_bow) # sample latent vectors\n",
    "\n",
    "    prob_topic = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob')(latents_bow) # inference of topic probabilities\n",
    "\n",
    "# decode bow\n",
    "with tf.variable_scope('shared', reuse=False):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "bow_embeddings = tf.nn.embedding_lookup(embeddings, bow_idxs) # embeddings of each bow features\n",
    "\n",
    "with tf.variable_scope('topic/dec', reuse=False):\n",
    "    topic_embeddings = tf.get_variable('topic_emb', [config.n_topic, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of topics\n",
    "\n",
    "    topic_bow = tf.nn.softmax(tf.matmul(topic_embeddings, bow_embeddings, transpose_b=True), 1) # bow vectors for each topic\n",
    "    logits_bow = tf_log(tf.matmul(prob_topic, topic_bow)) # predicted bow distribution\n",
    "\n",
    "# prior of each gaussian distribution (computed for each topic)\n",
    "with tf.variable_scope('topic/enc/infer', reuse=False):\n",
    "    means_topic = tf.keras.layers.Dense(units=config.dim_latent)(topic_bow)\n",
    "    logvars_topic = tf.keras.layers.Dense(units=config.dim_latent, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0))(topic_bow)\n",
    "sigma_topic = tf.exp(0.5 * logvars_topic)\n",
    "gauss_topic = tfd.Normal(loc=means_topic, scale=sigma_topic)    \n",
    "    \n",
    "# define losses\n",
    "topic_losses_recon = -tf.reduce_sum(tf.multiply(t_variables['bow'], logits_bow), 1)\n",
    "topic_loss_recon = tf.reduce_mean(topic_losses_recon) # negative log likelihood of each words\n",
    "\n",
    "topic_loss_kl = compute_kl_loss(means_bow, logvars_bow) # KL divergence b/w latent dist & gaussian std\n",
    "\n",
    "topic_embeddings_norm = topic_embeddings / tf.norm(topic_embeddings, axis=1, keepdims=True)\n",
    "topic_angles = tf.matmul(topic_embeddings_norm, tf.transpose(topic_embeddings_norm))\n",
    "topic_angles_mean = tf.reduce_mean(topic_angles, keepdims=True)\n",
    "topic_angles_vars = tf.reduce_mean(tf.square(topic_angles - topic_angles_mean))\n",
    "topic_loss_reg = topic_angles_vars - tf.squeeze(topic_angles_mean)\n",
    "\n",
    "# monitor\n",
    "n_bow = tf.reduce_sum(t_variables['bow'], 1)\n",
    "topic_ppls = tf.divide(topic_losses_recon, n_bow)\n",
    "topics_freq_bow_indices = tf.nn.top_k(topic_bow, 10, name='topic_freq_bow').indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic/enc/dropout/cond/Merge:0 : (21, 256)\n",
      "topic/enc/add:0 : (21, 32)\n",
      "topic/enc/prob/Softmax:0 : (21, 13)\n"
     ]
    }
   ],
   "source": [
    "debug_shape([hidden_bow, latents_bow, prob_topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic/dec/topic_emb:0 : (13, 256)\n",
      "topic/dec/Softmax:0 : (13, 2661)\n",
      "topic/dec/Log:0 : (21, 2661)\n",
      "topic/enc/infer/dense_2/BiasAdd:0 : (13, 32)\n",
      "Neg:0 : (21,)\n"
     ]
    }
   ],
   "source": [
    "debug_shape([topic_embeddings, topic_bow, logits_bow, means_topic, topic_losses_recon])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum_10:0 : [1.0000001  0.9999999  0.9999999  1.         1.         1.0000001\n",
      " 1.         1.         1.         1.         0.99999994 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.99999994 1.         0.99999994]\n"
     ]
    }
   ],
   "source": [
    "debug_value([tf.reduce_sum(prob_topic, 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "input_token_idxs = t_variables['input_token_idxs']\n",
    "batch_l, max_sent_l = tf.shape(input_token_idxs)[0], tf.shape(input_token_idxs)[1]\n",
    "sent_l = t_variables['sent_l']\n",
    "\n",
    "with tf.variable_scope('sent/enc', reuse=False):\n",
    "    # get word embedding\n",
    "    enc_input = tf.nn.embedding_lookup(embeddings, input_token_idxs)\n",
    "\n",
    "    # get sentence embedding\n",
    "    _, enc_state = dynamic_rnn(enc_input, sent_l, config.dim_hidden, t_variables['keep_prob'])\n",
    "\n",
    "    # TODO House Holder flow\n",
    "    prob_topic_infer = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax)(enc_state)\n",
    "    \n",
    "    # inference of each gaussian dist. parameter\n",
    "    enc_state_infer = tf.tile(tf.expand_dims(enc_state, 1), [1, config.n_topic, 1]) # tile over topics\n",
    "    means_topic_infer = tf.keras.layers.Dense(units=config.dim_latent)(enc_state_infer)\n",
    "    logvars_topic_infer = tf.keras.layers.Dense(units=config.dim_latent, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0))(enc_state_infer)\n",
    "    sigma_topic_infer = tf.exp(0.5 * logvars_topic_infer)\n",
    "    gauss_topic_infer = tfd.Normal(loc=means_topic_infer, scale=sigma_topic_infer)\n",
    "\n",
    "    # latent vectors from each gaussian dist.\n",
    "    latents_topic_infer = sample_latents(means_topic_infer, logvars_topic_infer) \n",
    "    # latent vector from gaussian mixture    \n",
    "    latents_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), latents_topic_infer, transpose_a=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder_1:0 : (132, 40)\n",
      "sent/enc/rnn/while/Exit_3:0 : (132, 512)\n",
      "sent/enc/Tile:0 : (132, 13, 512)\n",
      "sent/enc/add:0 : (132, 13, 32)\n",
      "sent/enc/dense/Softmax:0 : (132, 13)\n",
      "sent/enc/MatMul:0 : (132, 1, 32)\n"
     ]
    }
   ],
   "source": [
    "debug_shape([input_token_idxs, enc_state, enc_state_infer, latents_topic_infer, prob_topic_infer, latents_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum_9:0 : [1.0000001  1.         0.99999994 1.         1.         0.9999999\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         0.99999994 1.         0.99999994\n",
      " 1.         1.         1.         1.         1.         0.99999994\n",
      " 1.         1.         0.99999994 1.         1.         0.9999999\n",
      " 1.         1.         0.99999994 1.         0.99999994 1.\n",
      " 1.         1.         1.0000001  1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.9999999\n",
      " 1.         0.99999994 0.99999994 1.         1.         1.\n",
      " 1.         1.         1.         0.9999999  1.         1.\n",
      " 1.         1.         1.         1.         1.         0.99999994\n",
      " 0.99999994 1.         1.         1.         1.         0.9999999\n",
      " 1.         0.99999994 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.99999994\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.99999994 1.         1.         1.         1.\n",
      " 1.         0.99999994 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.99999994\n",
      " 1.         1.         1.         0.9999999  1.         0.9999999\n",
      " 0.99999994 1.         1.         1.         1.         1.\n",
      " 1.         0.99999994 1.         1.         1.         1.\n",
      " 0.99999994 1.         1.         1.         1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "debug_value([tf.reduce_sum(prob_topic_infer, 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for decoding\n",
    "dec_sent_l = tf.add(sent_l, 1)\n",
    "dec_input_idxs = t_variables['dec_input_idxs']\n",
    "dec_input = tf.nn.embedding_lookup(embeddings, dec_input_idxs)\n",
    "\n",
    "dec_latents_input = tf.tile(latents_input, [1, tf.shape(dec_input)[1], 1])\n",
    "dec_concat_input = tf.concat([dec_input, dec_latents_input], -1)\n",
    "\n",
    "# decode for training\n",
    "with tf.variable_scope('sent/dec/rnn', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32, reuse=tf.AUTO_REUSE):\n",
    "    dec_cell = tf.contrib.rnn.GRUCell(config.dim_hidden)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob = t_variables['keep_prob'])\n",
    "\n",
    "    dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu)(tf.squeeze(latents_input, 1))\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_concat_input, sequence_length=dec_sent_l)\n",
    "\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=dec_cell,\n",
    "        helper=helper,\n",
    "        initial_state=dec_initial_state)\n",
    "\n",
    "    dec_outputs, _, output_sent_l = tf.contrib.seq2seq.dynamic_decode(train_decoder)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(config.n_vocab, use_bias=False, name='out')\n",
    "    output_logits = output_layer(dec_outputs.rnn_output)\n",
    "    \n",
    "    output_token_idxs = tf.argmax(output_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concat:0 : (132, 41, 288)\n",
      "sent/dec/rnn/dense/Relu:0 : (132, 512)\n",
      "sent/dec/rnn/out/Tensordot:0 : (132, 41, 21867)\n",
      "sent/dec/rnn/ArgMax:0 : (132, 41)\n"
     ]
    }
   ],
   "source": [
    "debug_shape([dec_concat_input, dec_initial_state, output_logits, output_token_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language modeling cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target and mask\n",
    "dec_target_idxs = t_variables['dec_target_idxs']\n",
    "dec_mask_tokens = tf.sequence_mask(dec_sent_l, maxlen=max_sent_l+1, dtype=tf.float32)\n",
    "\n",
    "# nll for each token (averaged over batch & sentence)\n",
    "sent_loss_recon = tf.contrib.seq2seq.sequence_loss(output_logits, dec_target_idxs, dec_mask_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_l = t_variables['doc_l']\n",
    "mask_sents = tf.sequence_mask(doc_l)\n",
    "mask_sents_flatten = tf.reshape(mask_sents, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1]])\n",
    "\n",
    "prob_topic_tiled = tf.tile(tf.expand_dims(prob_topic, 1), [1, tf.shape(mask_sents)[1], 1])\n",
    "prob_topic_flatten = tf.reshape(prob_topic_tiled, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1], config.n_topic])\n",
    "prob_topic_sents = tf.boolean_mask(prob_topic_flatten, mask_sents_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferred mixture probabilities (computed for each sentence)\n",
    "categ_topic_infer = tfd.Categorical(probs=prob_topic_infer)\n",
    "\n",
    "# prior of mixture probabilities (computed for each document, tiled for each sentence)\n",
    "categ_topic = tfd.Categorical(probs=prob_topic_sents)\n",
    "\n",
    "sent_loss_kl_categ = tf.reduce_mean(tfd.kl_divergence(categ_topic_infer, categ_topic))\n",
    "\n",
    "# inference of each gaussian gaussribution (computed for each sentence)\n",
    "\n",
    "sent_loss_kl_gauss = tf.reduce_sum(tfd.kl_divergence(gauss_topic_infer, gauss_topic), -1)\n",
    "sent_loss_kl_gmm = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss), -1))\n",
    "\n",
    "sent_loss_kl = sent_loss_kl_categ + sent_loss_kl_gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder_4:0 : (21,)\n",
      "SequenceMask_1/Less:0 : (21, 9)\n",
      "Reshape:0 : (189,)\n"
     ]
    }
   ],
   "source": [
    "debug_shape([doc_l, mask_sents, mask_sents_flatten])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tile_1:0 : (21, 9, 13)\n",
      "Reshape_1:0 : (189, 13)\n",
      "boolean_mask/GatherV2:0 : (132, 13)\n",
      "sent/enc/dense/Softmax:0 : (132, 13)\n"
     ]
    }
   ],
   "source": [
    "debug_shape([prob_topic_tiled, prob_topic_flatten, prob_topic_sents, prob_topic_infer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceMask_1/Less:0 : [[ True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True False]\n",
      " [ True  True  True  True  True  True  True  True False]\n",
      " [ True  True  True  True  True  True  True  True False]\n",
      " [ True  True  True  True  True  True  True  True False]\n",
      " [ True  True  True  True  True  True  True False False]\n",
      " [ True  True  True  True  True  True  True False False]\n",
      " [ True  True  True  True  True  True  True False False]\n",
      " [ True  True  True  True  True  True  True False False]\n",
      " [ True  True  True  True  True  True False False False]\n",
      " [ True  True  True  True  True  True False False False]\n",
      " [ True  True  True  True  True  True False False False]\n",
      " [ True  True  True  True  True  True False False False]\n",
      " [ True  True  True  True  True  True False False False]\n",
      " [ True  True  True  True  True  True False False False]\n",
      " [ True  True  True  True  True False False False False]\n",
      " [ True  True  True  True  True False False False False]\n",
      " [ True  True  True  True  True False False False False]\n",
      " [ True  True  True  True  True False False False False]\n",
      " [ True  True  True  True False False False False False]\n",
      " [ True  True  True False False False False False False]]\n",
      "Reshape:0 : [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True False  True  True  True  True  True  True\n",
      "  True  True False  True  True  True  True  True  True  True  True False\n",
      "  True  True  True  True  True  True  True  True False  True  True  True\n",
      "  True  True  True  True False False  True  True  True  True  True  True\n",
      "  True False False  True  True  True  True  True  True  True False False\n",
      "  True  True  True  True  True  True  True False False  True  True  True\n",
      "  True  True  True False False False  True  True  True  True  True  True\n",
      " False False False  True  True  True  True  True  True False False False\n",
      "  True  True  True  True  True  True False False False  True  True  True\n",
      "  True  True  True False False False  True  True  True  True  True  True\n",
      " False False False  True  True  True  True  True False False False False\n",
      "  True  True  True  True  True False False False False  True  True  True\n",
      "  True  True False False False False  True  True  True  True  True False\n",
      " False False False  True  True  True  True False False False False False\n",
      "  True  True  True False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "debug_value([mask_sents, mask_sents_flatten])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent/enc/dense_5/BiasAdd:0 : (132, 13, 32)\n",
      "topic/enc/infer/dense_2/BiasAdd:0 : (13, 32)\n"
     ]
    }
   ],
   "source": [
    "debug_shape([means_topic_infer, means_topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tile_2:0 : (132, 13, 32)\n",
      "Sum_11:0 : [[0.00031778 0.00031761 0.00031753 ... 0.00031782 0.00031723 0.00031761]\n",
      " [0.00026743 0.00026744 0.00026729 ... 0.0002673  0.00026752 0.00026738]\n",
      " [0.00021236 0.00021253 0.00021226 ... 0.00021252 0.00021227 0.00021219]\n",
      " ...\n",
      " [0.00011392 0.00011369 0.00011354 ... 0.00011363 0.00011366 0.00011365]\n",
      " [0.00024045 0.00024016 0.00024023 ... 0.00024029 0.00024018 0.00024052]\n",
      " [0.00025667 0.00025701 0.00025676 ... 0.00025693 0.0002569  0.00025666]]\n",
      "Sum_18:0 : [[0.00031778 0.00031761 0.00031753 ... 0.00031782 0.00031723 0.00031761]\n",
      " [0.00026743 0.00026744 0.00026729 ... 0.0002673  0.00026752 0.00026738]\n",
      " [0.00021236 0.00021253 0.00021226 ... 0.00021252 0.00021227 0.00021219]\n",
      " ...\n",
      " [0.00011392 0.00011369 0.00011354 ... 0.00011363 0.00011366 0.00011365]\n",
      " [0.00024045 0.00024016 0.00024023 ... 0.00024029 0.00024018 0.00024052]\n",
      " [0.00025667 0.00025701 0.00025676 ... 0.00025693 0.0002569  0.00025666]]\n"
     ]
    }
   ],
   "source": [
    "means_topic_tmp = tf.tile(tf.expand_dims(means_topic, 0), [batch_l, 1, 1])\n",
    "sigma_topic_tmp = tf.tile(tf.expand_dims(sigma_topic, 0), [batch_l, 1, 1])\n",
    "\n",
    "gauss_topic_tmp = tfd.Normal(loc=means_topic_tmp, scale=sigma_topic_tmp)\n",
    "sent_loss_kl_gauss_tmp = tf.reduce_sum(tfd.kl_divergence(gauss_topic_infer, gauss_topic_tmp), -1)\n",
    "\n",
    "debug_shape([means_topic_tmp])\n",
    "debug_value([sent_loss_kl_gauss, sent_loss_kl_gauss_tmp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "beta = tf.Variable(config.beta, name='beta', trainable=False) if config.warmup > 0 else tf.constant(1., name='beta')\n",
    "update_beta = tf.assign_add(beta, 1./(config.warmup*len(train_batches)))\n",
    "sent_loss = sent_loss_recon + beta * sent_loss_kl\n",
    "\n",
    "topic_loss = topic_loss_recon + topic_loss_kl + config.reg * topic_loss_reg\n",
    "loss = topic_loss + sent_loss\n",
    "\n",
    "# define optimizer\n",
    "if config.opt == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif config.opt == 'Adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "    \n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_sents(token_idxs, config, idx_to_word):\n",
    "    sents = []\n",
    "    for sent_idxs in token_idxs:\n",
    "        tokens = []\n",
    "        for idx in sent_idxs:\n",
    "            if idx == config.EOS_IDX: break\n",
    "            tokens.append(idx_to_word[idx])\n",
    "        sent = ' '.join(tokens)\n",
    "        sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_batch, sent_loss_batch, ppls_batch = sess.run([loss, topic_loss, sent_loss, topic_ppls], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_batch, sent_loss_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "    loss_mean, topic_loss_mean, sent_loss_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    return loss_mean, topic_loss_mean, sent_loss_mean, ppl_mean\n",
    "\n",
    "def get_all_losses(sess, batches):\n",
    "    losses = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch = \\\n",
    "        sess.run([loss, topic_loss_recon, topic_loss_kl, sent_loss_recon, sent_loss_kl], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "    print('LOSS %.2f | TM NLL: %.2f, KL: %.4f | LM NLL: %.2f, KL: %.4f' %  np.mean(losses, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_batch):\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    pred_token_idxs = sess.run(output_token_idxs, feed_dict = feed_dict)\n",
    "    true_token_idxs = [sent_idxs for instance in sample_batch for sent_idxs in instance.token_idxs]\n",
    "    \n",
    "    assert len(pred_token_idxs) == len(true_token_idxs)\n",
    "    \n",
    "    pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "    true_sents = idxs_to_sents(true_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    for true_sent, pred_sent in zip(true_sents, pred_sents):        \n",
    "        print('True: %s' % true_sent)\n",
    "        print('Pred: %s' % pred_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "logs = []\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "007[s], 05[s], Ep: 00, Ct: 00000|TR LOSS: 351, PPL: 2662|TM NLL: 341, KL: 0.65 | LM NLL: 9.99, KL: 0.68|DE LOSS: 346, PPL: 2658, TM: 336, LM: 9.99|BETA: 0.000000\n",
      "100[s], 05[s], Ep: 00, Ct: 00500|TR LOSS: 326, PPL: 1560|TM NLL: 317, KL: 1.30 | LM NLL: 7.23, KL: 4.81|DE LOSS: 317, PPL: 1374, TM: 310, LM: 6.82|BETA: 0.034674\n",
      "100[s], 05[s], Ep: 00, Ct: 01000|TR LOSS: 323, PPL: 1450|TM NLL: 314, KL: 1.66 | LM NLL: 7.00, KL: 3.19|DE LOSS: 314, PPL: 1294, TM: 307, LM: 6.75|BETA: 0.069347\n",
      "090[s], 05[s], Ep: 01, Ct: 00000|TR LOSS: 321, PPL: 1387|TM NLL: 312, KL: 1.90 | LM NLL: 6.91, KL: 2.53|DE LOSS: 312, PPL: 1216, TM: 305, LM: 6.75|BETA: 0.100000\n",
      "101[s], 05[s], Ep: 01, Ct: 00500|TR LOSS: 319, PPL: 1328|TM NLL: 310, KL: 2.15 | LM NLL: 6.85, KL: 2.13|DE LOSS: 310, PPL: 1147, TM: 303, LM: 6.71|BETA: 0.134675\n",
      "101[s], 05[s], Ep: 01, Ct: 01000|TR LOSS: 318, PPL: 1289|TM NLL: 309, KL: 2.31 | LM NLL: 6.81, KL: 1.86|DE LOSS: 309, PPL: 1124, TM: 302, LM: 6.69|BETA: 0.169350\n",
      "090[s], 05[s], Ep: 02, Ct: 00000|TR LOSS: 317, PPL: 1261|TM NLL: 308, KL: 2.43 | LM NLL: 6.77, KL: 1.70|DE LOSS: 309, PPL: 1108, TM: 302, LM: 6.65|BETA: 0.200003\n",
      "101[s], 05[s], Ep: 02, Ct: 00500|TR LOSS: 317, PPL: 1237|TM NLL: 307, KL: 2.53 | LM NLL: 6.73, KL: 1.56|DE LOSS: 308, PPL: 1086, TM: 301, LM: 6.60|BETA: 0.234678\n",
      "101[s], 05[s], Ep: 02, Ct: 01000|TR LOSS: 316, PPL: 1218|TM NLL: 306, KL: 2.63 | LM NLL: 6.69, KL: 1.46|DE LOSS: 308, PPL: 1077, TM: 301, LM: 6.58|BETA: 0.269353\n",
      "089[s], 05[s], Ep: 03, Ct: 00000|TR LOSS: 315, PPL: 1202|TM NLL: 306, KL: 2.72 | LM NLL: 6.66, KL: 1.39|DE LOSS: 307, PPL: 1073, TM: 301, LM: 6.57|BETA: 0.300006\n",
      "102[s], 05[s], Ep: 03, Ct: 00500|TR LOSS: 315, PPL: 1186|TM NLL: 305, KL: 2.79 | LM NLL: 6.63, KL: 1.31|DE LOSS: 307, PPL: 1060, TM: 300, LM: 6.48|BETA: 0.334681\n",
      "101[s], 05[s], Ep: 03, Ct: 01000|TR LOSS: 315, PPL: 1174|TM NLL: 305, KL: 2.85 | LM NLL: 6.60, KL: 1.26|DE LOSS: 307, PPL: 1051, TM: 300, LM: 6.44|BETA: 0.369356\n",
      "090[s], 05[s], Ep: 04, Ct: 00000|TR LOSS: 314, PPL: 1163|TM NLL: 305, KL: 2.91 | LM NLL: 6.57, KL: 1.21|DE LOSS: 306, PPL: 1049, TM: 300, LM: 6.43|BETA: 0.400008\n",
      "101[s], 05[s], Ep: 04, Ct: 00500|TR LOSS: 314, PPL: 1152|TM NLL: 304, KL: 2.97 | LM NLL: 6.54, KL: 1.17|DE LOSS: 306, PPL: 1033, TM: 299, LM: 6.39|BETA: 0.434683\n",
      "101[s], 05[s], Ep: 04, Ct: 01000|TR LOSS: 313, PPL: 1143|TM NLL: 304, KL: 3.02 | LM NLL: 6.51, KL: 1.13|DE LOSS: 305, PPL: 1023, TM: 299, LM: 6.37|BETA: 0.469358\n",
      "089[s], 05[s], Ep: 05, Ct: 00000|TR LOSS: 313, PPL: 1134|TM NLL: 303, KL: 3.06 | LM NLL: 6.48, KL: 1.11|DE LOSS: 305, PPL: 1018, TM: 299, LM: 6.36|BETA: 0.500011\n",
      "102[s], 05[s], Ep: 05, Ct: 00500|TR LOSS: 313, PPL: 1125|TM NLL: 303, KL: 3.11 | LM NLL: 6.46, KL: 1.08|DE LOSS: 305, PPL: 1014, TM: 299, LM: 6.35|BETA: 0.534671\n",
      "100[s], 05[s], Ep: 05, Ct: 01000|TR LOSS: 313, PPL: 1117|TM NLL: 303, KL: 3.14 | LM NLL: 6.43, KL: 1.05|DE LOSS: 305, PPL: 1008, TM: 298, LM: 6.33|BETA: 0.569331\n",
      "090[s], 05[s], Ep: 06, Ct: 00000|TR LOSS: 312, PPL: 1111|TM NLL: 303, KL: 3.18 | LM NLL: 6.41, KL: 1.03|DE LOSS: 305, PPL: 1010, TM: 298, LM: 6.33|BETA: 0.599971\n",
      "101[s], 05[s], Ep: 06, Ct: 00500|TR LOSS: 312, PPL: 1104|TM NLL: 302, KL: 3.21 | LM NLL: 6.39, KL: 1.01|DE LOSS: 304, PPL: 997, TM: 298, LM: 6.30|BETA: 0.634631\n",
      "101[s], 05[s], Ep: 06, Ct: 01000|TR LOSS: 312, PPL: 1098|TM NLL: 302, KL: 3.24 | LM NLL: 6.36, KL: 1.00|DE LOSS: 304, PPL: 996, TM: 298, LM: 6.28|BETA: 0.669291\n",
      "090[s], 05[s], Ep: 07, Ct: 00000|TR LOSS: 312, PPL: 1092|TM NLL: 302, KL: 3.27 | LM NLL: 6.34, KL: 0.98|DE LOSS: 304, PPL: 1001, TM: 298, LM: 6.30|BETA: 0.699930\n",
      "102[s], 05[s], Ep: 07, Ct: 00500|TR LOSS: 312, PPL: 1087|TM NLL: 302, KL: 3.30 | LM NLL: 6.32, KL: 0.96|DE LOSS: 304, PPL: 992, TM: 298, LM: 6.27|BETA: 0.734591\n",
      "101[s], 05[s], Ep: 07, Ct: 01000|TR LOSS: 311, PPL: 1082|TM NLL: 301, KL: 3.32 | LM NLL: 6.30, KL: 0.95|DE LOSS: 304, PPL: 995, TM: 298, LM: 6.25|BETA: 0.769251\n",
      "088[s], 04[s], Ep: 08, Ct: 00000|TR LOSS: 311, PPL: 1078|TM NLL: 301, KL: 3.35 | LM NLL: 6.29, KL: 0.94|DE LOSS: 304, PPL: 990, TM: 298, LM: 6.28|BETA: 0.799890\n",
      "101[s], 05[s], Ep: 08, Ct: 00500|TR LOSS: 311, PPL: 1074|TM NLL: 301, KL: 3.37 | LM NLL: 6.27, KL: 0.93|DE LOSS: 304, PPL: 989, TM: 298, LM: 6.26|BETA: 0.834550\n",
      "101[s], 05[s], Ep: 08, Ct: 01000|TR LOSS: 311, PPL: 1070|TM NLL: 301, KL: 3.39 | LM NLL: 6.25, KL: 0.91|DE LOSS: 304, PPL: 985, TM: 298, LM: 6.28|BETA: 0.869210\n",
      "090[s], 05[s], Ep: 09, Ct: 00000|TR LOSS: 311, PPL: 1066|TM NLL: 301, KL: 3.41 | LM NLL: 6.23, KL: 0.90|DE LOSS: 304, PPL: 989, TM: 298, LM: 6.26|BETA: 0.899850\n",
      "101[s], 05[s], Ep: 09, Ct: 00500|TR LOSS: 311, PPL: 1062|TM NLL: 301, KL: 3.43 | LM NLL: 6.22, KL: 0.89|DE LOSS: 304, PPL: 982, TM: 298, LM: 6.26|BETA: 0.934510\n",
      "101[s], 05[s], Ep: 09, Ct: 01000|TR LOSS: 310, PPL: 1059|TM NLL: 301, KL: 3.45 | LM NLL: 6.20, KL: 0.88|DE LOSS: 304, PPL: 983, TM: 298, LM: 6.26|BETA: 0.969170\n",
      "090[s], 05[s], Ep: 10, Ct: 00000|TR LOSS: 310, PPL: 1056|TM NLL: 300, KL: 3.46 | LM NLL: 6.19, KL: 0.87|DE LOSS: 304, PPL: 979, TM: 298, LM: 6.31|BETA: 0.999810\n",
      "101[s], 05[s], Ep: 10, Ct: 00500|TR LOSS: 310, PPL: 1053|TM NLL: 300, KL: 3.48 | LM NLL: 6.17, KL: 0.87|DE LOSS: 304, PPL: 983, TM: 297, LM: 6.27|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 10, Ct: 01000|TR LOSS: 310, PPL: 1050|TM NLL: 300, KL: 3.50 | LM NLL: 6.16, KL: 0.86|DE LOSS: 304, PPL: 980, TM: 297, LM: 6.24|BETA: 1.034499\n",
      "089[s], 05[s], Ep: 11, Ct: 00000|TR LOSS: 310, PPL: 1048|TM NLL: 300, KL: 3.51 | LM NLL: 6.14, KL: 0.85|DE LOSS: 304, PPL: 982, TM: 298, LM: 6.26|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 11, Ct: 00500|TR LOSS: 310, PPL: 1045|TM NLL: 300, KL: 3.52 | LM NLL: 6.13, KL: 0.84|DE LOSS: 304, PPL: 980, TM: 297, LM: 6.22|BETA: 1.034499\n",
      "100[s], 05[s], Ep: 11, Ct: 01000|TR LOSS: 310, PPL: 1043|TM NLL: 300, KL: 3.54 | LM NLL: 6.11, KL: 0.83|DE LOSS: 303, PPL: 974, TM: 297, LM: 6.19|BETA: 1.034499\n",
      "089[s], 05[s], Ep: 12, Ct: 00000|TR LOSS: 310, PPL: 1040|TM NLL: 300, KL: 3.55 | LM NLL: 6.10, KL: 0.83|DE LOSS: 303, PPL: 975, TM: 297, LM: 6.21|BETA: 1.034499\n",
      "100[s], 05[s], Ep: 12, Ct: 00500|TR LOSS: 310, PPL: 1038|TM NLL: 300, KL: 3.56 | LM NLL: 6.09, KL: 0.82|DE LOSS: 304, PPL: 976, TM: 297, LM: 6.19|BETA: 1.034499\n",
      "100[s], 05[s], Ep: 12, Ct: 01000|TR LOSS: 310, PPL: 1036|TM NLL: 300, KL: 3.58 | LM NLL: 6.08, KL: 0.81|DE LOSS: 303, PPL: 974, TM: 297, LM: 6.16|BETA: 1.034499\n",
      "089[s], 05[s], Ep: 13, Ct: 00000|TR LOSS: 310, PPL: 1034|TM NLL: 300, KL: 3.59 | LM NLL: 6.07, KL: 0.81|DE LOSS: 303, PPL: 972, TM: 297, LM: 6.16|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 13, Ct: 00500|TR LOSS: 309, PPL: 1032|TM NLL: 299, KL: 3.60 | LM NLL: 6.05, KL: 0.80|DE LOSS: 303, PPL: 973, TM: 297, LM: 6.16|BETA: 1.034499\n",
      "098[s], 04[s], Ep: 13, Ct: 01000|TR LOSS: 309, PPL: 1030|TM NLL: 299, KL: 3.61 | LM NLL: 6.04, KL: 0.80|DE LOSS: 303, PPL: 975, TM: 297, LM: 6.13|BETA: 1.034499\n",
      "083[s], 04[s], Ep: 14, Ct: 00000|TR LOSS: 309, PPL: 1028|TM NLL: 299, KL: 3.62 | LM NLL: 6.03, KL: 0.79|DE LOSS: 303, PPL: 972, TM: 297, LM: 6.14|BETA: 1.034499\n",
      "095[s], 05[s], Ep: 14, Ct: 00500|TR LOSS: 309, PPL: 1026|TM NLL: 299, KL: 3.63 | LM NLL: 6.02, KL: 0.79|DE LOSS: 303, PPL: 968, TM: 297, LM: 6.11|BETA: 1.034499\n",
      "100[s], 05[s], Ep: 14, Ct: 01000|TR LOSS: 309, PPL: 1024|TM NLL: 299, KL: 3.64 | LM NLL: 6.01, KL: 0.78|DE LOSS: 303, PPL: 969, TM: 297, LM: 6.09|BETA: 1.034499\n",
      "086[s], 05[s], Ep: 15, Ct: 00000|TR LOSS: 309, PPL: 1023|TM NLL: 299, KL: 3.65 | LM NLL: 6.00, KL: 0.78|DE LOSS: 303, PPL: 970, TM: 297, LM: 6.10|BETA: 1.034499\n",
      "100[s], 05[s], Ep: 15, Ct: 00500|TR LOSS: 309, PPL: 1021|TM NLL: 299, KL: 3.66 | LM NLL: 5.99, KL: 0.77|DE LOSS: 303, PPL: 968, TM: 297, LM: 6.09|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 15, Ct: 01000|TR LOSS: 309, PPL: 1019|TM NLL: 299, KL: 3.67 | LM NLL: 5.98, KL: 0.77|DE LOSS: 303, PPL: 969, TM: 297, LM: 6.07|BETA: 1.034499\n",
      "089[s], 05[s], Ep: 16, Ct: 00000|TR LOSS: 309, PPL: 1018|TM NLL: 299, KL: 3.68 | LM NLL: 5.97, KL: 0.76|DE LOSS: 303, PPL: 966, TM: 297, LM: 6.09|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 16, Ct: 00500|TR LOSS: 309, PPL: 1016|TM NLL: 299, KL: 3.69 | LM NLL: 5.96, KL: 0.76|DE LOSS: 303, PPL: 968, TM: 297, LM: 6.05|BETA: 1.034499\n",
      "100[s], 05[s], Ep: 16, Ct: 01000|TR LOSS: 309, PPL: 1015|TM NLL: 299, KL: 3.69 | LM NLL: 5.95, KL: 0.76|DE LOSS: 303, PPL: 966, TM: 297, LM: 6.04|BETA: 1.034499\n",
      "089[s], 05[s], Ep: 17, Ct: 00000|TR LOSS: 309, PPL: 1014|TM NLL: 299, KL: 3.70 | LM NLL: 5.94, KL: 0.75|DE LOSS: 303, PPL: 964, TM: 297, LM: 6.06|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 17, Ct: 00500|TR LOSS: 309, PPL: 1012|TM NLL: 299, KL: 3.71 | LM NLL: 5.94, KL: 0.75|DE LOSS: 303, PPL: 962, TM: 297, LM: 6.03|BETA: 1.034499\n",
      "100[s], 05[s], Ep: 17, Ct: 01000|TR LOSS: 309, PPL: 1011|TM NLL: 299, KL: 3.72 | LM NLL: 5.93, KL: 0.74|DE LOSS: 303, PPL: 964, TM: 297, LM: 6.02|BETA: 1.034499\n",
      "089[s], 05[s], Ep: 18, Ct: 00000|TR LOSS: 309, PPL: 1010|TM NLL: 298, KL: 3.73 | LM NLL: 5.92, KL: 0.74|DE LOSS: 303, PPL: 958, TM: 297, LM: 6.02|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 18, Ct: 00500|TR LOSS: 309, PPL: 1008|TM NLL: 298, KL: 3.73 | LM NLL: 5.91, KL: 0.74|DE LOSS: 303, PPL: 961, TM: 297, LM: 6.00|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 18, Ct: 01000|TR LOSS: 308, PPL: 1007|TM NLL: 298, KL: 3.74 | LM NLL: 5.90, KL: 0.73|DE LOSS: 303, PPL: 962, TM: 297, LM: 6.01|BETA: 1.034499\n",
      "090[s], 05[s], Ep: 19, Ct: 00000|TR LOSS: 308, PPL: 1006|TM NLL: 298, KL: 3.75 | LM NLL: 5.89, KL: 0.73|DE LOSS: 303, PPL: 962, TM: 297, LM: 6.01|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 19, Ct: 00500|TR LOSS: 308, PPL: 1005|TM NLL: 298, KL: 3.75 | LM NLL: 5.89, KL: 0.73|DE LOSS: 303, PPL: 961, TM: 297, LM: 6.01|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 19, Ct: 01000|TR LOSS: 308, PPL: 1004|TM NLL: 298, KL: 3.76 | LM NLL: 5.88, KL: 0.72|DE LOSS: 303, PPL: 959, TM: 297, LM: 5.98|BETA: 1.034499\n",
      "089[s], 05[s], Ep: 20, Ct: 00000|TR LOSS: 308, PPL: 1002|TM NLL: 298, KL: 3.77 | LM NLL: 5.87, KL: 0.72|DE LOSS: 303, PPL: 958, TM: 297, LM: 6.00|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 20, Ct: 00500|TR LOSS: 308, PPL: 1001|TM NLL: 298, KL: 3.77 | LM NLL: 5.86, KL: 0.72|DE LOSS: 303, PPL: 957, TM: 297, LM: 5.97|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 20, Ct: 01000|TR LOSS: 308, PPL: 1000|TM NLL: 298, KL: 3.78 | LM NLL: 5.86, KL: 0.72|DE LOSS: 303, PPL: 958, TM: 297, LM: 5.96|BETA: 1.034499\n",
      "089[s], 05[s], Ep: 21, Ct: 00000|TR LOSS: 308, PPL: 999|TM NLL: 298, KL: 3.78 | LM NLL: 5.85, KL: 0.71|DE LOSS: 303, PPL: 956, TM: 297, LM: 5.98|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 21, Ct: 00500|TR LOSS: 308, PPL: 998|TM NLL: 298, KL: 3.79 | LM NLL: 5.84, KL: 0.71|DE LOSS: 303, PPL: 959, TM: 297, LM: 5.97|BETA: 1.034499\n",
      "100[s], 05[s], Ep: 21, Ct: 01000|TR LOSS: 308, PPL: 997|TM NLL: 298, KL: 3.80 | LM NLL: 5.83, KL: 0.71|DE LOSS: 302, PPL: 952, TM: 296, LM: 5.95|BETA: 1.034499\n",
      "089[s], 05[s], Ep: 22, Ct: 00000|TR LOSS: 308, PPL: 996|TM NLL: 298, KL: 3.80 | LM NLL: 5.83, KL: 0.71|DE LOSS: 303, PPL: 956, TM: 297, LM: 5.96|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 22, Ct: 00500|TR LOSS: 308, PPL: 995|TM NLL: 298, KL: 3.81 | LM NLL: 5.82, KL: 0.70|DE LOSS: 303, PPL: 955, TM: 297, LM: 5.94|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 22, Ct: 01000|TR LOSS: 308, PPL: 994|TM NLL: 298, KL: 3.81 | LM NLL: 5.81, KL: 0.70|DE LOSS: 302, PPL: 955, TM: 297, LM: 5.95|BETA: 1.034499\n",
      "090[s], 05[s], Ep: 23, Ct: 00000|TR LOSS: 308, PPL: 993|TM NLL: 298, KL: 3.82 | LM NLL: 5.81, KL: 0.70|DE LOSS: 303, PPL: 956, TM: 297, LM: 5.95|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 23, Ct: 00500|TR LOSS: 308, PPL: 992|TM NLL: 298, KL: 3.82 | LM NLL: 5.80, KL: 0.70|DE LOSS: 302, PPL: 952, TM: 296, LM: 5.94|BETA: 1.034499\n",
      "100[s], 05[s], Ep: 23, Ct: 01000|TR LOSS: 308, PPL: 992|TM NLL: 298, KL: 3.83 | LM NLL: 5.79, KL: 0.69|DE LOSS: 302, PPL: 953, TM: 296, LM: 5.92|BETA: 1.034499\n",
      "089[s], 05[s], Ep: 24, Ct: 00000|TR LOSS: 308, PPL: 991|TM NLL: 298, KL: 3.83 | LM NLL: 5.79, KL: 0.69|DE LOSS: 302, PPL: 954, TM: 297, LM: 5.93|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 24, Ct: 00500|TR LOSS: 308, PPL: 990|TM NLL: 298, KL: 3.84 | LM NLL: 5.78, KL: 0.69|DE LOSS: 302, PPL: 951, TM: 296, LM: 5.94|BETA: 1.034499\n",
      "100[s], 05[s], Ep: 24, Ct: 01000|TR LOSS: 308, PPL: 989|TM NLL: 298, KL: 3.84 | LM NLL: 5.78, KL: 0.69|DE LOSS: 302, PPL: 954, TM: 296, LM: 5.92|BETA: 1.034499\n",
      "089[s], 05[s], Ep: 25, Ct: 00000|TR LOSS: 308, PPL: 988|TM NLL: 298, KL: 3.85 | LM NLL: 5.77, KL: 0.68|DE LOSS: 302, PPL: 953, TM: 297, LM: 5.93|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 25, Ct: 00500|TR LOSS: 308, PPL: 987|TM NLL: 298, KL: 3.85 | LM NLL: 5.76, KL: 0.68|DE LOSS: 302, PPL: 954, TM: 297, LM: 5.92|BETA: 1.034499\n",
      "100[s], 05[s], Ep: 25, Ct: 01000|TR LOSS: 308, PPL: 987|TM NLL: 298, KL: 3.86 | LM NLL: 5.76, KL: 0.68|DE LOSS: 302, PPL: 950, TM: 296, LM: 5.92|BETA: 1.034499\n",
      "089[s], 05[s], Ep: 26, Ct: 00000|TR LOSS: 308, PPL: 986|TM NLL: 297, KL: 3.86 | LM NLL: 5.75, KL: 0.68|DE LOSS: 302, PPL: 949, TM: 296, LM: 5.92|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 26, Ct: 00500|TR LOSS: 308, PPL: 985|TM NLL: 297, KL: 3.86 | LM NLL: 5.75, KL: 0.68|DE LOSS: 302, PPL: 950, TM: 296, LM: 5.92|BETA: 1.034499\n",
      "100[s], 05[s], Ep: 26, Ct: 01000|TR LOSS: 308, PPL: 985|TM NLL: 297, KL: 3.87 | LM NLL: 5.74, KL: 0.67|DE LOSS: 302, PPL: 949, TM: 296, LM: 5.91|BETA: 1.034499\n",
      "089[s], 05[s], Ep: 27, Ct: 00000|TR LOSS: 307, PPL: 984|TM NLL: 297, KL: 3.87 | LM NLL: 5.74, KL: 0.67|DE LOSS: 302, PPL: 948, TM: 296, LM: 5.91|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 27, Ct: 00500|TR LOSS: 307, PPL: 983|TM NLL: 297, KL: 3.88 | LM NLL: 5.73, KL: 0.67|DE LOSS: 302, PPL: 951, TM: 296, LM: 5.90|BETA: 1.034499\n",
      "100[s], 05[s], Ep: 27, Ct: 01000|TR LOSS: 307, PPL: 982|TM NLL: 297, KL: 3.88 | LM NLL: 5.73, KL: 0.67|DE LOSS: 302, PPL: 951, TM: 297, LM: 5.88|BETA: 1.034499\n",
      "089[s], 05[s], Ep: 28, Ct: 00000|TR LOSS: 307, PPL: 982|TM NLL: 297, KL: 3.88 | LM NLL: 5.72, KL: 0.67|DE LOSS: 302, PPL: 949, TM: 296, LM: 5.90|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 28, Ct: 00500|TR LOSS: 307, PPL: 981|TM NLL: 297, KL: 3.89 | LM NLL: 5.71, KL: 0.66|DE LOSS: 302, PPL: 953, TM: 296, LM: 5.89|BETA: 1.034499\n",
      "100[s], 05[s], Ep: 28, Ct: 01000|TR LOSS: 307, PPL: 980|TM NLL: 297, KL: 3.89 | LM NLL: 5.71, KL: 0.66|DE LOSS: 302, PPL: 949, TM: 296, LM: 5.88|BETA: 1.034499\n",
      "089[s], 05[s], Ep: 29, Ct: 00000|TR LOSS: 307, PPL: 980|TM NLL: 297, KL: 3.89 | LM NLL: 5.70, KL: 0.66|DE LOSS: 302, PPL: 947, TM: 296, LM: 5.88|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 29, Ct: 00500|TR LOSS: 307, PPL: 979|TM NLL: 297, KL: 3.90 | LM NLL: 5.70, KL: 0.66|DE LOSS: 302, PPL: 950, TM: 296, LM: 5.88|BETA: 1.034499\n",
      "100[s], 05[s], Ep: 29, Ct: 01000|TR LOSS: 307, PPL: 979|TM NLL: 297, KL: 3.90 | LM NLL: 5.69, KL: 0.66|DE LOSS: 302, PPL: 948, TM: 296, LM: 5.87|BETA: 1.034499\n",
      "089[s], 05[s], Ep: 30, Ct: 00000|TR LOSS: 307, PPL: 978|TM NLL: 297, KL: 3.90 | LM NLL: 5.69, KL: 0.65|DE LOSS: 302, PPL: 948, TM: 296, LM: 5.89|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 30, Ct: 00500|TR LOSS: 307, PPL: 977|TM NLL: 297, KL: 3.91 | LM NLL: 5.68, KL: 0.65|DE LOSS: 302, PPL: 951, TM: 296, LM: 5.88|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 30, Ct: 01000|TR LOSS: 307, PPL: 977|TM NLL: 297, KL: 3.91 | LM NLL: 5.68, KL: 0.65|DE LOSS: 302, PPL: 951, TM: 296, LM: 5.89|BETA: 1.034499\n",
      "089[s], 05[s], Ep: 31, Ct: 00000|TR LOSS: 307, PPL: 976|TM NLL: 297, KL: 3.91 | LM NLL: 5.68, KL: 0.65|DE LOSS: 302, PPL: 948, TM: 296, LM: 5.87|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 31, Ct: 00500|TR LOSS: 307, PPL: 976|TM NLL: 297, KL: 3.92 | LM NLL: 5.67, KL: 0.65|DE LOSS: 302, PPL: 947, TM: 296, LM: 5.86|BETA: 1.034499\n",
      "100[s], 05[s], Ep: 31, Ct: 01000|TR LOSS: 307, PPL: 975|TM NLL: 297, KL: 3.92 | LM NLL: 5.67, KL: 0.65|DE LOSS: 302, PPL: 950, TM: 296, LM: 5.85|BETA: 1.034499\n",
      "089[s], 05[s], Ep: 32, Ct: 00000|TR LOSS: 307, PPL: 975|TM NLL: 297, KL: 3.92 | LM NLL: 5.66, KL: 0.64|DE LOSS: 302, PPL: 954, TM: 296, LM: 5.87|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 32, Ct: 00500|TR LOSS: 307, PPL: 974|TM NLL: 297, KL: 3.93 | LM NLL: 5.66, KL: 0.64|DE LOSS: 302, PPL: 949, TM: 296, LM: 5.86|BETA: 1.034499\n",
      "100[s], 05[s], Ep: 32, Ct: 01000|TR LOSS: 307, PPL: 974|TM NLL: 297, KL: 3.93 | LM NLL: 5.65, KL: 0.64|DE LOSS: 302, PPL: 948, TM: 296, LM: 5.85|BETA: 1.034499\n",
      "089[s], 05[s], Ep: 33, Ct: 00000|TR LOSS: 307, PPL: 973|TM NLL: 297, KL: 3.93 | LM NLL: 5.65, KL: 0.64|DE LOSS: 302, PPL: 950, TM: 296, LM: 5.88|BETA: 1.034499\n",
      "100[s], 05[s], Ep: 33, Ct: 00500|TR LOSS: 307, PPL: 972|TM NLL: 297, KL: 3.93 | LM NLL: 5.64, KL: 0.64|DE LOSS: 302, PPL: 948, TM: 296, LM: 5.86|BETA: 1.034499\n",
      "100[s], 05[s], Ep: 33, Ct: 01000|TR LOSS: 307, PPL: 972|TM NLL: 297, KL: 3.94 | LM NLL: 5.64, KL: 0.64|DE LOSS: 302, PPL: 948, TM: 296, LM: 5.85|BETA: 1.034499\n",
      "089[s], 05[s], Ep: 34, Ct: 00000|TR LOSS: 307, PPL: 971|TM NLL: 297, KL: 3.94 | LM NLL: 5.64, KL: 0.64|DE LOSS: 302, PPL: 951, TM: 296, LM: 5.87|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 34, Ct: 00500|TR LOSS: 307, PPL: 971|TM NLL: 297, KL: 3.94 | LM NLL: 5.63, KL: 0.63|DE LOSS: 302, PPL: 948, TM: 296, LM: 5.86|BETA: 1.034499\n",
      "100[s], 05[s], Ep: 34, Ct: 01000|TR LOSS: 307, PPL: 971|TM NLL: 297, KL: 3.95 | LM NLL: 5.63, KL: 0.63|DE LOSS: 302, PPL: 948, TM: 296, LM: 5.85|BETA: 1.034499\n",
      "089[s], 05[s], Ep: 35, Ct: 00000|TR LOSS: 307, PPL: 970|TM NLL: 297, KL: 3.95 | LM NLL: 5.62, KL: 0.63|DE LOSS: 302, PPL: 946, TM: 296, LM: 5.84|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 35, Ct: 00500|TR LOSS: 307, PPL: 970|TM NLL: 297, KL: 3.95 | LM NLL: 5.62, KL: 0.63|DE LOSS: 302, PPL: 949, TM: 296, LM: 5.84|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 35, Ct: 01000|TR LOSS: 307, PPL: 969|TM NLL: 297, KL: 3.95 | LM NLL: 5.61, KL: 0.63|DE LOSS: 302, PPL: 947, TM: 296, LM: 5.84|BETA: 1.034499\n",
      "089[s], 05[s], Ep: 36, Ct: 00000|TR LOSS: 307, PPL: 969|TM NLL: 297, KL: 3.96 | LM NLL: 5.61, KL: 0.63|DE LOSS: 302, PPL: 944, TM: 296, LM: 5.85|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 36, Ct: 00500|TR LOSS: 307, PPL: 968|TM NLL: 297, KL: 3.96 | LM NLL: 5.61, KL: 0.63|DE LOSS: 302, PPL: 948, TM: 296, LM: 5.85|BETA: 1.034499\n",
      "100[s], 05[s], Ep: 36, Ct: 01000|TR LOSS: 307, PPL: 968|TM NLL: 297, KL: 3.96 | LM NLL: 5.60, KL: 0.62|DE LOSS: 302, PPL: 948, TM: 296, LM: 5.85|BETA: 1.034499\n",
      "089[s], 05[s], Ep: 37, Ct: 00000|TR LOSS: 307, PPL: 967|TM NLL: 297, KL: 3.96 | LM NLL: 5.60, KL: 0.62|DE LOSS: 302, PPL: 950, TM: 296, LM: 5.85|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 37, Ct: 00500|TR LOSS: 307, PPL: 967|TM NLL: 297, KL: 3.97 | LM NLL: 5.59, KL: 0.62|DE LOSS: 302, PPL: 948, TM: 296, LM: 5.84|BETA: 1.034499\n",
      "100[s], 05[s], Ep: 37, Ct: 01000|TR LOSS: 307, PPL: 967|TM NLL: 297, KL: 3.97 | LM NLL: 5.59, KL: 0.62|DE LOSS: 302, PPL: 947, TM: 296, LM: 5.83|BETA: 1.034499\n",
      "089[s], 05[s], Ep: 38, Ct: 00000|TR LOSS: 307, PPL: 966|TM NLL: 297, KL: 3.97 | LM NLL: 5.59, KL: 0.62|DE LOSS: 302, PPL: 944, TM: 296, LM: 5.83|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 38, Ct: 00500|TR LOSS: 307, PPL: 966|TM NLL: 297, KL: 3.97 | LM NLL: 5.58, KL: 0.62|DE LOSS: 302, PPL: 948, TM: 296, LM: 5.83|BETA: 1.034499\n",
      "100[s], 05[s], Ep: 38, Ct: 01000|TR LOSS: 307, PPL: 965|TM NLL: 297, KL: 3.97 | LM NLL: 5.58, KL: 0.62|DE LOSS: 302, PPL: 948, TM: 296, LM: 5.83|BETA: 1.034499\n",
      "089[s], 05[s], Ep: 39, Ct: 00000|TR LOSS: 307, PPL: 965|TM NLL: 297, KL: 3.98 | LM NLL: 5.58, KL: 0.61|DE LOSS: 302, PPL: 947, TM: 296, LM: 5.85|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 39, Ct: 00500|TR LOSS: 307, PPL: 965|TM NLL: 297, KL: 3.98 | LM NLL: 5.57, KL: 0.61|DE LOSS: 302, PPL: 949, TM: 296, LM: 5.82|BETA: 1.034499\n",
      "100[s], 05[s], Ep: 39, Ct: 01000|TR LOSS: 307, PPL: 964|TM NLL: 297, KL: 3.98 | LM NLL: 5.57, KL: 0.61|DE LOSS: 302, PPL: 943, TM: 296, LM: 5.81|BETA: 1.034499\n",
      "089[s], 05[s], Ep: 40, Ct: 00000|TR LOSS: 307, PPL: 964|TM NLL: 297, KL: 3.98 | LM NLL: 5.57, KL: 0.61|DE LOSS: 302, PPL: 945, TM: 296, LM: 5.81|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 40, Ct: 00500|TR LOSS: 307, PPL: 963|TM NLL: 297, KL: 3.99 | LM NLL: 5.56, KL: 0.61|DE LOSS: 302, PPL: 946, TM: 296, LM: 5.82|BETA: 1.034499\n",
      "100[s], 05[s], Ep: 40, Ct: 01000|TR LOSS: 307, PPL: 963|TM NLL: 297, KL: 3.99 | LM NLL: 5.56, KL: 0.61|DE LOSS: 302, PPL: 945, TM: 296, LM: 5.82|BETA: 1.034499\n",
      "089[s], 05[s], Ep: 41, Ct: 00000|TR LOSS: 307, PPL: 963|TM NLL: 296, KL: 3.99 | LM NLL: 5.55, KL: 0.61|DE LOSS: 302, PPL: 948, TM: 296, LM: 5.83|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 41, Ct: 00500|TR LOSS: 306, PPL: 962|TM NLL: 296, KL: 3.99 | LM NLL: 5.55, KL: 0.61|DE LOSS: 302, PPL: 949, TM: 296, LM: 5.84|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 41, Ct: 01000|TR LOSS: 306, PPL: 962|TM NLL: 296, KL: 3.99 | LM NLL: 5.55, KL: 0.60|DE LOSS: 302, PPL: 945, TM: 296, LM: 5.82|BETA: 1.034499\n",
      "089[s], 05[s], Ep: 42, Ct: 00000|TR LOSS: 306, PPL: 962|TM NLL: 296, KL: 4.00 | LM NLL: 5.54, KL: 0.60|DE LOSS: 302, PPL: 946, TM: 296, LM: 5.83|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 42, Ct: 00500|TR LOSS: 306, PPL: 961|TM NLL: 296, KL: 4.00 | LM NLL: 5.54, KL: 0.60|DE LOSS: 302, PPL: 945, TM: 296, LM: 5.83|BETA: 1.034499\n",
      "100[s], 05[s], Ep: 42, Ct: 01000|TR LOSS: 306, PPL: 961|TM NLL: 296, KL: 4.00 | LM NLL: 5.54, KL: 0.60|DE LOSS: 302, PPL: 944, TM: 296, LM: 5.82|BETA: 1.034499\n",
      "089[s], 05[s], Ep: 43, Ct: 00000|TR LOSS: 306, PPL: 961|TM NLL: 296, KL: 4.00 | LM NLL: 5.53, KL: 0.60|DE LOSS: 302, PPL: 945, TM: 296, LM: 5.83|BETA: 1.034499\n",
      "101[s], 05[s], Ep: 43, Ct: 00500|TR LOSS: 306, PPL: 960|TM NLL: 296, KL: 4.00 | LM NLL: 5.53, KL: 0.60|DE LOSS: 302, PPL: 947, TM: 296, LM: 5.82|BETA: 1.034499\n",
      "100[s], 05[s], Ep: 43, Ct: 01000|TR LOSS: 306, PPL: 960|TM NLL: 296, KL: 4.01 | LM NLL: 5.53, KL: 0.60|DE LOSS: 302, PPL: 946, TM: 296, LM: 5.84|BETA: 1.034499\n",
      "089[s], 05[s], Ep: 44, Ct: 00000|TR LOSS: 306, PPL: 960|TM NLL: 296, KL: 4.01 | LM NLL: 5.52, KL: 0.60|DE LOSS: 302, PPL: 947, TM: 296, LM: 5.82|BETA: 1.034499\n",
      "100[s], 05[s], Ep: 44, Ct: 00500|TR LOSS: 306, PPL: 959|TM NLL: 296, KL: 4.01 | LM NLL: 5.52, KL: 0.60|DE LOSS: 302, PPL: 947, TM: 296, LM: 5.82|BETA: 1.034499\n",
      "100[s], 05[s], Ep: 44, Ct: 01000|TR LOSS: 306, PPL: 959|TM NLL: 296, KL: 4.01 | LM NLL: 5.52, KL: 0.59|DE LOSS: 302, PPL: 945, TM: 296, LM: 5.81|BETA: 1.034499\n",
      "089[s], 05[s], Ep: 45, Ct: 00000|TR LOSS: 306, PPL: 959|TM NLL: 296, KL: 4.01 | LM NLL: 5.52, KL: 0.59|DE LOSS: 302, PPL: 944, TM: 296, LM: 5.81|BETA: 1.034499\n",
      "100[s], 05[s], Ep: 45, Ct: 00500|TR LOSS: 306, PPL: 958|TM NLL: 296, KL: 4.01 | LM NLL: 5.51, KL: 0.59|DE LOSS: 302, PPL: 945, TM: 296, LM: 5.83|BETA: 1.034499\n",
      "100[s], 05[s], Ep: 45, Ct: 01000|TR LOSS: 306, PPL: 958|TM NLL: 296, KL: 4.02 | LM NLL: 5.51, KL: 0.59|DE LOSS: 302, PPL: 943, TM: 296, LM: 5.83|BETA: 1.034499\n",
      "True: two climbers have died in an avalanche on mount frances\n",
      "Pred: the say was the in the accident at the <unk>\n",
      "True: park rangers at denali national park confirmed wednesday that the two climbers\n",
      "Pred: the 's say <unk> <unk> airport in the that the city year were in and and and and and and and and and and and and and and and and\n",
      "True: one from canada and the other from japan\n",
      "Pred: the of the 's the department states the 's is are in ,\n",
      "True: died in the avalanche on the # foot peak\n",
      "Pred: the says the state , the city s , of was , is , and and and and and <unk> and and and and and <unk> and and and\n",
      "True: the dead were identified as # year old <unk> <unk> of <unk> , canada , and # year old <unk> <unk> of <unk> , <unk> , japan\n",
      "Pred: the state , taken\n",
      "True: the climbers were attempting a new route on the west face of mount frances when they were killed in the avalanche\n",
      "Pred: the state was not to # york and the # , and the\n",
      "True: a search was launched when they did not return to base camp on monday\n",
      "Pred: the new for found at the was was been to the\n",
      "True: rangers aboard a helicopter spotted a body lying in avalanche debris at the base of the mountain\n",
      "Pred: the say the # in the <unk> of a #\n",
      "True: both bodies were recovered wednesday\n",
      "Pred: the says of found\n",
      "True: park officials say records indicate these are the first two fatalities on mount frances\n",
      "Pred: the officials say the are the <unk> being first time weeks , the <unk>\n",
      "True: kodak says it will sell its document imaging and personalized imaging businesses to better focus on printing and business services as it works to emerge from bankruptcy protection\n",
      "Pred: the says the 's be a to in and <unk> <unk> <unk>\n",
      "True: eastman kodak co. said thursday that the sale of the units , along with cost cutting measures and the auction of its patent portfolio , will help it emerge from bankruptcy sometime in #\n",
      "Pred: the says is says thursday that the company of the company is the the # and in of <unk> company of\n",
      "True: kodak 's document imaging division makes scanners and offers related software and services\n",
      "Pred: the says # is <unk> of the to the the to and <unk>\n",
      "True: the personalized imaging business includes photo paper and still camera film products\n",
      "Pred: the says the the the the of and other\n",
      "True: it also offers <unk> photo products at theme parks and other venues\n",
      "Pred: the was is the to of and the and the and products ,\n",
      "True: the <unk> photography pioneer filed for bankruptcy protection in january\n",
      "Pred: the company says reports reports wednesday the ,\n",
      "True: it has kept operating while it tries to sell its digital imaging patents\n",
      "Pred: the was been a in and 's to be the <unk> <unk>\n",
      "True: so far , it has not found buyers\n",
      "Pred: the far , the 's been been the of\n",
      "True: four amish children are seriously hurt after a collision between a buggy and a sport utility vehicle in south central wisconsin\n",
      "Pred: the was <unk> says scheduled to in the # that interstate # and a # vehicle vehicle\n",
      "True: columbia county sheriff 's authorities say the horse and buggy was leaving a country store when the crash happened thursday morning in the town of <unk>\n",
      "Pred: the 's sheriff 's office say a crash was a and called the # in in he fire was at morning\n",
      "True: the four children were thrown from the horse and buggy\n",
      "Pred: the # year were taken in the and and <unk>\n",
      "True: two of the children were flown by <unk> to uw hospital in madison with life threatening injuries\n",
      "Pred: the says the # , not to a to the hospital\n",
      "True: the other two children were taken to divine <unk> hospital in portage with serious injuries\n",
      "Pred: the city of men were been in the hospital\n",
      "True: the suv driver was treated and released\n",
      "Pred: the state was was not and a ,\n",
      "True: names of the injured were not released\n",
      "Pred: the of the victims , not immediately released\n",
      "True: the investigation continues but the sheriff 's office says drugs or alcohol do not appear to be factors\n",
      "Pred: the says was to the investigation 's office\n",
      "True: april showers bring may flowers\n",
      "Pred: the # , the #\n",
      "True: but in a central florida cemetery , october rains nearly raised the dead\n",
      "Pred: the says the news pennsylvania city , the # , $ miles # 's in\n",
      "True: the vero beach news journal ( http : <unk> ) reports concrete vaults were unearthed this week in gifford cemetery after a tropical system dumped heavy rain across florida during the final weekend in october\n",
      "Pred: the company daily of the reports ( : //bit.ly/ ) reports the was is found in week , the and , <unk> # of\n",
      "True: cemetery spokeswoman <unk> hayes says october 's record setting rain loosened the soil , allowing the vaults to float to the surface\n",
      "Pred: the says <unk> <unk> says the # statement that the are the <unk> , but the to to the the the area\n",
      "True: she says no <unk> were visible\n",
      "Pred: the says the longer have not\n",
      "True: hayes says crews will have to wait until the soil <unk> to assess the damage and bury the vaults again\n",
      "Pred: the says the was be a be for the next\n",
      "True: officials say nearby vero beach got an estimated # inches of rain in october , making it the second wettest month on record\n",
      "Pred: the say the # was the out the by # of the\n",
      "True: residents told the newspaper a similar situation occurred during the # hurricane season\n",
      "Pred: the are the fire that <unk> call that in the end year season\n",
      "True: virginia beach is the final stop for a national academy of sciences report that examined the prospect of uranium mining in virginia\n",
      "Pred: the 's police a first # in the new guard of the and\n",
      "True: several members of the <unk> panel that delivered the report , titled `` uranium mining in virginia , `` will be in attendance thursday in the resort city to brief the public on their findings\n",
      "Pred: the people of the state of of the a bill of the\n",
      "True: the report examined various aspects of mining and processing uranium in virginia and concluded many hurdles must be overcome before the state ends a # year ban on mining the radioactive ore\n",
      "Pred: the company is the the of the and gas products\n",
      "True: virginia uranium inc. has proposed mining a deposit in pittsylvania county that is among the largest in the world\n",
      "Pred: the says the is been a to to of the county\n",
      "True: the company has faced fierce opposition from some southside residents and environmentalists\n",
      "Pred: the city of been a to to the of the to the\n",
      "True: they say mining is not worth the environmental risk\n",
      "Pred: the say the , expected to of # # of ,\n",
      "True: the company disagrees\n",
      "Pred: the city said it <unk> <unk> , , , and and and and and and and\n",
      "True: a man vying for the university of alaska 's top leadership role says he is expecting academic programs to become stronger but fewer in the next decade\n",
      "Pred: the judge has of a death of the 's <unk> school is is the has a a to to help the and the\n",
      "True: the juneau empire reports ( http : //bit.ly/ # <unk> # ) johnson is the university 's first choice to replace retiring president pat gamble in one of the state 's highest paying government jobs\n",
      "Pred: the says the the ( http : <unk> # <unk> ) ) ) that charged first of largest degree of the the\n",
      "True: state data shows gamble made $ # in #\n",
      "Pred: the officials say <unk> says the # million restitution\n",
      "True: johnson started his tour of the state in juneau and spoke with a small group of lawmakers , leaders and community members thursday night\n",
      "Pred: the says the the of the # 's # , the to the <unk> in to the and which to the and of\n",
      "True: he said consolidating programs among the three campuses will strengthen what the university offers and save money by avoiding <unk>\n",
      "Pred: the was the the will the state year , be the they state is will the the to the the\n",
      "True: johnson is among four finalists out of # applicants for the job\n",
      "Pred: the says the the of to of the to\n",
      "True: the board of regents is expected to make a decision by the week of july #\n",
      "Pred: the company of the the a to be the new to the state\n",
      "True: a cranston parish is selling its buildings to a local <unk> catholic community that was displaced by a fire in #\n",
      "Pred: the # year old has a <unk> for help new <unk> , the\n",
      "True: the roman catholic diocese of providence announced sunday that it has approved st. ann parish 's request to sell its church and rectory to the <unk> community\n",
      "Pred: the says <unk> <unk> is the is wednesday that the will a a louis <unk> , office for the the of\n",
      "True: the community is catholic and under the authority of the pope\n",
      "Pred: the news of a <unk> the the state\n",
      "True: the diocese says <unk> had <unk> at pawtucket 's st. george church until a fire destroyed the church and its adjacent hall in october , #\n",
      "Pred: the state is the 's been # to and and louis 's\n",
      "True: the community has since <unk> at st. raymond church in providence , but has sought a permanent location\n",
      "Pred: the state of says the the the <unk> <unk> in <unk> , the the been to lawsuit lawsuit to for\n",
      "True: st. ann is selling the buildings to the <unk> for $ #\n",
      "Pred: the say county of the # will help the and the # million\n",
      "True: the diocese says st. ann parishioners will still be able to use the church for sunday worship and special occasions\n",
      "Pred: the # of the 's <unk> of the <unk> , <unk> the the <unk> 's the <unk>\n",
      "True: former atlanta fire chief kelvin cochran has filed a federal complaint against the city claiming he was discriminated against because of his religion\n",
      "Pred: the <unk> county chief mike <unk> says been a lawsuit of the the city of the was arrested to the he the <unk>\n",
      "True: mayor kasim reed recently fired cochran over comments in his self published book `` who told you that you were naked .\n",
      "Pred: the michael <unk> says said the and the in the case <unk> ,\n",
      "True: the book <unk> homosexuality as `` <unk> `` and `` a sexual <unk> .\n",
      "Pred: the state is the , part of , the <unk> <unk> `` `` .\n",
      "True: city spokeswoman anne torres says the city plans to defend the mayor 's decision\n",
      "Pred: the officials say <unk> says the state of to be the 's\n",
      "True: reed has said he did n't dismiss the fire chief for his religious views , but for not <unk> the rules , including not getting clearance to write the book\n",
      "Pred: the says been the was to know the lawsuit\n",
      "True: cochran , who 's a deacon at elizabeth baptist church , says he got permission from the city 's ethics office , but was later told he also needed the mayor 's approval\n",
      "Pred: the says who , , , , , <unk> , , , , was the the the to 's #\n",
      "True: cochran filed his complaint with the equal employment opportunity commission\n",
      "Pred: the says a lawsuit wednesday a associated <unk> committee\n",
      "True: a vermont labor union is complaining that a large wind power project in sheffield is <unk> local <unk> and bringing in out of state crews , undermining the project 's hoped for economic benefits\n",
      "Pred: the # man has says has the the is the energy is the the and expected to\n",
      "True: michael <unk> , business agent for <unk> local # , says his union members are being bypassed in favor of a utah crew being brought in by the general contractor\n",
      "Pred: the <unk> , who <unk> , the , media , , the client is is a to to the of the <unk> 's who used\n",
      "True: <unk> , the madison , wis. based general contractor , has n't responded to requests for comment\n",
      "Pred: the says the state county says , the , , the been a to the for the\n",
      "True: but john lamontagne , spokesman for project developer first wind , says <unk> has been hiring some vermont subcontractors\n",
      "Pred: the the <unk> , the , <unk> , the , , says the will will <unk> of the 's and\n",
      "True: first wind estimated it would hire # workers during construction\n",
      "Pred: the degree guard is is the by million to the\n",
      "True: vermont 's deputy commissioner of labor says her department 's newport office has placed two workers on the project , including a security guard\n",
      "Pred: the 's police say mike <unk> says the 's of administration rate has been a <unk> to the the to but the # , and\n",
      "True: lamontagne says the # turbine , $ # million project is to be finished by year 's end\n",
      "Pred: the say the man year <unk> # # million in is expected be in by #\n",
      "True: ( ap )\n",
      "Pred: the ap )\n",
      "True: authorities have identified a body found in the west fork river as that of a woman who was swept into the water while fishing\n",
      "Pred: the say identified the man of in a # of of near a <unk> the #\n",
      "True: police tell media outlets that the state medical examiner 's office ruled sheila <unk> 's death as an accidental drowning\n",
      "Pred: the 's the outlets that that man 's examiner 's office says the the the death was a investigation\n",
      "True: police say the # year old <unk> resident was fishing with a friend on aug. # when she <unk> into the water above falls in marion county\n",
      "Pred: the says the # year old <unk> was was killed to a hospital and a ,\n",
      "True: she was then swept into the river\n",
      "Pred: the says arrested held in the head\n",
      "True: search crews found <unk> 's body <unk> from the worthington dam on saturday\n",
      "Pred: the of are the # body in the # fire of\n",
      "True: a collision near beaverton , ore. , involving a minivan and an ambulance has left the van driver hospitalized in critical condition\n",
      "Pred: the # in the 's says , says , , was not suv and been released scene of of the the condition\n",
      "True: however , the ambulance paramedics and their # year old patient fared much better\n",
      "Pred: the says the , , , other friends were old <unk> were <unk> of of\n",
      "True: washington county sheriff 's sgt . david thompson says a # year old beaverton woman was reported in critical condition thursday night at a portland area hospital\n",
      "Pred: the county of 's deputies . <unk> <unk> says the man year old man was the was in the condition\n",
      "True: he says two <unk> fire department firefighter paramedics were treated at a hospital and released after the thursday afternoon crash\n",
      "Pred: the was the people of and were the were reported\n",
      "True: thompson says # year old mary <unk> was being transported in the ambulance for a broken hip\n",
      "Pred: the said the year old <unk> <unk> was arrested held to a hospital\n",
      "True: she did not suffer any major injuries from the crash\n",
      "Pred: the say not say the details details\n",
      "True: police in durham are continuing to investigate the discovery of a woman 's body in a local lake\n",
      "Pred: the say the county investigating to determine a shooting of a # who body\n",
      "True: a person passing by <unk> lake called police on friday to report what turned out to be human remains\n",
      "Pred: the judge who the the man police the officer , the the the he was of the a\n",
      "True: police have not yet identified the body publicly , saying only that it is a white woman\n",
      "Pred: the was been yet yet the man of\n",
      "True: margaret lewis , who lived in the neighborhood , disappeared on dec. # , and her car was found days later near the lake\n",
      "Pred: the <unk> , who , , the # , was , saturday #\n",
      "True: police have n't said if the body found friday is that of the missing woman\n",
      "Pred: the say said released the the shooting was the\n",
      "True: police are asking anyone with information about lewis to call them\n",
      "Pred: the say the the to a to the\n",
      "True: ( ap )\n",
      "Pred: the ap )\n",
      "True: west virginians are paying an average # cents more for a gallon of gasoline than they did a week ago\n",
      "Pred: the virginia are expected a increase of million per than $ year of $\n",
      "True: according to aaa 's fuel gauge , the average price of self serve regular unleaded gasoline in west virginia is about $ #\n",
      "Pred: the to the tv , , , the company of of # <unk> unleaded , , prices the of , expected # percent million\n",
      "True: the national average price is $ # per gallon\n",
      "Pred: the state weather of of expected # cents gallon ,\n",
      "True: figures released tuesday by aaa east central show bridgeport had the highest price in the state at about $ # per gallon\n",
      "Pred: the say the that the # of the , says # highest average of the past\n",
      "True: the lowest prices , about $ # , were in morgantown and weirton\n",
      "Pred: the say estimate are the # percent percent or # # , #\n",
      "True: the president of an organization that represents north africa 's indigenous people says the algerian government 's treatment of the <unk> is the worst in the region\n",
      "Pred: the city of the 's of 's the carolina 's # <unk> to the city is has <unk> is the is has the <unk> of the nation\n",
      "True: <unk> <unk> , leader of the world <unk> congress , told the associated press on monday that <unk> were in a `` dangerous `` relationship of conflict with the algerian government\n",
      "Pred: the says , the of the , 's , , says the the press that wednesday that the will will the statement <unk> `` `` `` the\n",
      "True: asked if algerian treatment of <unk> was the worst in north africa , he said `` yes .\n",
      "Pred: the the the <unk> was the , stolen first of the ,\n",
      "True: <unk> , or <unk> , have their own language and culture and are found all over north africa\n",
      "Pred: the says the <unk> ,\n",
      "True: there are been regular riots between the <unk> and arab populations in the algerian desert town of <unk> , with eight deaths over the past months\n",
      "Pred: the were no # <unk> in the and and <unk> <unk> , the city area ,\n",
      "True: the algerian prime minister prompted demonstrations last week by making a derogatory comment about a mountain <unk> community\n",
      "Pred: the says <unk> minister <unk> <unk> to the to the state of <unk>\n",
      "True: state officials have ordered the closure of a central indiana church 's day care center after an inspection found a deacon accused of inappropriately touching a child last year near the center 's children\n",
      "Pred: the 's the been a state of the # virginia city\n",
      "True: indiana family and social services administration spokeswoman <unk> lemons says the little angels daycare and preschool must close by wednesday\n",
      "Pred: the officials members the services are are <unk> <unk> says the # <unk> will was will the the the the <unk>\n",
      "True: she tells the indianapolis star ( http : <unk> ) the agency revoked the center 's registration after a consultant working for the state found the accused deacon on the premises jan. #\n",
      "Pred: the says the <unk> 's ( the : //bit.ly/ # <unk> ) that the <unk> of <unk> and the <unk> of in the city\n",
      "True: lemons says the deacon has not been arrested or charged , but the state agency has `` substantiated a claim of sexual abuse .\n",
      "Pred: the says the is is been been released\n",
      "True: the agency had ordered white river baptist church in greenwood just south of indianapolis to keep the deacon off the property or lose its day care license\n",
      "Pred: the city says the a to to <unk> in the\n",
      "True: death penalty opponents and the attorney for a man convicted of a # murder for hire plot are appealing to missouri gov . jay nixon to stop what would be the first execution in the state in nearly two years\n",
      "Pred: the of the are the state are the new who in a man year in a a\n",
      "True: richard clay is scheduled to die on jan. #\n",
      "Pred: the say county a to begin in the #\n",
      "True: he was convicted of murder in the new madrid ( mah ' <unk> ) shooting death of randy <unk>\n",
      "Pred: the says sentenced in first and a death york death # ' <unk> )\n",
      "True: a judge threw out clay 's # conviction six years later and ordered a new trial , but an appeals court reinstated the conviction and death sentence in #\n",
      "Pred: the # has a to county trial year of years of , a a # sentence of the # appeals to\n",
      "True: clay 's case was one of seven in which appellate judges raised questions about the courtroom conduct of former statewide prosecutor kenny <unk> , who later served six terms in congress\n",
      "Pred: the county , in n't of the and the # court in the\n",
      "True: a group of investors have opened a $ # million plant in central puerto rico to produce aluminum cans\n",
      "Pred: the say of the are been a $ # million grant in the indiana ,\n",
      "True: caribbean can manufacturing company llc says it is capable of producing up to # cans per minute , and expects to expand operations to produce up to # cans per minute\n",
      "Pred: the says be <unk> to , the 's the of the <unk>\n",
      "True: company officials said friday that they will initially export cans to the dominican republic , trinidad and jamaica , and plan to expand to the u.s. and latin america\n",
      "Pred: the said say the that the <unk> be used in and the company <unk>\n",
      "True: the company said it currently produces cans for beers and soft drinks\n",
      "Pred: the 's said the will <unk> a to the and other <unk>\n",
      "True: the plant has created a total of # jobs\n",
      "Pred: the says is been the # of its percent in and in\n",
      "True: gov . dennis daugaard has canceled a special session of the south dakota legislature that was to deal with cost overruns in the construction of a new state veterans home in hot springs\n",
      "Pred: the says <unk> <unk> says signed the 's of on the state of state and would the the with the state\n",
      "True: in a letter to lawmakers , the governor says the special session is no longer necessary because the federal veterans administration has extended a grant for the project , giving the state until february to finalize construction plans\n",
      "Pred: the the , , the the the city is the 's will and the to would to of state government to\n",
      "True: he says that gives the state time to review project plans , seek a scaled back design and <unk> the project\n",
      "Pred: the says the 's the money 's to make the , to the will new to to of will the state of\n",
      "True: daugaard initially said the special session set for june # was necessary to avoid losing a $ # million grant from the veterans administration\n",
      "Pred: the says said that company was is the the #\n",
      "True: bids for the project came in much higher than projected\n",
      "Pred: the are the first will in the of than # percent\n",
      "True: authorities in los angeles are reminding people that it is a bad idea , as well as illegal , to shoot guns into the sky to celebrate the arrival of the new year\n",
      "Pred: the say western say are investigating a who they was a second of of the a as a as and the the and the area\n",
      "True: los angeles police and the sheriff 's department said in an advisory tuesday that significant numbers of people are still shooting weapons into the air , despite a campaign to stop the practice\n",
      "Pred: the says , say the state of the of the the investigation that that the was are the and were in in\n",
      "True: in # , # year old brian perez died while playing in the front yard of his home after someone in the area shot a weapon into the sky\n",
      "Pred: the the , the percent old <unk> <unk> was of being at the <unk> of of the <unk>\n",
      "True: anyone caught shooting in celebration could have their weapon confiscated and face a felony charge\n",
      "Pred: the say the of the\n",
      "True: in compton , sheriff 's deputies will deploy technology that can pinpoint gunfire to within a few feet of where the shots were fired\n",
      "Pred: the 's is # , <unk> , , , , , be the , the the home\n",
      "True: the czech government says south korean tire maker <unk> tire will invest $ # billion in a new plant in the country\n",
      "Pred: the company is ( the will <unk> is the the and will to # million in the\n",
      "True: foreign minister <unk> <unk> said monday the new factory to be built near the northwestern town of <unk> should create more than # jobs\n",
      "Pred: the president minister <unk> said the that bill york will be a by the end <unk> of\n",
      "True: the plant is set to open in two years\n",
      "Pred: the says is expected to # in the years\n",
      "True: the company supplies tires to major car plants in the czech republic and slovakia , including volkswagen 's <unk> auto and south korea 's kia and hyundai\n",
      "Pred: the u.s. is the is the the and\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-2b48ff361a65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_recon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_kl_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_recon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_kl_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppls_batch\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_recon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_kl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_recon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_kl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_ppls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mlosses_train\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_recon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_kl_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_recon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_kl_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "for epoch in range(config.epochs):\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = get_feed_dict(batch)\n",
    "        if config.warmup > 0 and beta_eval < 1.0: sess.run(update_beta)\n",
    "\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch, ppls_batch = \\\n",
    "        sess.run([opt, loss, topic_loss_recon, topic_loss_kl, sent_loss_recon, sent_loss_kl, topic_ppls], feed_dict = feed_dict)\n",
    "\n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if ct%config.log_period==0:\n",
    "            time_dev = time.time()\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, sent_loss_recon_train, sent_loss_kl_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_dev, sent_loss_dev, ppl_dev = get_loss(sess, dev_batches)\n",
    "\n",
    "            if config.warmup > 0: beta_eval = beta.eval(session=sess)\n",
    "\n",
    "            clear_output()\n",
    "            time_finish = time.time()\n",
    "            time_log = int(time_finish - time_start)\n",
    "            time_log_dev = int(time_finish - time_dev)\n",
    "            logs += [(time_log, time_log_dev, epoch, ct, loss_train, ppl_train, topic_loss_recon_train, topic_loss_kl_train, sent_loss_recon_train, sent_loss_kl_train, loss_dev, ppl_dev, topic_loss_dev, sent_loss_dev, beta_eval)]\n",
    "            for log in logs:\n",
    "                print('%03d[s], %02d[s], Ep: %02d, Ct: %05d|TR LOSS: %.0f, PPL: %.0f|TM NLL: %.0f, KL: %.2f | LM NLL: %.2f, KL: %.2f|DE LOSS: %.0f, PPL: %.0f, TM: %.0f, LM: %.2f|BETA: %.6f' %  log)\n",
    "\n",
    "            print_sample(batch)\n",
    "\n",
    "            time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : republican senate democratic house election party campaign governor republicans gov\n",
      "1 : court judge federal u.s. attorney lawsuit filed case district law\n",
      "2 : million company http water project reports federal department u.s. plant\n",
      "3 : vehicle car crash driver truck died driving killed hit struck\n",
      "4 : department http people reports health office law officers week security\n",
      "5 : percent million company cents share revenue billion rate shares average\n",
      "6 : national saturday event museum died day years u.s. center president\n",
      "7 : bill tax budget million health lawmakers house measure gov approved\n",
      "8 : school university students board education program president college schools district\n",
      "9 : fire firefighters area reported authorities home people sunday miles blaze\n",
      "10 : found home officers authorities shot woman sheriff shooting arrested body\n",
      "11 : charged guilty prosecutors charges court years prison pleaded attorney murder\n",
      "12 : service weather power national storm snow tuesday expected rain customers\n"
     ]
    }
   ],
   "source": [
    "# visualize topic\n",
    "topics_freq_bow_idxs = bow_idxs[sess.run(topics_freq_bow_indices)]\n",
    "for topic, topic_freq_bow_idxs in enumerate(topics_freq_bow_idxs):\n",
    "    print(topic, ':', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "_logvars, _means, _kl_losses, _latents, _output_logits = sess.run([logvars, means, kl_losses, latents, output_logits], feed_dict=feed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 32), (32, 32), (32,), (32, 32))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_logvars.shape, _means.shape, _kl_losses.shape, _latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_output_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dec_target_idxs_do' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-7de59bc2cc54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_output_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dec_target_idxs_do\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dec_mask_tokens_do\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_recon_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_kl_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_target_idxs_do\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_mask_tokens_do\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dec_target_idxs_do' is not defined"
     ]
    }
   ],
   "source": [
    "_output_logits, _dec_target_idxs_do, _dec_mask_tokens_do, _recon_loss, _kl_losses, _ = sess.run([output_logits, dec_target_idxs_do, dec_mask_tokens_do, recon_loss, kl_losses, opt], feed_dict=feed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 46)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_max(output_logits, 2).eval(session=sess, feed_dict=feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 46, 20000), (120, 46), (120, 46))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_output_logits.shape, _dec_target_idxs_do.shape, _dec_mask_tokens_do.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "_logits = np.exp(_output_logits) / np.sum(np.exp(_output_logits), 2)[:, :, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "_idxs = _dec_target_idxs_do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "_losses = np.array([[-np.log(_logits[i, j, _idxs[i, j]]) for j in range(_idxs.shape[1])] for i in range(_idxs.shape[0])]) * _dec_mask_tokens_do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.903732"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(_losses)/np.sum(_dec_mask_tokens_do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.903732"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_recon_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_kl_losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
