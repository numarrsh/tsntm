{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from data_structure import get_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "from topic_model import TopicModel\n",
    "\n",
    "from topic_beam_search_decoder import BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '5', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/apnews/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/nvdm_vae_tmp', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'apnews', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 50, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 500, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.1, 'lr')\n",
    "flags.DEFINE_float('reg', 1., 'regularization term')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_bool('warmup', True, 'flg of warming up')\n",
    "flags.DEFINE_integer('epochs_cycle', 5, 'number of epochs within a cycle')\n",
    "flags.DEFINE_float('r_cycle', 0.5, 'proportion used to increase beta within a cycle')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 10, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_bow', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_hidden_topic', 512, 'dim_hidden_topic')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "flags.DEFINE_bool('bidirectional', True, 'flg of bidirectional encoding')\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('PAD_IDX', word_to_idx[PAD], 'PAD_IDX')\n",
    "flags.DEFINE_integer('UNK_IDX', word_to_idx[UNK], 'UNK_IDX')\n",
    "flags.DEFINE_integer('BOS_IDX', word_to_idx[BOS], 'BOS_IDX')\n",
    "flags.DEFINE_integer('EOS_IDX', word_to_idx[EOS], 'EOS_IDX')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "maximum_iterations = max([max([instance.max_sent_l for instance in batch]) for ct, batch in dev_batches])\n",
    "flags.DEFINE_integer('maximum_iterations', maximum_iterations, 'maximum_iterations')\n",
    "\n",
    "flags.DEFINE_integer('cycle_steps', len(train_batches)*config.epochs_cycle, 'number of steps for each cycle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build language model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['bow'] = tf.placeholder(tf.float32, [None, config.dim_bow], name='bow')\n",
    "t_variables['input_token_idxs'] = tf.placeholder(tf.int32, [None, None], name='input_token_idxs')\n",
    "t_variables['dec_input_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_input_idxs')\n",
    "t_variables['dec_target_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_target_idxs')\n",
    "t_variables['batch_l'] = tf.placeholder(tf.int32, name='batch_l')\n",
    "t_variables['doc_l'] = tf.placeholder(tf.int32, [None], name='doc_l')\n",
    "t_variables['sent_l'] = tf.placeholder(tf.int32, [None], name='sent_l')\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train', assertion=False):\n",
    "    def token_dropout(sent_idxs):\n",
    "        sent_idxs_dropout = np.asarray(sent_idxs)\n",
    "        sent_idxs_dropout[np.random.rand(len(sent_idxs)) > config.word_keep_prob] = config.UNK_IDX\n",
    "        return list(sent_idxs_dropout)\n",
    "\n",
    "    bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "    \n",
    "    doc_l = np.array([len(instance.token_idxs) for instance in batch])\n",
    "    \n",
    "    feed_input_token_idxs_list = [sent_idxs for instance in batch for sent_idxs in instance.token_idxs]\n",
    "    feed_dec_input_idxs_list = [[config.BOS_IDX] + token_dropout(sent_idxs) for sent_idxs in feed_input_token_idxs_list]\n",
    "    feed_dec_target_idxs_list = [sent_idxs + [config.EOS_IDX]  for sent_idxs in feed_input_token_idxs_list]\n",
    "        \n",
    "    sent_l = np.array([len(sent_idxs) for sent_idxs in feed_input_token_idxs_list], np.int32)\n",
    "    batch_l = len(sent_l)\n",
    "    \n",
    "    feed_input_token_idxs = pad_sequences(feed_input_token_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_input_idxs = pad_sequences(feed_dec_input_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_target_idxs = pad_sequences(feed_dec_target_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    \n",
    "    if assertion:\n",
    "        index = 0\n",
    "        for instance in batch:\n",
    "            for line_idxs in instance.token_idxs:\n",
    "                assert feed_input_token_idxs_list[index] == line_idxs\n",
    "                index += 1\n",
    "        assert feed_input_token_idxs.shape[1] == np.max(sent_l)\n",
    "        assert feed_dec_input_idxs.shape[1] == np.max(sent_l) + 1\n",
    "        assert feed_dec_target_idxs.shape[1] == np.max(sent_l) + 1\n",
    "    \n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "\n",
    "    feed_dict = {\n",
    "                t_variables['bow']: bow, \n",
    "                t_variables['batch_l']: batch_l, t_variables['doc_l']: doc_l, t_variables['sent_l']: sent_l, \n",
    "                t_variables['input_token_idxs']: feed_input_token_idxs, t_variables['dec_input_idxs']: feed_dec_input_idxs, t_variables['dec_target_idxs']: feed_dec_target_idxs, \n",
    "                t_variables['keep_prob']: keep_prob\n",
    "    }\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     10,
     24,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "def check_shape(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()\n",
    "    \n",
    "def check_value(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()    \n",
    "    \n",
    "# sent_loss_kl_categ_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, tf_log(prob_topic_infer/prob_topic_sents)), 1))\n",
    "# debug_value([sent_loss_kl_categ, sent_loss_kl_categ_tmp])\n",
    "# sent_loss_kl_gauss_tmp = 0.5 * tf.reduce_sum(tf.exp(logvars_topic_infer-logvars_topic) + tf.square(means_topic - means_topic_infer) / tf.exp(logvars_topic) - 1 + (logvars_topic - logvars_topic_infer), -1)\n",
    "# sent_loss_kl_gmm_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss_tmp), -1))\n",
    "# debug_value([sent_loss_kl_gmm_tmp, sent_loss_kl_gmm])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encode bow\n",
    "with tf.variable_scope('topic/enc', reuse=False):\n",
    "    hidden_bow_ = tf.layers.Dense(units=config.dim_hidden_bow, activation=tf.nn.relu, name='hidden_bow')(t_variables['bow'])\n",
    "    hidden_bow = tf.layers.Dropout(t_variables['keep_prob'])(hidden_bow_)\n",
    "    means_bow = tf.layers.Dense(units=config.dim_latent_bow, name='mean_bow')(hidden_bow)\n",
    "    logvars_bow = tf.layers.Dense(units=config.dim_latent_bow, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_bow)\n",
    "    latents_bow = sample_latents(means_bow, logvars_bow) # sample latent vectors\n",
    "\n",
    "    prob_topic = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic')(latents_bow) # inference of topic probabilities\n",
    "\n",
    "# decode bow\n",
    "with tf.variable_scope('shared', reuse=False):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "bow_embeddings = tf.nn.embedding_lookup(embeddings, bow_idxs) # embeddings of each bow features\n",
    "\n",
    "with tf.variable_scope('topic/dec', reuse=False):\n",
    "    topic_bow = tf.get_variable('topic_bow', [config.n_topic, config.dim_bow], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of topics\n",
    "    logits_bow = tf_log(tf.nn.softmax(tf.tensordot(prob_topic, topic_bow, axes=[[1], [0]]), 1))\n",
    "\n",
    "    # prior of each gaussian distribution (computed for each topic)\n",
    "    hidden_topic = tf.layers.Dense(units=config.dim_hidden_topic, activation=tf.nn.relu, name='hidden_topic')(topic_bow)\n",
    "    means_topic = tf.layers.Dense(units=config.dim_latent, name='mean_topic')(hidden_topic)\n",
    "    logvars_topic = tf.layers.Dense(units=config.dim_latent, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_topic)\n",
    "    sigma_topic = tf.exp(0.5 * logvars_topic)\n",
    "    gauss_topic = tfd.Normal(loc=means_topic, scale=sigma_topic)    \n",
    "    \n",
    "# define losses\n",
    "topic_losses_recon = -tf.reduce_sum(tf.multiply(t_variables['bow'], logits_bow), 1)\n",
    "topic_loss_recon = tf.reduce_mean(topic_losses_recon) # negative log likelihood of each words\n",
    "\n",
    "topic_loss_kl = compute_kl_loss(means_bow, logvars_bow) # KL divergence b/w latent dist & gaussian std\n",
    "\n",
    "topic_bow_norm = topic_bow / tf.norm(topic_bow, axis=1, keepdims=True)\n",
    "topic_dots = tf.clip_by_value(tf.matmul(topic_bow_norm, tf.transpose(topic_bow_norm)), -1., 1.)\n",
    "topic_loss_reg = tf.reduce_mean(tf.square(topic_dots - tf.eye(config.n_topic)))\n",
    "# topic_angles = tf.acos(topic_dots)\n",
    "# topic_angles_mean = tf.reduce_mean(topic_angles)\n",
    "# topic_angles_vars = tf.reduce_mean(tf.square(topic_angles - topic_angles_mean))\n",
    "# topic_loss_reg = tf.exp(topic_angles_vars - topic_angles_mean)\n",
    "\n",
    "# monitor\n",
    "n_bow = tf.reduce_sum(t_variables['bow'], 1)\n",
    "topic_ppls = tf.divide(topic_losses_recon, n_bow)\n",
    "topics_freq_bow_indices = tf.nn.top_k(topic_bow, 10, name='topic_freq_bow').indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "input_token_idxs = t_variables['input_token_idxs']\n",
    "batch_l = t_variables['batch_l']\n",
    "sent_l = t_variables['sent_l']\n",
    "max_sent_l = tf.reduce_max(sent_l)\n",
    "\n",
    "with tf.variable_scope('sent/enc', reuse=False):\n",
    "    # get word embedding\n",
    "    enc_input = tf.nn.embedding_lookup(embeddings, input_token_idxs)\n",
    "\n",
    "    # get sentence embedding\n",
    "    _, enc_state = dynamic_bi_rnn(enc_input, sent_l, config.dim_hidden, t_variables['keep_prob'])\n",
    "\n",
    "    # TODO House Holder flow\n",
    "    hidden_topic_infer =  tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='hidden_topic_infer')(enc_state)\n",
    "    prob_topic_infer = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic_infer')(hidden_topic_infer)\n",
    "\n",
    "    w_mean_topic_infer = tf.get_variable('mean_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32)\n",
    "    b_mean_topic_infer = tf.get_variable('mean_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32)\n",
    "    means_topic_infer = tf.tensordot(enc_state, w_mean_topic_infer, axes=[[1], [1]]) + b_mean_topic_infer\n",
    "    \n",
    "    w_logvar_topic_infer = tf.get_variable('logvar_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    b_logvar_topic_infer = tf.get_variable('logvar_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    logvars_topic_infer = tf.tensordot(enc_state, w_logvar_topic_infer, axes=[[1], [1]]) + b_logvar_topic_infer\n",
    "    sigma_topic_infer = tf.exp(0.5 * logvars_topic_infer)\n",
    "    gauss_topic_infer = tfd.Normal(loc=means_topic_infer, scale=sigma_topic_infer)\n",
    "    \n",
    "    # latent vectors from each gaussian dist.\n",
    "    latents_topic_infer = sample_latents(means_topic_infer, logvars_topic_infer) \n",
    "    # latent vector from gaussian mixture\n",
    "    latents_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), latents_topic_infer, transpose_a=True)\n",
    "    \n",
    "    # for beam search\n",
    "    means_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), means_topic_infer, transpose_a=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for decoding\n",
    "dec_sent_l = tf.add(sent_l, 1)\n",
    "dec_input_idxs = t_variables['dec_input_idxs']\n",
    "dec_input = tf.nn.embedding_lookup(embeddings, dec_input_idxs)\n",
    "\n",
    "dec_latents_input = tf.tile(latents_input, [1, tf.shape(dec_input)[1], 1])\n",
    "dec_concat_input = tf.concat([dec_input, dec_latents_input], -1)\n",
    "\n",
    "# decode for training\n",
    "with tf.variable_scope('sent/dec/rnn', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32, reuse=False):\n",
    "    dec_cell = tf.contrib.rnn.GRUCell(config.dim_hidden)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob = t_variables['keep_prob'])\n",
    "\n",
    "    dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(latents_input, 1))\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_concat_input, sequence_length=dec_sent_l)\n",
    "\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=dec_cell,\n",
    "        helper=helper,\n",
    "        initial_state=dec_initial_state)\n",
    "\n",
    "    dec_outputs, _, output_sent_l = tf.contrib.seq2seq.dynamic_decode(train_decoder)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(config.n_vocab, use_bias=False, name='out')\n",
    "    output_logits = output_layer(dec_outputs.rnn_output)\n",
    "    \n",
    "    output_token_idxs = tf.argmax(output_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tokens = tf.fill([batch_l], config.BOS_IDX)\n",
    "end_token = config.EOS_IDX\n",
    "\n",
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    infer_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(means_input, 1))\n",
    "    beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(infer_dec_initial_state, multiplier=config.beam_width)\n",
    "    beam_latents_input = tf.contrib.seq2seq.tile_batch(tf.squeeze(means_input, 1), multiplier=config.beam_width) # added\n",
    "    \n",
    "    beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=beam_latents_input)\n",
    "\n",
    "    beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    beam_output_token_idxs = beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    inter_means_input = tf.placeholder(tf.float32, [None, config.dim_latent])\n",
    "    \n",
    "    inter_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(inter_means_input)\n",
    "    inter_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(inter_dec_initial_state, multiplier=config.beam_width)\n",
    "    inter_beam_latents_input = tf.contrib.seq2seq.tile_batch(inter_means_input, multiplier=config.beam_width) # added\n",
    "    \n",
    "    inter_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=inter_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=inter_beam_latents_input)\n",
    "\n",
    "    inter_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inter_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    inter_beam_output_token_idxs = inter_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    topic_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic)\n",
    "    topic_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(topic_dec_initial_state, multiplier=config.beam_width)\n",
    "    topic_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic, multiplier=config.beam_width) # added\n",
    "    \n",
    "    topic_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=topic_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=topic_beam_latents_input)\n",
    "\n",
    "    topic_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        topic_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    topic_beam_output_token_idxs = topic_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language modeling cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target and mask\n",
    "dec_target_idxs = t_variables['dec_target_idxs']\n",
    "dec_mask_tokens = tf.sequence_mask(dec_sent_l, maxlen=max_sent_l+1, dtype=tf.float32)\n",
    "\n",
    "# nll for each token (averaged over batch & sentence)\n",
    "sent_loss_recon = tf.contrib.seq2seq.sequence_loss(output_logits, dec_target_idxs, dec_mask_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_l = t_variables['doc_l']\n",
    "mask_sents = tf.sequence_mask(doc_l)\n",
    "mask_sents_flatten = tf.reshape(mask_sents, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1]])\n",
    "\n",
    "prob_topic_tiled = tf.tile(tf.expand_dims(prob_topic, 1), [1, tf.shape(mask_sents)[1], 1])\n",
    "prob_topic_flatten = tf.reshape(prob_topic_tiled, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1], config.n_topic])\n",
    "prob_topic_sents = tf.boolean_mask(prob_topic_flatten, mask_sents_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferred mixture probabilities (computed for each sentence)\n",
    "categ_topic_infer = tfd.Categorical(probs=prob_topic_infer)\n",
    "\n",
    "# prior of mixture probabilities (computed for each document, tiled for each sentence)\n",
    "categ_topic = tfd.Categorical(probs=prob_topic_sents)\n",
    "\n",
    "sent_loss_kl_categ = tf.reduce_mean(tfd.kl_divergence(categ_topic_infer, categ_topic))\n",
    "\n",
    "# inference of each gaussian gaussribution (computed for each sentence)\n",
    "\n",
    "sent_loss_kl_gauss = tf.reduce_sum(tfd.kl_divergence(gauss_topic_infer, gauss_topic), -1)\n",
    "sent_loss_kl_gmm = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss), -1))\n",
    "\n",
    "sent_loss_kl = sent_loss_kl_categ + sent_loss_kl_gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "tau = tf.cast(tf.divide(tf.mod(global_step, tf.constant(config.cycle_steps)), tf.constant(config.cycle_steps)), dtype=tf.float32)\n",
    "beta = tf.minimum(1., tau/config.r_cycle)\n",
    "\n",
    "sent_loss = sent_loss_recon + beta * sent_loss_kl\n",
    "\n",
    "topic_loss = topic_loss_recon + topic_loss_kl + config.reg * topic_loss_reg\n",
    "loss = topic_loss + sent_loss\n",
    "\n",
    "# define optimizer\n",
    "if config.opt == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif config.opt == 'Adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "    \n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_sents(token_idxs, config, idx_to_word):\n",
    "    sents = []\n",
    "    for sent_idxs in token_idxs:\n",
    "        tokens = []\n",
    "        for idx in sent_idxs:\n",
    "            if idx == config.EOS_IDX: break\n",
    "            tokens.append(idx_to_word[idx])\n",
    "        sent = ' '.join(tokens)\n",
    "        sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_batch, sent_loss_batch, ppls_batch = sess.run([loss, topic_loss, sent_loss, topic_ppls], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_batch, sent_loss_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "    loss_mean, topic_loss_mean, sent_loss_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    return loss_mean, topic_loss_mean, sent_loss_mean, ppl_mean\n",
    "\n",
    "def get_all_losses(sess, batches):\n",
    "    losses = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch = \\\n",
    "        sess.run([loss, topic_loss_recon, topic_loss_kl, sent_loss_recon, sent_loss_kl], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "    print('LOSS %.2f | TM NLL: %.2f, KL: %.4f | LM NLL: %.2f, KL: %.4f' %  np.mean(losses, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_batch):\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    pred_token_idxs = sess.run(output_token_idxs, feed_dict = feed_dict)\n",
    "    true_token_idxs = [sent_idxs for instance in sample_batch for sent_idxs in instance.token_idxs]\n",
    "    \n",
    "    assert len(pred_token_idxs) == len(true_token_idxs)\n",
    "    \n",
    "    pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "    true_sents = idxs_to_sents(true_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    for i, (true_sent, pred_sent) in enumerate(zip(true_sents, pred_sents)):        \n",
    "        print(i, 'True: %s' % true_sent)\n",
    "        print(i, 'Pred: %s' % pred_sent)\n",
    "\n",
    "def print_topic_sample():\n",
    "    pred_topics_freq_bow_indices, pred_topic_token_idxs = sess.run([topics_freq_bow_indices, topic_beam_output_token_idxs], \n",
    "                                                                                                           feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "    pred_topic_sents = idxs_to_sents(pred_topic_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Topic Samples-----------')\n",
    "    for i, (topic_freq_bow_idxs, pred_topic_sent) in enumerate(zip(topics_freq_bow_idxs, pred_topic_sents)):\n",
    "        print(i, ' bow:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' sent:', pred_topic_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "logs = []\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "saver = tf.train.Saver(max_to_keep=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "010[s], Ep: 00, Ct: 00000|TR LOSS: 362, PPL: 2663|TM NLL: 351, KL: 0.71, REG:0.00 | LM NLL: 10.34, KL: 1.52|DE LOSS: 347, PPL: 2658, TM: 337, LM: 10.34|BETA: 0.000139\n",
      "0 True: a former ancestry.com executive accused of sexually abusing his daughter 's teenage friend has waived a preliminary hearing in the case\n",
      "0 Pred: kurt witt weiland weiland weiland wemhoff wtnh wheels lewisburg wheels wheels wheels wheels timeline wheels wheels wheels ponce wheels wheels wheels wheels heading wheels wheels wheels wheels wheels wheels pssas wheels wheels debolt ponce wheels wheels wheels wheels wheels\n",
      "1 True: court records show that # year old daniel d. taggart of highland appeared in court tuesday to waive the right to have prosecutors lay out the evidence against him\n",
      "1 Pred: dark reached pollster pellegrini norbert presley pitcher theatre dagestan irregular stidd advertisement folks massacre investigations advertisement norbert dagestan plaguing cher shows advertisement folks theatre dagestan about advertisement about advertisement advertisement about advertisement u.s. greer about closing murtha pellegrini cabe\n",
      "2 True: he is facing # charges , including rape , sodomy and sexual abuse\n",
      "2 Pred: restoration snowmobiles blodgett clustered waltham hometowns pisgah demleitner holdups hometowns sculptures wilderness overlook unlike fakes whyte harnois orphan sands epicenter slaughterhouse assistants holdups pawned maddox slaughterhouse watauga overlook agromod unlike ground fakes maddox whyte marie cambridge pawned sculptures reardon\n",
      "3 True: he has not entered a plea\n",
      "3 Pred: boc python legislation legislation bloodied straw legislation hollister straw legislation buh hollister hollister hollister neonatal hollister buh embarrassing neonatal hollister sea hollister hollister embarrassing pups buh embarrassing neonatal hollister hollister buh embarrassing buh hollister embarrassing buh hollister buh buh\n",
      "4 True: the # year old girl told police she visited taggart 's daughter in july #\n",
      "4 Pred: garrett cds underwater comfortable soo lutcher handwriting looks exit handwriting tanning breed traylor traylor worthy worthy soo ballroom soo breed breed cronin nethken worthy aizenberg soo breed worthy dena'ina breed handwriting alleyne handwriting breed worthy worthy blumenthal soo traylor\n",
      "5 True: he allegedly provided alcohol and suggested games , including a `` <unk> `` contest with sex toys as prizes\n",
      "5 Pred: essentially scanner soler roque giveaways panepinto blessing blessing hair blessing blessing tightening embassies blessing . blessing blessing faster blessing spokesperson lifting misstating embassies spokesperson blessing faster nicolas blessing embassies faster corps closely streams kilmartin blessing difficulty faster deficits faster\n",
      "6 True: the girl claims the sexual abuse began when taggart put on a pornographic video after his daughter fell asleep\n",
      "6 Pred: uncovered budgeting budgeting fleeing fed sunshine mack surgeries truckee artistic lanza dupri macmullin stessel stessel responders mack mack trumped overlay dupri eluding presentations excess stessel grass challenged ava glimpse surgeries dupri wheel responders presentations trumped fletcher trumped trumped mack\n",
      "7 True: ancestry.com said taggart is a former executive who left the company # years ago\n",
      "7 Pred: steward guard defenses lumina spencer already luz luz ridiculous conciliation homecoming unpaid luz luz traces cypriot palestine ada allergan conciliation gamble luz conciliation luz ancestry uncomfortable copter traces uncomfortable inspire traces ridiculous groundbreaking traces cypriot conciliation chickasha pup gamble\n",
      "8 True: taggart 's attorney <unk> <unk> declined to comment tuesday\n",
      "8 Pred: gable van simulate volunteering labeled collin specialized gilpin fallout taxes collin babe apart gilpin mudslides bp mudslides namesake mudslides mudslides lapel o'neal cords mudslides deeper expires tribeca lapel gilpin mudslides mudslides mudslides exhausted galleria mudslides gilpin lapel lapel mudslides\n",
      "9 True: minnie driver is joining the cast of nbc 's `` peter pan live !\n",
      "9 Pred: witt sparta litto schooner gusty heineman schooner gusty gusty gusty prot mulvaney forlizzi mulvaney gusty velez mulvaney velez gusty gusty velez velez wtnh deming toney parkersburg aluminum velez aluminum toney espinola gusty tight tightening gusty eastman gusty gusty toney\n",
      "10 True: she will play the character wendy darling as an adult and <unk> the musical , the network said wednesday\n",
      "10 Pred: homegrown austrian extra fed celia buddy inquiries waldington watts harrold watts watts harrold inquiries gaskin channels watts watts inquiries inquiries photos inquiries watts inquiries watts exhausted livestock channels writes watts pursue watts harrold watts watts channels topeka watts harrold\n",
      "11 True: driver co stars in the nbc comedy `` about a boy .\n",
      "11 Pred: faucet delayed welcome simi disarm borrow welcome josh strawberry josh josh josh borrow venues ridenour borrow mating borrow no venues //tinyurl.com/ borrow //tinyurl.com/ strawberry salon o'hare mating borrow borrow no no mating ridenour pr borrow borrow strawberry mating substances\n",
      "12 True: nbc also announced wednesday that it will air a special , `` the making of peter pan live !\n",
      "12 Pred: thriving fedele pemberton participated taxes flushed straps pemberton tripp tripp tripp historical fogel unrest snuffed historical campos snuffed historical maes acworth mauck tractor odlin fong historical lopez singer historical lattarulo straps fogel odlin acworth historical seas dozens fogel deceased\n",
      "13 True: , on nov. #\n",
      "13 Pred: terminate nathan nathan nathan read nathan carleton sleisher nathan nathan pockets nathan web nathan bartmier contradicted grice forcibly nathan subordinate winston issuance nathan skilling mammals opening babysitting hometowns nathan terrible terrible honolulu duffie release nathan contradicted wcax winston nathan\n",
      "14 True: allison williams is starring in the title role in `` peter pan live !\n",
      "14 Pred: miscalculated commercial ncsl trisha sausage swap laughs tree motorboat vocational tree medicinal raucous tree diana rainwater mouse recklessness hubs tree mouse recklessness rovi recklessness laughs rovi tree dowda freedoms freedoms hammoud tree sorts exercising entrusted clearing sorts outbreak diana\n",
      "15 True: , which also includes christopher <unk> , christian <unk> and kelli o'hara in the cast\n",
      "15 Pred: fayard negligence arledge arledge fishburne arledge pup bork pup arledge pup bosch pup tomah concedes havard pup pup simpler pup fakes cromwell fishburne gothner inert pup jeanerette donato cheered gothner gothner cuomo gothner inert fakes arevalos gothner pup pup\n",
      "16 True: the musical is scheduled to air dec. # on nbc\n",
      "16 Pred: catalyst novavax flammable stays oakhurst competitiveness contenders blow deaf contenders deaf competitiveness invaded competitiveness wheelers competitiveness competitiveness competitiveness competitions plain competitiveness deaf invaded performs competitiveness invaded uncertain invaded plain invaded invaded competitiveness granger blow blow deaf competitiveness invaded invaded\n",
      "17 True: a new poll shows retired army col. mike mccalister shoring up his support in a four way race for the republican nomination for the u.s. senate\n",
      "17 Pred: jauregui dred mcnamara dred medford d'amico faggard daluz arsenic onassis laufer medford buoys onassis flycatcher focusing might danbury senators laufer enactment onassis rockies onassis laufer laufer strict enactment onassis enactment carden adolf subduing stacy medford reservation spoken enactment weekday\n",
      "18 True: mccalister was favored by # percent in a recent random telephone survey of # republican voters by quinnipiac university\n",
      "18 Pred: forcible adjournment donoyan nrg westerly builds schoolyard castano schoolyard schoolyard schoolyard his schoolyard qantas odds sag iii loops donoyan iii kroy schoolyard streetcar ramps sag ramps exhibits machinists salamander streetcar exhibits insist qantas iii odds schoolyard beau schoolyard exhibits\n",
      "19 True: former u.s. sen. george lemieux received # percent , while # percent said they are undecided\n",
      "19 Pred: moosie sky subside mara unpopular incoherent weekley weekley communicate infirmary communicate insight baroni weekley weekley boeing lewisburg infirmary weekley singer cartagena communicate smoke bernice insight teacher communicate infirmary extremist reburied mccomb communicate infirmary honorable administrative dubuque weekley acworth teacher\n",
      "20 True: mccalister , who lives in plant city , received # percent of the vote in last year 's republican gubernatorial primary\n",
      "20 Pred: griffiths schoolyard heidelberg sluter vulnerabilities aquatic complaining eickhoff complaining vehr sluter liberal gaudens nerheim legislation institutional liberal appraised carole enforcer keeler hotels batch richard acknowledging breeze batch batch batch liberal batch batch stockholder sick stabbed liberal stockholder sick sick\n",
      "21 True: winter park businessman craig miller was favored by # percent and former state rep. adam hasner by # percent in the senate survey\n",
      "21 Pred: tractors valuable mobilization treating raw skeletons skeletons process pssas widlak skeletons skeletons process carleton tying skeletons nw skeletons closing widlak nw accumulate process cm skeletons scuffle release huge association bits taft about cms unspecified childress gordman nw coloradans process\n",
      "22 True: the margin of error in the survey was # percentage points\n",
      "22 Pred: spears herald grunion ethel amendatory amendatory amendatory functionally chris functionally functionally functionally functionally functionally amendatory discontinue functionally functionally functionally chris functionally discontinue functionally functionally amendatory discontinue discontinue discontinue cab functionally archer mcgill functionally functionally discontinue discontinue amendatory discontinue planning\n",
      "23 True: the republican nominee will face democratic u.s. sen. bill nelson , who is seeking a third term next november\n",
      "23 Pred: secular hound slave dried turbine india courtroom india snuka courtroom sinha snuka elum tice ramnarine herrera kcfw sinha india sinha sinha elum inhalation kipp sharfstein india smoke sinha smoke remorse kipp remorse courtroom valentin courtroom india blacked remorse elum\n",
      "24 True: thousands of teenagers will be sporting blue <unk> jackets in louisville beginning wednesday as the national <unk> organization convention returns to the city for the first time since #\n",
      "24 Pred: guard recharge amarillo horse hose lopez steelworkers handmade lopez lopez closet lopez coakley lopez nicaragua har lopez lopez lopez bunny ammo patches regional lopez lopez kolo petit regional ammo step regional ammo lopez forums plus wlox lopez ammo emperor\n",
      "25 True: the courier journal says the four day gathering is expected to bring close to # people to the city , generating about $ # million for the local and state economy\n",
      "25 Pred: referenced antwerp bit stephan hancock sentence hurdles hurdles rogoff provocative mcmahon criminal mcmahon grice masonry sayre mcmahon finances belarus read sanctuary kcfw stringer finances finances mcmahon mcmahon wsu pell finances mcmahon masonry orchard pell finances sky hurdles riskier mcmahon\n",
      "26 True: the convention was held in louisville from # to # and will alternate between louisville and indianapolis for the next several years\n",
      "26 Pred: claus carleton cited vz nathan nathan paez ketchum actor nathan rorls nathan paez legislation schmaltz paez forcibly oilfield ackman disappointment gretna schmaltz distributor racetracks mcclure ackman alliances forcibly overpasses alliances actor lindstrom lucky actor stoneking mcclure coca mishap nofoa\n",
      "27 True: numerous events are planned during the convention\n",
      "27 Pred: carleton speaking cadaver parade whitmarsh kashmir joking tsegaye pickets pickets pickets tsegaye tsegaye retraction irritating joking rudd lustyik catalina scientists orlando parade mater tsegaye ravenswood lustyik tsegaye whitmarsh tsegaye undressed fowl pickets scams raw joking wrtv rudd pickets pickets\n",
      "28 True: one activity being held thursday is the chevron products co. 's <unk> tractor restoration competition finals\n",
      "28 Pred: publicity publicity publicity publicity duped ney auditorium duped duped duped duped duped duped ney forums ney ney contributing berthold bulb ney duped ney duped loop interagency duped duped ney ney duped berthold rerouted asphyxia ney foreclosed bulb asphyxia ney\n",
      "29 True: the grand champion will win $ #\n",
      "29 Pred: nestande joel restructure accustomed borrowers borrowers borrowers borrowers springleaf borrowers bidders overzealous dryden held kitv staten whereabouts bidders bvd bidders gottlieb televised held borrowers testifying borrowers borrowers buccaneers held testifying bowles staten borrowers bidders borrowers bidders borrowers luna borrowers\n",
      "30 True: some attending the convention will participate in the national days of service and are expected to provide more than # hours of community service\n",
      "30 Pred: descend actual crandall overcoming ellen tri ira tri westlake phone deputy phone singer deputy deputy uab deputy phone deputy deputy deputy phone deputy tri deputy uab deputy uab deputy deputy firearms uso singer phone deputy deputy uab deputy uab\n",
      "31 True: the last of four men to plead guilty in connection with the fatal shooting of a man in albany in # has been sentenced to # years in state prison\n",
      "31 Pred: hightower artifact hawkeye exhaust naimi restricted breaker stoneking breaker vacate alcohol warehouse tight camille instituted warehouse camille warehouse instituted camille fwp nicaragua instituted nicaragua warehouse tight warehouse camille velez fwp warehouse warehouse warehouse blasting camille camille thibodaux warehouse warehouse\n",
      "32 True: prosecutors say monday that # year old antwan <unk> was sentenced in albany county court , where he pleaded guilty in january to a robbery charge in the killing of # year old alex duncan\n",
      "32 Pred: refilling cared shuai gf disbursed puc lohs julene ricks disbursed overdose overdose lohs verbally schoolyard lohs suzanne captivity followed julene copy disbursed disbursed galvin disbursed tale hottenstein captivity overdose odds complain kimberly tale captivity curd complain followed firework hottenstein\n",
      "33 True: police say duncan was killed in september # in front of an albany neighborhood business\n",
      "33 Pred: perimeter majors cosgrove courier spaceshiptwo courier courier recreational velasco courier recreational senjem victory marcos spaceshiptwo baltazar stowe lemaster elevate rapides lemaster elevate lemaster spoken elevate rapides engineering presumably baltazar engineering spoken stowe tanner hertz blender unpaid lemaster poythress cbp\n",
      "34 True: authorities say <unk> , shaquan oliver , bernard white and <unk> bean tried to rob duncan , who was shot in the chest and died at albany medical center\n",
      "34 Pred: a.m. sisak picker castaneda ammo exchange transported squatters went those gagne officer exchange went went stephan went accelerate hiatus went went went exchange hawksley sentiments transported mchenry exchange portrays those hiatus bottles exchange bianchi faster stephan plans deandre shocked\n",
      "35 True: oliver , white and bean pleaded guilty to robbery charges in january\n",
      "35 Pred: consumers paralysis bbc bbc bbc levy bae property meridien sentiments wmaq scientific inadmissible cyber uncertain wmaq parada patrolman parada essence parada inadmissible essence essence parada wmaq inadmissible locating inadmissible rosemarie essence inadmissible patt levy uncertain parada biking scientific missey\n",
      "36 True: they were given sentences ranging from # years to # years in state prison\n",
      "36 Pred: plea swerved pony exhaustion exhaustion condor coincided rescind airventure exhaustion leo exhaustion rescind guinto interpretive overloaded exhaustion fullerton july exhaustion exhaustion bylaw exhaustion exhaustion exhaustion extremely transmitter airventure exhaustion eurozone extremely airventure interpretive snowmass extremely exhaustion extremely airventure airventure\n",
      "37 True: iowa agriculture officials say this year 's spring planting is among the slowest in a decade with only # percent of the state 's corn crop in the ground\n",
      "37 Pred: li symbol nadeau interagency interagency ney interagency interagency rainwater ney ney ney shale ney twisters focusing varughese ney ney hold covington twisters targeting focusing ney ney varughese focusing hometowns lawson ney fwp tempered covington lacerations ney focusing ney ney\n",
      "38 True: that compares to # percent of the state 's corn being planted at the same time last year\n",
      "38 Pred: hefner lumina pitt lumina cam cendoya cam cam lumina islam cendoya cam regional joh lumina marshes joh rehberg weber kansans cam joh kansans cam cam eickhoff cam joh undersea tad efficient cam marvell harming handed joh procedural cam lumina\n",
      "39 True: in a news release monday , iowa secretary of agriculture bill northey says topsoil moisture was rated at # percent adequate and # percent surplus\n",
      "39 Pred: scanner yuchnitz scanner powdered espada unearthed unearthed espada parkersburg gordman incinerated gordman fatality unearthed backcountry a.d. unearthed his unearthed snowmobile minerals backcountry incinerated parkersburg pelvic gordman espada pint unearthed incinerated patten parkersburg a.d. prosthetic pint unearthed misspending shawano mcqueen\n",
      "40 True: <unk> moisture is rated at # percent adequate and # percent surplus\n",
      "40 Pred: outage dark dioguardi confiscate bowden mulling confiscate confiscate slater maui bentinck jean sale bentinck maternity garrison bih vying sour slater dunster madame vying pitcher deficiencies premise sour vying confiscate demonstrated sour sour adel sour bellefontaine housing vying groggins ackman\n",
      "41 True: he says field work last week was virtually suspended because of the wet , cool conditions\n",
      "41 Pred: maize diapers xwb loans closing filter shawnee # tweaking filter # might # closing fwp florez fwp fwp assert fwp fwp lac fwp loans filter defaulted fwp closing filter imagined lac fwp pastries fwp fwp filter fwp fwp fwp\n",
      "42 True: northey says some farmers are beginning to worry about <unk> if corn does n't <unk> and develop\n",
      "42 Pred: housekeeper astra inclined mother mother steinbach tv radios collisions transfer racetracks mother racetracks navajo racetracks behaving forcing mother xuzhou fawns increased navajo racetracks stidd navajo navajo racetracks racetracks navajo steinbach nofoa navajo steinbach navajo navajo racetracks racetracks racetracks fourths\n",
      "43 True: a brazilian man has been charged with two counts of conspiracy to commit a felony after being accused of trying to get driver 's licenses for two other men\n",
      "43 Pred: disagrees bioscrip ncube dobry pierson mortality romes universe congregate bribery inert mulholland advances basler spokespeople bustos nofoa inert bribery nofoa mulholland nofoa sayre biofuel shots hostility mulholland nofoa nofoa nofoa cessation sayre bikers sayre linn nofoa evading inert awol\n",
      "44 True: krqe television in albuquerque reports that state police allege cristian sobral was paid $ # by two other brazilian men to fly them to new mexico from new jersey\n",
      "44 Pred: philosophy mizell slot philosophy infect resist inventor pastries raffled seeping searched odds convince thad marie gangs pastries tunica seeping replica dime nassau replica hobgood cue pastries devillier damage devillier hobgood riverdale pastries concerts refinance ragsdale ragsdale ebony refinance decample\n",
      "45 True: state police say sobral was trying to get driver 's licenses for the men friday afternoon at a state motor vehicle division office in albuquerque , using a city address\n",
      "45 Pred: tommie steelworkers paez steelworkers thoma thoma ellenwood expressed redmond madagascar expressed roy mauck ha mauck crystalline infestations mauck mauck l.com/ brockport northern parachute brockport mauck mauck corralled steelworkers nathan expressed expressed expressed nathan northern orthopedic mauck mary ketamine rob\n",
      "46 True: a taxation and revenue agent remembered seeing sobral at the office with different people a couple of times over the past year\n",
      "46 Pred: occasionally affects appointed supervision refueling supervision lowman analytical lowman lowman supervision analytical supervision lowman lowman refueling lowman connor lowman springtime lowman refueling respondents supervision lowman deruwe scarborough supervision refueling ames po links refueling supervision refueling refueling refueling refueling springtime\n",
      "47 True: the other two men are charged with forgery\n",
      "47 Pred: orangutan anguiano buddies androscoggin amusement crammed balked retain mag singer khan captured participate ineligible bottles gail bottles extermination singer retain sunflowers bednarik southdown sunglasses fact southdown uw sunflowers bednarik sunflowers bottles bottles leaking southdown bottles exchange resolves denounced bednarik\n",
      "48 True: all three remained jailed monday in the bernalillo county metropolitan detention center\n",
      "48 Pred: spillway lawns kitchens coatings consultations consultations consultations pemiscot uribe stroger supplemental consultations consultations consultations consultations consultations consultations oils stroger lalla consultations consultations pse consultations toney professors supplemental thal consultations zeferino toughest pemiscot oils toney conimicut supplemental pssas manor conimicut\n",
      "49 True: the university of southern maine 's president has rescinded the raises of two top administrators in the wake of revelations that the school gave out large pay hikes to dozens of employees at a time of financial uncertainty\n",
      "49 Pred: concealing products salvador marble weighs prone deceased gunnison hiv/aids cognitive unspecified drayton vulnerability raid pavlik weighs taipei drayton leclaire weighs pavlik contacts alerted mergers weighs diede automatic ige adl shake raid weighs amy weighs mergers improvements banco shoffner dannelly\n",
      "50 True: president selma <unk> announced wednesday she pulled raises given last summer to her chief of staff and the university 's public affairs director\n",
      "50 Pred: texting hershey fainting refrain carroll instituted mcloud khazaee khazaee downstream signings khazaee bulgaria khazaee recovery khazaee khazaee imposes khazaee khazaee rockies khazaee baltazar ewa khazaee khazaee khazaee juvenile kimberly khazaee cashier bulgaria engines khazaee khazaee khazaee sheriff khazaee khazaee\n",
      "51 True: both had received pay hikes of around # percent that bumped their annual salaries to $ #\n",
      "51 Pred: palazzo upholding setting beardsley defied titanium ellingson jenks ellingson charges ardo mickle charges mckissick vitamin mckissick grief jonas dealing fawwaz defied mickle greenwald pastor casing appoints subduing flamer fatalities nathanial mckissick orderly deteriorating stessel attached bingman subject regard apache\n",
      "52 True: the portland press herald ( http : <unk> # ) reports that <unk> wrote in an email to staff that `` legitimate concerns have been raised `` about the raises\n",
      "52 Pred: retallick beneath selab beaded virus exhaustion denny tilton marked sonora toward cautiously johnathan wight went tinker toward calchi backgrounds main economy tinker assistant stipends snipe intercontinental ranger attractive extremism mower extremely freezer exhaustion wight attractive beatz went wight johnathan\n",
      "53 True: she pulled raises of two staffers whose increases had generated the greatest concern\n",
      "53 Pred: exciting bueche grateful bustos prayer ramping muskogee ramping ramirez rd children connell tutoring weak admissible superheroes golden superheroes alleviate envelopes diede ramping connell alabamians ostracized superheroes alabamians poythress poythress dime mating weak persistent weak connell aloha connell mccollum poythress\n",
      "54 True: the university faces $ # million in budget cuts next year and the pay of most workers has been frozen\n",
      "54 Pred: conceding buh juvenile taps safe refrain safe waterhouse retallick cotromano chadds chadds chadds cotromano chadds chadds safe safe waterhouse mothers cotromano safe safe mothers mothers chadds kenens safe lakeview safe safe luppino mothers safe safe safe kenens safe safe\n",
      "55 True: the city of tracy will be allowed to purchase a # acre tract of land from the federal government to build a solar energy project after a bill passed giving the stalled deal the green light\n",
      "55 Pred: raiding raiding chalet redistricting dobry dobry dobry chalet records johnson records lounge bet tension mcnamara closing paulding sinus immediately $ juvenile juvenile $ juvenile dena'ina juvenile horrible $ chained hutton johnson mortality juvenile $ hutton juvenile $ transmitter juvenile\n",
      "56 True: u.s. rep. jeff denham , r turlock , on wednesday said the president approved the tracy land conveyance bill , which waived use restrictions on the land\n",
      "56 Pred: solve dorn luna thrusters spoleto deposed karaca loma urban kilovolt ballard amarillo amarillo celebration avalanche hollatz green amarillo gorder seaside ballard biased amarillo constant crescent urban amarillo celebration amarillo green urban high green urban rewarding green green biased green\n",
      "57 True: the land on schulte road was conveyed previously to tracy # years ago with the condition that it be used only for recreational or educational purposes\n",
      "57 Pred: uncovered radioshack largest violates harming reclaimed harming sleisher chalet sleisher bathon harming bih canadian harming sleisher harming repeatedly harming unaccredited qualities sleisher sleisher parade sleisher sleisher unaccredited unaccredited cut subside harming harming chat sleisher //bit.ly/uh weak reclaimed renovations harming\n",
      "58 True: denham said the project will help create jobs in california 's central valley , one of the most economically hard hit areas in the nation\n",
      "58 Pred: bus anticipate rang apply capsized kancamagus hot o aug. conditioning capsized backlog selma exploding bujak statistically extradition eberhart tonopah eberhart selma eberhart defamation defamation eberhart reston o noeldechen titles defamation shine violators falcons sebastian billerica falcons defamation medicines falcons\n",
      "59 True: tracy mayor brent <unk> said in a statement that the project could create up to # jobs\n",
      "59 Pred: overcoming adams cochran rawson years years rawson rawson selimovic askins selimovic askins rawson selimovic rawson rawson askins rawson askins doubles rawson rawson selimovic askins trailhead trailhead rawson rawson rawson malloy years askins rawson askins selimovic askins diversification engines selimovic\n",
      "60 True: maryland state police say four people were injured after a trooper 's patrol car and another vehicle collided in a harford county intersection as the trooper responded to a call\n",
      "60 Pred: vocational room earle holthaus reopen holthaus vatican vatican holthaus elzea enactment holthaus reaches holthaus holthaus barth holthaus enactment holthaus elzea deliveryman holthaus spoken holthaus spell nevadans nokia holthaus holthaus elzea unconstitutionally holthaus holthaus holthaus barth vatican holthaus paternity spell\n",
      "61 True: police say the trooper was responding to a call on sunday afternoon with lights and sirens on when the crash occurred at the intersection of state route # and state route # in bel air\n",
      "61 Pred: bush vandalism nasty longstanding receipts flatbed cookman cookman details brentwood malfunctioning apache cookman cookman cookman cookman cookman register cookman failings cookman cookman cookman cookman cookman cookman cookman cookman cookman cookman cookman cookman cookman cookman failings cookman failings cookman cookman\n",
      "62 True: police say the trooper and one of the three people in the other vehicle were flown to the university of maryland shock trauma center in baltimore\n",
      "62 Pred: garrett betts betts betts betts underrepresented underrepresented vallejo dekalb township betts plaster township culbertson township hermann township betts haller munster township almond chip township bedlion township prevented plaster township disturbing reputation outlaws plaster rosneft betts meanwhile rosneft tod haller\n",
      "63 True: the other two people in the other vehicle were taken to johns hopkins bayview hospital\n",
      "63 Pred: interrupt walkway user plano askins user user rangeland shearer suriname sanny askins arose dani commonwealth mixed aol thor preserved suriname joblessness hospitality preserved askins preparedness juvenile losey destroy mostly thor seawall seawall bioscience sittenfeld aol facade statutes baumrucker shearer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Topic Samples-----------\n",
      "0  bow: sexual albany tuesday declined sentenced sex video january guilty began\n",
      "0  sent: cawley marry hatched hatched hatched hatched hatched hatched hatched contribute hatched contribute manicured manicured manicured manicured manicured manicured manicured manicured manicured manicured manicured manicured drier manicured manicured manicured manicured manicured drier manicured manicured manicured manicured manicured drier manicured manicured drier\n",
      "1  bow: special january percentage albany retired park wrote ranging ground telephone\n",
      "1  sent: plea plea sentence sentence shelby shelby shelby shelby shelby shelby shelby shelby madame madame referendum referendum referendum referendum referendum referendum referendum shortly shortly foreclosed cancers cancers cancers referendum referendum referendum referendum referendum referendum referendum referendum referendum referendum referendum referendum referendum\n",
      "2  bow: percent adult making wednesday cast years air television peter beginning\n",
      "2  sent: sitter sitter uw uw uw srs srs srs srs paperless cruff paperless cruff paperless cruff cruff paperless cruff cruff paperless cruff cruff paperless cruff cruff paperless cruff cruff paperless cruff cruff paperless cruff cruff cruff paperless cruff cruff paperless cruff\n",
      "3  bow: driver felony commit prison activity christian authorities grand means gunshot\n",
      "3  sent: plea plea pregnancies mclinn aggression aggression aggression aggression assertions ephron ephron assertions ephron ephron ephron deluge deluge sick deluge sick sick ephron ephron ephron deluge sick ephron deluge sick deluge sick ephron ephron ephron deluge sick ephron deluge sick ephron\n",
      "4  bow: wednesday restrictions valley charges accused million special bodies crew retiring\n",
      "4  sent: plea plea intergovernmental mortality annette mortality annette annette mortality annette mortality annette annette annette annette annette annette carleton annette annette annette carleton transmitter transmitter transmitter transmitter transmitter transmitter transmitter transmitter transmitter transmitter transmitter transmitter transmitter transmitter transmitter transmitter transmitter transmitter\n",
      "5  bow: percent trooper maryland people afternoon flown responded occurred center time\n",
      "5  sent: dasig dasig dasig dasig icebreaking vying vying vying vying vying vying vying vying vying vying vying vying vying vying vying vying vying vying vying vying vying vying vying vying vying vying vying vying vying vying vying vying vying vying vying\n",
      "6  bow: percent conspiracy affairs financial large courier christopher pick locations matt\n",
      "6  sent: picasso hlebak hlebak deferred deferred deferred deferred deferred deferred deferred deferred panned deferred deferred deferred deferred deferred deferred deferred deferred deferred deferred deferred deferred deferred deferred deferred deferred deferred deferred deferred deferred deferred deferred deferred deferred deferred deferred deferred deferred\n",
      "7  bow: develop deal areas plant nation u.s. evidence prevent friends art\n",
      "7  sent: picasso schlussel icebreaking icebreaking icebreaking icebreaking icebreaking icebreaking operatives pressly badlands octavia badlands badlands nypd nypd nypd nypd nypd nypd nypd nypd nypd nypd nypd nypd nypd nypd darby nypd nypd nypd nypd nypd nypd nypd nypd nypd nypd darby\n",
      "8  bow: percent university tuesday staff president pulled wrote raised miller craig\n",
      "8  sent: prue schlussel schlussel icebreaking pony darby pony darby pony darby pony darby pony darby pony darby culvert darby pony darby darby pony culvert darby darby pony darby darby pony culvert darby darby pony darby darby pony iffland iffland iffland darby\n",
      "9  bow: convention national days gathering driver hours expected blue journal commit\n",
      "9  sent: marrero marrero diaz diaz processes processes processes processes processes processes processes truckloads processes processes processes processes processes processes processes processes processes processes processes processes processes processes processes processes processes processes processes processes processes processes processes processes processes processes processes processes\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-50e0e0534bec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_recon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_kl_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_reg_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_recon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_kl_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_kl_categ_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_kl_gmm_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppls_batch\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_recon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_kl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_recon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_kl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_kl_categ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_kl_gmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_ppls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msent_loss_kl_batch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if len(logs) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "while epoch < config.epochs:\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = get_feed_dict(batch)\n",
    "\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, sent_loss_kl_categ_batch, sent_loss_kl_gmm_batch, ppls_batch = \\\n",
    "        sess.run([opt, loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, sent_loss_kl_categ, sent_loss_kl_gmm, topic_ppls], feed_dict = feed_dict)\n",
    "   \n",
    "        if sent_loss_kl_batch == np.inf:\n",
    "            print('Nan occured')\n",
    "            ckpt = tf.train.get_checkpoint_state(config.modeldir)\n",
    "            model_checkpoint_path = ckpt.all_model_checkpoint_paths[-1]\n",
    "            saver.restore(sess, model_checkpoint_path)            \n",
    "            break\n",
    "            \n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if ct%config.log_period==0:\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train, sent_loss_recon_train, sent_loss_kl_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_dev, sent_loss_dev, ppl_dev = get_loss(sess, dev_batches)\n",
    "\n",
    "            if config.warmup: beta_eval = beta.eval(session=sess)\n",
    "            global_step_log = sess.run(tf.train.get_global_step())            \n",
    "            \n",
    "#             if loss_dev < loss_min:\n",
    "#                 loss_min = loss_dev\n",
    "#                 saver.save(sess, config.modelpath, global_step=global_step_log)\n",
    "\n",
    "            clear_output()\n",
    "\n",
    "            time_finish = time.time()\n",
    "            time_log = int(time_finish - time_start)\n",
    "            logs += [(time_log, epoch, ct, loss_train, ppl_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train, sent_loss_recon_train, sent_loss_kl_train, loss_dev, ppl_dev, topic_loss_dev, sent_loss_dev, beta_eval)]\n",
    "            for log in logs:\n",
    "                print('%03d[s], Ep: %02d, Ct: %05d|TR LOSS: %.0f, PPL: %.0f|TM NLL: %.0f, KL: %.2f, REG:%.2f | LM NLL: %.2f, KL: %.2f|DE LOSS: %.0f, PPL: %.0f, TM: %.0f, LM: %.2f|BETA: %.6f' %  log)\n",
    "\n",
    "            print_sample(batch)\n",
    "\n",
    "            time_start = time.time()\n",
    "            \n",
    "            print_topic_sample()\n",
    "                \n",
    "    epoch += 1\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364.04877"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364.04126"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_loss_recon_batch+topic_loss_kl_batch+topic_loss_reg_batch+sent_loss_recon_batch\n",
    "# +sent_loss_kl_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_prob_topic, _prob_topic_sents, _prob_topic_infer, _means_topic_infer = debug_value([prob_topic, prob_topic_sents, prob_topic_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_i = 4\n",
    "_prob_topic_sents[batch_i], _prob_topic_infer[batch_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_means_topic_infer[0][:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_means_topic, b_means_topic = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"topic/dec/mean_topic\")\n",
    "\n",
    "pred_topic_embeddings, pred_topic_bow, pred_means_topic, pred_logvars_topic, pred_token_idxs, _w_means_topic, _b_means_topic, _w_mean_topic_infer = \\\n",
    "                                sess.run([topic_embeddings, topic_bow, means_topic, logvars_topic, topic_beam_output_token_idxs, w_means_topic, b_means_topic, w_mean_topic_infer], \n",
    "                                         feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "\n",
    "pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "\n",
    "pred_topics_freq_bow_indices = np.argsort(pred_topic_bow, 1)[:, ::-1][:, :10]\n",
    "pred_topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idxs in pred_topics_freq_bow_idxs:\n",
    "    print([idx_to_word[idx] for idx in idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_topic_embeddings[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_topic_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_w_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_w_mean_topic_infer[:, :10, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_enc_state_infer, _means_topic_infer = debug_value([enc_state_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_enc_state_infer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_means_topic_infer[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
