{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from data_structure import get_batches\n",
    "from hntm import HierarchicalNeuralTopicModel\n",
    "from nhdp import nestedHierarchicalNeuralTopicModel\n",
    "from tsgntm import TreeStructuredGaussianNeuralTopicModel\n",
    "from tree import get_descendant_idxs\n",
    "from evaluation import get_hierarchical_affinity, get_topic_specialization, print_topic_sample, print_topic_year\n",
    "from configure import get_config\n",
    "\n",
    "pd.set_option('display.max_columns', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(nb_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu\n",
    "np.random.seed(config.seed)\n",
    "random.seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.path_data,'rb'))\n",
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)\n",
    "config.dim_bow = len(bow_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     6
    ]
   },
   "outputs": [],
   "source": [
    "def debug(variable, sample_batch=None):\n",
    "    if sample_batch is None: sample_batch = test_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch, mode='test')\n",
    "    _variable = sess.run(variable, feed_dict=feed_dict)\n",
    "    return _variable\n",
    "\n",
    "def check(variable):\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sample_batch = test_batches[0]\n",
    "    feed_dict = model.get_feed_dict(sample_batch, mode='test', assertion=True)\n",
    "    _variable = sess.run(variable, feed_dict=feed_dict)\n",
    "    return _variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "TopicModels = {'hntm': HierarchicalNeuralTopicModel, 'nhdp': nestedHierarchicalNeuralTopicModel, 'tsgntm': TreeStructuredGaussianNeuralTopicModel}\n",
    "TopicModel = TopicModels[config.model]\n",
    "model = TopicModel(config)\n",
    "sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(max_to_keep=config.max_to_keep)\n",
    "update_tree_flg = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     24
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = []\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "ppl_min = np.inf\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','','','','','','VALID:','','','','','','TEST:','', 'SPEC:', '', '', 'HIER:', ''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL', 'GAUSS', 'REG','LOSS','PPL','NLL','KL', 'GAUSS','REG','LOSS','PPL', '1', '2', '3', 'CHILD', 'OTHER']]))))\n",
    "\n",
    "cmd_rm = 'rm -r %s' % config.dir_model\n",
    "res = subprocess.call(cmd_rm.split())\n",
    "cmd_mk = 'mkdir %s' % config.dir_model\n",
    "res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "def update_checkpoint(config, checkpoint, global_step):\n",
    "    checkpoint.append(config.path_model + '-%i' % global_step)\n",
    "    if len(checkpoint) > config.max_to_keep:\n",
    "        path_model = checkpoint.pop(0) + '.*'\n",
    "        for p in glob.glob(path_model):\n",
    "            os.remove(p)\n",
    "    cPickle.dump(checkpoint, open(config.path_checkpoint, 'wb'))\n",
    "    \n",
    "def validate(sess, batches, model):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    prob_topic_list = []\n",
    "    n_bow_list = []\n",
    "    n_topics_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = model.get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch, ppls_batch, prob_topic_batch, n_bow_batch, n_topics_batch \\\n",
    "            = sess.run([model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_gauss, model.topic_loss_reg, model.topic_ppls, model.prob_topic, model.n_bow, model.n_topics], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "        prob_topic_list.append(prob_topic_batch)\n",
    "        n_bow_list.append(n_bow_batch)\n",
    "        n_topics_list.append(n_topics_batch)\n",
    "    loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_gauss_mean, topic_loss_reg_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    \n",
    "    probs_topic = np.concatenate(prob_topic_list, 0)\n",
    "    \n",
    "    n_bow = np.concatenate(n_bow_list, 0)\n",
    "    n_topics = np.concatenate(n_topics_list, 0)\n",
    "    probs_topic_mean = np.sum(n_topics, 0) / np.sum(n_bow)\n",
    "    \n",
    "    return loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_gauss_mean, topic_loss_reg_mean, ppl_mean, probs_topic_mean    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train & validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th colspan=\"5\" halign=\"left\"></th>\n",
       "      <th>VALID:</th>\n",
       "      <th colspan=\"5\" halign=\"left\"></th>\n",
       "      <th>TEST:</th>\n",
       "      <th></th>\n",
       "      <th>SPEC:</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "      <th>HIER:</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>GAUSS</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>GAUSS</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>CHILD</th>\n",
       "      <th>OTHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>999</td>\n",
       "      <td>1361.66</td>\n",
       "      <td>1686</td>\n",
       "      <td>1353.42</td>\n",
       "      <td>2.94</td>\n",
       "      <td>4.95</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1299.60</td>\n",
       "      <td>1547</td>\n",
       "      <td>1291.75</td>\n",
       "      <td>2.20</td>\n",
       "      <td>5.36</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1302.12</td>\n",
       "      <td>1525</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>1999</td>\n",
       "      <td>1351.68</td>\n",
       "      <td>1596</td>\n",
       "      <td>1343.68</td>\n",
       "      <td>2.99</td>\n",
       "      <td>4.66</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1296.18</td>\n",
       "      <td>1529</td>\n",
       "      <td>1287.80</td>\n",
       "      <td>4.15</td>\n",
       "      <td>3.83</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1295.76</td>\n",
       "      <td>1485</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>295</td>\n",
       "      <td>1347.32</td>\n",
       "      <td>1555</td>\n",
       "      <td>1339.22</td>\n",
       "      <td>3.27</td>\n",
       "      <td>4.48</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1287.50</td>\n",
       "      <td>1458</td>\n",
       "      <td>1278.67</td>\n",
       "      <td>4.24</td>\n",
       "      <td>4.34</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1292.52</td>\n",
       "      <td>1457</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>1295</td>\n",
       "      <td>1343.89</td>\n",
       "      <td>1529</td>\n",
       "      <td>1335.72</td>\n",
       "      <td>3.43</td>\n",
       "      <td>4.40</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1284.97</td>\n",
       "      <td>1431</td>\n",
       "      <td>1276.39</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.29</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1289.12</td>\n",
       "      <td>1426</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>2295</td>\n",
       "      <td>1341.35</td>\n",
       "      <td>1509</td>\n",
       "      <td>1333.08</td>\n",
       "      <td>3.55</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1285.70</td>\n",
       "      <td>1435</td>\n",
       "      <td>1276.00</td>\n",
       "      <td>4.75</td>\n",
       "      <td>4.55</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1289.12</td>\n",
       "      <td>1426</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6000</th>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>591</td>\n",
       "      <td>1340.01</td>\n",
       "      <td>1494</td>\n",
       "      <td>1331.56</td>\n",
       "      <td>3.66</td>\n",
       "      <td>4.45</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1285.56</td>\n",
       "      <td>1433</td>\n",
       "      <td>1275.84</td>\n",
       "      <td>4.44</td>\n",
       "      <td>4.92</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1289.12</td>\n",
       "      <td>1426</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     TRAIN:                                    VALID:        \\\n",
       "     Time Ep    Ct     LOSS   PPL      NLL    KL GAUSS   REG     LOSS   PPL   \n",
       "1000   64  0   999  1361.66  1686  1353.42  2.94  4.95  0.35  1299.60  1547   \n",
       "2000   56  0  1999  1351.68  1596  1343.68  2.99  4.66  0.35  1296.18  1529   \n",
       "3000   57  1   295  1347.32  1555  1339.22  3.27  4.48  0.35  1287.50  1458   \n",
       "4000   57  1  1295  1343.89  1529  1335.72  3.43  4.40  0.35  1284.97  1431   \n",
       "5000   57  1  2295  1341.35  1509  1333.08  3.55  4.38  0.35  1285.70  1435   \n",
       "6000   52  2   591  1340.01  1494  1331.56  3.66  4.45  0.34  1285.56  1433   \n",
       "\n",
       "                                   TEST:       SPEC:             HIER:        \n",
       "          NLL    KL GAUSS   REG     LOSS   PPL     1     2     3 CHILD OTHER  \n",
       "1000  1291.75  2.20  5.36  0.29  1302.12  1525  0.21  0.13  0.14  0.96  0.88  \n",
       "2000  1287.80  4.15  3.83  0.40  1295.76  1485  0.17  0.11  0.14  0.95  0.86  \n",
       "3000  1278.67  4.24  4.34  0.25  1292.52  1457  0.31  0.19  0.16  0.94  0.78  \n",
       "4000  1276.39  4.05  4.29  0.24  1289.12  1426  0.46  0.22  0.18  0.93  0.80  \n",
       "5000  1276.00  4.75  4.55  0.41  1289.12  1426  0.23  0.21  0.20  0.93  0.79  \n",
       "6000  1275.84  4.44  4.92  0.36  1289.12  1426  0.23  0.18  0.16  0.96  0.81  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 R: 1.000 P: 0.018 great place time good service back love staff friendly nice\n",
      "   1 R: 0.593 P: 0.097 good place food great service back time friendly love staff\n",
      "     11 R: 0.190 P: 0.190 food good chicken place restaurant service delicious great ordered fresh\n",
      "     12 R: 0.083 P: 0.083 food good place service great back delicious ordered time chicken\n",
      "     13 R: 0.056 P: 0.056 food good place great service time back friendly nice ordered\n",
      "     14 R: 0.050 P: 0.050 food good service place great back chicken time ordered restaurant\n",
      "     15 R: 0.118 P: 0.118 pizza food good great cheese service fries burger salad menu\n",
      "   2 R: 0.197 P: 0.021 place good great love back service staff time friendly nice\n",
      "     21 R: 0.043 P: 0.043 place coffee great good drinks love cream delicious bar ice\n",
      "     22 R: 0.082 P: 0.082 coffee place great cream ice good shop drinks nice store\n",
      "     23 R: 0.018 P: 0.018 place great good love time friendly nice service staff back\n",
      "     24 R: 0.015 P: 0.015 place great good time nice love staff service friendly back\n",
      "     25 R: 0.019 P: 0.019 place great good time love back friendly nice service staff\n",
      "   3 R: 0.007 P: 0.001 great good place time back service food love friendly nice\n",
      "     31 R: 0.002 P: 0.002 great service good place time back nice food amazing friendly\n",
      "     32 R: 0.001 P: 0.001 great place good time service back friendly love staff nice\n",
      "     33 R: 0.001 P: 0.001 good place great back service time love staff friendly nice\n",
      "     34 R: 0.001 P: 0.001 great place time good service back staff love nice friendly\n",
      "     35 R: 0.002 P: 0.002 place time great love back service nice staff friendly good\n",
      "   4 R: 0.008 P: 0.001 place great time good back love service friendly staff nice\n",
      "     41 R: 0.002 P: 0.002 great place service time good back staff love friendly nice\n",
      "     42 R: 0.002 P: 0.002 great time place service good back nice friendly staff love\n",
      "     43 R: 0.001 P: 0.001 great place good love back time nice friendly service clean\n",
      "     44 R: 0.001 P: 0.001 place great time good service friendly love back staff nice\n",
      "     45 R: 0.002 P: 0.002 place great time back service good love friendly nice staff\n",
      "   5 R: 0.177 P: 0.019 great time car service back job recommend staff clean hair\n",
      "     51 R: 0.045 P: 0.045 nails job time great nail salon car place back clean\n",
      "     52 R: 0.030 P: 0.030 great place time service back good staff nice friendly love\n",
      "     53 R: 0.021 P: 0.021 time great back service place staff friendly recommend good love\n",
      "     54 R: 0.019 P: 0.019 great service time place back friendly staff car nice good\n",
      "     55 R: 0.043 P: 0.043 great time service back nails salon job experience place car\n",
      "[31.502605 29.347668 31.31292  31.52913  31.545929 32.       26.445423\n",
      " 29.39727  29.358372 29.467062 28.777594 31.06486  31.178837 31.24517\n",
      " 31.30726  31.322214 31.373737 31.517872 31.50816  31.514172 31.513716\n",
      " 31.442286 31.543047 31.539946 31.54129  31.527689 32.       32.\n",
      " 32.       32.       32.      ]\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "while epoch < config.n_epochs:\n",
    "    # train\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = model.get_feed_dict(batch)\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch, ppls_batch, global_step_log = \\\n",
    "        sess.run([model.opt, model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_gauss, model.topic_loss_reg, model.topic_ppls, tf.train.get_global_step()], feed_dict = feed_dict)\n",
    "\n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "#         if global_step_log % config.log_period == 0:\n",
    "        if global_step_log % 1000 == 0:\n",
    "            # validate\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_gauss_train, topic_loss_reg_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_gauss_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "\n",
    "            # test\n",
    "            if ppl_dev < ppl_min:\n",
    "                ppl_min = ppl_dev\n",
    "                loss_test, _, _, _, _, ppl_test, _ = validate(sess, test_batches, model)\n",
    "                saver.save(sess, config.path_model, global_step=global_step_log)\n",
    "                cPickle.dump(config, open(config.path_config % global_step_log, 'wb'))\n",
    "                update_checkpoint(config, checkpoint, global_step_log)\n",
    "            \n",
    "            # visualize topic\n",
    "            topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "            topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "            topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "            topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "            descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "            recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "            \n",
    "            depth_specs = get_topic_specialization(sess, model, instances_test)\n",
    "            hierarchical_affinities = get_hierarchical_affinity(sess, model)\n",
    "            \n",
    "            # log\n",
    "            clear_output()\n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_gauss_train, '%.2f'%topic_loss_reg_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_gauss_dev, '%.2f'%topic_loss_reg_dev, \\\n",
    "                    '%.2f'%loss_test, '%.0f'%ppl_test, \\\n",
    "                    '%.2f'%depth_specs[1], '%.2f'%depth_specs[2], '%.2f'%depth_specs[3], \\\n",
    "                    '%.2f'%hierarchical_affinities[0], '%.2f'%hierarchical_affinities[1]],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "            cPickle.dump(log_df, open(os.path.join(config.path_log), 'wb'))\n",
    "            print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)\n",
    "            print(np.sum(debug(model.topic_logvars), 1))\n",
    "            \n",
    "            # update tree\n",
    "            if not config.static:\n",
    "                config.tree_idxs, update_tree_flg = model.update_tree(topic_prob_topic, recur_prob_topic)\n",
    "                if update_tree_flg:\n",
    "                    print(config.tree_idxs)\n",
    "                    name_variables = {tensor.name: variable for tensor, variable in zip(tf.global_variables(), sess.run(tf.global_variables()))} # store paremeters\n",
    "                    if 'sess' in globals(): sess.close()\n",
    "                    model = HierarchicalNeuralTopicModel(config)\n",
    "                    sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "                    name_tensors = {tensor.name: tensor for tensor in tf.global_variables()}\n",
    "                    sess.run([name_tensors[name].assign(variable) for name, variable in name_variables.items()]) # restore parameters\n",
    "                    saver = tf.train.Saver(max_to_keep=1)\n",
    "                \n",
    "            time_start = time.time()\n",
    "\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "    epoch += 1\n",
    "\n",
    "loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "display(log_df)\n",
    "print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_year(sample_batches):\n",
    "    probs_topics = []\n",
    "    years = []\n",
    "    for i, sample_batch in sample_batches:\n",
    "        probs_topics_batch = sess.run(model.prob_topic, feed_dict=model.get_feed_dict(sample_batch, mode='test'))\n",
    "        years_batch = [instance.year for instance in sample_batch]\n",
    "        probs_topics += [probs_topics_batch]\n",
    "        years += years_batch\n",
    "    probs_topics = np.concatenate(probs_topics)\n",
    "    years = np.array(years)\n",
    "\n",
    "    topic_years = years.dot(probs_topics) / np.sum(probs_topics, 0)\n",
    "    topic_year = {model.topic_idxs[i]: year for i, year in enumerate(topic_years)}\n",
    "    return topic_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Avg Year: 2005 level analysis english systems domain lexical linguistic test process lexicon\n",
      "   1 Avg Year: 2000 grammar structure rules semantic form representation rule lexical type structures\n",
      "     11 Avg Year: 2006 verb semantic syntactic verbs event relations discourse argument relation arguments\n",
      "     12 Avg Year: 2007 tree parsing dependency parser grammar trees node parse rules syntactic\n",
      "   2 Avg Year: 2008 english languages dictionary errors morphological chinese lexical rules corpora pos\n",
      "     21 Avg Year: 2009 entity sense relations relation semantic entities wordnet senses lexical patterns\n",
      "     22 Avg Year: 2010 translation english source alignment phrase target languages sentences systems parallel\n",
      "   3 Avg Year: 2008 models probability training algorithm probabilities search parameters sequence segmentation gram\n",
      "     31 Avg Year: 2011 features feature training performance learning classifier classification accuracy test class\n",
      "     32 Avg Year: 2015 models network vector embeddings neural training vectors representations embedding input\n",
      "   4 Avg Year: 2007 knowledge semantic terms term natural values context type representation concept\n",
      "     41 Avg Year: 2009 question query terms documents answer questions document domain term web\n",
      "     42 Avg Year: 2010 similarity topic document sentences method score scores measure clustering pairs\n",
      "   5 Avg Year: 2007 user annotation resources project tools tool database research interface linguistic\n",
      "     51 Avg Year: 2007 speech dialogue user speaker utterance utterances recognition spoken human speakers\n",
      "     52 Avg Year: 2013 sentiment tweets polarity twitter negative opinion positive social emotion annotators\n"
     ]
    }
   ],
   "source": [
    "sample_batches = get_batches(instances_train, config.batch_size)\n",
    "topic_year = get_topic_year(sample_batches)\n",
    "print_topic_year(sess, model, topic_freq_tokens=topic_freq_tokens, topic_year=topic_year)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
