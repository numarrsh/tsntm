{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from data_structure import get_batches, get_test_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "from topic_model import TopicModel\n",
    "\n",
    "from topic_beam_search_decoder import BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '4', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/bags/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/tglm_vae', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'bags', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 50, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 500, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.1, 'lr')\n",
    "flags.DEFINE_float('reg', 1., 'regularization term')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_bool('warmup', True, 'flg of warming up')\n",
    "flags.DEFINE_integer('epochs_cycle', 5, 'number of epochs within a cycle')\n",
    "flags.DEFINE_float('r_cycle', 0.5, 'proportion used to increase beta within a cycle')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 30, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_bow', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_hidden_topic', 512, 'dim_hidden_topic')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "flags.DEFINE_bool('bidirectional', True, 'flg of bidirectional encoding')\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_test_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('PAD_IDX', word_to_idx[PAD], 'PAD_IDX')\n",
    "flags.DEFINE_integer('UNK_IDX', word_to_idx[UNK], 'UNK_IDX')\n",
    "flags.DEFINE_integer('BOS_IDX', word_to_idx[BOS], 'BOS_IDX')\n",
    "flags.DEFINE_integer('EOS_IDX', word_to_idx[EOS], 'EOS_IDX')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "maximum_iterations = max([max([instance.max_sent_l for instance in batch]) for ct, batch in dev_batches])\n",
    "flags.DEFINE_integer('maximum_iterations', maximum_iterations, 'maximum_iterations')\n",
    "\n",
    "flags.DEFINE_integer('cycle_steps', len(train_batches)*config.epochs_cycle, 'number of steps for each cycle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build language model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['bow'] = tf.placeholder(tf.float32, [None, config.dim_bow], name='bow')\n",
    "t_variables['input_token_idxs'] = tf.placeholder(tf.int32, [None, None], name='input_token_idxs')\n",
    "t_variables['dec_input_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_input_idxs')\n",
    "t_variables['dec_target_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_target_idxs')\n",
    "t_variables['batch_l'] = tf.placeholder(tf.int32, name='batch_l')\n",
    "t_variables['doc_l'] = tf.placeholder(tf.int32, [None], name='doc_l')\n",
    "t_variables['sent_l'] = tf.placeholder(tf.int32, [None], name='sent_l')\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train', assertion=False):\n",
    "    def token_dropout(sent_idxs):\n",
    "        sent_idxs_dropout = np.asarray(sent_idxs)\n",
    "        sent_idxs_dropout[np.random.rand(len(sent_idxs)) > config.word_keep_prob] = config.UNK_IDX\n",
    "        return list(sent_idxs_dropout)\n",
    "\n",
    "    bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "    \n",
    "    doc_l = np.array([len(instance.token_idxs) for instance in batch])\n",
    "    \n",
    "    feed_input_token_idxs_list = [sent_idxs for instance in batch for sent_idxs in instance.token_idxs]\n",
    "    feed_dec_input_idxs_list = [[config.BOS_IDX] + token_dropout(sent_idxs) for sent_idxs in feed_input_token_idxs_list]\n",
    "    feed_dec_target_idxs_list = [sent_idxs + [config.EOS_IDX]  for sent_idxs in feed_input_token_idxs_list]\n",
    "        \n",
    "    sent_l = np.array([len(sent_idxs) for sent_idxs in feed_input_token_idxs_list], np.int32)\n",
    "    batch_l = len(sent_l)\n",
    "    \n",
    "    feed_input_token_idxs = pad_sequences(feed_input_token_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_input_idxs = pad_sequences(feed_dec_input_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_target_idxs = pad_sequences(feed_dec_target_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    \n",
    "    if assertion:\n",
    "        index = 0\n",
    "        for instance in batch:\n",
    "            for line_idxs in instance.token_idxs:\n",
    "                assert feed_input_token_idxs_list[index] == line_idxs\n",
    "                index += 1\n",
    "        assert feed_input_token_idxs.shape[1] == np.max(sent_l)\n",
    "        assert feed_dec_input_idxs.shape[1] == np.max(sent_l) + 1\n",
    "        assert feed_dec_target_idxs.shape[1] == np.max(sent_l) + 1\n",
    "    \n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "\n",
    "    feed_dict = {\n",
    "                t_variables['bow']: bow, \n",
    "                t_variables['batch_l']: batch_l, t_variables['doc_l']: doc_l, t_variables['sent_l']: sent_l, \n",
    "                t_variables['input_token_idxs']: feed_input_token_idxs, t_variables['dec_input_idxs']: feed_dec_input_idxs, t_variables['dec_target_idxs']: feed_dec_target_idxs, \n",
    "                t_variables['keep_prob']: keep_prob\n",
    "    }\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     10,
     24,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables):\n",
    "    sample_batch = dev_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "def check_shape(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()\n",
    "    \n",
    "def check_value(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()    \n",
    "    \n",
    "# sent_loss_kl_categ_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, tf_log(prob_topic_infer/prob_topic_sents)), 1))\n",
    "# debug_value([sent_loss_kl_categ, sent_loss_kl_categ_tmp])\n",
    "# sent_loss_kl_gauss_tmp = 0.5 * tf.reduce_sum(tf.exp(logvars_topic_infer-logvars_topic) + tf.square(means_topic - means_topic_infer) / tf.exp(logvars_topic) - 1 + (logvars_topic - logvars_topic_infer), -1)\n",
    "# sent_loss_kl_gmm_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss_tmp), -1))\n",
    "# debug_value([sent_loss_kl_gmm_tmp, sent_loss_kl_gmm])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encode bow\n",
    "with tf.variable_scope('topic/enc', reuse=False):\n",
    "    hidden_bow_ = tf.layers.Dense(units=config.dim_hidden_bow, activation=tf.nn.relu, name='hidden_bow')(t_variables['bow'])\n",
    "    hidden_bow = tf.layers.Dropout(t_variables['keep_prob'])(hidden_bow_)\n",
    "    means_bow = tf.layers.Dense(units=config.dim_latent_bow, name='mean_bow')(hidden_bow)\n",
    "    logvars_bow = tf.layers.Dense(units=config.dim_latent_bow, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_bow)\n",
    "    latents_bow = sample_latents(means_bow, logvars_bow) # sample latent vectors\n",
    "\n",
    "    prob_topic = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic')(latents_bow) # inference of topic probabilities\n",
    "\n",
    "# decode bow\n",
    "with tf.variable_scope('shared', reuse=False):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "bow_embeddings = tf.nn.embedding_lookup(embeddings, bow_idxs) # embeddings of each bow features\n",
    "\n",
    "with tf.variable_scope('topic/dec', reuse=False):\n",
    "    topic_embeddings = tf.get_variable('topic_emb', [config.n_topic, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of topics\n",
    "\n",
    "    topic_bow = tf.nn.softmax(tf.matmul(topic_embeddings, bow_embeddings, transpose_b=True), 1) # bow vectors for each topic\n",
    "    logits_bow = tf_log(tf.matmul(prob_topic, topic_bow)) # predicted bow distribution\n",
    "\n",
    "    # prior of each gaussian distribution (computed for each topic)\n",
    "    hidden_topic = tf.layers.Dense(units=config.dim_hidden_topic, activation=tf.nn.relu, name='hidden_topic')(topic_bow)\n",
    "    means_topic = tf.layers.Dense(units=config.dim_latent, name='mean_topic')(hidden_topic)\n",
    "    logvars_topic = tf.layers.Dense(units=config.dim_latent, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_topic)\n",
    "    sigma_topic = tf.exp(0.5 * logvars_topic)\n",
    "    gauss_topic = tfd.Normal(loc=means_topic, scale=sigma_topic)    \n",
    "    \n",
    "# define losses\n",
    "topic_losses_recon = -tf.reduce_sum(tf.multiply(t_variables['bow'], logits_bow), 1)\n",
    "topic_loss_recon = tf.reduce_mean(topic_losses_recon) # negative log likelihood of each words\n",
    "\n",
    "topic_loss_kl = compute_kl_loss(means_bow, logvars_bow) # KL divergence b/w latent dist & gaussian std\n",
    "\n",
    "topic_bow_norm = topic_bow / tf.norm(topic_bow, axis=1, keepdims=True)\n",
    "topic_dots = tf.clip_by_value(tf.matmul(topic_bow_norm, tf.transpose(topic_bow_norm)), -1., 1.)\n",
    "topic_loss_reg = tf.reduce_mean(tf.square(topic_dots - tf.eye(config.n_topic)))\n",
    "# topic_angles = tf.acos(topic_dots)\n",
    "# topic_angles_mean = tf.reduce_mean(topic_angles)\n",
    "# topic_angles_vars = tf.reduce_mean(tf.square(topic_angles - topic_angles_mean))\n",
    "# topic_loss_reg = tf.exp(topic_angles_vars - topic_angles_mean)\n",
    "\n",
    "# monitor\n",
    "n_bow = tf.reduce_sum(t_variables['bow'], 1)\n",
    "topic_ppls = tf.divide(topic_losses_recon, tf.maximum(1e-5, n_bow))\n",
    "topics_freq_bow_indices = tf.nn.top_k(topic_bow, 10, name='topic_freq_bow').indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "input_token_idxs = t_variables['input_token_idxs']\n",
    "batch_l = t_variables['batch_l']\n",
    "sent_l = t_variables['sent_l']\n",
    "max_sent_l = tf.reduce_max(sent_l)\n",
    "\n",
    "with tf.variable_scope('sent/enc', reuse=False):\n",
    "    # get word embedding\n",
    "    enc_input = tf.nn.embedding_lookup(embeddings, input_token_idxs)\n",
    "\n",
    "    # get sentence embedding\n",
    "    _, enc_state = dynamic_bi_rnn(enc_input, sent_l, config.dim_hidden, t_variables['keep_prob'])\n",
    "\n",
    "    # TODO House Holder flow\n",
    "    hidden_topic_infer =  tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='hidden_topic_infer')(enc_state)\n",
    "    prob_topic_infer = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic_infer')(hidden_topic_infer)\n",
    "\n",
    "    w_mean_topic_infer = tf.get_variable('mean_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32)\n",
    "    b_mean_topic_infer = tf.get_variable('mean_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32)\n",
    "    means_topic_infer = tf.tensordot(enc_state, w_mean_topic_infer, axes=[[1], [1]]) + b_mean_topic_infer\n",
    "    \n",
    "    w_logvar_topic_infer = tf.get_variable('logvar_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    b_logvar_topic_infer = tf.get_variable('logvar_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    logvars_topic_infer = tf.tensordot(enc_state, w_logvar_topic_infer, axes=[[1], [1]]) + b_logvar_topic_infer\n",
    "    sigma_topic_infer = tf.exp(0.5 * logvars_topic_infer)\n",
    "    gauss_topic_infer = tfd.Normal(loc=means_topic_infer, scale=sigma_topic_infer)\n",
    "    \n",
    "    # latent vectors from each gaussian dist.\n",
    "    latents_topic_infer = sample_latents(means_topic_infer, logvars_topic_infer) \n",
    "    # latent vector from gaussian mixture\n",
    "    latents_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), latents_topic_infer, transpose_a=True)\n",
    "    \n",
    "    # for beam search\n",
    "    means_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), means_topic_infer, transpose_a=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for decoding\n",
    "dec_sent_l = tf.add(sent_l, 1)\n",
    "dec_input_idxs = t_variables['dec_input_idxs']\n",
    "dec_input = tf.nn.embedding_lookup(embeddings, dec_input_idxs)\n",
    "\n",
    "dec_latents_input = tf.tile(latents_input, [1, tf.shape(dec_input)[1], 1])\n",
    "dec_concat_input = tf.concat([dec_input, dec_latents_input], -1)\n",
    "\n",
    "# decode for training\n",
    "with tf.variable_scope('sent/dec/rnn', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32, reuse=False):\n",
    "    dec_cell = tf.contrib.rnn.GRUCell(config.dim_hidden)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob = t_variables['keep_prob'])\n",
    "\n",
    "    dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(latents_input, 1))\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_concat_input, sequence_length=dec_sent_l)\n",
    "\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=dec_cell,\n",
    "        helper=helper,\n",
    "        initial_state=dec_initial_state)\n",
    "\n",
    "    dec_outputs, _, output_sent_l = tf.contrib.seq2seq.dynamic_decode(train_decoder)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(config.n_vocab, use_bias=False, name='out')\n",
    "    output_logits = output_layer(dec_outputs.rnn_output)\n",
    "    \n",
    "    output_token_idxs = tf.argmax(output_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tokens = tf.fill([batch_l], config.BOS_IDX)\n",
    "end_token = config.EOS_IDX\n",
    "\n",
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    infer_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(means_input, 1))\n",
    "    beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(infer_dec_initial_state, multiplier=config.beam_width)\n",
    "    beam_latents_input = tf.contrib.seq2seq.tile_batch(tf.squeeze(means_input, 1), multiplier=config.beam_width) # added\n",
    "    \n",
    "    beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=beam_latents_input)\n",
    "\n",
    "    beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    beam_output_token_idxs = beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    inter_means_input = tf.placeholder(tf.float32, [None, config.dim_latent])\n",
    "    \n",
    "    inter_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(inter_means_input)\n",
    "    inter_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(inter_dec_initial_state, multiplier=config.beam_width)\n",
    "    inter_beam_latents_input = tf.contrib.seq2seq.tile_batch(inter_means_input, multiplier=config.beam_width) # added\n",
    "    \n",
    "    inter_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=inter_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=inter_beam_latents_input)\n",
    "\n",
    "    inter_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inter_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    inter_beam_output_token_idxs = inter_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    topic_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic)\n",
    "    topic_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(topic_dec_initial_state, multiplier=config.beam_width)\n",
    "    topic_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic, multiplier=config.beam_width) # added\n",
    "    \n",
    "    topic_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=topic_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=topic_beam_latents_input)\n",
    "\n",
    "    topic_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        topic_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    topic_beam_output_token_idxs = topic_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    means_topic_summary = tf.reduce_mean(means_topic_infer, 0)\n",
    "    \n",
    "    summary_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic_summary)\n",
    "    summary_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(summary_dec_initial_state, multiplier=config.beam_width)\n",
    "    summary_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic_summary, multiplier=config.beam_width) # added\n",
    "    \n",
    "    summary_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=summary_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width,\n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=summary_beam_latents_input)\n",
    "\n",
    "    summary_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        summary_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    summary_beam_output_token_idxs = summary_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language modeling cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target and mask\n",
    "dec_target_idxs = t_variables['dec_target_idxs']\n",
    "dec_mask_tokens = tf.sequence_mask(dec_sent_l, maxlen=max_sent_l+1, dtype=tf.float32)\n",
    "\n",
    "# nll for each token (averaged over batch & sentence)\n",
    "sent_loss_recon = tf.contrib.seq2seq.sequence_loss(output_logits, dec_target_idxs, dec_mask_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_l = t_variables['doc_l']\n",
    "mask_sents = tf.sequence_mask(doc_l)\n",
    "mask_sents_flatten = tf.reshape(mask_sents, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1]])\n",
    "\n",
    "prob_topic_tiled = tf.tile(tf.expand_dims(prob_topic, 1), [1, tf.shape(mask_sents)[1], 1])\n",
    "prob_topic_flatten = tf.reshape(prob_topic_tiled, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1], config.n_topic])\n",
    "prob_topic_sents = tf.boolean_mask(prob_topic_flatten, mask_sents_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferred mixture probabilities (computed for each sentence)\n",
    "categ_topic_infer = tfd.Categorical(probs=prob_topic_infer)\n",
    "\n",
    "# prior of mixture probabilities (computed for each document, tiled for each sentence)\n",
    "categ_topic = tfd.Categorical(probs=prob_topic_sents)\n",
    "\n",
    "sent_loss_kl_categ = tf.reduce_mean(tfd.kl_divergence(categ_topic_infer, categ_topic))\n",
    "\n",
    "# inference of each gaussian gaussribution (computed for each sentence)\n",
    "\n",
    "sent_loss_kl_gauss = tf.reduce_sum(tfd.kl_divergence(gauss_topic_infer, gauss_topic), -1)\n",
    "sent_loss_kl_gmm = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss), -1))\n",
    "\n",
    "sent_loss_kl = sent_loss_kl_categ + sent_loss_kl_gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "tau = tf.cast(tf.divide(tf.mod(global_step, tf.constant(config.cycle_steps)), tf.constant(config.cycle_steps)), dtype=tf.float32)\n",
    "beta = tf.minimum(1., tau/config.r_cycle)\n",
    "\n",
    "sent_loss = sent_loss_recon + beta * sent_loss_kl\n",
    "\n",
    "topic_loss = topic_loss_recon + topic_loss_kl + config.reg * topic_loss_reg\n",
    "loss = topic_loss + sent_loss\n",
    "\n",
    "# define optimizer\n",
    "if config.opt == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif config.opt == 'Adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "    \n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_sents(token_idxs, config, idx_to_word):\n",
    "    sents = []\n",
    "    for sent_idxs in token_idxs:\n",
    "        tokens = []\n",
    "        for idx in sent_idxs:\n",
    "            if idx == config.EOS_IDX: break\n",
    "            tokens.append(idx_to_word[idx])\n",
    "        sent = ' '.join(tokens)\n",
    "        sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, ppls_batch \\\n",
    "            = sess.run([loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, topic_ppls], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "    loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, sent_loss_recon_mean, sent_loss_kl_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    return loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, sent_loss_recon_mean, sent_loss_kl_mean, ppl_mean\n",
    "\n",
    "def get_all_losses(sess, batches):\n",
    "    losses = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch = \\\n",
    "        sess.run([loss, topic_loss_recon, topic_loss_kl, sent_loss_recon, sent_loss_kl], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "    print('LOSS %.2f | TM NLL: %.2f, KL: %.4f | LM NLL: %.2f, KL: %.4f' %  np.mean(losses, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_batch):\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    pred_token_idxs = sess.run(output_token_idxs, feed_dict = feed_dict)\n",
    "    true_token_idxs = [sent_idxs for instance in sample_batch for sent_idxs in instance.token_idxs]\n",
    "    \n",
    "    assert len(pred_token_idxs) == len(true_token_idxs)\n",
    "    \n",
    "    pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "    true_sents = idxs_to_sents(true_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    for i, (true_sent, pred_sent) in enumerate(zip(true_sents, pred_sents)):        \n",
    "        print(i, 'TRUE: %s' % true_sent)\n",
    "        print(i, 'PRED: %s' % pred_sent)\n",
    "\n",
    "def print_topic_sample():\n",
    "    pred_topics_freq_bow_indices, pred_topic_token_idxs = sess.run([topics_freq_bow_indices, topic_beam_output_token_idxs], \n",
    "                                                                                                           feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "    pred_topic_sents = idxs_to_sents(pred_topic_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Topic Samples-----------')\n",
    "    for i, (topic_freq_bow_idxs, pred_topic_sent) in enumerate(zip(topics_freq_bow_idxs, pred_topic_sents)):\n",
    "        print(i, ' BOW:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' SENTENCE:', pred_topic_sent)\n",
    "        \n",
    "def print_summary(test_batch):\n",
    "    feed_dict = get_feed_dict(test_batch)\n",
    "    feed_dict[t_variables['batch_l']] = config.n_topic\n",
    "    feed_dict[t_variables['keep_prob']] = 1.\n",
    "    pred_topics_freq_bow_indices, pred_summary_token_idxs = sess.run([topics_freq_bow_indices, summary_beam_output_token_idxs], feed_dict=feed_dict)\n",
    "    pred_summary_sents = idxs_to_sents(pred_summary_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Output sentences for each topic-----------')\n",
    "    print('Item idx:', test_batch[0].item_idx)\n",
    "    for i, (topic_freq_bow_idxs, pred_summary_sent) in enumerate(zip(topics_freq_bow_idxs, pred_summary_sents)):\n",
    "        print(i, ' BOW:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' SENTENCE:', pred_summary_sent)\n",
    "        \n",
    "    print('-----------Summaries-----------')\n",
    "    for i, summary in enumerate(test_batch[0].summaries):\n",
    "        print('SUMMARY %i :'%i, '\\n', summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "saver = tf.train.Saver(max_to_keep=10)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','TM','','','','LM','','VALID:','TM','','','','LM','', ''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL','REG','NLL','KL','LOSS','PPL','NLL','KL','REG','NLL','KL', 'Beta']]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>LM</th>\n",
       "      <th></th>\n",
       "      <th>VALID:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>LM</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>133.50</td>\n",
       "      <td>1036</td>\n",
       "      <td>122.98</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.97</td>\n",
       "      <td>9.12</td>\n",
       "      <td>1.15</td>\n",
       "      <td>126.60</td>\n",
       "      <td>1034</td>\n",
       "      <td>116.19</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.97</td>\n",
       "      <td>9.12</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>121.47</td>\n",
       "      <td>598</td>\n",
       "      <td>114.37</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.66</td>\n",
       "      <td>6.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>111.65</td>\n",
       "      <td>533</td>\n",
       "      <td>105.29</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.45</td>\n",
       "      <td>5.74</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>120.07</td>\n",
       "      <td>576</td>\n",
       "      <td>113.30</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.52</td>\n",
       "      <td>6.05</td>\n",
       "      <td>0.82</td>\n",
       "      <td>111.37</td>\n",
       "      <td>524</td>\n",
       "      <td>105.00</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.32</td>\n",
       "      <td>5.75</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>1500</td>\n",
       "      <td>119.50</td>\n",
       "      <td>561</td>\n",
       "      <td>112.81</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.44</td>\n",
       "      <td>5.94</td>\n",
       "      <td>0.72</td>\n",
       "      <td>110.80</td>\n",
       "      <td>504</td>\n",
       "      <td>104.32</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.22</td>\n",
       "      <td>5.62</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>119.19</td>\n",
       "      <td>549</td>\n",
       "      <td>112.51</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.38</td>\n",
       "      <td>5.86</td>\n",
       "      <td>0.65</td>\n",
       "      <td>110.46</td>\n",
       "      <td>498</td>\n",
       "      <td>104.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.18</td>\n",
       "      <td>5.48</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119.09</td>\n",
       "      <td>545</td>\n",
       "      <td>112.41</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.35</td>\n",
       "      <td>5.82</td>\n",
       "      <td>0.62</td>\n",
       "      <td>110.30</td>\n",
       "      <td>496</td>\n",
       "      <td>103.96</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.16</td>\n",
       "      <td>5.36</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2776</th>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>118.79</td>\n",
       "      <td>537</td>\n",
       "      <td>112.12</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.31</td>\n",
       "      <td>5.74</td>\n",
       "      <td>0.57</td>\n",
       "      <td>109.84</td>\n",
       "      <td>481</td>\n",
       "      <td>103.49</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.13</td>\n",
       "      <td>5.23</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3276</th>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>118.44</td>\n",
       "      <td>529</td>\n",
       "      <td>111.78</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.29</td>\n",
       "      <td>5.67</td>\n",
       "      <td>0.53</td>\n",
       "      <td>109.59</td>\n",
       "      <td>475</td>\n",
       "      <td>103.33</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.11</td>\n",
       "      <td>5.13</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3776</th>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>118.35</td>\n",
       "      <td>523</td>\n",
       "      <td>111.70</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.26</td>\n",
       "      <td>5.61</td>\n",
       "      <td>0.50</td>\n",
       "      <td>109.43</td>\n",
       "      <td>472</td>\n",
       "      <td>103.13</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.06</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4276</th>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>118.13</td>\n",
       "      <td>517</td>\n",
       "      <td>111.50</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.24</td>\n",
       "      <td>5.55</td>\n",
       "      <td>0.48</td>\n",
       "      <td>109.37</td>\n",
       "      <td>467</td>\n",
       "      <td>103.02</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4551</th>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>117.99</td>\n",
       "      <td>514</td>\n",
       "      <td>111.36</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.23</td>\n",
       "      <td>5.52</td>\n",
       "      <td>0.47</td>\n",
       "      <td>109.23</td>\n",
       "      <td>467</td>\n",
       "      <td>102.98</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4.95</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5051</th>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>117.82</td>\n",
       "      <td>510</td>\n",
       "      <td>111.19</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.21</td>\n",
       "      <td>5.47</td>\n",
       "      <td>0.45</td>\n",
       "      <td>109.14</td>\n",
       "      <td>464</td>\n",
       "      <td>102.84</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.90</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5551</th>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>117.67</td>\n",
       "      <td>505</td>\n",
       "      <td>111.05</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.20</td>\n",
       "      <td>5.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>108.98</td>\n",
       "      <td>455</td>\n",
       "      <td>102.60</td>\n",
       "      <td>1.21</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4.86</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6051</th>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>1500</td>\n",
       "      <td>117.51</td>\n",
       "      <td>502</td>\n",
       "      <td>110.89</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.19</td>\n",
       "      <td>5.39</td>\n",
       "      <td>0.42</td>\n",
       "      <td>108.98</td>\n",
       "      <td>457</td>\n",
       "      <td>102.62</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4.81</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6551</th>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>117.42</td>\n",
       "      <td>498</td>\n",
       "      <td>110.81</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.18</td>\n",
       "      <td>5.35</td>\n",
       "      <td>0.41</td>\n",
       "      <td>108.74</td>\n",
       "      <td>450</td>\n",
       "      <td>102.39</td>\n",
       "      <td>1.24</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4.77</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6826</th>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>117.38</td>\n",
       "      <td>497</td>\n",
       "      <td>110.76</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.33</td>\n",
       "      <td>0.41</td>\n",
       "      <td>108.88</td>\n",
       "      <td>455</td>\n",
       "      <td>102.53</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4.76</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7326</th>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>117.31</td>\n",
       "      <td>494</td>\n",
       "      <td>110.70</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.16</td>\n",
       "      <td>5.30</td>\n",
       "      <td>0.40</td>\n",
       "      <td>108.78</td>\n",
       "      <td>451</td>\n",
       "      <td>102.45</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4.72</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7826</th>\n",
       "      <td>72</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>117.22</td>\n",
       "      <td>491</td>\n",
       "      <td>110.61</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.15</td>\n",
       "      <td>5.27</td>\n",
       "      <td>0.39</td>\n",
       "      <td>108.66</td>\n",
       "      <td>448</td>\n",
       "      <td>102.28</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.69</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8326</th>\n",
       "      <td>61</td>\n",
       "      <td>3</td>\n",
       "      <td>1500</td>\n",
       "      <td>117.14</td>\n",
       "      <td>489</td>\n",
       "      <td>110.53</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.15</td>\n",
       "      <td>5.24</td>\n",
       "      <td>0.39</td>\n",
       "      <td>108.67</td>\n",
       "      <td>452</td>\n",
       "      <td>102.35</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.67</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8826</th>\n",
       "      <td>73</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>116.99</td>\n",
       "      <td>487</td>\n",
       "      <td>110.39</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.14</td>\n",
       "      <td>5.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>108.55</td>\n",
       "      <td>446</td>\n",
       "      <td>102.18</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.64</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9101</th>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>116.97</td>\n",
       "      <td>485</td>\n",
       "      <td>110.36</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.14</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.38</td>\n",
       "      <td>108.38</td>\n",
       "      <td>441</td>\n",
       "      <td>102.03</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.62</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9601</th>\n",
       "      <td>56</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "      <td>116.88</td>\n",
       "      <td>483</td>\n",
       "      <td>110.27</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.13</td>\n",
       "      <td>5.18</td>\n",
       "      <td>0.37</td>\n",
       "      <td>108.45</td>\n",
       "      <td>443</td>\n",
       "      <td>102.09</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10101</th>\n",
       "      <td>68</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>116.82</td>\n",
       "      <td>481</td>\n",
       "      <td>110.21</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.13</td>\n",
       "      <td>5.15</td>\n",
       "      <td>0.37</td>\n",
       "      <td>108.37</td>\n",
       "      <td>441</td>\n",
       "      <td>102.01</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.57</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10601</th>\n",
       "      <td>69</td>\n",
       "      <td>4</td>\n",
       "      <td>1500</td>\n",
       "      <td>116.74</td>\n",
       "      <td>479</td>\n",
       "      <td>110.12</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.12</td>\n",
       "      <td>5.13</td>\n",
       "      <td>0.37</td>\n",
       "      <td>108.34</td>\n",
       "      <td>438</td>\n",
       "      <td>102.00</td>\n",
       "      <td>1.42</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.55</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11101</th>\n",
       "      <td>57</td>\n",
       "      <td>4</td>\n",
       "      <td>2000</td>\n",
       "      <td>116.69</td>\n",
       "      <td>477</td>\n",
       "      <td>110.08</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.12</td>\n",
       "      <td>5.11</td>\n",
       "      <td>0.37</td>\n",
       "      <td>108.45</td>\n",
       "      <td>441</td>\n",
       "      <td>102.04</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.53</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11376</th>\n",
       "      <td>47</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>116.66</td>\n",
       "      <td>476</td>\n",
       "      <td>110.04</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.11</td>\n",
       "      <td>5.10</td>\n",
       "      <td>0.37</td>\n",
       "      <td>107.93</td>\n",
       "      <td>436</td>\n",
       "      <td>101.89</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.52</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11876</th>\n",
       "      <td>56</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>116.61</td>\n",
       "      <td>475</td>\n",
       "      <td>110.00</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.11</td>\n",
       "      <td>5.08</td>\n",
       "      <td>0.37</td>\n",
       "      <td>107.96</td>\n",
       "      <td>435</td>\n",
       "      <td>101.76</td>\n",
       "      <td>1.66</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.48</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12376</th>\n",
       "      <td>68</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>116.52</td>\n",
       "      <td>473</td>\n",
       "      <td>109.91</td>\n",
       "      <td>1.22</td>\n",
       "      <td>0.11</td>\n",
       "      <td>5.06</td>\n",
       "      <td>0.38</td>\n",
       "      <td>107.75</td>\n",
       "      <td>428</td>\n",
       "      <td>101.55</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.45</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12876</th>\n",
       "      <td>56</td>\n",
       "      <td>5</td>\n",
       "      <td>1500</td>\n",
       "      <td>116.45</td>\n",
       "      <td>471</td>\n",
       "      <td>109.85</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.04</td>\n",
       "      <td>0.38</td>\n",
       "      <td>107.77</td>\n",
       "      <td>425</td>\n",
       "      <td>101.42</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.42</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13376</th>\n",
       "      <td>56</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>116.38</td>\n",
       "      <td>469</td>\n",
       "      <td>109.77</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.02</td>\n",
       "      <td>0.38</td>\n",
       "      <td>107.88</td>\n",
       "      <td>429</td>\n",
       "      <td>101.57</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.41</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15151</th>\n",
       "      <td>59</td>\n",
       "      <td>6</td>\n",
       "      <td>1500</td>\n",
       "      <td>116.17</td>\n",
       "      <td>464</td>\n",
       "      <td>109.55</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.09</td>\n",
       "      <td>4.96</td>\n",
       "      <td>0.40</td>\n",
       "      <td>107.76</td>\n",
       "      <td>419</td>\n",
       "      <td>101.20</td>\n",
       "      <td>1.86</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.37</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15651</th>\n",
       "      <td>60</td>\n",
       "      <td>6</td>\n",
       "      <td>2000</td>\n",
       "      <td>116.13</td>\n",
       "      <td>463</td>\n",
       "      <td>109.51</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.09</td>\n",
       "      <td>4.94</td>\n",
       "      <td>0.40</td>\n",
       "      <td>107.91</td>\n",
       "      <td>423</td>\n",
       "      <td>101.33</td>\n",
       "      <td>1.86</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.35</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15926</th>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>116.12</td>\n",
       "      <td>462</td>\n",
       "      <td>109.49</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.09</td>\n",
       "      <td>4.94</td>\n",
       "      <td>0.40</td>\n",
       "      <td>107.81</td>\n",
       "      <td>422</td>\n",
       "      <td>101.24</td>\n",
       "      <td>1.86</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.33</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16426</th>\n",
       "      <td>59</td>\n",
       "      <td>7</td>\n",
       "      <td>500</td>\n",
       "      <td>116.07</td>\n",
       "      <td>460</td>\n",
       "      <td>109.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.92</td>\n",
       "      <td>0.40</td>\n",
       "      <td>107.90</td>\n",
       "      <td>421</td>\n",
       "      <td>101.28</td>\n",
       "      <td>1.87</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.33</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16926</th>\n",
       "      <td>71</td>\n",
       "      <td>7</td>\n",
       "      <td>1000</td>\n",
       "      <td>116.03</td>\n",
       "      <td>459</td>\n",
       "      <td>109.38</td>\n",
       "      <td>1.43</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.91</td>\n",
       "      <td>0.40</td>\n",
       "      <td>107.61</td>\n",
       "      <td>409</td>\n",
       "      <td>100.95</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.32</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17426</th>\n",
       "      <td>69</td>\n",
       "      <td>7</td>\n",
       "      <td>1500</td>\n",
       "      <td>115.97</td>\n",
       "      <td>458</td>\n",
       "      <td>109.31</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.89</td>\n",
       "      <td>0.40</td>\n",
       "      <td>107.55</td>\n",
       "      <td>412</td>\n",
       "      <td>100.93</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.30</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17926</th>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "      <td>2000</td>\n",
       "      <td>115.95</td>\n",
       "      <td>457</td>\n",
       "      <td>109.28</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.88</td>\n",
       "      <td>0.40</td>\n",
       "      <td>107.71</td>\n",
       "      <td>414</td>\n",
       "      <td>101.04</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.29</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18201</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>115.93</td>\n",
       "      <td>456</td>\n",
       "      <td>109.26</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.87</td>\n",
       "      <td>0.40</td>\n",
       "      <td>107.56</td>\n",
       "      <td>409</td>\n",
       "      <td>100.83</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.28</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18701</th>\n",
       "      <td>78</td>\n",
       "      <td>8</td>\n",
       "      <td>500</td>\n",
       "      <td>115.89</td>\n",
       "      <td>455</td>\n",
       "      <td>109.21</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.86</td>\n",
       "      <td>0.40</td>\n",
       "      <td>107.54</td>\n",
       "      <td>407</td>\n",
       "      <td>100.79</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19201</th>\n",
       "      <td>67</td>\n",
       "      <td>8</td>\n",
       "      <td>1000</td>\n",
       "      <td>115.84</td>\n",
       "      <td>453</td>\n",
       "      <td>109.16</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4.85</td>\n",
       "      <td>0.40</td>\n",
       "      <td>107.51</td>\n",
       "      <td>410</td>\n",
       "      <td>100.87</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.26</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19701</th>\n",
       "      <td>66</td>\n",
       "      <td>8</td>\n",
       "      <td>1500</td>\n",
       "      <td>115.83</td>\n",
       "      <td>452</td>\n",
       "      <td>109.13</td>\n",
       "      <td>1.53</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4.84</td>\n",
       "      <td>0.40</td>\n",
       "      <td>107.47</td>\n",
       "      <td>409</td>\n",
       "      <td>100.76</td>\n",
       "      <td>2.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.25</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201</th>\n",
       "      <td>55</td>\n",
       "      <td>8</td>\n",
       "      <td>2000</td>\n",
       "      <td>115.79</td>\n",
       "      <td>451</td>\n",
       "      <td>109.09</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4.83</td>\n",
       "      <td>0.40</td>\n",
       "      <td>107.47</td>\n",
       "      <td>405</td>\n",
       "      <td>100.75</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.23</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20476</th>\n",
       "      <td>51</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>115.77</td>\n",
       "      <td>451</td>\n",
       "      <td>109.07</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4.82</td>\n",
       "      <td>0.40</td>\n",
       "      <td>107.38</td>\n",
       "      <td>405</td>\n",
       "      <td>100.74</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.23</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20976</th>\n",
       "      <td>58</td>\n",
       "      <td>9</td>\n",
       "      <td>500</td>\n",
       "      <td>115.74</td>\n",
       "      <td>450</td>\n",
       "      <td>109.02</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4.81</td>\n",
       "      <td>0.40</td>\n",
       "      <td>107.51</td>\n",
       "      <td>406</td>\n",
       "      <td>100.75</td>\n",
       "      <td>2.12</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.22</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21476</th>\n",
       "      <td>69</td>\n",
       "      <td>9</td>\n",
       "      <td>1000</td>\n",
       "      <td>115.71</td>\n",
       "      <td>449</td>\n",
       "      <td>108.99</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4.80</td>\n",
       "      <td>0.40</td>\n",
       "      <td>107.34</td>\n",
       "      <td>404</td>\n",
       "      <td>100.64</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.20</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21976</th>\n",
       "      <td>58</td>\n",
       "      <td>9</td>\n",
       "      <td>1500</td>\n",
       "      <td>115.67</td>\n",
       "      <td>448</td>\n",
       "      <td>108.94</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4.79</td>\n",
       "      <td>0.40</td>\n",
       "      <td>107.41</td>\n",
       "      <td>405</td>\n",
       "      <td>100.73</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.20</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22476</th>\n",
       "      <td>58</td>\n",
       "      <td>9</td>\n",
       "      <td>2000</td>\n",
       "      <td>115.65</td>\n",
       "      <td>447</td>\n",
       "      <td>108.92</td>\n",
       "      <td>1.62</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.78</td>\n",
       "      <td>0.40</td>\n",
       "      <td>107.43</td>\n",
       "      <td>408</td>\n",
       "      <td>100.74</td>\n",
       "      <td>2.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.18</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22751</th>\n",
       "      <td>52</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>115.63</td>\n",
       "      <td>446</td>\n",
       "      <td>108.90</td>\n",
       "      <td>1.62</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.77</td>\n",
       "      <td>0.40</td>\n",
       "      <td>107.02</td>\n",
       "      <td>407</td>\n",
       "      <td>100.78</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.17</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23251</th>\n",
       "      <td>84</td>\n",
       "      <td>10</td>\n",
       "      <td>500</td>\n",
       "      <td>115.59</td>\n",
       "      <td>445</td>\n",
       "      <td>108.86</td>\n",
       "      <td>1.64</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.76</td>\n",
       "      <td>0.41</td>\n",
       "      <td>106.76</td>\n",
       "      <td>402</td>\n",
       "      <td>100.48</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.13</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23751</th>\n",
       "      <td>55</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>115.57</td>\n",
       "      <td>444</td>\n",
       "      <td>108.84</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.75</td>\n",
       "      <td>0.41</td>\n",
       "      <td>106.88</td>\n",
       "      <td>399</td>\n",
       "      <td>100.52</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.11</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24251</th>\n",
       "      <td>56</td>\n",
       "      <td>10</td>\n",
       "      <td>1500</td>\n",
       "      <td>115.53</td>\n",
       "      <td>443</td>\n",
       "      <td>108.80</td>\n",
       "      <td>1.67</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.74</td>\n",
       "      <td>0.41</td>\n",
       "      <td>106.87</td>\n",
       "      <td>397</td>\n",
       "      <td>100.42</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.11</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24751</th>\n",
       "      <td>55</td>\n",
       "      <td>10</td>\n",
       "      <td>2000</td>\n",
       "      <td>115.50</td>\n",
       "      <td>443</td>\n",
       "      <td>108.76</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.73</td>\n",
       "      <td>0.42</td>\n",
       "      <td>107.05</td>\n",
       "      <td>402</td>\n",
       "      <td>100.58</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.09</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25026</th>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>115.47</td>\n",
       "      <td>442</td>\n",
       "      <td>108.74</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.72</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.89</td>\n",
       "      <td>394</td>\n",
       "      <td>100.41</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.08</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25526</th>\n",
       "      <td>59</td>\n",
       "      <td>11</td>\n",
       "      <td>500</td>\n",
       "      <td>115.44</td>\n",
       "      <td>441</td>\n",
       "      <td>108.71</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.71</td>\n",
       "      <td>0.42</td>\n",
       "      <td>107.07</td>\n",
       "      <td>404</td>\n",
       "      <td>100.54</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.08</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26026</th>\n",
       "      <td>59</td>\n",
       "      <td>11</td>\n",
       "      <td>1000</td>\n",
       "      <td>115.41</td>\n",
       "      <td>440</td>\n",
       "      <td>108.67</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.70</td>\n",
       "      <td>0.42</td>\n",
       "      <td>107.01</td>\n",
       "      <td>397</td>\n",
       "      <td>100.42</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.08</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26526</th>\n",
       "      <td>60</td>\n",
       "      <td>11</td>\n",
       "      <td>1500</td>\n",
       "      <td>115.38</td>\n",
       "      <td>440</td>\n",
       "      <td>108.64</td>\n",
       "      <td>1.73</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>107.02</td>\n",
       "      <td>395</td>\n",
       "      <td>100.38</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.08</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27026</th>\n",
       "      <td>59</td>\n",
       "      <td>11</td>\n",
       "      <td>2000</td>\n",
       "      <td>115.35</td>\n",
       "      <td>439</td>\n",
       "      <td>108.61</td>\n",
       "      <td>1.74</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.68</td>\n",
       "      <td>0.43</td>\n",
       "      <td>107.25</td>\n",
       "      <td>400</td>\n",
       "      <td>100.48</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.06</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27301</th>\n",
       "      <td>33</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>115.34</td>\n",
       "      <td>438</td>\n",
       "      <td>108.59</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.68</td>\n",
       "      <td>0.43</td>\n",
       "      <td>106.89</td>\n",
       "      <td>394</td>\n",
       "      <td>100.22</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.05</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27801</th>\n",
       "      <td>59</td>\n",
       "      <td>12</td>\n",
       "      <td>500</td>\n",
       "      <td>115.32</td>\n",
       "      <td>438</td>\n",
       "      <td>108.56</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4.67</td>\n",
       "      <td>0.43</td>\n",
       "      <td>106.85</td>\n",
       "      <td>390</td>\n",
       "      <td>100.14</td>\n",
       "      <td>2.23</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.06</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28301</th>\n",
       "      <td>60</td>\n",
       "      <td>12</td>\n",
       "      <td>1000</td>\n",
       "      <td>115.30</td>\n",
       "      <td>437</td>\n",
       "      <td>108.54</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4.66</td>\n",
       "      <td>0.43</td>\n",
       "      <td>107.01</td>\n",
       "      <td>394</td>\n",
       "      <td>100.20</td>\n",
       "      <td>2.28</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.06</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      TRAIN:    TM                        LM        VALID:  \\\n",
       "      Time  Ep    Ct    LOSS   PPL     NLL    KL   REG   NLL    KL    LOSS   \n",
       "1       15   0     0  133.50  1036  122.98  0.43  0.97  9.12  1.15  126.60   \n",
       "501     74   0   500  121.47   598  114.37  0.08  0.66  6.32  1.04  111.65   \n",
       "1001    72   0  1000  120.07   576  113.30  0.13  0.52  6.05  0.82  111.37   \n",
       "1501    73   0  1500  119.50   561  112.81  0.24  0.44  5.94  0.72  110.80   \n",
       "2001    75   0  2000  119.19   549  112.51  0.36  0.38  5.86  0.65  110.46   \n",
       "2276    56   1     0  119.09   545  112.41  0.41  0.35  5.82  0.62  110.30   \n",
       "2776    74   1   500  118.79   537  112.12  0.50  0.31  5.74  0.57  109.84   \n",
       "3276    72   1  1000  118.44   529  111.78  0.58  0.29  5.67  0.53  109.59   \n",
       "3776    72   1  1500  118.35   523  111.70  0.65  0.26  5.61  0.50  109.43   \n",
       "4276    72   1  2000  118.13   517  111.50  0.71  0.24  5.55  0.48  109.37   \n",
       "4551    46   2     0  117.99   514  111.36  0.74  0.23  5.52  0.47  109.23   \n",
       "5051    72   2   500  117.82   510  111.19  0.78  0.21  5.47  0.45  109.14   \n",
       "5551    71   2  1000  117.67   505  111.05  0.83  0.20  5.43  0.43  108.98   \n",
       "6051    61   2  1500  117.51   502  110.89  0.87  0.19  5.39  0.42  108.98   \n",
       "6551    72   2  2000  117.42   498  110.81  0.90  0.18  5.35  0.41  108.74   \n",
       "6826    33   3     0  117.38   497  110.76  0.92  0.17  5.33  0.41  108.88   \n",
       "7326    62   3   500  117.31   494  110.70  0.96  0.16  5.30  0.40  108.78   \n",
       "7826    72   3  1000  117.22   491  110.61  0.99  0.15  5.27  0.39  108.66   \n",
       "8326    61   3  1500  117.14   489  110.53  1.02  0.15  5.24  0.39  108.67   \n",
       "8826    73   3  2000  116.99   487  110.39  1.04  0.14  5.21  0.38  108.55   \n",
       "9101    42   4     0  116.97   485  110.36  1.06  0.14  5.20  0.38  108.38   \n",
       "9601    56   4   500  116.88   483  110.27  1.09  0.13  5.18  0.37  108.45   \n",
       "10101   68   4  1000  116.82   481  110.21  1.11  0.13  5.15  0.37  108.37   \n",
       "10601   69   4  1500  116.74   479  110.12  1.14  0.12  5.13  0.37  108.34   \n",
       "11101   57   4  2000  116.69   477  110.08  1.16  0.12  5.11  0.37  108.45   \n",
       "11376   47   5     0  116.66   476  110.04  1.17  0.11  5.10  0.37  107.93   \n",
       "11876   56   5   500  116.61   475  110.00  1.20  0.11  5.08  0.37  107.96   \n",
       "12376   68   5  1000  116.52   473  109.91  1.22  0.11  5.06  0.38  107.75   \n",
       "12876   56   5  1500  116.45   471  109.85  1.25  0.10  5.04  0.38  107.77   \n",
       "13376   56   5  2000  116.38   469  109.77  1.28  0.10  5.02  0.38  107.88   \n",
       "...    ...  ..   ...     ...   ...     ...   ...   ...   ...   ...     ...   \n",
       "15151   59   6  1500  116.17   464  109.55  1.36  0.09  4.96  0.40  107.76   \n",
       "15651   60   6  2000  116.13   463  109.51  1.38  0.09  4.94  0.40  107.91   \n",
       "15926   33   7     0  116.12   462  109.49  1.39  0.09  4.94  0.40  107.81   \n",
       "16426   59   7   500  116.07   460  109.43  1.41  0.08  4.92  0.40  107.90   \n",
       "16926   71   7  1000  116.03   459  109.38  1.43  0.08  4.91  0.40  107.61   \n",
       "17426   69   7  1500  115.97   458  109.31  1.45  0.08  4.89  0.40  107.55   \n",
       "17926   60   7  2000  115.95   457  109.28  1.47  0.08  4.88  0.40  107.71   \n",
       "18201   32   8     0  115.93   456  109.26  1.48  0.08  4.87  0.40  107.56   \n",
       "18701   78   8   500  115.89   455  109.21  1.49  0.08  4.86  0.40  107.54   \n",
       "19201   67   8  1000  115.84   453  109.16  1.51  0.07  4.85  0.40  107.51   \n",
       "19701   66   8  1500  115.83   452  109.13  1.53  0.07  4.84  0.40  107.47   \n",
       "20201   55   8  2000  115.79   451  109.09  1.54  0.07  4.83  0.40  107.47   \n",
       "20476   51   9     0  115.77   451  109.07  1.55  0.07  4.82  0.40  107.38   \n",
       "20976   58   9   500  115.74   450  109.02  1.57  0.07  4.81  0.40  107.51   \n",
       "21476   69   9  1000  115.71   449  108.99  1.59  0.07  4.80  0.40  107.34   \n",
       "21976   58   9  1500  115.67   448  108.94  1.60  0.07  4.79  0.40  107.41   \n",
       "22476   58   9  2000  115.65   447  108.92  1.62  0.06  4.78  0.40  107.43   \n",
       "22751   52  10     0  115.63   446  108.90  1.62  0.06  4.77  0.40  107.02   \n",
       "23251   84  10   500  115.59   445  108.86  1.64  0.06  4.76  0.41  106.76   \n",
       "23751   55  10  1000  115.57   444  108.84  1.65  0.06  4.75  0.41  106.88   \n",
       "24251   56  10  1500  115.53   443  108.80  1.67  0.06  4.74  0.41  106.87   \n",
       "24751   55  10  2000  115.50   443  108.76  1.68  0.06  4.73  0.42  107.05   \n",
       "25026   30  11     0  115.47   442  108.74  1.69  0.06  4.72  0.42  106.89   \n",
       "25526   59  11   500  115.44   441  108.71  1.70  0.06  4.71  0.42  107.07   \n",
       "26026   59  11  1000  115.41   440  108.67  1.71  0.06  4.70  0.42  107.01   \n",
       "26526   60  11  1500  115.38   440  108.64  1.73  0.06  4.69  0.43  107.02   \n",
       "27026   59  11  2000  115.35   439  108.61  1.74  0.06  4.68  0.43  107.25   \n",
       "27301   33  12     0  115.34   438  108.59  1.75  0.06  4.68  0.43  106.89   \n",
       "27801   59  12   500  115.32   438  108.56  1.76  0.05  4.67  0.43  106.85   \n",
       "28301   60  12  1000  115.30   437  108.54  1.77  0.05  4.66  0.43  107.01   \n",
       "\n",
       "         TM                        LM               \n",
       "        PPL     NLL    KL   REG   NLL    KL   Beta  \n",
       "1      1034  116.19  0.32  0.97  9.12  1.01  0.000  \n",
       "501     533  105.29  0.11  0.45  5.74  0.75  0.088  \n",
       "1001    524  105.00  0.21  0.32  5.75  0.51  0.176  \n",
       "1501    504  104.32  0.51  0.22  5.62  0.47  0.264  \n",
       "2001    498  104.00  0.65  0.18  5.48  0.44  0.352  \n",
       "2276    496  103.96  0.67  0.16  5.36  0.36  0.400  \n",
       "2776    481  103.49  0.84  0.13  5.23  0.31  0.488  \n",
       "3276    475  103.33  0.85  0.11  5.13  0.28  0.576  \n",
       "3776    472  103.13  0.95  0.10  5.06  0.29  0.664  \n",
       "4276    467  103.02  1.04  0.09  5.00  0.30  0.752  \n",
       "4551    467  102.98  1.01  0.07  4.95  0.28  0.800  \n",
       "5051    464  102.84  1.11  0.06  4.90  0.27  0.888  \n",
       "5551    455  102.60  1.21  0.05  4.86  0.27  0.976  \n",
       "6051    457  102.62  1.23  0.05  4.81  0.27  1.000  \n",
       "6551    450  102.39  1.24  0.04  4.77  0.30  1.000  \n",
       "6826    455  102.53  1.26  0.04  4.76  0.28  1.000  \n",
       "7326    451  102.45  1.29  0.04  4.72  0.29  1.000  \n",
       "7826    448  102.28  1.36  0.03  4.69  0.30  1.000  \n",
       "8326    452  102.35  1.33  0.03  4.67  0.29  1.000  \n",
       "8826    446  102.18  1.40  0.03  4.64  0.30  1.000  \n",
       "9101    441  102.03  1.41  0.03  4.62  0.29  1.000  \n",
       "9601    443  102.09  1.41  0.02  4.60  0.32  1.000  \n",
       "10101   441  102.01  1.47  0.02  4.57  0.29  1.000  \n",
       "10601   438  102.00  1.42  0.02  4.55  0.35  1.000  \n",
       "11101   441  102.04  1.52  0.02  4.53  0.33  1.000  \n",
       "11376   436  101.89  1.50  0.02  4.52  0.34  0.000  \n",
       "11876   435  101.76  1.66  0.02  4.48  0.51  0.088  \n",
       "12376   428  101.55  1.65  0.02  4.45  0.51  0.176  \n",
       "12876   425  101.42  1.77  0.02  4.42  0.52  0.264  \n",
       "13376   429  101.57  1.71  0.02  4.41  0.49  0.352  \n",
       "...     ...     ...   ...   ...   ...   ...    ...  \n",
       "15151   419  101.20  1.86  0.02  4.37  0.47  0.664  \n",
       "15651   423  101.33  1.86  0.02  4.35  0.46  0.752  \n",
       "15926   422  101.24  1.86  0.02  4.33  0.46  0.800  \n",
       "16426   421  101.28  1.87  0.02  4.33  0.45  0.888  \n",
       "16926   409  100.95  1.92  0.02  4.32  0.42  0.976  \n",
       "17426   412  100.93  1.92  0.01  4.30  0.38  1.000  \n",
       "17926   414  101.04  1.95  0.01  4.29  0.42  1.000  \n",
       "18201   409  100.83  2.00  0.01  4.28  0.44  1.000  \n",
       "18701   407  100.79  2.05  0.01  4.28  0.40  1.000  \n",
       "19201   410  100.87  1.95  0.01  4.26  0.41  1.000  \n",
       "19701   409  100.76  2.04  0.01  4.25  0.40  1.000  \n",
       "20201   405  100.75  2.05  0.01  4.23  0.42  1.000  \n",
       "20476   405  100.74  1.98  0.01  4.23  0.42  1.000  \n",
       "20976   406  100.75  2.12  0.01  4.22  0.41  1.000  \n",
       "21476   404  100.64  2.06  0.01  4.20  0.42  1.000  \n",
       "21976   405  100.73  2.06  0.01  4.20  0.41  1.000  \n",
       "22476   408  100.74  2.09  0.01  4.18  0.42  1.000  \n",
       "22751   407  100.78  2.06  0.01  4.17  0.41  0.000  \n",
       "23251   402  100.48  2.10  0.01  4.13  0.58  0.088  \n",
       "23751   399  100.52  2.13  0.01  4.11  0.60  0.176  \n",
       "24251   397  100.42  2.18  0.01  4.11  0.57  0.264  \n",
       "24751   402  100.58  2.17  0.01  4.09  0.56  0.352  \n",
       "25026   394  100.41  2.17  0.01  4.08  0.54  0.400  \n",
       "25526   404  100.54  2.18  0.01  4.08  0.54  0.488  \n",
       "26026   397  100.42  2.16  0.01  4.08  0.58  0.576  \n",
       "26526   395  100.38  2.21  0.01  4.08  0.52  0.664  \n",
       "27026   400  100.48  2.21  0.01  4.06  0.64  0.752  \n",
       "27301   394  100.22  2.21  0.01  4.05  0.49  0.800  \n",
       "27801   390  100.14  2.23  0.01  4.06  0.47  0.888  \n",
       "28301   394  100.20  2.28  0.01  4.06  0.48  0.976  \n",
       "\n",
       "[63 rows x 18 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Output sentences for each topic-----------\n",
      "Item idx: B000VB7EFW\n",
      "0  BOW: zipper velcro flap zip closed stitching open zippers close side\n",
      "0  SENTENCE: the only thing i have is that it is a little bit to fit my # & # # ; laptop\n",
      "1  BOW: keyboard mac love apple color easy hard protector pro clean\n",
      "1  SENTENCE: the keyboard cover fits perfectly\n",
      "2  BOW: months year broke weeks quality week back started time years\n",
      "2  SENTENCE: i would definitely recommend this product\n",
      "3  BOW: quality made perfect inside size bought big carry recommend inch\n",
      "3  SENTENCE: i have a # . # `` laptop and it fits perfectly\n",
      "4  BOW: listed fair test sewn offered website odor trust refund stated\n",
      "4  SENTENCE: i have a # . # inch laptop and it fits perfectly\n",
      "5  BOW: odor chemical smells impact stated smell tab thickness port fair\n",
      "5  SENTENCE: it is a great case for the price\n",
      "6  BOW: ; & pro laptops big tablet description hp retina air\n",
      "6  SENTENCE: the sleeve fits my # & # # ; macbook pro # & # # ; macbook pro perfectly\n",
      "7  BOW: air bit protect pro hard protection job smell bought :\n",
      "7  SENTENCE: the case is a little difficult to put on the laptop , but it does not fit\n",
      "8  BOW: ! love awesome fast amazing absolutely daughter loves shipping compliments\n",
      "8  SENTENCE: love love ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "9  BOW: back handle strap straps shoulder heavy pack work zippers bags\n",
      "9  SENTENCE: i have a # `` laptop and this bag has plenty of room for my laptop and other accessories\n",
      "10  BOW: scratches shell retina hard screen easily nice scratch speck cover\n",
      "10  SENTENCE: it fits perfectly and the keyboard cover fits perfectly\n",
      "11  BOW: pocket power netbook mouse inside padding cord zipper drive usb\n",
      "11  SENTENCE: there is also enough room for the power cord , mouse , mouse , etc .\n",
      "12  BOW: nice price made - inside material quality big small design\n",
      "12  SENTENCE: it is a little bit to fit my macbook pro\n",
      "13  BOW: cover nice easily put makes easy ! feel pretty recommend\n",
      "13  SENTENCE: the keyboard cover fits perfectly , and the keyboard cover does not fit perfectly\n",
      "14  BOW: 'm 've - ... quality thing time : made pretty\n",
      "14  SENTENCE: i would definitely recommend this product to anyone who wants to protect your laptop\n",
      "15  BOW: room ipad charger cords extra accessories carry pocket kindle padded\n",
      "15  SENTENCE: there is plenty of room for the power cord , power cord , mouse , pens , etc .\n",
      "16  BOW: perfectly pro recommend mac perfect protects air book protect easy\n",
      "16  SENTENCE: the keyboard cover fits perfectly\n",
      "17  BOW: price buy quality ... time bought happy purchase worth $\n",
      "17  SENTENCE: i love the color\n",
      "18  BOW: ! highly perfect loves awesome college school ... love absolutely\n",
      "18  SENTENCE: love love ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "19  BOW: item arrived shipping amazon received ordered days wrong shipped description\n",
      "19  SENTENCE: i love this case\n",
      "20  BOW: sleeve protection inch neoprene inside snug sleeves chromebook extra soft\n",
      "20  SENTENCE: the sleeve fits my # & # # ; macbook pro # & # # ; macbook pro # & # # ; macbook pro # & # # ; macbook pro # & # # ; macbook pro #\n",
      "21  BOW: pockets pocket small hold strap compartment room large front carry\n",
      "21  SENTENCE: there is plenty of room for the power cord , power cords , pens , pens , etc .\n",
      "22  BOW: cheaply smells website ugly realized terrible stated late bucks offered\n",
      "22  SENTENCE: it is a great case for the price\n",
      "23  BOW: return service customer received company seller send shipping contacted order\n",
      "23  SENTENCE: i would definitely recommend this product to anyone who wants to protect your laptop\n",
      "24  BOW: color picture pink ordered blue purple red bright received green\n",
      "24  SENTENCE: i love the color and the keyboard cover\n",
      "25  BOW: clothes lenses seat lens canon shoulders bottle loaded overhead airplane\n",
      "25  SENTENCE: i have a # . # inch laptop and it fits perfectly\n",
      "26  BOW: carry comfortable compartments lots books pockets space stuff plenty camera\n",
      "26  SENTENCE: plenty of room for my laptop , books , pens , pens , pens , etc .\n",
      "27  BOW: fair test sewn trust pulled offered reviewer port weak stated\n",
      "27  SENTENCE: it is a great case for the price\n",
      "28  BOW: bottom top plastic back part piece rubber corner cracked feet\n",
      "28  SENTENCE: it does not fit the macbook pro # & # # ;\n",
      "29  BOW: love recommend ! perfect highly absolutely awesome purchase super happy\n",
      "29  SENTENCE: i love this product ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "-----------Summaries-----------\n",
      "SUMMARY 0 : \n",
      " This is a very well made bag, nice construction, lots of pockets.\n",
      "the straps are very comfortable.\n",
      "and protects everything inside.\n",
      "It says\n",
      "it fits a 17inch notebook,\n",
      "however it did not.\n",
      "after using the pack for less than a month,\n",
      "it is ripping out already.\n",
      "SUMMARY 1 : \n",
      " This is a very well made bag, nice construction, lots of pockets.\n",
      "The quality is excellent\n",
      "and it is very durable.\n",
      "the straps are very comfortable.\n",
      "and protects everything inside.\n",
      "The color is a true red\n",
      "and it fits nicely.\n",
      "it is ripping out already.\n",
      "and it doesnt fit.\n",
      "It's just not the lightest backpack\n",
      "because some zipper teeth were not aligned.\n",
      "SUMMARY 2 : \n",
      " The quality is excellent\n",
      "and I love all the pockets and compartments.\n",
      "and it is very durable.\n",
      "and can't beleive the price\n",
      "and protects everything inside.\n",
      "The laptop\n",
      "doesn't fit in it.\n",
      "it is ripping out already.\n",
      "It's just not the lightest backpack\n",
      "0 TRUE: the item arrived within good time , and was well packaged\n",
      "0 PRED: i color was quickly a days\n",
      "1 TRUE: the vga pin side was protected by a plastic cover which easily slid off in preparation for use\n",
      "1 PRED: the case is on of n't , the hard ,\n",
      "2 TRUE: the product appeared intact with no visible sign of wear and tear\n",
      "2 PRED: i quality is is\n",
      "3 TRUE: unfortunately , when tested on multiple computers , it did not work properly\n",
      "3 PRED: i , i i the it <unk> , the 's n't a\n",
      "4 TRUE: a picture appeared on the desired monitor , but not a clear one by any means\n",
      "4 PRED: the little , to the bottom , , but it a good of\n",
      "5 TRUE: it was black and white with lines running throughout , making any sort of possibility of <unk> the picture <unk> at best\n",
      "5 PRED: i was a and the and the\n",
      "6 TRUE: i reported this to the seller , and within two to three days i received a full refund -lrb- # plus # . # shipping and handling -rrb-\n",
      "6 PRED: i love this case anyone seller\n",
      "7 TRUE: i wish the product had worked , but i am impressed with the <unk> of the seller , `` <unk> ``\n",
      "7 PRED: i would it case was a more but i i not with it\n",
      "8 TRUE: i brought this from amazon since it had an offer\n",
      "8 PRED: i have the product my and i was a # more\n",
      "9 TRUE: the item was delivered # day prior to the mentioned date\n",
      "9 PRED: i love was a\n",
      "10 TRUE: i was not able to track this properly during the delivery\n",
      "10 PRED: i was a to to get the case\n",
      "11 TRUE: when i opened the package , i was shocked to see a small back pack than what i expected\n",
      "11 PRED: i i got it case , i was a to find the case case for i the\n",
      "12 TRUE: it does n't had any review comments when i brought this , means never buy an item with no review comments : -rrb-\n",
      "12 PRED: i was n't have any problem about\n",
      "13 TRUE: this bag can not even hold my # `` laptop which has an extra battery\n",
      "13 PRED: i is is be a with my laptop `` laptop , is a in room\n",
      "14 TRUE: my old hp laptop backpack which costs around # dollar is a lot bigger than this and i could n't find much weight difference between both\n",
      "14 PRED: i # bag laptop fits was it\n",
      "15 TRUE: so if you are looking to carry your # `` or # `` laptop , please do buy this else go on searching and you will surely find a bigger backpack for a cheaper\n",
      "15 PRED: i far you are looking for carry a laptop `` laptop this `` # , this have n't this one\n",
      "16 TRUE: i was a little hesitant about the quality of this product at first\n",
      "16 PRED: i was very little disappointed to this quality of\n",
      "17 TRUE: but it totally surprised me\n",
      "17 PRED: i i i love\n",
      "18 TRUE: i love it ! it was super easy to put together\n",
      "18 PRED: i love it\n",
      "19 TRUE: my favorite feature of this is the legs to prop up the computer at an angle\n",
      "19 PRED: i was color is the case a color color the the macbook apple of the apple\n",
      "20 TRUE: it makes typing so much more comfortable\n",
      "20 PRED: i was a , easily as i\n",
      "21 TRUE: the color is also really nice\n",
      "21 PRED: i color is great , and\n",
      "22 TRUE: love , love , love this case\n",
      "22 PRED: i the love the love this case\n",
      "23 TRUE: i bought this for my new lenovo t #\n",
      "23 PRED: i is this for # # # # # .\n",
      "24 TRUE: the quality of the sleeve is great with lots of padding , as one would expect for a samsonite product\n",
      "24 PRED: i case is the bag is very\n",
      "25 TRUE: however , my t # can only fit in without the battery\n",
      "25 PRED: the , the # is # be be the the the zipper ,\n",
      "26 TRUE: -lrb- it is not impossible to zip the sleeve with the battery on , but it takes a lot of effort and probably will break the zipper in a few days .\n",
      "26 PRED: it is is a a to fit the # # the case , the but it it a lot of of\n",
      "27 TRUE: i decided to keep this product anyhow and carry the battery separately , given the negative reviews about the <unk> sleeves\n",
      "27 PRED: i bought to get it one because , the it laptop in\n",
      "28 TRUE: but think twice if that 's what you want to do\n",
      "28 PRED: i have it is a a a i 're to to n't\n",
      "29 TRUE: i do n't have a dslr\n",
      "29 PRED: i i n't recommend to lot bag\n",
      "30 TRUE: i have an early model of the <unk> and a camcorder\n",
      "30 PRED: i love to had # and the case and i few of ,\n",
      "31 TRUE: i bought this as it holds everything for both cameras when we travel for <unk> <unk> and vacations\n",
      "31 PRED: i was this bag a backpack my i my and and i travel\n",
      "32 TRUE: it holds more than i can put into it - regarding my camera stuff\n",
      "32 PRED: i is my than and and not in a\n",
      "33 TRUE: it does hold the larger tripod as well , but , the smaller one i have is perfect for it\n",
      "33 PRED: the is n't my laptop and , well as but i i bag bag is have had\n",
      "34 TRUE: this is a great bag\n",
      "34 PRED: i is a great bag\n",
      "35 TRUE: i used it on a # -day trip to japan and it held my canon t # i digital slr as well as passports , maps , etc .\n",
      "35 PRED: i have this for my trip trip trip and my and to has up laptop and # . have it and and\n",
      "36 TRUE: the only thing i did n't like about it was that the padding on the shoulder strap kept slipping down from where i wanted it\n",
      "36 PRED: i only thing i that n't like the the is a the is is the front is is the the the the the the to to\n",
      "37 TRUE: the other minor drawback is the clip for the front flap & # # ; its a little difficult to open and close quickly\n",
      "37 PRED: the only side pockets is the main pocket the charger is is # # ; laptop & bit bit\n",
      "38 TRUE: i am still very happy with the bag and would recommend to others\n",
      "38 PRED: i am very happy happy with this quality\n",
      "39 TRUE: l <unk> at this price it would be adequate but wow , it is very nice\n",
      "39 PRED: i , , the price , is be been for i ,\n",
      "40 TRUE: plenty of padding , extra pockets , and easy openings\n",
      "40 PRED: i of room , and , , and other to carry\n",
      "41 TRUE: plenty of room for my # . # `` laptop plus my external # tb drive with all necessary cables and room left over\n",
      "41 PRED: the of room for the # . # inch laptop , the laptop cord . # # a # drive\n",
      "42 TRUE: this is a <unk> buy it now\n",
      "42 PRED: i is a great\n",
      "43 TRUE: do not buy this bag if you value your laptop at all ! !\n",
      "43 PRED: i n't recommend this bag\n",
      "44 TRUE: the design of the shoulder strap clips are such that they detach from the bag without warning\n",
      "44 PRED: i bag is the bag is is is the as i can a to bag\n",
      "45 TRUE: this has happened on two separate occasions and both times damaging my laptop\n",
      "45 PRED: i is a for the years and\n",
      "46 TRUE: which brings me to the second major flaw ; there 's absolutely no padding for this bag in the even the straps do become unclipped\n",
      "46 PRED: i is the , the the bag <unk> , the bag a not longer\n",
      "47 TRUE: the backpack is a downsize from a much larger one\n",
      "47 PRED: i bag is a very , the shoulder of bag\n",
      "48 TRUE: it does not hold a # inch laptop but my smaller ipad and laptop both fit fine\n",
      "48 PRED: i is not fit the the ; laptop , the # laptop , the fits\n",
      "49 TRUE: i really like the compartments and shoulder straps . the color definitely stands out from the other boring black backpacks around\n",
      "49 PRED: i am like this bag\n",
      "50 TRUE: if you like compact and efficient you will like this\n",
      "50 PRED: i you are a , you , this not this one\n",
      "51 TRUE: its all right except i fear that the zippers will & # # ; <unk> out & # # ; on me prematurely\n",
      "51 PRED: i a a for for would it i zipper is be # # ; laptop & # # # ;\n",
      "52 TRUE: its too bad there is no more decent work <unk> anymore\n",
      "52 PRED: i a good and is no problems\n",
      "53 TRUE: we the consumer are pretty much screwed as to finding anything of quality\n",
      "53 PRED: i have bag is not , in , a as a else the ,\n",
      "54 TRUE: the case came exactly as described , and i am very happy with it\n",
      "54 PRED: i color was in and well\n",
      "55 TRUE: it was delivered very quickly as well\n",
      "55 PRED: i was exactly\n",
      "56 TRUE: the case fits my computer extremely well and will protect it nicely\n",
      "56 PRED: the is is perfectly macbook perfectly well\n",
      "57 TRUE: we have yet to really <unk> this case , but so far , so good\n",
      "57 PRED: i have n't that have like it case\n",
      "58 TRUE: we bought it for a my passport drive and the hd fits snugly into the case\n",
      "58 PRED: i is this for my # # # and it sleeve is perfectly\n",
      "59 TRUE: so snugly that it takes a little bit of effort in getting the drive out\n",
      "59 PRED: the case is is is a little bit of protection to the the laptop in the\n",
      "60 TRUE: i bought this last month and so far it has been great\n",
      "60 PRED: i love this bag for\n",
      "61 TRUE: it is nicely padded and soft on the inside and my macbook pro # `` fits perfectly , definitely recommend it\n",
      "61 PRED: the is a , and does\n",
      "62 TRUE: the sleeve protects <unk> against water and scratches , but it will not protect it if dropped\n",
      "62 PRED: the case is my macbook scratches scratches scratches , the it 's not fit the from you\n",
      "63 TRUE: it could also probably use a handle , but other than that i love it\n",
      "63 PRED: i is not a a a more or but it than that i is it\n"
     ]
    }
   ],
   "source": [
    "if len(log_df) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "while epoch < config.epochs:\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = get_feed_dict(batch)\n",
    "\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, sent_loss_kl_categ_batch, sent_loss_kl_gmm_batch, ppls_batch = \\\n",
    "        sess.run([opt, loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, sent_loss_kl_categ, sent_loss_kl_gmm, topic_ppls], feed_dict = feed_dict)\n",
    "   \n",
    "        if sent_loss_kl_batch == np.inf:\n",
    "            print('Nan occured')\n",
    "            ckpt = tf.train.get_checkpoint_state(config.modeldir)\n",
    "            model_checkpoint_path = ckpt.all_model_checkpoint_paths[-1]\n",
    "            saver.restore(sess, model_checkpoint_path)            \n",
    "            break\n",
    "            \n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if ct%config.log_period==0:\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train, sent_loss_recon_train, sent_loss_kl_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, sent_loss_recon_dev, sent_loss_kl_dev, ppl_dev = get_loss(sess, dev_batches)\n",
    "            global_step_log, beta_eval = sess.run([tf.train.get_global_step(), beta])\n",
    "            \n",
    "            if loss_dev < loss_min:\n",
    "                loss_min = loss_dev\n",
    "                saver.save(sess, config.modelpath, global_step=global_step_log)\n",
    "\n",
    "            clear_output()\n",
    "    \n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_reg_train, '%.2f'%sent_loss_recon_train, '%.2f'%sent_loss_kl_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_reg_dev, '%.2f'%sent_loss_recon_dev, '%.2f'%sent_loss_kl_dev,  '%.3f'%beta_eval],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "\n",
    "            print_summary(test_batches[1][1])\n",
    "            print_sample(batch)\n",
    "            \n",
    "            time_start = time.time()\n",
    "            \n",
    "    epoch += 1\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_prob_topic, _prob_topic_sents, _prob_topic_infer, _means_topic_infer = debug_value([prob_topic, prob_topic_sents, prob_topic_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_i = 4\n",
    "_prob_topic_sents[batch_i], _prob_topic_infer[batch_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_means_topic_infer[0][:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_means_topic, b_means_topic = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"topic/dec/mean_topic\")\n",
    "\n",
    "pred_topic_embeddings, pred_topic_bow, pred_means_topic, pred_logvars_topic, pred_token_idxs, _w_means_topic, _b_means_topic, _w_mean_topic_infer = \\\n",
    "                                sess.run([topic_embeddings, topic_bow, means_topic, logvars_topic, topic_beam_output_token_idxs, w_means_topic, b_means_topic, w_mean_topic_infer], \n",
    "                                         feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "\n",
    "pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "\n",
    "pred_topics_freq_bow_indices = np.argsort(pred_topic_bow, 1)[:, ::-1][:, :10]\n",
    "pred_topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idxs in pred_topics_freq_bow_idxs:\n",
    "    print([idx_to_word[idx] for idx in idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_topic_embeddings[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_topic_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_w_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_w_mean_topic_infer[:, :10, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_enc_state_infer, _means_topic_infer = debug_value([enc_state_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_enc_state_infer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_means_topic_infer[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
