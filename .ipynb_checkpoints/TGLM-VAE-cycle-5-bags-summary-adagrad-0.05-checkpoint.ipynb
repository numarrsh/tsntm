{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from data_structure import get_batches, get_test_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "from topic_model import TopicModel\n",
    "\n",
    "from topic_beam_search_decoder import BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '1', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/bags/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/tglm_vae_tmp1', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'bags', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 50, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 500, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.05, 'lr')\n",
    "flags.DEFINE_float('reg', 1., 'regularization term')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_bool('warmup', True, 'flg of warming up')\n",
    "flags.DEFINE_integer('epochs_cycle', 5, 'number of epochs within a cycle')\n",
    "flags.DEFINE_float('r_cycle', 0.5, 'proportion used to increase beta within a cycle')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 10, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_bow', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_hidden_topic', 512, 'dim_hidden_topic')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "flags.DEFINE_bool('bidirectional', True, 'flg of bidirectional encoding')\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_test_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('PAD_IDX', word_to_idx[PAD], 'PAD_IDX')\n",
    "flags.DEFINE_integer('UNK_IDX', word_to_idx[UNK], 'UNK_IDX')\n",
    "flags.DEFINE_integer('BOS_IDX', word_to_idx[BOS], 'BOS_IDX')\n",
    "flags.DEFINE_integer('EOS_IDX', word_to_idx[EOS], 'EOS_IDX')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "maximum_iterations = max([max([instance.max_sent_l for instance in batch]) for ct, batch in dev_batches])\n",
    "flags.DEFINE_integer('maximum_iterations', maximum_iterations, 'maximum_iterations')\n",
    "\n",
    "flags.DEFINE_integer('cycle_steps', len(train_batches)*config.epochs_cycle, 'number of steps for each cycle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build language model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['bow'] = tf.placeholder(tf.float32, [None, config.dim_bow], name='bow')\n",
    "t_variables['input_token_idxs'] = tf.placeholder(tf.int32, [None, None], name='input_token_idxs')\n",
    "t_variables['dec_input_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_input_idxs')\n",
    "t_variables['dec_target_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_target_idxs')\n",
    "t_variables['batch_l'] = tf.placeholder(tf.int32, name='batch_l')\n",
    "t_variables['doc_l'] = tf.placeholder(tf.int32, [None], name='doc_l')\n",
    "t_variables['sent_l'] = tf.placeholder(tf.int32, [None], name='sent_l')\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train', assertion=False):\n",
    "    def token_dropout(sent_idxs):\n",
    "        sent_idxs_dropout = np.asarray(sent_idxs)\n",
    "        sent_idxs_dropout[np.random.rand(len(sent_idxs)) > config.word_keep_prob] = config.UNK_IDX\n",
    "        return list(sent_idxs_dropout)\n",
    "\n",
    "    bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "    \n",
    "    doc_l = np.array([len(instance.token_idxs) for instance in batch])\n",
    "    \n",
    "    feed_input_token_idxs_list = [sent_idxs for instance in batch for sent_idxs in instance.token_idxs]\n",
    "    feed_dec_input_idxs_list = [[config.BOS_IDX] + token_dropout(sent_idxs) for sent_idxs in feed_input_token_idxs_list]\n",
    "    feed_dec_target_idxs_list = [sent_idxs + [config.EOS_IDX]  for sent_idxs in feed_input_token_idxs_list]\n",
    "        \n",
    "    sent_l = np.array([len(sent_idxs) for sent_idxs in feed_input_token_idxs_list], np.int32)\n",
    "    batch_l = len(sent_l)\n",
    "    \n",
    "    feed_input_token_idxs = pad_sequences(feed_input_token_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_input_idxs = pad_sequences(feed_dec_input_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_target_idxs = pad_sequences(feed_dec_target_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    \n",
    "    if assertion:\n",
    "        index = 0\n",
    "        for instance in batch:\n",
    "            for line_idxs in instance.token_idxs:\n",
    "                assert feed_input_token_idxs_list[index] == line_idxs\n",
    "                index += 1\n",
    "        assert feed_input_token_idxs.shape[1] == np.max(sent_l)\n",
    "        assert feed_dec_input_idxs.shape[1] == np.max(sent_l) + 1\n",
    "        assert feed_dec_target_idxs.shape[1] == np.max(sent_l) + 1\n",
    "    \n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "\n",
    "    feed_dict = {\n",
    "                t_variables['bow']: bow, \n",
    "                t_variables['batch_l']: batch_l, t_variables['doc_l']: doc_l, t_variables['sent_l']: sent_l, \n",
    "                t_variables['input_token_idxs']: feed_input_token_idxs, t_variables['dec_input_idxs']: feed_dec_input_idxs, t_variables['dec_target_idxs']: feed_dec_target_idxs, \n",
    "                t_variables['keep_prob']: keep_prob\n",
    "    }\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     10,
     24,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables):\n",
    "    sample_batch = dev_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "def check_shape(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()\n",
    "    \n",
    "def check_value(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()    \n",
    "    \n",
    "# sent_loss_kl_categ_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, tf_log(prob_topic_infer/prob_topic_sents)), 1))\n",
    "# debug_value([sent_loss_kl_categ, sent_loss_kl_categ_tmp])\n",
    "# sent_loss_kl_gauss_tmp = 0.5 * tf.reduce_sum(tf.exp(logvars_topic_infer-logvars_topic) + tf.square(means_topic - means_topic_infer) / tf.exp(logvars_topic) - 1 + (logvars_topic - logvars_topic_infer), -1)\n",
    "# sent_loss_kl_gmm_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss_tmp), -1))\n",
    "# debug_value([sent_loss_kl_gmm_tmp, sent_loss_kl_gmm])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encode bow\n",
    "with tf.variable_scope('topic/enc', reuse=False):\n",
    "    hidden_bow_ = tf.layers.Dense(units=config.dim_hidden_bow, activation=tf.nn.relu, name='hidden_bow')(t_variables['bow'])\n",
    "    hidden_bow = tf.layers.Dropout(t_variables['keep_prob'])(hidden_bow_)\n",
    "    means_bow = tf.layers.Dense(units=config.dim_latent_bow, name='mean_bow')(hidden_bow)\n",
    "    logvars_bow = tf.layers.Dense(units=config.dim_latent_bow, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_bow)\n",
    "    latents_bow = sample_latents(means_bow, logvars_bow) # sample latent vectors\n",
    "\n",
    "    prob_topic = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic')(latents_bow) # inference of topic probabilities\n",
    "\n",
    "# decode bow\n",
    "with tf.variable_scope('shared', reuse=False):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "bow_embeddings = tf.nn.embedding_lookup(embeddings, bow_idxs) # embeddings of each bow features\n",
    "\n",
    "with tf.variable_scope('topic/dec', reuse=False):\n",
    "    topic_embeddings = tf.get_variable('topic_emb', [config.n_topic, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of topics\n",
    "\n",
    "    topic_bow = tf.nn.softmax(tf.matmul(topic_embeddings, bow_embeddings, transpose_b=True), 1) # bow vectors for each topic\n",
    "    logits_bow = tf_log(tf.matmul(prob_topic, topic_bow)) # predicted bow distribution\n",
    "\n",
    "    # prior of each gaussian distribution (computed for each topic)\n",
    "    hidden_topic = tf.layers.Dense(units=config.dim_hidden_topic, activation=tf.nn.relu, name='hidden_topic')(topic_bow)\n",
    "    means_topic = tf.layers.Dense(units=config.dim_latent, name='mean_topic')(hidden_topic)\n",
    "    logvars_topic = tf.layers.Dense(units=config.dim_latent, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_topic)\n",
    "    sigma_topic = tf.exp(0.5 * logvars_topic)\n",
    "    gauss_topic = tfd.Normal(loc=means_topic, scale=sigma_topic)    \n",
    "    \n",
    "# define losses\n",
    "topic_losses_recon = -tf.reduce_sum(tf.multiply(t_variables['bow'], logits_bow), 1)\n",
    "topic_loss_recon = tf.reduce_mean(topic_losses_recon) # negative log likelihood of each words\n",
    "\n",
    "topic_loss_kl = compute_kl_loss(means_bow, logvars_bow) # KL divergence b/w latent dist & gaussian std\n",
    "\n",
    "topic_bow_norm = topic_bow / tf.norm(topic_bow, axis=1, keepdims=True)\n",
    "topic_dots = tf.clip_by_value(tf.matmul(topic_bow_norm, tf.transpose(topic_bow_norm)), -1., 1.)\n",
    "topic_loss_reg = tf.reduce_mean(tf.square(topic_dots - tf.eye(config.n_topic)))\n",
    "# topic_angles = tf.acos(topic_dots)\n",
    "# topic_angles_mean = tf.reduce_mean(topic_angles)\n",
    "# topic_angles_vars = tf.reduce_mean(tf.square(topic_angles - topic_angles_mean))\n",
    "# topic_loss_reg = tf.exp(topic_angles_vars - topic_angles_mean)\n",
    "\n",
    "# monitor\n",
    "n_bow = tf.reduce_sum(t_variables['bow'], 1)\n",
    "topic_ppls = tf.divide(topic_losses_recon, tf.maximum(1e-5, n_bow))\n",
    "topics_freq_bow_indices = tf.nn.top_k(topic_bow, 10, name='topic_freq_bow').indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "input_token_idxs = t_variables['input_token_idxs']\n",
    "batch_l = t_variables['batch_l']\n",
    "sent_l = t_variables['sent_l']\n",
    "max_sent_l = tf.reduce_max(sent_l)\n",
    "\n",
    "with tf.variable_scope('sent/enc', reuse=False):\n",
    "    # get word embedding\n",
    "    enc_input = tf.nn.embedding_lookup(embeddings, input_token_idxs)\n",
    "\n",
    "    # get sentence embedding\n",
    "    _, enc_state = dynamic_bi_rnn(enc_input, sent_l, config.dim_hidden, t_variables['keep_prob'])\n",
    "\n",
    "    # TODO House Holder flow\n",
    "    hidden_topic_infer =  tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='hidden_topic_infer')(enc_state)\n",
    "    prob_topic_infer = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic_infer')(hidden_topic_infer)\n",
    "\n",
    "    w_mean_topic_infer = tf.get_variable('mean_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32)\n",
    "    b_mean_topic_infer = tf.get_variable('mean_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32)\n",
    "    means_topic_infer = tf.tensordot(enc_state, w_mean_topic_infer, axes=[[1], [1]]) + b_mean_topic_infer\n",
    "    \n",
    "    w_logvar_topic_infer = tf.get_variable('logvar_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    b_logvar_topic_infer = tf.get_variable('logvar_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    logvars_topic_infer = tf.tensordot(enc_state, w_logvar_topic_infer, axes=[[1], [1]]) + b_logvar_topic_infer\n",
    "    sigma_topic_infer = tf.exp(0.5 * logvars_topic_infer)\n",
    "    gauss_topic_infer = tfd.Normal(loc=means_topic_infer, scale=sigma_topic_infer)\n",
    "    \n",
    "    # latent vectors from each gaussian dist.\n",
    "    latents_topic_infer = sample_latents(means_topic_infer, logvars_topic_infer) \n",
    "    # latent vector from gaussian mixture\n",
    "    latents_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), latents_topic_infer, transpose_a=True)\n",
    "    \n",
    "    # for beam search\n",
    "    means_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), means_topic_infer, transpose_a=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for decoding\n",
    "dec_sent_l = tf.add(sent_l, 1)\n",
    "dec_input_idxs = t_variables['dec_input_idxs']\n",
    "dec_input = tf.nn.embedding_lookup(embeddings, dec_input_idxs)\n",
    "\n",
    "dec_latents_input = tf.tile(latents_input, [1, tf.shape(dec_input)[1], 1])\n",
    "dec_concat_input = tf.concat([dec_input, dec_latents_input], -1)\n",
    "\n",
    "# decode for training\n",
    "with tf.variable_scope('sent/dec/rnn', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32, reuse=False):\n",
    "    dec_cell = tf.contrib.rnn.GRUCell(config.dim_hidden)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob = t_variables['keep_prob'])\n",
    "\n",
    "    dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(latents_input, 1))\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_concat_input, sequence_length=dec_sent_l)\n",
    "\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=dec_cell,\n",
    "        helper=helper,\n",
    "        initial_state=dec_initial_state)\n",
    "\n",
    "    dec_outputs, _, output_sent_l = tf.contrib.seq2seq.dynamic_decode(train_decoder)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(config.n_vocab, use_bias=False, name='out')\n",
    "    output_logits = output_layer(dec_outputs.rnn_output)\n",
    "    \n",
    "    output_token_idxs = tf.argmax(output_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tokens = tf.fill([batch_l], config.BOS_IDX)\n",
    "end_token = config.EOS_IDX\n",
    "\n",
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    infer_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(means_input, 1))\n",
    "    beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(infer_dec_initial_state, multiplier=config.beam_width)\n",
    "    beam_latents_input = tf.contrib.seq2seq.tile_batch(tf.squeeze(means_input, 1), multiplier=config.beam_width) # added\n",
    "    \n",
    "    beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=beam_latents_input)\n",
    "\n",
    "    beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    beam_output_token_idxs = beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    inter_means_input = tf.placeholder(tf.float32, [None, config.dim_latent])\n",
    "    \n",
    "    inter_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(inter_means_input)\n",
    "    inter_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(inter_dec_initial_state, multiplier=config.beam_width)\n",
    "    inter_beam_latents_input = tf.contrib.seq2seq.tile_batch(inter_means_input, multiplier=config.beam_width) # added\n",
    "    \n",
    "    inter_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=inter_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=inter_beam_latents_input)\n",
    "\n",
    "    inter_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inter_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    inter_beam_output_token_idxs = inter_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    topic_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic)\n",
    "    topic_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(topic_dec_initial_state, multiplier=config.beam_width)\n",
    "    topic_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic, multiplier=config.beam_width) # added\n",
    "    \n",
    "    topic_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=topic_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=topic_beam_latents_input)\n",
    "\n",
    "    topic_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        topic_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    topic_beam_output_token_idxs = topic_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    means_topic_summary = tf.reduce_mean(means_topic_infer, 0)\n",
    "    \n",
    "    summary_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic_summary)\n",
    "    summary_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(summary_dec_initial_state, multiplier=config.beam_width)\n",
    "    summary_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic_summary, multiplier=config.beam_width) # added\n",
    "    \n",
    "    summary_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=summary_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width,\n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=summary_beam_latents_input)\n",
    "\n",
    "    summary_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        summary_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    summary_beam_output_token_idxs = summary_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language modeling cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target and mask\n",
    "dec_target_idxs = t_variables['dec_target_idxs']\n",
    "dec_mask_tokens = tf.sequence_mask(dec_sent_l, maxlen=max_sent_l+1, dtype=tf.float32)\n",
    "\n",
    "# nll for each token (averaged over batch & sentence)\n",
    "sent_loss_recon = tf.contrib.seq2seq.sequence_loss(output_logits, dec_target_idxs, dec_mask_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_l = t_variables['doc_l']\n",
    "mask_sents = tf.sequence_mask(doc_l)\n",
    "mask_sents_flatten = tf.reshape(mask_sents, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1]])\n",
    "\n",
    "prob_topic_tiled = tf.tile(tf.expand_dims(prob_topic, 1), [1, tf.shape(mask_sents)[1], 1])\n",
    "prob_topic_flatten = tf.reshape(prob_topic_tiled, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1], config.n_topic])\n",
    "prob_topic_sents = tf.boolean_mask(prob_topic_flatten, mask_sents_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferred mixture probabilities (computed for each sentence)\n",
    "categ_topic_infer = tfd.Categorical(probs=prob_topic_infer)\n",
    "\n",
    "# prior of mixture probabilities (computed for each document, tiled for each sentence)\n",
    "categ_topic = tfd.Categorical(probs=prob_topic_sents)\n",
    "\n",
    "sent_loss_kl_categ = tf.reduce_mean(tfd.kl_divergence(categ_topic_infer, categ_topic))\n",
    "\n",
    "# inference of each gaussian gaussribution (computed for each sentence)\n",
    "\n",
    "sent_loss_kl_gauss = tf.reduce_sum(tfd.kl_divergence(gauss_topic_infer, gauss_topic), -1)\n",
    "sent_loss_kl_gmm = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss), -1))\n",
    "\n",
    "sent_loss_kl = sent_loss_kl_categ + sent_loss_kl_gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "tau = tf.cast(tf.divide(tf.mod(global_step, tf.constant(config.cycle_steps)), tf.constant(config.cycle_steps)), dtype=tf.float32)\n",
    "beta = tf.minimum(1., tau/config.r_cycle)\n",
    "\n",
    "sent_loss = sent_loss_recon + beta * sent_loss_kl\n",
    "\n",
    "topic_loss = topic_loss_recon + topic_loss_kl + config.reg * topic_loss_reg\n",
    "loss = topic_loss + sent_loss\n",
    "\n",
    "# define optimizer\n",
    "if config.opt == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif config.opt == 'Adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "    \n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_sents(token_idxs, config, idx_to_word):\n",
    "    sents = []\n",
    "    for sent_idxs in token_idxs:\n",
    "        tokens = []\n",
    "        for idx in sent_idxs:\n",
    "            if idx == config.EOS_IDX: break\n",
    "            tokens.append(idx_to_word[idx])\n",
    "        sent = ' '.join(tokens)\n",
    "        sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, ppls_batch \\\n",
    "            = sess.run([loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, topic_ppls], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "    loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, sent_loss_recon_mean, sent_loss_kl_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    return loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, sent_loss_recon_mean, sent_loss_kl_mean, ppl_mean\n",
    "\n",
    "def get_all_losses(sess, batches):\n",
    "    losses = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch = \\\n",
    "        sess.run([loss, topic_loss_recon, topic_loss_kl, sent_loss_recon, sent_loss_kl], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "    print('LOSS %.2f | TM NLL: %.2f, KL: %.4f | LM NLL: %.2f, KL: %.4f' %  np.mean(losses, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_batch):\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    pred_token_idxs = sess.run(output_token_idxs, feed_dict = feed_dict)\n",
    "    true_token_idxs = [sent_idxs for instance in sample_batch for sent_idxs in instance.token_idxs]\n",
    "    \n",
    "    assert len(pred_token_idxs) == len(true_token_idxs)\n",
    "    \n",
    "    pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "    true_sents = idxs_to_sents(true_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    for i, (true_sent, pred_sent) in enumerate(zip(true_sents, pred_sents)):        \n",
    "        print(i, 'TRUE: %s' % true_sent)\n",
    "        print(i, 'PRED: %s' % pred_sent)\n",
    "\n",
    "def print_topic_sample():\n",
    "    pred_topics_freq_bow_indices, pred_topic_token_idxs = sess.run([topics_freq_bow_indices, topic_beam_output_token_idxs], \n",
    "                                                                                                           feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "    pred_topic_sents = idxs_to_sents(pred_topic_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Topic Samples-----------')\n",
    "    for i, (topic_freq_bow_idxs, pred_topic_sent) in enumerate(zip(topics_freq_bow_idxs, pred_topic_sents)):\n",
    "        print(i, ' BOW:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' SENTENCE:', pred_topic_sent)\n",
    "        \n",
    "def print_summary(test_batch):\n",
    "    feed_dict = get_feed_dict(test_batch)\n",
    "    feed_dict[t_variables['batch_l']] = config.n_topic\n",
    "    feed_dict[t_variables['keep_prob']] = 1.\n",
    "    pred_topics_freq_bow_indices, pred_summary_token_idxs = sess.run([topics_freq_bow_indices, summary_beam_output_token_idxs], feed_dict=feed_dict)\n",
    "    pred_summary_sents = idxs_to_sents(pred_summary_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Output sentences for each topic-----------')\n",
    "    print('Item idx:', test_batch[0].item_idx)\n",
    "    for i, (topic_freq_bow_idxs, pred_summary_sent) in enumerate(zip(topics_freq_bow_idxs, pred_summary_sents)):\n",
    "        print(i, ' BOW:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' SENTENCE:', pred_summary_sent)\n",
    "        \n",
    "    print('-----------Summaries-----------')\n",
    "    for i, summary in enumerate(test_batch[0].summaries):\n",
    "        print('SUMMARY %i :'%i, '\\n', summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "saver = tf.train.Saver(max_to_keep=10)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','TM','','','','LM','','VALID:','TM','','','','LM','', ''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL','REG','NLL','KL','LOSS','PPL','NLL','KL','REG','NLL','KL', 'Beta']]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>LM</th>\n",
       "      <th></th>\n",
       "      <th>VALID:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>LM</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>123.45</td>\n",
       "      <td>1035</td>\n",
       "      <td>113.05</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.90</td>\n",
       "      <td>9.13</td>\n",
       "      <td>1.46</td>\n",
       "      <td>126.48</td>\n",
       "      <td>1034</td>\n",
       "      <td>116.13</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.90</td>\n",
       "      <td>9.13</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>121.19</td>\n",
       "      <td>598</td>\n",
       "      <td>113.84</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.63</td>\n",
       "      <td>6.50</td>\n",
       "      <td>1.38</td>\n",
       "      <td>111.52</td>\n",
       "      <td>527</td>\n",
       "      <td>105.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.47</td>\n",
       "      <td>5.77</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>120.10</td>\n",
       "      <td>568</td>\n",
       "      <td>113.13</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.49</td>\n",
       "      <td>6.15</td>\n",
       "      <td>1.07</td>\n",
       "      <td>111.01</td>\n",
       "      <td>510</td>\n",
       "      <td>104.48</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.27</td>\n",
       "      <td>5.71</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>1500</td>\n",
       "      <td>119.68</td>\n",
       "      <td>553</td>\n",
       "      <td>112.78</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.03</td>\n",
       "      <td>0.91</td>\n",
       "      <td>110.78</td>\n",
       "      <td>507</td>\n",
       "      <td>104.24</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.69</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>119.22</td>\n",
       "      <td>542</td>\n",
       "      <td>112.33</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.33</td>\n",
       "      <td>5.96</td>\n",
       "      <td>0.80</td>\n",
       "      <td>110.57</td>\n",
       "      <td>497</td>\n",
       "      <td>103.96</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.13</td>\n",
       "      <td>5.67</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>118.98</td>\n",
       "      <td>536</td>\n",
       "      <td>112.10</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.31</td>\n",
       "      <td>5.93</td>\n",
       "      <td>0.75</td>\n",
       "      <td>110.24</td>\n",
       "      <td>488</td>\n",
       "      <td>103.61</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.11</td>\n",
       "      <td>5.67</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2776</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>118.70</td>\n",
       "      <td>529</td>\n",
       "      <td>111.80</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.27</td>\n",
       "      <td>5.90</td>\n",
       "      <td>0.68</td>\n",
       "      <td>110.28</td>\n",
       "      <td>483</td>\n",
       "      <td>103.52</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.66</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3276</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>118.41</td>\n",
       "      <td>523</td>\n",
       "      <td>111.50</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.24</td>\n",
       "      <td>5.87</td>\n",
       "      <td>0.63</td>\n",
       "      <td>110.10</td>\n",
       "      <td>478</td>\n",
       "      <td>103.30</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.09</td>\n",
       "      <td>5.64</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3776</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>118.35</td>\n",
       "      <td>517</td>\n",
       "      <td>111.42</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.22</td>\n",
       "      <td>5.85</td>\n",
       "      <td>0.58</td>\n",
       "      <td>109.95</td>\n",
       "      <td>474</td>\n",
       "      <td>103.13</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.07</td>\n",
       "      <td>5.63</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4276</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>118.21</td>\n",
       "      <td>512</td>\n",
       "      <td>111.27</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.20</td>\n",
       "      <td>5.83</td>\n",
       "      <td>0.55</td>\n",
       "      <td>109.83</td>\n",
       "      <td>469</td>\n",
       "      <td>103.04</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.07</td>\n",
       "      <td>5.56</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4551</th>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>118.16</td>\n",
       "      <td>510</td>\n",
       "      <td>111.21</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.20</td>\n",
       "      <td>5.81</td>\n",
       "      <td>0.53</td>\n",
       "      <td>109.75</td>\n",
       "      <td>467</td>\n",
       "      <td>102.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.06</td>\n",
       "      <td>5.52</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5051</th>\n",
       "      <td>62</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>118.02</td>\n",
       "      <td>506</td>\n",
       "      <td>111.07</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.18</td>\n",
       "      <td>5.79</td>\n",
       "      <td>0.50</td>\n",
       "      <td>109.62</td>\n",
       "      <td>462</td>\n",
       "      <td>102.83</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.45</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5551</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>117.88</td>\n",
       "      <td>503</td>\n",
       "      <td>110.93</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.76</td>\n",
       "      <td>0.48</td>\n",
       "      <td>109.61</td>\n",
       "      <td>465</td>\n",
       "      <td>102.89</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.04</td>\n",
       "      <td>5.38</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6051</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>1500</td>\n",
       "      <td>117.80</td>\n",
       "      <td>500</td>\n",
       "      <td>110.85</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.16</td>\n",
       "      <td>5.74</td>\n",
       "      <td>0.46</td>\n",
       "      <td>109.52</td>\n",
       "      <td>464</td>\n",
       "      <td>102.88</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>5.30</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6551</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>117.68</td>\n",
       "      <td>497</td>\n",
       "      <td>110.74</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.15</td>\n",
       "      <td>5.71</td>\n",
       "      <td>0.45</td>\n",
       "      <td>109.37</td>\n",
       "      <td>463</td>\n",
       "      <td>102.79</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>5.24</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6826</th>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>117.66</td>\n",
       "      <td>496</td>\n",
       "      <td>110.72</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.14</td>\n",
       "      <td>5.69</td>\n",
       "      <td>0.44</td>\n",
       "      <td>109.18</td>\n",
       "      <td>456</td>\n",
       "      <td>102.57</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.03</td>\n",
       "      <td>5.21</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7326</th>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>117.57</td>\n",
       "      <td>493</td>\n",
       "      <td>110.63</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.14</td>\n",
       "      <td>5.66</td>\n",
       "      <td>0.43</td>\n",
       "      <td>109.07</td>\n",
       "      <td>451</td>\n",
       "      <td>102.47</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.03</td>\n",
       "      <td>5.16</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7826</th>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>117.49</td>\n",
       "      <td>491</td>\n",
       "      <td>110.56</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.13</td>\n",
       "      <td>5.64</td>\n",
       "      <td>0.42</td>\n",
       "      <td>109.12</td>\n",
       "      <td>453</td>\n",
       "      <td>102.52</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.11</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8326</th>\n",
       "      <td>53</td>\n",
       "      <td>3</td>\n",
       "      <td>1500</td>\n",
       "      <td>117.43</td>\n",
       "      <td>489</td>\n",
       "      <td>110.51</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.12</td>\n",
       "      <td>5.61</td>\n",
       "      <td>0.41</td>\n",
       "      <td>109.18</td>\n",
       "      <td>458</td>\n",
       "      <td>102.65</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.07</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8826</th>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>117.37</td>\n",
       "      <td>487</td>\n",
       "      <td>110.45</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.12</td>\n",
       "      <td>5.58</td>\n",
       "      <td>0.40</td>\n",
       "      <td>109.08</td>\n",
       "      <td>457</td>\n",
       "      <td>102.57</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.05</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9101</th>\n",
       "      <td>39</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>117.31</td>\n",
       "      <td>487</td>\n",
       "      <td>110.40</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.11</td>\n",
       "      <td>5.57</td>\n",
       "      <td>0.40</td>\n",
       "      <td>108.90</td>\n",
       "      <td>450</td>\n",
       "      <td>102.37</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.03</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9601</th>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "      <td>117.23</td>\n",
       "      <td>485</td>\n",
       "      <td>110.33</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.11</td>\n",
       "      <td>5.55</td>\n",
       "      <td>0.39</td>\n",
       "      <td>109.11</td>\n",
       "      <td>455</td>\n",
       "      <td>102.52</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10101</th>\n",
       "      <td>52</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>117.13</td>\n",
       "      <td>483</td>\n",
       "      <td>110.24</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.52</td>\n",
       "      <td>0.38</td>\n",
       "      <td>109.01</td>\n",
       "      <td>455</td>\n",
       "      <td>102.55</td>\n",
       "      <td>1.21</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.97</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10601</th>\n",
       "      <td>71</td>\n",
       "      <td>4</td>\n",
       "      <td>1500</td>\n",
       "      <td>117.12</td>\n",
       "      <td>482</td>\n",
       "      <td>110.23</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.50</td>\n",
       "      <td>0.38</td>\n",
       "      <td>108.85</td>\n",
       "      <td>451</td>\n",
       "      <td>102.37</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.95</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11101</th>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>2000</td>\n",
       "      <td>117.05</td>\n",
       "      <td>480</td>\n",
       "      <td>110.16</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.48</td>\n",
       "      <td>0.37</td>\n",
       "      <td>108.91</td>\n",
       "      <td>453</td>\n",
       "      <td>102.43</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.93</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11376</th>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>117.04</td>\n",
       "      <td>480</td>\n",
       "      <td>110.16</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>5.47</td>\n",
       "      <td>0.37</td>\n",
       "      <td>108.64</td>\n",
       "      <td>455</td>\n",
       "      <td>102.48</td>\n",
       "      <td>1.24</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.90</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     TRAIN:    TM                        LM        VALID:  \\\n",
       "      Time Ep    Ct    LOSS   PPL     NLL    KL   REG   NLL    KL    LOSS   \n",
       "1       14  0     0  123.45  1035  113.05  0.37  0.90  9.13  1.46  126.48   \n",
       "501     62  0   500  121.19   598  113.84  0.17  0.63  6.50  1.38  111.52   \n",
       "1001    71  0  1000  120.10   568  113.13  0.26  0.49  6.15  1.07  111.01   \n",
       "1501    71  0  1500  119.68   553  112.78  0.39  0.40  6.03  0.91  110.78   \n",
       "2001    63  0  2000  119.22   542  112.33  0.49  0.33  5.96  0.80  110.57   \n",
       "2276    39  1     0  118.98   536  112.10  0.54  0.31  5.93  0.75  110.24   \n",
       "2776    52  1   500  118.70   529  111.80  0.61  0.27  5.90  0.68  110.28   \n",
       "3276    63  1  1000  118.41   523  111.50  0.67  0.24  5.87  0.63  110.10   \n",
       "3776    62  1  1500  118.35   517  111.42  0.72  0.22  5.85  0.58  109.95   \n",
       "4276    62  1  2000  118.21   512  111.27  0.77  0.20  5.83  0.55  109.83   \n",
       "4551    40  2     0  118.16   510  111.21  0.79  0.20  5.81  0.53  109.75   \n",
       "5051    62  2   500  118.02   506  111.07  0.82  0.18  5.79  0.50  109.62   \n",
       "5551    64  2  1000  117.88   503  110.93  0.86  0.17  5.76  0.48  109.61   \n",
       "6051    64  2  1500  117.80   500  110.85  0.89  0.16  5.74  0.46  109.52   \n",
       "6551    64  2  2000  117.68   497  110.74  0.91  0.15  5.71  0.45  109.37   \n",
       "6826    41  3     0  117.66   496  110.72  0.93  0.14  5.69  0.44  109.18   \n",
       "7326    64  3   500  117.57   493  110.63  0.95  0.14  5.66  0.43  109.07   \n",
       "7826    55  3  1000  117.49   491  110.56  0.97  0.13  5.64  0.42  109.12   \n",
       "8326    53  3  1500  117.43   489  110.51  0.99  0.12  5.61  0.41  109.18   \n",
       "8826    52  3  2000  117.37   487  110.45  1.01  0.12  5.58  0.40  109.08   \n",
       "9101    39  4     0  117.31   487  110.40  1.02  0.11  5.57  0.40  108.90   \n",
       "9601    53  4   500  117.23   485  110.33  1.04  0.11  5.55  0.39  109.11   \n",
       "10101   52  4  1000  117.13   483  110.24  1.06  0.10  5.52  0.38  109.01   \n",
       "10601   71  4  1500  117.12   482  110.23  1.08  0.10  5.50  0.38  108.85   \n",
       "11101   53  4  2000  117.05   480  110.16  1.09  0.10  5.48  0.37  108.91   \n",
       "11376   39  5     0  117.04   480  110.16  1.10  0.09  5.47  0.37  108.64   \n",
       "\n",
       "         TM                        LM               \n",
       "        PPL     NLL    KL   REG   NLL    KL   Beta  \n",
       "1      1034  116.13  0.33  0.90  9.13  1.40  0.000  \n",
       "501     527  105.05  0.15  0.47  5.77  0.78  0.088  \n",
       "1001    510  104.48  0.44  0.27  5.71  0.66  0.176  \n",
       "1501    507  104.24  0.56  0.17  5.69  0.46  0.264  \n",
       "2001    497  103.96  0.69  0.13  5.67  0.36  0.352  \n",
       "2276    488  103.61  0.71  0.11  5.67  0.37  0.400  \n",
       "2776    483  103.52  0.84  0.10  5.66  0.35  0.488  \n",
       "3276    478  103.30  0.90  0.09  5.64  0.30  0.576  \n",
       "3776    474  103.13  0.93  0.07  5.63  0.30  0.664  \n",
       "4276    469  103.04  0.97  0.07  5.56  0.26  0.752  \n",
       "4551    467  102.97  0.98  0.06  5.52  0.28  0.800  \n",
       "5051    462  102.83  1.04  0.05  5.45  0.28  0.888  \n",
       "5551    465  102.89  1.05  0.04  5.38  0.25  0.976  \n",
       "6051    464  102.88  1.04  0.04  5.30  0.26  1.000  \n",
       "6551    463  102.79  1.04  0.03  5.24  0.27  1.000  \n",
       "6826    456  102.57  1.09  0.03  5.21  0.27  1.000  \n",
       "7326    451  102.47  1.16  0.03  5.16  0.27  1.000  \n",
       "7826    453  102.52  1.18  0.02  5.11  0.28  1.000  \n",
       "8326    458  102.65  1.18  0.02  5.07  0.25  1.000  \n",
       "8826    457  102.57  1.18  0.02  5.05  0.26  1.000  \n",
       "9101    450  102.37  1.20  0.02  5.03  0.28  1.000  \n",
       "9601    455  102.52  1.28  0.02  5.00  0.28  1.000  \n",
       "10101   455  102.55  1.21  0.02  4.97  0.27  1.000  \n",
       "10601   451  102.37  1.25  0.02  4.95  0.27  1.000  \n",
       "11101   453  102.43  1.25  0.01  4.93  0.29  1.000  \n",
       "11376   455  102.48  1.24  0.02  4.90  0.29  0.000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Output sentences for each topic-----------\n",
      "Item idx: B000VB7EFW\n",
      "0  BOW: refund smells sold bucks cheaply brown saved sound terrible told\n",
      "0  SENTENCE: i bought this case\n",
      "1  BOW: price quality 'm made time - 've back thing purchased\n",
      "1  SENTENCE: i bought this case\n",
      "2  BOW: cover color bottom keyboard pro mac apple hard top scratches\n",
      "2  SENTENCE: i bought this case\n",
      "3  BOW: ! love perfect recommend ... highly buy color cute perfectly\n",
      "3  SENTENCE: i bought this case\n",
      "4  BOW: sleeve ; & protection pro protect perfectly air snug bit\n",
      "4  SENTENCE: i bought this case\n",
      "5  BOW: closed toshiba fabric metal device returned plug stitching inexpensive lining\n",
      "5  SENTENCE: i bought this case\n",
      "6  BOW: carry pockets strap room shoulder back handle compartment space straps\n",
      "6  SENTENCE: i bought this case\n",
      "7  BOW: bought nice buy recommend ... item $ months amazon put\n",
      "7  SENTENCE: i bought this case\n",
      "8  BOW: seam local well-made pulled ground folder waterproof world average abuse\n",
      "8  SENTENCE: i bought this case\n",
      "9  BOW: pocket inside zipper small ipad room padding power charger mouse\n",
      "9  SENTENCE: i bought this case\n",
      "-----------Summaries-----------\n",
      "SUMMARY 0 : \n",
      " This is a very well made bag, nice construction, lots of pockets.\n",
      "the straps are very comfortable.\n",
      "and protects everything inside.\n",
      "It says\n",
      "it fits a 17inch notebook,\n",
      "however it did not.\n",
      "after using the pack for less than a month,\n",
      "it is ripping out already.\n",
      "SUMMARY 1 : \n",
      " This is a very well made bag, nice construction, lots of pockets.\n",
      "The quality is excellent\n",
      "and it is very durable.\n",
      "the straps are very comfortable.\n",
      "and protects everything inside.\n",
      "The color is a true red\n",
      "and it fits nicely.\n",
      "it is ripping out already.\n",
      "and it doesnt fit.\n",
      "It's just not the lightest backpack\n",
      "because some zipper teeth were not aligned.\n",
      "SUMMARY 2 : \n",
      " The quality is excellent\n",
      "and I love all the pockets and compartments.\n",
      "and it is very durable.\n",
      "and can't beleive the price\n",
      "and protects everything inside.\n",
      "The laptop\n",
      "doesn't fit in it.\n",
      "it is ripping out already.\n",
      "It's just not the lightest backpack\n",
      "0 TRUE: it 's pretty awesome\n",
      "0 PRED: i 's a a\n",
      "1 TRUE: i got this for both work and personal use\n",
      "1 PRED: i , the bag my # and i\n",
      "2 TRUE: my # `` laptop fits in there great , with lots of room for whatever else you might need\n",
      "2 PRED: i laptop `` # , # # is the the the\n",
      "3 TRUE: plenty of padding\n",
      "3 PRED: i is the , , , ,\n",
      "4 TRUE: more than enough pockets , which helps keep things organized\n",
      "4 PRED: i , the for and and is the the\n",
      "5 TRUE: good quality all-around\n",
      "5 PRED: i case is the and\n",
      "6 TRUE: straps are nice , and work well for those of us with broad shoulders\n",
      "6 PRED: i , the for and the , the the\n",
      "7 TRUE: very pleased with the purchase\n",
      "7 PRED: i good of the case of\n",
      "8 TRUE: the case fit perfectly with my macbook pro # with an ipearl laptop case\n",
      "8 PRED: i case is the\n",
      "9 TRUE: it fits snuggly within the case , the zipper feels sturdy , design is simple with minimal brand showing\n",
      "9 PRED: i 's my # the laptop\n",
      "10 TRUE: the greatest thing about the case is the material\n",
      "10 PRED: i case is is the case is the case is\n",
      "11 TRUE: it is perfectly thick and <unk> , not completely of course , but more than sufficient for most minimal drop\n",
      "11 PRED: i 's a for the the\n",
      "12 TRUE: the reason i purchased this case was because of my previous incident of water spill in my bag\n",
      "12 PRED: i case is the the case\n",
      "13 TRUE: well , i have n't tried putting my laptop inside and pour water on it , but most water was absorbed when i put a few drops of water on the empty case\n",
      "13 PRED: i , the the a be to\n",
      "14 TRUE: i believe it would sufficiently protect the laptop from minor water spill inside your back pack\n",
      "14 PRED: i have this for be to the\n",
      "15 TRUE: i moved up to real world by eliminating the standard issue pc bag and <unk>\n",
      "15 PRED: i have this to the the , the\n",
      "16 TRUE: this backpack is organized to manage my # years of carrying a bag that is so simple and convenient\n",
      "16 PRED: i the the a , carry\n",
      "17 TRUE: here is my advice , get rid of that pc bag and get this xenon backpack\n",
      "17 PRED: i , a laptop and but the , the i\n",
      "18 TRUE: if not this one ... . . find one ... but will not find the quality , design , professional design , ease of carry and price .\n",
      "18 PRED: i you the is a it and #\n",
      "19 TRUE: i shopped and shopped\n",
      "19 PRED: i bought this this to and\n",
      "20 TRUE: save a back ache and time\n",
      "20 PRED: i 's great of , the\n",
      "21 TRUE: buy this from amazon because you will save about $ # as compared to ... .\n",
      "21 PRED: i , case a\n",
      "22 TRUE: overall i love the bag\n",
      "22 PRED: i , have this case\n",
      "23 TRUE: it is built well\n",
      "23 PRED: i 's a for and and\n",
      "24 TRUE: the only thing i would change is the inner lining is attached by velcro to the bag wall and it comes undone unless your pretty careful\n",
      "24 PRED: i case thing is would have to the the\n",
      "25 TRUE: not a big deal\n",
      "25 PRED: i have little case\n",
      "26 TRUE: i would be sure to buy from this maker again\n",
      "26 PRED: i the the the it the\n",
      "27 TRUE: they need to sell a wider variety of colors\n",
      "27 PRED: i have a the\n",
      "28 TRUE: i sort of wished i never ordered this product\n",
      "28 PRED: i have this the , would it this case\n",
      "29 TRUE: i use a backpack for school and usually end up putting my laptop in it instead of carrying this messenger bag on the side\n",
      "29 PRED: i have the # to the for the the\n",
      "30 TRUE: the good : it 's stylish , matches my laptop 's design , and it 's spacious enough for the laptop , power cords and mouse\n",
      "30 PRED: i case is is is a\n",
      "31 TRUE: the bad : it 's a little too spacious and i wish now that i had gotten a backpack instead of a messenger bag\n",
      "31 PRED: i only is the the the\n",
      "32 TRUE: unless you really love the look and prefer a messenger bag , i would pass this up for something more efficient\n",
      "32 PRED: i , have like a case of the the\n",
      "33 TRUE: bottom case i received did not fit my new model macbook air\n",
      "33 PRED: i , is the the\n",
      "34 TRUE: they sent me a replacement that also did not fit\n",
      "34 PRED: i was the to the\n",
      "35 TRUE: i finally decided to go with a different brand\n",
      "35 PRED: i would this this get\n",
      "36 TRUE: material is stiff and cheap\n",
      "36 PRED: i , a for the\n",
      "37 TRUE: feels as though it will chip easily\n",
      "37 PRED: i , the , is\n",
      "38 TRUE: purchased as a christmas gift\n",
      "38 PRED: i the the good of\n",
      "39 TRUE: product received was damaged and appeared used ; not new\n",
      "39 PRED: i , a a\n",
      "40 TRUE: searched for a telephone number for product distributor & could only find an address , no telephone number to <unk> <unk> of product\n",
      "40 PRED: i is a # , , the\n",
      "41 TRUE: barely fits a # . # `` notebook\n",
      "41 PRED: i , my # and # # # and\n",
      "42 TRUE: excellent quality ... however , i believe the description should say , the back of the backpack and the straps are not leather\n",
      "42 PRED: i is for the , and is , laptop , the\n",
      "43 TRUE: nor is the inside ... only the sides and front outside\n",
      "43 PRED: i , a case of the bag case\n",
      "44 TRUE: but even so , it 's very well made\n",
      "44 PRED: i i the it it 's a good\n",
      "45 TRUE: my husband loved it and if he loves it , then all is well in the world\n",
      "45 PRED: i case is the the the i have it\n",
      "46 TRUE: i was n't sure how i would like the design in person and i love it\n",
      "46 PRED: i is a fit the it would have it case\n",
      "47 TRUE: it has a stripped design instead of the blue pattern on the other side which i think looks perfect\n",
      "47 PRED: i is a little of for and the case\n",
      "48 TRUE: the only thing i do n't like is sometimes when i pull the zipper fast it will get stuck on the inside fabric , besides that it is well worth your money\n",
      "48 PRED: i only is is have n't have the a\n",
      "49 TRUE: great buy for the price !\n",
      "49 PRED: i is this the case is\n",
      "50 TRUE: looks great but wound up not working out\n",
      "50 PRED: i the the the\n",
      "51 TRUE: the thread is trying to un do itself and the hole <unk> front of the bag is big enough for things to fall out\n",
      "51 PRED: i is a a and the the the\n",
      "52 TRUE: nice for a weekend or overnight trips but i feel like it 's not strong enough to hold\n",
      "52 PRED: i , my # , # , , i 'm to\n",
      "53 TRUE: after # months of use the case works fine\n",
      "53 PRED: i the . , the , case\n",
      "54 TRUE: the keyboard cover is lifting a little under the space bar and it wo n't go down\n",
      "54 PRED: i case is is the and great bit of the\n",
      "55 TRUE: it seems like it got stretched out somehow\n",
      "55 PRED: i 's a the is a the\n",
      "56 TRUE: i bought this as a gift for my husband and he raves about it\n",
      "56 PRED: i have this for the little of the laptop\n",
      "57 TRUE: have n't had it long but it seems very sturdy and has plenty of pockets and space inside\n",
      "57 PRED: i is have the\n",
      "58 TRUE: it goes through security at the airport without having to remove the laptop - my husband loves that feature\n",
      "58 PRED: i 's a a and the and\n",
      "59 TRUE: the color is not the same as the picture , and the clips that hold the bottom protective cover on keeps coming off\n",
      "59 PRED: i bag is a a case of the case\n",
      "60 TRUE: it is very annoying that when i go to pick it up , it comes unhooked and i have to <unk> it\n",
      "60 PRED: i 's a a and i is the\n",
      "61 TRUE: would not buy this one ! ! !\n",
      "61 PRED: i is a this case\n",
      "62 TRUE: i 've had this for well over a month , no tears , no thread hanging , no snags with the zip\n",
      "62 PRED: i have this a the the # the #\n",
      "63 TRUE: i still have room to fit my charger or my ipad mini with the case and cover , or a couple notebooks\n",
      "63 PRED: i would recommend a for a a laptop and # laptop and\n"
     ]
    }
   ],
   "source": [
    "if len(log_df) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "while epoch < config.epochs:\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = get_feed_dict(batch)\n",
    "\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, sent_loss_kl_categ_batch, sent_loss_kl_gmm_batch, ppls_batch = \\\n",
    "        sess.run([opt, loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, sent_loss_kl_categ, sent_loss_kl_gmm, topic_ppls], feed_dict = feed_dict)\n",
    "   \n",
    "        if sent_loss_kl_batch == np.inf:\n",
    "            print('Nan occured')\n",
    "            ckpt = tf.train.get_checkpoint_state(config.modeldir)\n",
    "            model_checkpoint_path = ckpt.all_model_checkpoint_paths[-1]\n",
    "            saver.restore(sess, model_checkpoint_path)            \n",
    "            break\n",
    "            \n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if ct%config.log_period==0:\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train, sent_loss_recon_train, sent_loss_kl_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, sent_loss_recon_dev, sent_loss_kl_dev, ppl_dev = get_loss(sess, dev_batches)\n",
    "            global_step_log, beta_eval = sess.run([tf.train.get_global_step(), beta])\n",
    "            \n",
    "            if loss_dev < loss_min:\n",
    "                loss_min = loss_dev\n",
    "                saver.save(sess, config.modelpath, global_step=global_step_log)\n",
    "\n",
    "            clear_output()\n",
    "    \n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_reg_train, '%.2f'%sent_loss_recon_train, '%.2f'%sent_loss_kl_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_reg_dev, '%.2f'%sent_loss_recon_dev, '%.2f'%sent_loss_kl_dev,  '%.3f'%beta_eval],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "\n",
    "            print_summary(test_batches[1][1])\n",
    "            print_sample(batch)\n",
    "            \n",
    "            time_start = time.time()\n",
    "            \n",
    "    epoch += 1\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_prob_topic, _prob_topic_sents, _prob_topic_infer, _means_topic_infer = debug_value([prob_topic, prob_topic_sents, prob_topic_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_i = 4\n",
    "_prob_topic_sents[batch_i], _prob_topic_infer[batch_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_means_topic_infer[0][:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_means_topic, b_means_topic = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"topic/dec/mean_topic\")\n",
    "\n",
    "pred_topic_embeddings, pred_topic_bow, pred_means_topic, pred_logvars_topic, pred_token_idxs, _w_means_topic, _b_means_topic, _w_mean_topic_infer = \\\n",
    "                                sess.run([topic_embeddings, topic_bow, means_topic, logvars_topic, topic_beam_output_token_idxs, w_means_topic, b_means_topic, w_mean_topic_infer], \n",
    "                                         feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "\n",
    "pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "\n",
    "pred_topics_freq_bow_indices = np.argsort(pred_topic_bow, 1)[:, ::-1][:, :10]\n",
    "pred_topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idxs in pred_topics_freq_bow_idxs:\n",
    "    print([idx_to_word[idx] for idx in idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_topic_embeddings[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_topic_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_w_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_w_mean_topic_infer[:, :10, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_enc_state_infer, _means_topic_infer = debug_value([enc_state_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_enc_state_infer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_means_topic_infer[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
