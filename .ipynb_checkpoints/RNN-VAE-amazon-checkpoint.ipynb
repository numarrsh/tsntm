{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import pdb\n",
    "from six.moves import zip_longest\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import debug as tf_debug\n",
    "import _pickle as cPickle\n",
    "import random\n",
    "\n",
    "from data_structure import load_data\n",
    "from components import dynamic_rnn, dynamic_bi_rnn, DiagonalGaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '1', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('datadir', 'data', 'directory of input data')\n",
    "flags.DEFINE_string('dataname', 'sports_sents.pkl', 'name of data')\n",
    "flags.DEFINE_string('modeldir', 'NAS/model', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'sports', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 10, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 32, 'batch size')\n",
    "flags.DEFINE_integer('log_period', 100, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adam', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.001, 'lr')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 1.0, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_integer('warmup', 10, 'warmup period for KL')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 10, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_latent')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_output')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "config = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(config.datadir, config.dataname)\n",
    "data_train, data_dev, data_test, word_to_idx, idx_to_word = cPickle.load(open(data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouper(iterable, n, fillvalue=None, shorten=True, num_groups=None, shuffle=False):\n",
    "    args = [iter(iterable)] * n\n",
    "    out = zip_longest(*args, fillvalue=fillvalue)\n",
    "    out = list(out)\n",
    "    if shuffle: random.shuffle(out)\n",
    "    if num_groups is not None:\n",
    "        default = (fillvalue,) * n\n",
    "        assert isinstance(num_groups, int)\n",
    "        out = list(each for each, _ in zip_longest(out, range(num_groups), fillvalue=default))\n",
    "    if shorten:\n",
    "        assert fillvalue is None\n",
    "        out = (tuple(e for e in each if e is not None) for each in out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = grouper(data_train, config.batch_size, shuffle=True)\n",
    "dev_batches = list(grouper(data_dev, config.batch_size))\n",
    "test_batches = list(grouper(data_test, config.batch_size))\n",
    "num_train_batches = len(list(grouper(data_train, config.batch_size, shuffle=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('PAD_IDX', word_to_idx[PAD], 'PAD_IDX')\n",
    "flags.DEFINE_integer('UNK_IDX', word_to_idx[UNK], 'UNK_IDX')\n",
    "flags.DEFINE_integer('BOS_IDX', word_to_idx[BOS], 'BOS_IDX')\n",
    "flags.DEFINE_integer('EOS_IDX', word_to_idx[EOS], 'EOS_IDX')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "\n",
    "maximum_iterations = max([max([len(sent_idx) for sent_idx in batch]) for batch in dev_batches])\n",
    "flags.DEFINE_integer('maximum_iterations', maximum_iterations, 'maximum_iterations')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train'):\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    sent_l = [len(sent_idxs) for sent_idxs in batch]\n",
    "    dec_sent_l = [len(sent_idxs)+1 for sent_idxs in batch]\n",
    "    max_sent_l = max(sent_l)\n",
    "\n",
    "    token_idxs_matrix = np.zeros([batch_size, max_sent_l], np.int32)\n",
    "    dec_input_idxs_matrix = np.zeros([batch_size, max_sent_l+1], np.int32)\n",
    "    dec_target_idxs_matrix = np.zeros([batch_size, max_sent_l+1], np.int32)\n",
    "    \n",
    "    for i, sent_idxs in enumerate(batch):\n",
    "        token_idxs_matrix[i, :len(sent_idxs)] = np.asarray(sent_idxs)\n",
    "\n",
    "        sent_idxs_dropout = np.asarray(sent_idxs)\n",
    "        sent_idxs_dropout[np.random.rand(len(sent_idxs)) > config.word_keep_prob] = config.UNK_IDX\n",
    "        dec_input_idxs_matrix[i, :len(sent_idxs)+1] = np.concatenate([[config.BOS_IDX], sent_idxs_dropout])\n",
    "\n",
    "        dec_target_idxs_matrix[i, :len(sent_idxs)+1] = np.asarray(sent_idxs + [config.EOS_IDX])\n",
    "\n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "\n",
    "    feed_dict = {\n",
    "                t_variables['token_idxs']: token_idxs_matrix,\n",
    "                t_variables['dec_input_idxs']: dec_input_idxs_matrix, t_variables['dec_target_idxs']: dec_target_idxs_matrix, \n",
    "                t_variables['batch_l']: batch_size, t_variables['sent_l']: sent_l, t_variables['dec_sent_l']: dec_sent_l,\n",
    "                t_variables['keep_prob']: keep_prob}\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_shape(variables):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        sample_batch = test_batches[0]\n",
    "        feed_dict = get_feed_dict(sample_batch)\n",
    "        _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "            \n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fed variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32)\n",
    "t_variables['batch_l'] = tf.placeholder(tf.int32, [])\n",
    "t_variables['token_idxs'] = tf.placeholder(tf.int32, [None, None])\n",
    "t_variables['dec_input_idxs'] = tf.placeholder(tf.int32, [None, None])\n",
    "t_variables['dec_target_idxs'] = tf.placeholder(tf.int32, [None, None])\n",
    "t_variables['sent_l'] = tf.placeholder(tf.int32, [None])\n",
    "t_variables['dec_sent_l'] = tf.placeholder(tf.int32, [None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trained variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = tf.float32\n",
    "\n",
    "dim_hidden = config.dim_hidden\n",
    "dim_latent = config.dim_latent\n",
    "\n",
    "# with tf.variable_scope('emb', reuse=tf.AUTO_REUSE):\n",
    "#     embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=dtype, initializer=tf.random_uniform_initializer(-1., 1.))\n",
    "    \n",
    "# with tf.variable_scope('enc', reuse=tf.AUTO_REUSE):\n",
    "#     w_enc = tf.get_variable('w', [dim_hidden, 2 * dim_latent], dtype=dtype, initializer=tf.random_uniform_initializer(-1., 1.))\n",
    "#     b_enc = tf.get_variable('b', [2 * dim_latent], dtype=dtype, initializer=tf.random_uniform_initializer(-1., 1.))\n",
    "    \n",
    "# with tf.variable_scope('dec', reuse=tf.AUTO_REUSE):\n",
    "#     w_dec = tf.get_variable('w', [dim_latent, dim_hidden], dtype=dtype, initializer=tf.random_uniform_initializer(-1., 1.))\n",
    "#     b_dec = tf.get_variable('b', [dim_hidden], dtype=dtype, initializer=tf.random_uniform_initializer(-1., 1.))\n",
    "\n",
    "if config.warmup > 0:\n",
    "    beta = tf.Variable(0.1, name='beta', trainable=False)    \n",
    "    \n",
    "with tf.variable_scope('emb'):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=dtype, initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "with tf.variable_scope('enc'):\n",
    "    w_enc = tf.get_variable('w_enc', [dim_hidden, 2 * dim_latent], dtype=dtype)\n",
    "    b_enc = tf.get_variable('b_enc', [2 * dim_latent], dtype=dtype)\n",
    "    \n",
    "with tf.variable_scope('dec'):\n",
    "    w_dec = tf.get_variable('w_dec', [dim_latent, dim_hidden], dtype=dtype)\n",
    "    b_dec = tf.get_variable('b_dec', [dim_hidden], dtype=dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encode sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "batch_l = t_variables['batch_l']\n",
    "sent_l = t_variables['sent_l']\n",
    "max_sent_l = tf.reduce_max(sent_l)\n",
    "token_idxs = t_variables['token_idxs']\n",
    "\n",
    "# get sentence embedding\n",
    "enc_input = tf.nn.embedding_lookup(embeddings, token_idxs)\n",
    "_, enc_state = dynamic_rnn(enc_input, sent_l, dim_hidden, t_variables['keep_prob'], cell_name='Model/sent')\n",
    "\n",
    "# encode to parameter \n",
    "means_logvars = tf.nn.relu(tf.matmul(enc_state, w_enc) + b_enc)\n",
    "means, logvars = tf.split(means_logvars, 2, 1)\n",
    "\n",
    "# reparameterize\n",
    "noises = tf.random_normal(tf.shape(means))\n",
    "latents = means + tf.exp(0.5 * logvars) * noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder_2:0 : (32, 3)\n",
      "embedding_lookup:0 : (32, 3, 256)\n",
      "Model/sent/rnn/while/Exit_3:0 : (32, 512)\n",
      "split:0 : (32, 32)\n",
      "split:1 : (32, 32)\n",
      "add_1:0 : (32, 32)\n"
     ]
    }
   ],
   "source": [
    "debug_shape([token_idxs, enc_input, enc_state, means, logvars, latents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decode sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for decoding\n",
    "dec_input_idxs = t_variables['dec_input_idxs']\n",
    "dec_input = tf.nn.embedding_lookup(embeddings, dec_input_idxs)\n",
    "\n",
    "dec_latent_input = tf.tile(tf.expand_dims(latents, 1), [1, tf.shape(dec_input_idxs)[1], 1])\n",
    "dec_concat_input = tf.concat([dec_input, dec_latent_input], 2)\n",
    "\n",
    "# decode for training\n",
    "dec_sent_l = t_variables['dec_sent_l']\n",
    "\n",
    "with tf.variable_scope('Model/sent/dec', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32, reuse=tf.AUTO_REUSE):\n",
    "    dec_cell = tf.contrib.rnn.GRUCell(dim_hidden)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob = t_variables['keep_prob'])\n",
    "\n",
    "    dec_initial_state = tf.nn.relu(tf.matmul(latents, w_dec) + b_dec)\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_concat_input, sequence_length=dec_sent_l)\n",
    "\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=dec_cell,\n",
    "        helper=helper,\n",
    "        initial_state=dec_initial_state)\n",
    "\n",
    "    dec_outputs, _, output_sent_l = tf.contrib.seq2seq.dynamic_decode(train_decoder)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(config.n_vocab, use_bias=False, name=\"output_projection\")\n",
    "    output_logits = output_layer(dec_outputs.rnn_output)\n",
    "    \n",
    "    output_token_idxs = tf.argmax(output_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concat:0 : (32, 4, 288)\n",
      "Model/sent/dec/output_projection/Tensordot:0 : (32, 4, 20000)\n",
      "Model/sent/dec/ArgMax:0 : (32, 4)\n"
     ]
    }
   ],
   "source": [
    "debug_shape([dec_concat_input, output_logits, output_token_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define cost & optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target and mask\n",
    "dec_target_idxs = t_variables['dec_target_idxs']\n",
    "max_dec_sent_l = tf.reduce_max(dec_sent_l)\n",
    "dec_mask_tokens = tf.sequence_mask(dec_sent_l, maxlen=max_dec_sent_l, dtype=tf.float32)\n",
    "\n",
    "recon_loss = tf.contrib.seq2seq.sequence_loss(output_logits, dec_target_idxs, dec_mask_tokens) # nll for each token (averaged over batch & sentence)\n",
    "\n",
    "# define loss\n",
    "kl_losses = tf.reduce_sum(-0.5 * (logvars - tf.square(means) - tf.exp(logvars) + 1.0), 1) # sum over latent dimentsion    \n",
    "kl_loss = tf.reduce_mean(kl_losses, [0]) #mean of kl_losses over batches\n",
    "\n",
    "loss = recon_loss + beta * kl_loss\n",
    "\n",
    "# define optimizer\n",
    "if (config.opt == 'Adam'):\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif (config.opt == 'Adagrad'):\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "elif (config.opt == 'Sgd'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(config.lr)\n",
    "    \n",
    "    \n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum:0 : (32,)\n"
     ]
    }
   ],
   "source": [
    "debug_shape([kl_losses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_sent(line_idxs, config, idx_to_word):\n",
    "    tokens = []\n",
    "    for idx in line_idxs:\n",
    "        if idx == config.EOS_IDX: break\n",
    "        tokens.append(idx_to_word[idx])\n",
    "    sent = ' '.join(tokens)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    for batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch = sess.run(loss, feed_dict = feed_dict)\n",
    "        losses += [loss_batch]        \n",
    "    loss_mean = np.mean(losses)\n",
    "    return loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_batch):\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    pred_token_idxs = sess.run(output_token_idxs, feed_dict = feed_dict)\n",
    "    true_token_idxs = sample_batch\n",
    "    \n",
    "    assert len(pred_token_idxs) == len(true_token_idxs)\n",
    "    \n",
    "    for true_sent_idxs, pred_sent_idxs in zip(true_token_idxs, pred_token_idxs):\n",
    "        true_sent = idxs_to_sent(true_sent_idxs, config, idx_to_word)\n",
    "        pred_sent = idxs_to_sent(pred_sent_idxs, config, idx_to_word)\n",
    "\n",
    "        print('True: %s' % true_sent)\n",
    "        print('Pred: %s' % pred_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "logs = []\n",
    "losses_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 0.001\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Step: 0 | LOSS TRAIN: 9.91, DEV: 9.89 | NLL: 20069.90, KL: 0.1855| BETA: 0.001008\n",
      "Epoch: 1, Step: 100 | LOSS TRAIN: 6.98, DEV: 6.41 | NLL: 1056.81, KL: 14.7306| BETA: 0.001016\n",
      "Epoch: 1, Step: 200 | LOSS TRAIN: 6.66, DEV: 6.29 | NLL: 770.65, KL: 15.5133| BETA: 0.001024\n",
      "Epoch: 1, Step: 300 | LOSS TRAIN: 6.52, DEV: 6.08 | NLL: 667.73, KL: 17.5398| BETA: 0.001032\n",
      "Epoch: 1, Step: 400 | LOSS TRAIN: 6.41, DEV: 5.92 | NLL: 595.06, KL: 19.6154| BETA: 0.001040\n",
      "Epoch: 1, Step: 500 | LOSS TRAIN: 6.29, DEV: 5.80 | NLL: 525.86, KL: 22.0996| BETA: 0.001047\n",
      "Epoch: 1, Step: 600 | LOSS TRAIN: 6.19, DEV: 5.68 | NLL: 477.76, KL: 24.3948| BETA: 0.001055\n",
      "Epoch: 1, Step: 700 | LOSS TRAIN: 6.11, DEV: 5.62 | NLL: 436.99, KL: 26.3502| BETA: 0.001063\n",
      "Epoch: 1, Step: 800 | LOSS TRAIN: 6.04, DEV: 5.54 | NLL: 406.53, KL: 28.3587| BETA: 0.001071\n",
      "Epoch: 1, Step: 900 | LOSS TRAIN: 5.98, DEV: 5.46 | NLL: 381.65, KL: 30.3355| BETA: 0.001079\n",
      "Epoch: 1, Step: 1000 | LOSS TRAIN: 5.92, DEV: 5.38 | NLL: 360.19, KL: 32.2337| BETA: 0.001087\n",
      "Epoch: 1, Step: 1100 | LOSS TRAIN: 5.87, DEV: 5.35 | NLL: 340.75, KL: 34.1649| BETA: 0.001095\n",
      "Epoch: 1, Step: 1200 | LOSS TRAIN: 5.82, DEV: 5.25 | NLL: 323.66, KL: 36.1527| BETA: 0.001103\n",
      "Epoch: 1, Step: 1300 | LOSS TRAIN: 5.77, DEV: 5.19 | NLL: 306.98, KL: 38.3236| BETA: 0.001111\n",
      "Epoch: 1, Step: 1400 | LOSS TRAIN: 5.72, DEV: 5.16 | NLL: 292.48, KL: 40.5959| BETA: 0.001119\n",
      "Epoch: 1, Step: 1500 | LOSS TRAIN: 5.68, DEV: 5.10 | NLL: 279.16, KL: 42.8240| BETA: 0.001126\n",
      "Epoch: 1, Step: 1600 | LOSS TRAIN: 5.64, DEV: 5.04 | NLL: 268.27, KL: 44.8995| BETA: 0.001134\n",
      "Epoch: 1, Step: 1700 | LOSS TRAIN: 5.61, DEV: 5.00 | NLL: 258.17, KL: 46.9320| BETA: 0.001142\n",
      "Epoch: 1, Step: 1800 | LOSS TRAIN: 5.57, DEV: 4.96 | NLL: 247.53, KL: 48.9340| BETA: 0.001150\n",
      "Epoch: 1, Step: 1900 | LOSS TRAIN: 5.53, DEV: 4.93 | NLL: 239.36, KL: 50.9173| BETA: 0.001158\n",
      "Epoch: 1, Step: 2000 | LOSS TRAIN: 5.51, DEV: 4.89 | NLL: 232.20, KL: 52.6314| BETA: 0.001166\n",
      "Epoch: 1, Step: 2100 | LOSS TRAIN: 5.48, DEV: 4.86 | NLL: 225.70, KL: 54.2655| BETA: 0.001174\n",
      "Epoch: 1, Step: 2200 | LOSS TRAIN: 5.45, DEV: 4.86 | NLL: 219.51, KL: 55.7766| BETA: 0.001182\n",
      "Epoch: 1, Step: 2300 | LOSS TRAIN: 5.43, DEV: 4.82 | NLL: 213.08, KL: 57.3227| BETA: 0.001190\n",
      "Epoch: 1, Step: 2400 | LOSS TRAIN: 5.40, DEV: 4.81 | NLL: 206.76, KL: 58.9328| BETA: 0.001198\n",
      "Epoch: 1, Step: 2500 | LOSS TRAIN: 5.37, DEV: 4.81 | NLL: 200.71, KL: 60.4099| BETA: 0.001206\n",
      "Epoch: 1, Step: 2600 | LOSS TRAIN: 5.35, DEV: 4.75 | NLL: 195.93, KL: 61.7870| BETA: 0.001213\n",
      "Epoch: 1, Step: 2700 | LOSS TRAIN: 5.33, DEV: 4.74 | NLL: 191.17, KL: 63.0377| BETA: 0.001221\n",
      "Epoch: 1, Step: 2800 | LOSS TRAIN: 5.31, DEV: 4.72 | NLL: 187.20, KL: 64.2915| BETA: 0.001229\n",
      "Epoch: 1, Step: 2900 | LOSS TRAIN: 5.28, DEV: 4.70 | NLL: 182.99, KL: 65.4662| BETA: 0.001237\n",
      "Epoch: 1, Step: 3000 | LOSS TRAIN: 5.26, DEV: 4.69 | NLL: 178.76, KL: 66.6695| BETA: 0.001245\n",
      "Epoch: 1, Step: 3100 | LOSS TRAIN: 5.24, DEV: 4.68 | NLL: 175.14, KL: 67.8097| BETA: 0.001253\n",
      "Epoch: 1, Step: 3200 | LOSS TRAIN: 5.23, DEV: 4.65 | NLL: 171.75, KL: 68.8111| BETA: 0.001261\n",
      "Epoch: 1, Step: 3300 | LOSS TRAIN: 5.21, DEV: 4.64 | NLL: 169.09, KL: 69.7508| BETA: 0.001269\n",
      "Epoch: 1, Step: 3400 | LOSS TRAIN: 5.19, DEV: 4.62 | NLL: 165.60, KL: 70.7360| BETA: 0.001277\n",
      "Epoch: 1, Step: 3500 | LOSS TRAIN: 5.18, DEV: 4.61 | NLL: 162.59, KL: 71.6865| BETA: 0.001285\n",
      "Epoch: 1, Step: 3600 | LOSS TRAIN: 5.16, DEV: 4.58 | NLL: 159.55, KL: 72.6658| BETA: 0.001293\n",
      "Epoch: 1, Step: 3700 | LOSS TRAIN: 5.14, DEV: 4.58 | NLL: 157.11, KL: 73.6048| BETA: 0.001300\n",
      "Epoch: 1, Step: 3800 | LOSS TRAIN: 5.13, DEV: 4.56 | NLL: 154.61, KL: 74.4785| BETA: 0.001308\n",
      "Epoch: 1, Step: 3900 | LOSS TRAIN: 5.11, DEV: 4.56 | NLL: 151.44, KL: 75.4631| BETA: 0.001316\n",
      "Epoch: 1, Step: 4000 | LOSS TRAIN: 5.10, DEV: 4.55 | NLL: 149.06, KL: 76.3830| BETA: 0.001324\n",
      "Epoch: 1, Step: 4100 | LOSS TRAIN: 5.08, DEV: 4.53 | NLL: 146.42, KL: 77.2507| BETA: 0.001332\n",
      "Epoch: 1, Step: 4200 | LOSS TRAIN: 5.07, DEV: 4.51 | NLL: 144.36, KL: 78.0811| BETA: 0.001340\n",
      "Epoch: 1, Step: 4300 | LOSS TRAIN: 5.05, DEV: 4.50 | NLL: 141.90, KL: 78.9638| BETA: 0.001348\n",
      "Epoch: 1, Step: 4400 | LOSS TRAIN: 5.04, DEV: 4.47 | NLL: 139.96, KL: 79.8004| BETA: 0.001356\n",
      "Epoch: 1, Step: 4500 | LOSS TRAIN: 5.03, DEV: 4.47 | NLL: 138.18, KL: 80.4973| BETA: 0.001364\n",
      "Epoch: 1, Step: 4600 | LOSS TRAIN: 5.01, DEV: 4.46 | NLL: 136.09, KL: 81.2997| BETA: 0.001372\n",
      "Epoch: 1, Step: 4700 | LOSS TRAIN: 5.00, DEV: 4.46 | NLL: 133.89, KL: 82.1148| BETA: 0.001379\n",
      "Epoch: 1, Step: 4800 | LOSS TRAIN: 4.99, DEV: 4.44 | NLL: 132.16, KL: 82.8512| BETA: 0.001387\n",
      "Epoch: 1, Step: 4900 | LOSS TRAIN: 4.98, DEV: 4.44 | NLL: 130.58, KL: 83.5362| BETA: 0.001395\n",
      "Epoch: 1, Step: 5000 | LOSS TRAIN: 4.97, DEV: 4.41 | NLL: 129.22, KL: 84.1994| BETA: 0.001403\n",
      "Epoch: 1, Step: 5100 | LOSS TRAIN: 4.96, DEV: 4.40 | NLL: 127.70, KL: 84.8343| BETA: 0.001411\n",
      "Epoch: 1, Step: 5200 | LOSS TRAIN: 4.95, DEV: 4.39 | NLL: 126.28, KL: 85.4720| BETA: 0.001419\n",
      "Epoch: 1, Step: 5300 | LOSS TRAIN: 4.93, DEV: 4.37 | NLL: 124.49, KL: 86.1874| BETA: 0.001427\n",
      "Epoch: 1, Step: 5400 | LOSS TRAIN: 4.92, DEV: 4.38 | NLL: 123.02, KL: 86.8418| BETA: 0.001435\n",
      "Epoch: 1, Step: 5500 | LOSS TRAIN: 4.91, DEV: 4.37 | NLL: 121.78, KL: 87.4563| BETA: 0.001443\n",
      "Epoch: 1, Step: 5600 | LOSS TRAIN: 4.90, DEV: 4.40 | NLL: 120.19, KL: 88.1961| BETA: 0.001451\n",
      "Epoch: 1, Step: 5700 | LOSS TRAIN: 4.89, DEV: 4.36 | NLL: 119.01, KL: 88.7305| BETA: 0.001459\n",
      "Epoch: 1, Step: 5800 | LOSS TRAIN: 4.88, DEV: 4.37 | NLL: 117.50, KL: 89.3923| BETA: 0.001466\n",
      "Epoch: 1, Step: 5900 | LOSS TRAIN: 4.87, DEV: 4.35 | NLL: 116.20, KL: 90.0012| BETA: 0.001474\n",
      "Epoch: 1, Step: 6000 | LOSS TRAIN: 4.86, DEV: 4.35 | NLL: 114.67, KL: 90.6688| BETA: 0.001482\n",
      "Epoch: 1, Step: 6100 | LOSS TRAIN: 4.85, DEV: 4.33 | NLL: 113.26, KL: 91.2810| BETA: 0.001490\n",
      "Epoch: 1, Step: 6200 | LOSS TRAIN: 4.84, DEV: 4.32 | NLL: 112.11, KL: 91.8129| BETA: 0.001498\n",
      "Epoch: 1, Step: 6300 | LOSS TRAIN: 4.83, DEV: 4.30 | NLL: 111.04, KL: 92.3095| BETA: 0.001506\n",
      "Epoch: 1, Step: 6400 | LOSS TRAIN: 4.82, DEV: 4.31 | NLL: 110.15, KL: 92.7467| BETA: 0.001514\n",
      "Epoch: 1, Step: 6500 | LOSS TRAIN: 4.81, DEV: 4.30 | NLL: 108.90, KL: 93.3319| BETA: 0.001522\n",
      "Epoch: 1, Step: 6600 | LOSS TRAIN: 4.81, DEV: 4.30 | NLL: 108.05, KL: 93.7486| BETA: 0.001530\n",
      "Epoch: 1, Step: 6700 | LOSS TRAIN: 4.80, DEV: 4.29 | NLL: 106.99, KL: 94.2602| BETA: 0.001538\n",
      "Epoch: 1, Step: 6800 | LOSS TRAIN: 4.79, DEV: 4.27 | NLL: 106.19, KL: 94.6578| BETA: 0.001545\n",
      "Epoch: 1, Step: 6900 | LOSS TRAIN: 4.78, DEV: 4.25 | NLL: 105.30, KL: 95.1147| BETA: 0.001553\n",
      "Epoch: 1, Step: 7000 | LOSS TRAIN: 4.78, DEV: 4.27 | NLL: 104.42, KL: 95.5970| BETA: 0.001561\n",
      "Epoch: 1, Step: 7100 | LOSS TRAIN: 4.77, DEV: 4.25 | NLL: 103.64, KL: 95.9858| BETA: 0.001569\n",
      "Epoch: 1, Step: 7200 | LOSS TRAIN: 4.76, DEV: 4.26 | NLL: 102.71, KL: 96.4289| BETA: 0.001577\n",
      "Epoch: 1, Step: 7300 | LOSS TRAIN: 4.75, DEV: 4.26 | NLL: 101.83, KL: 96.8569| BETA: 0.001585\n",
      "Epoch: 1, Step: 7400 | LOSS TRAIN: 4.75, DEV: 4.26 | NLL: 101.30, KL: 97.1732| BETA: 0.001593\n",
      "Epoch: 1, Step: 7500 | LOSS TRAIN: 4.74, DEV: 4.25 | NLL: 100.55, KL: 97.5166| BETA: 0.001601\n",
      "Epoch: 1, Step: 7600 | LOSS TRAIN: 4.74, DEV: 4.23 | NLL: 99.86, KL: 97.8528| BETA: 0.001609\n",
      "Epoch: 1, Step: 7700 | LOSS TRAIN: 4.73, DEV: 4.24 | NLL: 99.12, KL: 98.1811| BETA: 0.001617\n",
      "Epoch: 1, Step: 7800 | LOSS TRAIN: 4.72, DEV: 4.22 | NLL: 98.48, KL: 98.4774| BETA: 0.001625\n",
      "Epoch: 1, Step: 7900 | LOSS TRAIN: 4.72, DEV: 4.23 | NLL: 97.79, KL: 98.8137| BETA: 0.001632\n",
      "Epoch: 1, Step: 8000 | LOSS TRAIN: 4.71, DEV: 4.22 | NLL: 97.05, KL: 99.1838| BETA: 0.001640\n",
      "Epoch: 1, Step: 8100 | LOSS TRAIN: 4.71, DEV: 4.21 | NLL: 96.32, KL: 99.5066| BETA: 0.001648\n",
      "Epoch: 1, Step: 8200 | LOSS TRAIN: 4.70, DEV: 4.20 | NLL: 95.75, KL: 99.7649| BETA: 0.001656\n",
      "Epoch: 1, Step: 8300 | LOSS TRAIN: 4.69, DEV: 4.20 | NLL: 95.13, KL: 100.0453| BETA: 0.001664\n",
      "Epoch: 1, Step: 8400 | LOSS TRAIN: 4.69, DEV: 4.20 | NLL: 94.39, KL: 100.3460| BETA: 0.001672\n",
      "Epoch: 1, Step: 8500 | LOSS TRAIN: 4.68, DEV: 4.20 | NLL: 93.71, KL: 100.6376| BETA: 0.001680\n",
      "Epoch: 1, Step: 8600 | LOSS TRAIN: 4.68, DEV: 4.20 | NLL: 93.26, KL: 100.8296| BETA: 0.001688\n",
      "Epoch: 1, Step: 8700 | LOSS TRAIN: 4.67, DEV: 4.20 | NLL: 92.55, KL: 101.1576| BETA: 0.001696\n",
      "Epoch: 1, Step: 8800 | LOSS TRAIN: 4.66, DEV: 4.20 | NLL: 91.84, KL: 101.4558| BETA: 0.001704\n",
      "Epoch: 1, Step: 8900 | LOSS TRAIN: 4.66, DEV: 4.18 | NLL: 91.28, KL: 101.7123| BETA: 0.001712\n",
      "Epoch: 1, Step: 9000 | LOSS TRAIN: 4.65, DEV: 4.19 | NLL: 90.58, KL: 102.0275| BETA: 0.001719\n",
      "Epoch: 1, Step: 9100 | LOSS TRAIN: 4.65, DEV: 4.18 | NLL: 90.02, KL: 102.2718| BETA: 0.001727\n",
      "Epoch: 1, Step: 9200 | LOSS TRAIN: 4.64, DEV: 4.19 | NLL: 89.46, KL: 102.5047| BETA: 0.001735\n",
      "Epoch: 1, Step: 9300 | LOSS TRAIN: 4.64, DEV: 4.18 | NLL: 88.97, KL: 102.7335| BETA: 0.001743\n",
      "Epoch: 1, Step: 9400 | LOSS TRAIN: 4.63, DEV: 4.18 | NLL: 88.41, KL: 102.9743| BETA: 0.001751\n",
      "Epoch: 1, Step: 9500 | LOSS TRAIN: 4.62, DEV: 4.18 | NLL: 87.83, KL: 103.2275| BETA: 0.001759\n",
      "Epoch: 1, Step: 9600 | LOSS TRAIN: 4.62, DEV: 4.17 | NLL: 87.40, KL: 103.4038| BETA: 0.001767\n",
      "Epoch: 1, Step: 9700 | LOSS TRAIN: 4.61, DEV: 4.17 | NLL: 86.88, KL: 103.6223| BETA: 0.001775\n",
      "Epoch: 1, Step: 9800 | LOSS TRAIN: 4.61, DEV: 4.17 | NLL: 86.28, KL: 103.8599| BETA: 0.001783\n",
      "Epoch: 1, Step: 9900 | LOSS TRAIN: 4.60, DEV: 4.17 | NLL: 85.82, KL: 104.0302| BETA: 0.001791\n",
      "Epoch: 1, Step: 10000 | LOSS TRAIN: 4.60, DEV: 4.18 | NLL: 85.23, KL: 104.2691| BETA: 0.001798\n",
      "Epoch: 1, Step: 10100 | LOSS TRAIN: 4.59, DEV: 4.16 | NLL: 84.75, KL: 104.4719| BETA: 0.001806\n",
      "Epoch: 1, Step: 10200 | LOSS TRAIN: 4.59, DEV: 4.16 | NLL: 84.33, KL: 104.6544| BETA: 0.001814\n",
      "Epoch: 1, Step: 10300 | LOSS TRAIN: 4.58, DEV: 4.16 | NLL: 83.82, KL: 104.8650| BETA: 0.001822\n",
      "Epoch: 1, Step: 10400 | LOSS TRAIN: 4.58, DEV: 4.14 | NLL: 83.40, KL: 105.0335| BETA: 0.001830\n",
      "Epoch: 1, Step: 10500 | LOSS TRAIN: 4.57, DEV: 4.14 | NLL: 83.03, KL: 105.1871| BETA: 0.001838\n",
      "Epoch: 1, Step: 10600 | LOSS TRAIN: 4.57, DEV: 4.11 | NLL: 82.74, KL: 105.2826| BETA: 0.001846\n",
      "Epoch: 1, Step: 10700 | LOSS TRAIN: 4.57, DEV: 4.14 | NLL: 82.26, KL: 105.4602| BETA: 0.001854\n",
      "Epoch: 1, Step: 10800 | LOSS TRAIN: 4.56, DEV: 4.15 | NLL: 81.72, KL: 105.6560| BETA: 0.001862\n",
      "Epoch: 1, Step: 10900 | LOSS TRAIN: 4.56, DEV: 4.13 | NLL: 81.32, KL: 105.8165| BETA: 0.001870\n",
      "Epoch: 1, Step: 11000 | LOSS TRAIN: 4.55, DEV: 4.13 | NLL: 81.00, KL: 105.9409| BETA: 0.001878\n",
      "Epoch: 1, Step: 11100 | LOSS TRAIN: 4.55, DEV: 4.13 | NLL: 80.55, KL: 106.1223| BETA: 0.001885\n",
      "Epoch: 1, Step: 11200 | LOSS TRAIN: 4.55, DEV: 4.12 | NLL: 80.21, KL: 106.2531| BETA: 0.001893\n",
      "Epoch: 1, Step: 11300 | LOSS TRAIN: 4.54, DEV: 4.12 | NLL: 79.88, KL: 106.3779| BETA: 0.001901\n",
      "Epoch: 1, Step: 11400 | LOSS TRAIN: 4.54, DEV: 4.12 | NLL: 79.41, KL: 106.5613| BETA: 0.001909\n",
      "Epoch: 1, Step: 11500 | LOSS TRAIN: 4.53, DEV: 4.13 | NLL: 79.08, KL: 106.6898| BETA: 0.001917\n",
      "Epoch: 1, Step: 11600 | LOSS TRAIN: 4.53, DEV: 4.13 | NLL: 78.76, KL: 106.8060| BETA: 0.001925\n",
      "Epoch: 1, Step: 11700 | LOSS TRAIN: 4.53, DEV: 4.12 | NLL: 78.46, KL: 106.9058| BETA: 0.001933\n",
      "Epoch: 1, Step: 11800 | LOSS TRAIN: 4.52, DEV: 4.10 | NLL: 78.08, KL: 107.0402| BETA: 0.001941\n",
      "Epoch: 1, Step: 11900 | LOSS TRAIN: 4.52, DEV: 4.09 | NLL: 77.80, KL: 107.1350| BETA: 0.001949\n",
      "Epoch: 1, Step: 12000 | LOSS TRAIN: 4.52, DEV: 4.11 | NLL: 77.48, KL: 107.2466| BETA: 0.001957\n",
      "Epoch: 1, Step: 12100 | LOSS TRAIN: 4.51, DEV: 4.09 | NLL: 77.15, KL: 107.3568| BETA: 0.001965\n",
      "Epoch: 1, Step: 12200 | LOSS TRAIN: 4.51, DEV: 4.11 | NLL: 76.75, KL: 107.4950| BETA: 0.001972\n",
      "Epoch: 1, Step: 12300 | LOSS TRAIN: 4.50, DEV: 4.08 | NLL: 76.47, KL: 107.5858| BETA: 0.001980\n",
      "Epoch: 1, Step: 12400 | LOSS TRAIN: 4.50, DEV: 4.08 | NLL: 76.26, KL: 107.6327| BETA: 0.001988\n",
      "Epoch: 1, Step: 12500 | LOSS TRAIN: 4.50, DEV: 4.10 | NLL: 75.87, KL: 107.7752| BETA: 0.001996\n",
      "Epoch: 1, Step: 12600 | LOSS TRAIN: 4.49, DEV: 4.09 | NLL: 75.53, KL: 107.8878| BETA: 0.002004\n",
      "Epoch: 2, Step: 0 | LOSS TRAIN: 4.49, DEV: 4.08 | NLL: 75.42, KL: 107.9201| BETA: 0.002012\n",
      "Epoch: 2, Step: 100 | LOSS TRAIN: 4.49, DEV: 4.09 | NLL: 75.06, KL: 108.0390| BETA: 0.002020\n",
      "Epoch: 2, Step: 200 | LOSS TRAIN: 4.48, DEV: 4.09 | NLL: 74.73, KL: 108.1296| BETA: 0.002028\n",
      "Epoch: 2, Step: 300 | LOSS TRAIN: 4.48, DEV: 4.10 | NLL: 74.42, KL: 108.2310| BETA: 0.002036\n",
      "Epoch: 2, Step: 400 | LOSS TRAIN: 4.48, DEV: 4.09 | NLL: 74.14, KL: 108.3026| BETA: 0.002044\n",
      "Epoch: 2, Step: 500 | LOSS TRAIN: 4.47, DEV: 4.09 | NLL: 73.81, KL: 108.4007| BETA: 0.002051\n",
      "Epoch: 2, Step: 600 | LOSS TRAIN: 4.47, DEV: 4.08 | NLL: 73.48, KL: 108.4911| BETA: 0.002059\n",
      "Epoch: 2, Step: 700 | LOSS TRAIN: 4.47, DEV: 4.08 | NLL: 73.20, KL: 108.5663| BETA: 0.002067\n",
      "Epoch: 2, Step: 800 | LOSS TRAIN: 4.46, DEV: 4.08 | NLL: 72.91, KL: 108.6445| BETA: 0.002075\n",
      "Epoch: 2, Step: 900 | LOSS TRAIN: 4.46, DEV: 4.08 | NLL: 72.59, KL: 108.7439| BETA: 0.002083\n",
      "Epoch: 2, Step: 1000 | LOSS TRAIN: 4.45, DEV: 4.10 | NLL: 72.20, KL: 108.8713| BETA: 0.002091\n",
      "Epoch: 2, Step: 1100 | LOSS TRAIN: 4.45, DEV: 4.10 | NLL: 71.92, KL: 108.9472| BETA: 0.002099\n",
      "Epoch: 2, Step: 1200 | LOSS TRAIN: 4.45, DEV: 4.08 | NLL: 71.62, KL: 109.0308| BETA: 0.002107\n",
      "Epoch: 2, Step: 1300 | LOSS TRAIN: 4.44, DEV: 4.09 | NLL: 71.33, KL: 109.0905| BETA: 0.002115\n",
      "Epoch: 2, Step: 1400 | LOSS TRAIN: 4.44, DEV: 4.09 | NLL: 71.04, KL: 109.1755| BETA: 0.002123\n",
      "Epoch: 2, Step: 1500 | LOSS TRAIN: 4.44, DEV: 4.10 | NLL: 70.66, KL: 109.3021| BETA: 0.002131\n",
      "Epoch: 2, Step: 1600 | LOSS TRAIN: 4.43, DEV: 4.08 | NLL: 70.43, KL: 109.3365| BETA: 0.002138\n",
      "Epoch: 2, Step: 1700 | LOSS TRAIN: 4.43, DEV: 4.08 | NLL: 70.15, KL: 109.4087| BETA: 0.002146\n",
      "Epoch: 2, Step: 1800 | LOSS TRAIN: 4.43, DEV: 4.06 | NLL: 69.95, KL: 109.4312| BETA: 0.002154\n",
      "Epoch: 2, Step: 1900 | LOSS TRAIN: 4.42, DEV: 4.06 | NLL: 69.70, KL: 109.4832| BETA: 0.002162\n",
      "Epoch: 2, Step: 2000 | LOSS TRAIN: 4.42, DEV: 4.06 | NLL: 69.48, KL: 109.5245| BETA: 0.002170\n",
      "Epoch: 2, Step: 2100 | LOSS TRAIN: 4.42, DEV: 4.06 | NLL: 69.31, KL: 109.5341| BETA: 0.002178\n",
      "Epoch: 2, Step: 2200 | LOSS TRAIN: 4.42, DEV: 4.07 | NLL: 68.99, KL: 109.6335| BETA: 0.002186\n",
      "Epoch: 2, Step: 2300 | LOSS TRAIN: 4.41, DEV: 4.07 | NLL: 68.75, KL: 109.6925| BETA: 0.002194\n",
      "True: loli have commented on other reviews regarding the size discrepancy of this ball and i do suggest you reading the comments\n",
      "Pred: i have tried the the reviews of the other and and them product but they do not\n",
      "True: in lieu of that place the ball between a wall and another straight vertical surface that is 30 from the wall\n",
      "Pred: to addition of a the a a the of <unk> <unk> the the edge is is is the seconds the\n",
      "True: my others are on a 308 and a <unk> and even with the bigger calibers i have not had any issues\n",
      "Pred: these daughter have not a a and i couple of i have not first\n",
      "True: i bought my treadmill over 10 years ago and since that time the prices of motorized treadmills have really gone up\n",
      "Pred: i am this of of the years ago and the the i i old is the use i i\n",
      "True: my replacement mount also secured much more solidly than the one that came with the scope and it were quality made\n",
      "Pred: my wife i was came in better the in the other of i in the of of the was the\n",
      "True: panning the scope around on the target one sees variation in the focus clarity which suggests surface <unk> of the lenses\n",
      "Pred: in the the of the the top is the the in the same of of the the\n",
      "True: considering the rather low price for a variable power scope i consider it a mediocre value and give it 3 stars\n",
      "Pred: as a price a a i a great price and i would this a great scope\n",
      "True: so in short i discovered the hard way that you do infact need an airgun rated scope for any springer airgun\n",
      "Pred: so far the this can the fact plastic to you can can you to be the for for for more or\n",
      "True: if i would have known this i would not have needed to spend additional money to get an airgun rated scope\n",
      "Pred: if i would not would i would would be be to to buy on money\n",
      "True: it is pretty basic and does not have a scale or something to help you zero it in for different distances\n",
      "Pred: it also also light if you not have a <unk> to <unk> to be keep in for in a a\n",
      "True: it fit my seat post without one of the shims though so i do not know if that makes a difference\n",
      "Pred: it also the other in the the the the but but i i do not think that you is it better difference\n",
      "True: if you can fix these things as i did than you found the basket for you and for your topeak rack\n",
      "Pred: if you are not these for for you as not you have on rings for the\n",
      "True: cons it is not really a one size fits all design and the handle is too <unk> for any significant weight\n",
      "Pred: also it is not a a knife of but is the is it handle is not thick\n",
      "True: also topeak has pretty good customer service so if the mounting area ever does break they will replace it for free\n",
      "Pred: just for is been much customer service you far you are the you to not the are not the\n",
      "True: so with this i will say that this pump could put around 85 90 psi on a <unk> tire pretty easily\n",
      "Pred: so i this i will not it it little will be a a rounds degrees\n",
      "True: after switching to the schrader valve set up it was very difficult to get this on and off the valve stems\n",
      "Pred: after putting to to it it it it it is not easy to get the to the\n",
      "True: the pump came attached to a piece of cardboard but the cardboard had not one sentence of instruction printed on it\n",
      "Pred: the entire itself in a the small of plastic so the only is is the of\n",
      "True: it got a bit warm but it worked and that 's pretty much all you 'd ask for in a pump\n",
      "Pred: it has a little of but it is well you it will not more you you be\n",
      "True: this is really a get you home pump and is just too much work to inflate a tire to full pressure\n",
      "Pred: it is not a little to have to up it not to long to to keep to small to the\n",
      "True: it cones with a nifty mount that fits under the water bottle mounting and lets it nestle beside the water bottle\n",
      "Pred: that with a a bottle ball on the the the valve bottle cage the the the\n",
      "True: you have to pull the lever away from the pump to get into the t position to lock it in place\n",
      "Pred: you have to the the the to from the bottom to get it the gun\n",
      "True: i find that unscrewing the gray cap sliding it over the valve and then screwing back onto the pump works best\n",
      "Pred: i just it the the the bottle on the the the top and it it it the the gun\n",
      "True: 3 ) at first i thought this pump was hard to use because it required a lot of muscle to pressurize\n",
      "Pred: update of i i i am it would was a to a and i was a little of a to the\n",
      "True: i was surprised though that for as small a stroke as it has it seemed to fill the tire pretty quickly\n",
      "Pred: i was looking at how it a a as week of i was held the to be with air\n",
      "True: i would give it 5 stars if it did not take me two <unk> to figure out how to use it\n",
      "Pred: i would give it 5 stars if it is not take it to stars to get it\n",
      "True: i used to carry a co2 inflator with a spare cartridge just in case i got a flat on a ride\n",
      "Pred: i put a put a few cartridge on a few tire and a a when put a few patch\n",
      "True: like will screw in to your spare water bottle cage spot on the upright tube or strap underneath your horizontal tube\n",
      "Pred: if it keep in the the bike with bottle with in on your front of\n",
      "True: other features it converts easily from presta to shrader which is important as i have bikes with both types of valves\n",
      "Pred: the <unk> that has to the the valves and and is not to a as to to the\n",
      "True: then i tried the bottle cage the bolts would not go in as the holes in the mount were too small\n",
      "Pred: once i had the screw in the straps would not be in the long result\n",
      "True: i should knock off a star for that but i am not going to because everything else about it is great\n",
      "Pred: i have have a star star rating it but i do not sure to do it did\n",
      "True: the valve head being on a flexible tube is great and helps to not put wear on the tube 's valve\n",
      "Pred: the bottle comes comes a and <unk> bag that easy for you you you go it it the ground\n",
      "True: my first trip out after putting this on my bike i realized my pressure was low about five miles from home\n",
      "Pred: my first bike was was about the on my first in had it bike in in\n"
     ]
    }
   ],
   "source": [
    "while epoch <= config.epochs:\n",
    "    train_batches = grouper(data_train, config.batch_size, shuffle=True)\n",
    "    epoch += 1\n",
    "    ct = 0\n",
    "    for ct, batch in enumerate(train_batches):\n",
    "        feed_dict = get_feed_dict(batch)\n",
    "        if config.warmup > 0: sess.run(beta.assign(np.minimum(1., beta_eval+ 1./(config.warmup*num_train_batches))))\n",
    "\n",
    "        _, loss_batch, kl_loss_batch, recon_loss_batch = sess.run([opt, loss, kl_loss, recon_loss], feed_dict = feed_dict)\n",
    "        losses_train += [[loss_batch, kl_loss_batch, recon_loss_batch]]\n",
    "\n",
    "        if ct%config.log_period==0:\n",
    "            loss_train, kl_loss_train, recon_loss_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(recon_loss_train)\n",
    "            if config.warmup > 0: beta_eval = beta.eval(session=sess)\n",
    "            loss_dev = get_loss(sess, dev_batches)\n",
    "\n",
    "#             if loss_dev <= loss_min:\n",
    "#                 loss_min = loss_dev\n",
    "#                 loss_test = get_loss(sess, test_batches)\n",
    "\n",
    "            clear_output()\n",
    "\n",
    "            logs += [(epoch, ct, loss_train, loss_dev, ppl_train, kl_loss_train, beta_eval)]\n",
    "            for log in logs:\n",
    "                print('Epoch: %i, Step: %i | LOSS TRAIN: %.2f, DEV: %.2f | NLL: %.2f, KL: %.4f| BETA: %.6f' %  log)\n",
    "\n",
    "            print_sample(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 | LOSS TRAIN: 9.91, DEV: 9.91, TEST: 9.91 | NLL: 19989.35, KL: 0.2202| BETA: 0.010008\n",
      "Step: 1000 | LOSS TRAIN: 6.59, DEV: 6.28, TEST: 6.26 | NLL: 705.00, KL: 3.5135| BETA: 0.010016\n",
      "Step: 2000 | LOSS TRAIN: 6.43, DEV: 6.18, TEST: 6.16 | NLL: 602.32, KL: 2.7705| BETA: 0.010024\n",
      "Step: 3000 | LOSS TRAIN: 6.34, DEV: 6.11, TEST: 6.09 | NLL: 551.30, KL: 2.6194| BETA: 0.010032\n",
      "Step: 4000 | LOSS TRAIN: 6.26, DEV: 5.94, TEST: 5.92 | NLL: 512.02, KL: 2.5567| BETA: 0.010040\n",
      "Step: 5000 | LOSS TRAIN: 6.19, DEV: 5.80, TEST: 5.78 | NLL: 475.38, KL: 2.5735| BETA: 0.010047\n",
      "Step: 6000 | LOSS TRAIN: 6.12, DEV: 5.73, TEST: 5.71 | NLL: 442.81, KL: 2.6369| BETA: 0.010055\n",
      "Step: 7000 | LOSS TRAIN: 6.06, DEV: 5.66, TEST: 5.66 | NLL: 415.69, KL: 2.7330| BETA: 0.010063\n",
      "Step: 8000 | LOSS TRAIN: 6.00, DEV: 5.61, TEST: 5.59 | NLL: 393.43, KL: 2.8561| BETA: 0.010071\n",
      "Step: 9000 | LOSS TRAIN: 5.96, DEV: 5.57, TEST: 5.55 | NLL: 374.77, KL: 3.0316| BETA: 0.010079\n",
      "Step: 10000 | LOSS TRAIN: 5.91, DEV: 5.53, TEST: 5.51 | NLL: 357.67, KL: 3.2351| BETA: 0.010087\n",
      "Step: 11000 | LOSS TRAIN: 5.87, DEV: 5.48, TEST: 5.47 | NLL: 343.60, KL: 3.4164| BETA: 0.010095\n",
      "Step: 12000 | LOSS TRAIN: 5.84, DEV: 5.44, TEST: 5.44 | NLL: 331.91, KL: 3.5717| BETA: 0.010103\n"
     ]
    }
   ],
   "source": [
    "for log in logs:\n",
    "            print('Step: %i | LOSS TRAIN: %.2f, DEV: %.2f, TEST: %.2f | NLL: %.2f, KL: %.4f| BETA: %.6f' %  log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "_logvars, _means, _kl_losses, _latents, _output_logits = sess.run([logvars, means, kl_losses, latents, output_logits], feed_dict=feed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 32), (32, 32), (32,), (32, 32))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_logvars.shape, _means.shape, _kl_losses.shape, _latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_output_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dec_target_idxs_do' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-7de59bc2cc54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_output_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dec_target_idxs_do\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dec_mask_tokens_do\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_recon_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_kl_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_target_idxs_do\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_mask_tokens_do\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dec_target_idxs_do' is not defined"
     ]
    }
   ],
   "source": [
    "_output_logits, _dec_target_idxs_do, _dec_mask_tokens_do, _recon_loss, _kl_losses, _ = sess.run([output_logits, dec_target_idxs_do, dec_mask_tokens_do, recon_loss, kl_losses, opt], feed_dict=feed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 46)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_max(output_logits, 2).eval(session=sess, feed_dict=feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 46, 20000), (120, 46), (120, 46))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_output_logits.shape, _dec_target_idxs_do.shape, _dec_mask_tokens_do.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "_logits = np.exp(_output_logits) / np.sum(np.exp(_output_logits), 2)[:, :, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "_idxs = _dec_target_idxs_do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "_losses = np.array([[-np.log(_logits[i, j, _idxs[i, j]]) for j in range(_idxs.shape[1])] for i in range(_idxs.shape[0])]) * _dec_mask_tokens_do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.903732"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(_losses)/np.sum(_dec_mask_tokens_do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.903732"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_recon_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_kl_losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
