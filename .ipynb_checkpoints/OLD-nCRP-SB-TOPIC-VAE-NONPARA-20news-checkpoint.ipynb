{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "# %matplotlib nbagg\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import hmean\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from data_structure import get_batches, get_test_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "\n",
    "from topic_beam_search_decoder import BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '3', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/old/20news/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/topic_vae', 'directory of model')\n",
    "flags.DEFINE_string('modelname', '20news', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 1000, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 3000, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "# flags.DEFINE_string('opt', 'Adam', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.01, 'lr')\n",
    "flags.DEFINE_float('reg', 1., 'regularization term')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_bool('warmup', True, 'flg of warming up')\n",
    "flags.DEFINE_integer('epochs_cycle', 5, 'number of epochs within a cycle')\n",
    "flags.DEFINE_float('r_cycle', 0.5, 'proportion used to increase beta within a cycle')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 20, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_bow', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_hidden_topic', 512, 'dim_hidden_topic')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "flags.DEFINE_bool('bidirectional', True, 'flg of bidirectional encoding')\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_batches(instances, batch_size, iterator=False):\n",
    "    iter_instances = iter(instances)\n",
    "    n_batch = len(instances)//batch_size\n",
    "    \n",
    "    batches = [(i_batch, [next(iter_instances) for i_doc in range(batch_size)]) for i_batch in range(n_batch)]\n",
    "    \n",
    "    if iterator: batches = iter(batches)\n",
    "    return batches\n",
    "\n",
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "flags.DEFINE_integer('cycle_steps', len(train_batches)*config.epochs_cycle, 'number of steps for each cycle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     10,
     24,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables, model):\n",
    "    sample_batch = dev_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, model, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "def check_shape(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()\n",
    "    \n",
    "def check_value(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doubly rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoublyRNNCell:\n",
    "    def __init__(self, dim_hidden, output_layer=None):\n",
    "        self.dim_hidden = dim_hidden\n",
    "        \n",
    "        self.ancestral_layer=tf.layers.Dense(units=dim_hidden, activation=tf.nn.tanh, name='ancestral')\n",
    "        self.fraternal_layer=tf.layers.Dense(units=dim_hidden, activation=tf.nn.tanh, name='fraternal')\n",
    "        self.hidden_layer = tf.layers.Dense(units=dim_hidden, name='hidden')\n",
    "        \n",
    "        self.output_layer=output_layer\n",
    "        \n",
    "    def __call__(self, state_ancestral, state_fraternal, reuse=True):\n",
    "        with tf.variable_scope('input', reuse=reuse):\n",
    "            state_ancestral = self.ancestral_layer(state_ancestral)\n",
    "            state_fraternal = self.fraternal_layer(state_fraternal)\n",
    "\n",
    "        with tf.variable_scope('output', reuse=reuse):\n",
    "            state_hidden = self.hidden_layer(state_ancestral + state_fraternal)\n",
    "            if self.output_layer is not None: \n",
    "                output = self.output_layer(state_hidden)\n",
    "            else:\n",
    "                output = state_hidden\n",
    "            \n",
    "        return output, state_hidden\n",
    "    \n",
    "    def get_initial_state(self, name):\n",
    "        initial_state = tf.get_variable(name, [1, self.dim_hidden], dtype=tf.float32)\n",
    "        return initial_state\n",
    "    \n",
    "    def get_zero_state(self, name):\n",
    "        zero_state = tf.zeros([1, self.dim_hidden], dtype=tf.float32, name=name)\n",
    "        return zero_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubly_rnn(dim_hidden, tree_idxs, initial_state_parent=None, initial_state_sibling=None, output_layer=None, name=''):\n",
    "    outputs, states_parent = {}, {}\n",
    "    \n",
    "    with tf.variable_scope(name, reuse=False):\n",
    "        doubly_rnn_cell = DoublyRNNCell(dim_hidden, output_layer)\n",
    "\n",
    "        if initial_state_parent is None: \n",
    "            initial_state_parent = doubly_rnn_cell.get_initial_state('init_state_parent')\n",
    "#             initial_state_parent = doubly_rnn_cell.get_zero_state('init_state_parent')\n",
    "        if initial_state_sibling is None: \n",
    "#             initial_state_sibling = doubly_rnn_cell.get_initial_state('init_state_sibling')\n",
    "            initial_state_sibling = doubly_rnn_cell.get_zero_state('init_state_sibling')\n",
    "        output, state_sibling = doubly_rnn_cell(initial_state_parent, initial_state_sibling, reuse=False)\n",
    "        outputs[0], states_parent[0] = output, state_sibling\n",
    "\n",
    "        for parent_idx, child_idxs in tree_idxs.items():\n",
    "            state_parent = states_parent[parent_idx]\n",
    "            state_sibling = initial_state_sibling\n",
    "            for child_idx in child_idxs:\n",
    "                output, state_sibling = doubly_rnn_cell(state_parent, state_sibling)\n",
    "                outputs[child_idx], states_parent[child_idx] = output, state_sibling\n",
    "\n",
    "    return outputs, states_parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell:\n",
    "    def __init__(self, dim_hidden, output_layer=None):\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.hidden_layer = tf.layers.Dense(units=dim_hidden, activation=tf.nn.tanh, name='hidden')\n",
    "        self.output_layer=output_layer\n",
    "        \n",
    "    def __call__(self, state, reuse=True):\n",
    "        with tf.variable_scope('output', reuse=reuse):\n",
    "            state_hidden = self.hidden_layer(state)\n",
    "            if self.output_layer is not None: \n",
    "                output = self.output_layer(state_hidden)\n",
    "            else:\n",
    "                output = state_hidden\n",
    "            \n",
    "        return output, state_hidden\n",
    "    \n",
    "    def get_initial_state(self, name):\n",
    "        initial_state = tf.get_variable(name, [1, self.dim_hidden], dtype=tf.float32)\n",
    "        return initial_state\n",
    "    \n",
    "    def get_zero_state(self, name):\n",
    "        zero_state = tf.zeros([1, self.dim_hidden], dtype=tf.float32, name=name)\n",
    "        return zero_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(dim_hidden, max_depth, initial_state=None, output_layer=None, name=''):\n",
    "    outputs, states_hidden = [], []\n",
    "    \n",
    "    with tf.variable_scope(name, reuse=False):\n",
    "        rnn_cell = RNNCell(dim_hidden, output_layer)\n",
    "\n",
    "        if initial_state is not None: \n",
    "            state_hidden = initial_state\n",
    "        else:\n",
    "            state_hidden = rnn_cell.get_initial_state('init_state')\n",
    "#             state_hidden = rnn_cell.get_zero_state('init_state_parent')\n",
    "        \n",
    "        for depth in range(max_depth):\n",
    "            if depth == 0:                \n",
    "                output, state_hidden = rnn_cell(state_hidden, reuse=False)\n",
    "            else:\n",
    "                output, state_hidden = rnn_cell(state_hidden, reuse=True)\n",
    "            outputs.append(output)\n",
    "            states_hidden.append(state_hidden)\n",
    "\n",
    "    outputs = tf.concat(outputs, 1)\n",
    "    states_hidden = tf.concat(states_hidden, 0)\n",
    "    return outputs, states_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nCRP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "code_folding": [
     2,
     25,
     48,
     81
    ]
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, config, tree_idxs):\n",
    "        def get_depth(parent_idx=0, tree_depth=None, depth=1):\n",
    "            if tree_depth is None: tree_depth={0: depth}\n",
    "\n",
    "            child_idxs = tree_idxs[parent_idx]\n",
    "            depth +=1\n",
    "            for child_idx in child_idxs:\n",
    "                tree_depth[child_idx] = depth\n",
    "                if child_idx in tree_idxs: get_depth(child_idx, tree_depth, depth)\n",
    "            return tree_depth\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        self.t_variables = {}\n",
    "        \n",
    "        self.tree_idxs = tree_idxs\n",
    "        self.topic_idxs = [0] + [idx for child_idxs in tree_idxs.values() for idx in child_idxs]\n",
    "        self.child_to_parent_idxs = {child_idx: parent_idx for parent_idx, child_idxs in self.tree_idxs.items() for child_idx in child_idxs}\n",
    "        self.tree_depth = get_depth()\n",
    "        self.n_depth = max(self.tree_depth.values())\n",
    "        \n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        def nCRP(tree_sticks_topic):\n",
    "            tree_prob_topic = {}\n",
    "            tree_prob_leaf = {}\n",
    "            # calculate topic probability and save\n",
    "            tree_prob_topic[0] = 1.\n",
    "\n",
    "            for parent_idx, child_idxs in self.tree_idxs.items():\n",
    "                rest_prob_topic = tree_prob_topic[parent_idx]\n",
    "                for child_idx in child_idxs:\n",
    "                    stick_topic = tree_sticks_topic[child_idx]\n",
    "                    if child_idx == child_idxs[-1]:\n",
    "                        prob_topic = rest_prob_topic * 1.\n",
    "                    else:\n",
    "                        prob_topic = rest_prob_topic * stick_topic\n",
    "\n",
    "                    if not child_idx in self.tree_idxs: # leaf childs\n",
    "                        tree_prob_leaf[child_idx] = prob_topic\n",
    "                    else:\n",
    "                        tree_prob_topic[child_idx] = prob_topic\n",
    "\n",
    "                    rest_prob_topic -= prob_topic\n",
    "            return tree_prob_leaf\n",
    "\n",
    "        def sbp(sticks_depth, max_depth):\n",
    "            prob_depth_list = []\n",
    "            rest_prob_depth = 1.\n",
    "            for depth in range(max_depth):\n",
    "                stick_depth = tf.expand_dims(sticks_depth[:, depth], 1)\n",
    "                if depth == max_depth -1:\n",
    "                    prob_depth = rest_prob_depth * 1.\n",
    "                else:\n",
    "                    prob_depth = rest_prob_depth * stick_depth\n",
    "                prob_depth_list.append(prob_depth)\n",
    "                rest_prob_depth -= prob_depth\n",
    "\n",
    "            prob_depth = tf.concat(prob_depth_list, 1)\n",
    "            return prob_depth\n",
    "       \n",
    "        def get_prob_topic(tree_prob_leaf, prob_depth):\n",
    "            def get_ancestor_idxs(leaf_idx, ancestor_idxs = None):\n",
    "                if ancestor_idxs is None: ancestor_idxs = [leaf_idx]\n",
    "                parent_idx = self.child_to_parent_idxs[leaf_idx]\n",
    "                ancestor_idxs += [parent_idx]\n",
    "                if parent_idx in self.child_to_parent_idxs: get_ancestor_idxs(parent_idx, ancestor_idxs)\n",
    "                return ancestor_idxs[::-1]\n",
    "            \n",
    "            tree_prob_topic = defaultdict(float)\n",
    "            leaf_ancestor_idxs = {leaf_idx: get_ancestor_idxs(leaf_idx) for leaf_idx in tree_prob_leaf}\n",
    "            for leaf_idx, ancestor_idxs in leaf_ancestor_idxs.items():\n",
    "                prob_leaf = tree_prob_leaf[leaf_idx]\n",
    "                for i, ancestor_idx in enumerate(ancestor_idxs):\n",
    "                    prob_ancestor = prob_leaf * tf.expand_dims(prob_depth[:, i], -1)\n",
    "                    tree_prob_topic[ancestor_idx] += prob_ancestor\n",
    "            prob_topic = tf.concat([tree_prob_topic[topic_idx] for topic_idx in self.topic_idxs], -1)\n",
    "            return prob_topic\n",
    "        \n",
    "        def get_tree_topic_bow(tree_topic_embeddings):\n",
    "            def softmax_with_temperature(logits, axis=None, name=None, temperature=1.):\n",
    "                if axis is None:\n",
    "                    axis = -1\n",
    "                return tf.exp(logits / temperature) / tf.reduce_sum(tf.exp(logits / temperature), axis=axis)\n",
    "\n",
    "            tree_topic_bow = {}\n",
    "            for topic_idx, depth in self.tree_depth.items():\n",
    "                topic_embedding = tree_topic_embeddings[topic_idx]\n",
    "                temperature = tf.constant(10 ** (1./depth), dtype=tf.float32)\n",
    "                logits = tf.matmul(topic_embedding, self.bow_embeddings, transpose_b=True)\n",
    "                tree_topic_bow[topic_idx] = softmax_with_temperature(logits, axis=-1, temperature=temperature)\n",
    "            return tree_topic_bow\n",
    "\n",
    "#         def get_tree_mask_reg():\n",
    "#             def get_descendant_idxs(parent_idx, descendant_idxs = None):\n",
    "#                 if descendant_idxs is None: descendant_idxs = []\n",
    "\n",
    "#                 child_idxs = self.tree_idxs[parent_idx]\n",
    "#                 descendant_idxs += child_idxs\n",
    "#                 for child_idx in child_idxs:\n",
    "#                     if child_idx in self.tree_idxs: get_descendant_idxs(child_idx, descendant_idxs)\n",
    "#                 return descendant_idxs\n",
    "\n",
    "#             tree_mask_reg = np.ones([len(self.topic_idxs), len(self.topic_idxs)], dtype=np.float32)\n",
    "#             parent_to_descendant_idxs = {parent_idx: get_descendant_idxs(parent_idx) for parent_idx in self.tree_idxs}\n",
    "\n",
    "#             for parent_idx, descendant_idxs in parent_to_descendant_idxs.items():\n",
    "#                 for descendant_idx in descendant_idxs:\n",
    "#                     parent_index = self.topic_idxs.index(parent_idx)\n",
    "#                     descendant_index = self.topic_idxs.index(descendant_idx)\n",
    "#                     tree_mask_reg[parent_index, descendant_index] = tree_mask_reg[descendant_index, parent_index] = 0.\n",
    "                    \n",
    "#             tree_mask_reg[0, 0] = 0.        \n",
    "#             for parent_idx, child_idxs in self.tree_idxs.items():\n",
    "#                 for child_idx1 in child_idxs:\n",
    "#                     for child_idx2 in child_idxs:\n",
    "#                         tree_mask_reg[self.topic_idxs.index(child_idx1), self.topic_idxs.index(child_idx2)] = 0.                    \n",
    "                    \n",
    "#             return tree_mask_reg\n",
    "        \n",
    "        def get_tree_mask_reg(all_child_idxs):        \n",
    "            tree_mask_reg = np.zeros([len(all_child_idxs), len(all_child_idxs)], dtype=np.float32)\n",
    "            for child_idxs in self.tree_idxs.values():\n",
    "                for child_idx1 in child_idxs:\n",
    "                    for child_idx2 in child_idxs:\n",
    "                        if self.tree_depth[child_idx1] == self.tree_depth[child_idx2]:\n",
    "                            child_index1 = all_child_idxs.index(child_idx1)\n",
    "                            child_index2 = all_child_idxs.index(child_idx2)\n",
    "                            tree_mask_reg[child_index1, child_index2] = tree_mask_reg[child_index2, child_index1] = 1.\n",
    "                            \n",
    "            return tree_mask_reg\n",
    "       \n",
    "        # -------------- Build Model --------------\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.t_variables['bow'] = tf.placeholder(tf.float32, [None, self.config.dim_bow])\n",
    "        self.t_variables['keep_prob'] = tf.placeholder(tf.float32)\n",
    "        \n",
    "        # encode bow\n",
    "        with tf.variable_scope('topic/enc', reuse=False):\n",
    "            hidden_bow_ = tf.layers.Dense(units=self.config.dim_hidden_bow, activation=tf.nn.tanh, name='hidden_bow')(self.t_variables['bow'])\n",
    "            hidden_bow = tf.layers.Dropout(self.t_variables['keep_prob'])(hidden_bow_)\n",
    "            means_bow = tf.layers.Dense(units=self.config.dim_latent_bow, name='mean_bow')(hidden_bow)\n",
    "            logvars_bow = tf.layers.Dense(units=self.config.dim_latent_bow, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_bow')(hidden_bow)\n",
    "            latents_bow = sample_latents(means_bow, logvars_bow) # sample latent vectors\n",
    "            prob_layer = lambda h: tf.nn.sigmoid(tf.matmul(latents_bow, h, transpose_b=True))\n",
    "\n",
    "            tree_sticks_topic, tree_states_sticks_topic = doubly_rnn(self.config.dim_latent_bow, self.tree_idxs, output_layer=prob_layer, name='sticks_topic')\n",
    "            tree_prob_leaf = nCRP(tree_sticks_topic)\n",
    "            self.tree_prob_leaf = tree_prob_leaf\n",
    "#             prob_depth = tf.layers.Dense(units=self.n_depth, activation=tf.nn.softmax, name='prob_depth')(latents_bow) # inference of topic probabilities\n",
    "            sticks_depth, _ = rnn(config.dim_latent_bow, self.n_depth, output_layer=prob_layer, name='prob_depth')\n",
    "            prob_depth = sbp(sticks_depth, self.n_depth)\n",
    "            self.prob_depth = prob_depth\n",
    "\n",
    "            prob_topic = get_prob_topic(tree_prob_leaf, prob_depth)\n",
    "            self.prob_topic = prob_topic # n_batch x n_topic\n",
    "\n",
    "        # decode bow\n",
    "        with tf.variable_scope('shared', reuse=False):\n",
    "            self.bow_embeddings = tf.get_variable('emb', [self.config.dim_bow, self.config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "        with tf.variable_scope('topic/dec', reuse=False):\n",
    "        #     tree_topic_embeddings, tree_states_topic_embeddings = doubly_rnn(self.config.dim_emb, self.tree_idxs, name='emb_topic')\n",
    "            emb_layer = lambda h: tf.layers.Dense(units=self.config.dim_emb, name='output')(tf.nn.tanh(h))\n",
    "            tree_topic_embeddings, tree_states_topic_embeddings = doubly_rnn(self.config.dim_emb, self.tree_idxs, output_layer=emb_layer, name='emb_topic')\n",
    "#             topic_embeddings = tf.get_variable('topic_emb', [len(self.topic_idxs), self.config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of topics\n",
    "#             tree_topic_embeddings = {topic_idx: tf.expand_dims(topic_embeddings[self.topic_idxs.index(topic_idx)], 0) for topic_idx in self.topic_idxs}\n",
    "\n",
    "            tree_topic_bow = get_tree_topic_bow(tree_topic_embeddings) # bow vectors for each topic\n",
    "\n",
    "            topic_bow = tf.concat([tree_topic_bow[topic_idx] for topic_idx in self.topic_idxs], 0) # KxV\n",
    "            self.topic_bow = topic_bow\n",
    "            logits_bow = tf_log(tf.matmul(prob_topic, topic_bow)) # predicted bow distribution N_Batch x  V\n",
    "            self.logits_bow = logits_bow\n",
    "            \n",
    "        # define losses\n",
    "        self.topic_losses_recon = -tf.reduce_sum(tf.multiply(self.t_variables['bow'], logits_bow), 1)\n",
    "        self.topic_loss_recon = tf.reduce_mean(self.topic_losses_recon) # negative log likelihood of each words\n",
    "\n",
    "        self.topic_loss_kl = compute_kl_loss(means_bow, logvars_bow) # KL divergence b/w latent dist & gaussian std\n",
    "\n",
    "#         topic_bow_norm = topic_bow / tf.norm(topic_bow, axis=1, keepdims=True)\n",
    "#         self.topic_dots = tf.clip_by_value(tf.matmul(topic_bow_norm, tf.transpose(topic_bow_norm)), -1., 1.)\n",
    "        \n",
    "        self.topic_embeddings = tf.concat([tree_topic_embeddings[topic_idx] for topic_idx in self.topic_idxs], 0) # temporary\n",
    "#         topic_embeddings_norm = self.topic_embeddings / tf.norm(self.topic_embeddings, axis=1, keepdims=True)\n",
    "#         self.topic_dots = tf.clip_by_value(tf.matmul(topic_embeddings_norm, tf.transpose(topic_embeddings_norm)), -1., 1.)        \n",
    "        \n",
    "#         self.tree_mask_reg = get_tree_mask_reg()\n",
    "#         self.topic_losses_reg = tf.square(self.topic_dots - tf.eye(len(self.topic_idxs))) * self.tree_mask_reg\n",
    "#         self.topic_loss_reg = tf.reduce_sum(self.topic_losses_reg) / tf.reduce_sum(self.tree_mask_reg)\n",
    "\n",
    "        all_child_idxs = list(self.child_to_parent_idxs.keys())\n",
    "        self.diff_topic_embeddings = tf.concat([tree_topic_embeddings[child_idx] - tree_topic_embeddings[self.child_to_parent_idxs[child_idx]] for child_idx in all_child_idxs], axis=0)\n",
    "        diff_topic_embeddings_norm = self.diff_topic_embeddings / tf.norm(self.diff_topic_embeddings, axis=1, keepdims=True)\n",
    "        self.topic_dots = tf.clip_by_value(tf.matmul(diff_topic_embeddings_norm, tf.transpose(diff_topic_embeddings_norm)), -1., 1.)        \n",
    "        \n",
    "        self.tree_mask_reg = get_tree_mask_reg(all_child_idxs)\n",
    "        self.topic_losses_reg = tf.square(self.topic_dots - tf.eye(len(all_child_idxs))) * self.tree_mask_reg\n",
    "        self.topic_loss_reg = tf.reduce_sum(self.topic_losses_reg) / tf.reduce_sum(self.tree_mask_reg)\n",
    "\n",
    "        self.global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "\n",
    "        self.loss = self.topic_loss_recon + self.topic_loss_kl + self.config.reg * self.topic_loss_reg\n",
    "\n",
    "        # define optimizer\n",
    "        if self.config.opt == 'Adam':\n",
    "            optimizer = tf.train.AdamOptimizer(self.config.lr)\n",
    "        elif self.config.opt == 'Adagrad':\n",
    "            optimizer = tf.train.AdagradOptimizer(self.config.lr)\n",
    "\n",
    "        self.grad_vars = optimizer.compute_gradients(self.loss)\n",
    "        self.clipped_grad_vars = [(tf.clip_by_value(grad, -self.config.grad_clip, self.config.grad_clip), var) for grad, var in self.grad_vars]\n",
    "        self.opt = optimizer.apply_gradients(self.clipped_grad_vars, global_step=self.global_step)\n",
    "\n",
    "        # monitor\n",
    "        self.n_bow = tf.reduce_sum(self.t_variables['bow'], 1)\n",
    "        self.topic_ppls = tf.divide(self.topic_losses_recon, tf.maximum(1e-5, self.n_bow))\n",
    "        self.topics_freq_bow_indices = tf.nn.top_k(topic_bow, 10, name='topic_freq_bow').indices\n",
    "    \n",
    "        # growth criteria\n",
    "#         self.dist_bow = -tf.matmul(self.t_variables['bow'], tf.log(topic_bow), transpose_b=True)\n",
    "#         self.rads_bow = tf.divide(tf.multiply(self.dist_bow, prob_topic), tf.expand_dims(self.n_bow, -1))\n",
    "        self.n_topics = tf.multiply(tf.expand_dims(self.n_bow, -1), prob_topic)\n",
    "        \n",
    "        self.arcs_bow = tf.acos(tf.matmul(tf.linalg.l2_normalize(self.bow_embeddings, axis=-1), tf.linalg.l2_normalize(self.topic_embeddings, axis=-1), transpose_b=True)) # n_vocab x n_topic\n",
    "        self.rads_bow = tf.multiply(tf.matmul(self.t_variables['bow'], self.arcs_bow), self.prob_topic) # n_batch x n_topic\n",
    "    \n",
    "    def get_feed_dict(self, batch, mode='train'):\n",
    "        bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "        keep_prob = self.config.keep_prob if mode == 'train' else 1.0\n",
    "        feed_dict = {\n",
    "                    self.t_variables['bow']: bow, \n",
    "                    self.t_variables['keep_prob']: keep_prob\n",
    "        }\n",
    "        return  feed_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_loss(sess, batches, model):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    rads_bow_list = []\n",
    "    prob_topic_list = []\n",
    "    n_bow_list = []\n",
    "    n_topics_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = model.get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, ppls_batch, rads_bow_batch, prob_topic_batch, n_bow_batch, n_topics_batch \\\n",
    "            = sess.run([model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_reg, model.topic_ppls, model.rads_bow, model.prob_topic, model.n_bow, model.n_topics], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "        rads_bow_list.append(rads_bow_batch)\n",
    "        prob_topic_list.append(prob_topic_batch)\n",
    "        n_bow_list.append(n_bow_batch)\n",
    "        n_topics_list.append(n_topics_batch)\n",
    "    loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    \n",
    "    probs_topic = np.concatenate(prob_topic_list, 0)\n",
    "    \n",
    "    n_bow = np.concatenate(n_bow_list, 0)\n",
    "    n_topics = np.concatenate(n_topics_list, 0)\n",
    "    probs_topic_mean = np.sum(n_topics, 0) / np.sum(n_bow)\n",
    "    \n",
    "    rads_bow = np.concatenate(rads_bow_list, 0)\n",
    "    rads_bow_mean = np.cos(np.sum(rads_bow, 0) / np.sum(n_topics, 0))\n",
    "    \n",
    "    return loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, ppl_mean, rads_bow_mean, probs_topic_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def print_topic_sample(tree_idxs, sess=None, model=None, topic_rad_bow=None, topic_prob_topic=None, parent_idx=0, topics_freq_bow_idxs=None, depth = 0):\n",
    "    if topics_freq_bow_idxs is None:\n",
    "        topics_freq_bow_idxs = bow_idxs[sess.run(model.topics_freq_bow_indices)]\n",
    "        topic_freq_bow_idxs = topics_freq_bow_idxs[model.topic_idxs.index(parent_idx)]\n",
    "        rad_bow = topic_rad_bow[parent_idx]\n",
    "        prob_topic = topic_prob_topic[parent_idx]\n",
    "        print(parent_idx, 'R: %.3f' % rad_bow, 'P: %.3f' % prob_topic, ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "    \n",
    "    child_idxs = tree_idxs[parent_idx]\n",
    "    depth += 1\n",
    "    for child_idx in child_idxs:\n",
    "        topic_freq_bow_idxs = topics_freq_bow_idxs[model.topic_idxs.index(child_idx)]\n",
    "        rad_bow = topic_rad_bow[child_idx]\n",
    "        prob_topic = topic_prob_topic[child_idx]\n",
    "        print('  '*depth, child_idx, 'R: %.2f' % rad_bow, 'P: %.3f' % prob_topic, ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        \n",
    "        if child_idx in tree_idxs: print_topic_sample(tree_idxs, model=model, topic_rad_bow=topic_rad_bow, topic_prob_topic=topic_prob_topic, parent_idx=child_idx, topics_freq_bow_idxs=topics_freq_bow_idxs, depth=depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recur_topic_idxs(model, parent_idx, recur_topic_idxs = None):\n",
    "    if recur_topic_idxs is None: recur_topic_idxs = [parent_idx]\n",
    "\n",
    "    if parent_idx in model.tree_idxs:\n",
    "        child_idxs = model.tree_idxs[parent_idx]\n",
    "        recur_topic_idxs += child_idxs\n",
    "        for child_idx in child_idxs:\n",
    "            if child_idx in model.tree_idxs: get_recur_topic_idxs(model, child_idx, recur_topic_idxs)\n",
    "    return recur_topic_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def update_tree(recur_prob_topic, topic_prob_topic, model, add_threshold=0.3, remove_threshold=0.1):    \n",
    "    assert len(model.topic_idxs) == len(recur_prob_topic) == len(topic_prob_topic)\n",
    "    update_tree_flg = False\n",
    "    \n",
    "    def add_topic(topic_idx, tree_idxs):\n",
    "        if topic_idx in tree_idxs:\n",
    "            child_idx = min([10*topic_idx+i for i in range(1, 10) if 10*topic_idx+i not in tree_idxs[topic_idx]])\n",
    "            tree_idxs[topic_idx].append(child_idx)        \n",
    "        else:\n",
    "            child_idx = 10*topic_idx+1\n",
    "            tree_idxs[topic_idx] = [10*topic_idx+1]\n",
    "        return tree_idxs, child_idx\n",
    "    \n",
    "    added_tree_idxs = copy.deepcopy(model.tree_idxs)\n",
    "    for parent_idx, child_idxs in model.tree_idxs.items():\n",
    "#         rad_bow = topic_rad_bow[parent_idx]\n",
    "        rad_bow = topic_prob_topic[parent_idx]\n",
    "        if rad_bow > add_threshold:\n",
    "            update_tree_flg = True\n",
    "            for depth in range(model.tree_depth[parent_idx], model.n_depth):\n",
    "                added_tree_idxs, parent_idx = add_topic(parent_idx, added_tree_idxs)\n",
    "    \n",
    "    def remove_topic(parent_idx, child_idx, tree_idxs):\n",
    "        if parent_idx in tree_idxs:\n",
    "            tree_idxs[parent_idx].remove(child_idx)\n",
    "            if child_idx in tree_idxs:\n",
    "                tree_idxs.pop(child_idx)    \n",
    "        return tree_idxs\n",
    "    \n",
    "    removed_tree_idxs = copy.deepcopy(added_tree_idxs)\n",
    "    for parent_idx, child_idxs in model.tree_idxs.items():\n",
    "#         probs_child = np.array([topic_prob_topic[child_idx] for child_idx in child_idxs])\n",
    "        probs_child = np.array([recur_prob_topic[child_idx] for child_idx in child_idxs])\n",
    "#         prob_child = np.min(probs_child)\n",
    "#         child_idx = child_idxs[np.argmin(probs_child)]\n",
    "        for prob_child, child_idx in zip(probs_child, child_idxs):\n",
    "            if prob_child < remove_threshold:\n",
    "                update_tree_flg = True\n",
    "                removed_tree_idxs = remove_topic(parent_idx, child_idx, removed_tree_idxs)\n",
    "                if parent_idx in removed_tree_idxs:\n",
    "                    if len(removed_tree_idxs[parent_idx]) == 0:\n",
    "                        ancestor_idx = model.child_to_parent_idxs[parent_idx]\n",
    "                        removed_tree_idxs = remove_topic(ancestor_idx, parent_idx, removed_tree_idxs)\n",
    "    return removed_tree_idxs, update_tree_flg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','TM','','','','VALID:','TM','','',''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL','REG','LOSS','PPL','NLL','KL','REG']]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree_idxs = {0:[1, 2], \n",
    "#           1:[10, 11], 2:[20, 21]}\n",
    "\n",
    "tree_idxs = {0:[1, 2, 3, 4, 5], \n",
    "              1:[11, 12], 2:[21, 22], 3:[31, 32], 4:[41, 42], 5:[51, 52]}\n",
    "\n",
    "# tree_idxs = {0:[1, 2, 3], \n",
    "#               1:[10, 11, 12], 2:[20, 21, 22], 3:[30, 31, 32]}\n",
    "\n",
    "\n",
    "if 'sess' in globals(): sess.close()\n",
    "model = Model(config, tree_idxs)\n",
    "sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "name_variables = {tensor.name: variable for tensor, variable in zip(tf.global_variables(), sess.run(tf.global_variables()))}\n",
    "saver = tf.train.Saver(max_to_keep=10)\n",
    "update_tree_flg = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>VALID:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>75</td>\n",
       "      <td>28</td>\n",
       "      <td>99</td>\n",
       "      <td>600.61</td>\n",
       "      <td>888</td>\n",
       "      <td>597.80</td>\n",
       "      <td>2.38</td>\n",
       "      <td>0.43</td>\n",
       "      <td>607.18</td>\n",
       "      <td>1250</td>\n",
       "      <td>603.64</td>\n",
       "      <td>3.09</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>67</td>\n",
       "      <td>57</td>\n",
       "      <td>24</td>\n",
       "      <td>600.17</td>\n",
       "      <td>871</td>\n",
       "      <td>596.76</td>\n",
       "      <td>3.05</td>\n",
       "      <td>0.36</td>\n",
       "      <td>576.08</td>\n",
       "      <td>935</td>\n",
       "      <td>571.90</td>\n",
       "      <td>3.91</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>78</td>\n",
       "      <td>85</td>\n",
       "      <td>124</td>\n",
       "      <td>598.01</td>\n",
       "      <td>857</td>\n",
       "      <td>594.12</td>\n",
       "      <td>3.55</td>\n",
       "      <td>0.34</td>\n",
       "      <td>571.98</td>\n",
       "      <td>870</td>\n",
       "      <td>567.37</td>\n",
       "      <td>4.35</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>95</td>\n",
       "      <td>114</td>\n",
       "      <td>49</td>\n",
       "      <td>597.31</td>\n",
       "      <td>846</td>\n",
       "      <td>593.12</td>\n",
       "      <td>3.87</td>\n",
       "      <td>0.32</td>\n",
       "      <td>569.33</td>\n",
       "      <td>856</td>\n",
       "      <td>564.50</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>76</td>\n",
       "      <td>142</td>\n",
       "      <td>149</td>\n",
       "      <td>596.19</td>\n",
       "      <td>836</td>\n",
       "      <td>591.82</td>\n",
       "      <td>4.07</td>\n",
       "      <td>0.31</td>\n",
       "      <td>568.00</td>\n",
       "      <td>837</td>\n",
       "      <td>563.03</td>\n",
       "      <td>4.74</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>79</td>\n",
       "      <td>171</td>\n",
       "      <td>74</td>\n",
       "      <td>595.39</td>\n",
       "      <td>829</td>\n",
       "      <td>590.88</td>\n",
       "      <td>4.22</td>\n",
       "      <td>0.30</td>\n",
       "      <td>567.52</td>\n",
       "      <td>828</td>\n",
       "      <td>562.47</td>\n",
       "      <td>4.80</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35000</th>\n",
       "      <td>78</td>\n",
       "      <td>199</td>\n",
       "      <td>174</td>\n",
       "      <td>595.03</td>\n",
       "      <td>823</td>\n",
       "      <td>590.43</td>\n",
       "      <td>4.33</td>\n",
       "      <td>0.29</td>\n",
       "      <td>567.15</td>\n",
       "      <td>829</td>\n",
       "      <td>562.09</td>\n",
       "      <td>4.84</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40000</th>\n",
       "      <td>86</td>\n",
       "      <td>228</td>\n",
       "      <td>99</td>\n",
       "      <td>594.20</td>\n",
       "      <td>818</td>\n",
       "      <td>589.52</td>\n",
       "      <td>4.42</td>\n",
       "      <td>0.28</td>\n",
       "      <td>567.02</td>\n",
       "      <td>821</td>\n",
       "      <td>561.92</td>\n",
       "      <td>4.90</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45000</th>\n",
       "      <td>77</td>\n",
       "      <td>257</td>\n",
       "      <td>24</td>\n",
       "      <td>593.96</td>\n",
       "      <td>813</td>\n",
       "      <td>589.21</td>\n",
       "      <td>4.49</td>\n",
       "      <td>0.27</td>\n",
       "      <td>566.93</td>\n",
       "      <td>823</td>\n",
       "      <td>561.77</td>\n",
       "      <td>4.96</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000</th>\n",
       "      <td>77</td>\n",
       "      <td>285</td>\n",
       "      <td>124</td>\n",
       "      <td>593.36</td>\n",
       "      <td>810</td>\n",
       "      <td>588.56</td>\n",
       "      <td>4.55</td>\n",
       "      <td>0.26</td>\n",
       "      <td>566.87</td>\n",
       "      <td>818</td>\n",
       "      <td>561.82</td>\n",
       "      <td>4.86</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55000</th>\n",
       "      <td>86</td>\n",
       "      <td>314</td>\n",
       "      <td>49</td>\n",
       "      <td>593.16</td>\n",
       "      <td>806</td>\n",
       "      <td>588.32</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0.25</td>\n",
       "      <td>566.73</td>\n",
       "      <td>818</td>\n",
       "      <td>561.61</td>\n",
       "      <td>4.92</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60000</th>\n",
       "      <td>93</td>\n",
       "      <td>342</td>\n",
       "      <td>149</td>\n",
       "      <td>592.80</td>\n",
       "      <td>804</td>\n",
       "      <td>587.92</td>\n",
       "      <td>4.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>566.56</td>\n",
       "      <td>816</td>\n",
       "      <td>561.47</td>\n",
       "      <td>4.90</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65000</th>\n",
       "      <td>77</td>\n",
       "      <td>371</td>\n",
       "      <td>74</td>\n",
       "      <td>592.52</td>\n",
       "      <td>801</td>\n",
       "      <td>587.59</td>\n",
       "      <td>4.68</td>\n",
       "      <td>0.24</td>\n",
       "      <td>566.64</td>\n",
       "      <td>818</td>\n",
       "      <td>561.55</td>\n",
       "      <td>4.91</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70000</th>\n",
       "      <td>77</td>\n",
       "      <td>399</td>\n",
       "      <td>174</td>\n",
       "      <td>592.42</td>\n",
       "      <td>799</td>\n",
       "      <td>587.47</td>\n",
       "      <td>4.71</td>\n",
       "      <td>0.24</td>\n",
       "      <td>566.41</td>\n",
       "      <td>817</td>\n",
       "      <td>561.26</td>\n",
       "      <td>4.97</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75000</th>\n",
       "      <td>78</td>\n",
       "      <td>428</td>\n",
       "      <td>99</td>\n",
       "      <td>592.06</td>\n",
       "      <td>797</td>\n",
       "      <td>587.08</td>\n",
       "      <td>4.74</td>\n",
       "      <td>0.23</td>\n",
       "      <td>566.43</td>\n",
       "      <td>812</td>\n",
       "      <td>561.28</td>\n",
       "      <td>4.97</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80000</th>\n",
       "      <td>80</td>\n",
       "      <td>457</td>\n",
       "      <td>24</td>\n",
       "      <td>591.98</td>\n",
       "      <td>795</td>\n",
       "      <td>586.99</td>\n",
       "      <td>4.76</td>\n",
       "      <td>0.23</td>\n",
       "      <td>566.22</td>\n",
       "      <td>812</td>\n",
       "      <td>561.03</td>\n",
       "      <td>5.01</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85000</th>\n",
       "      <td>74</td>\n",
       "      <td>485</td>\n",
       "      <td>124</td>\n",
       "      <td>591.69</td>\n",
       "      <td>793</td>\n",
       "      <td>586.68</td>\n",
       "      <td>4.78</td>\n",
       "      <td>0.23</td>\n",
       "      <td>566.31</td>\n",
       "      <td>812</td>\n",
       "      <td>561.18</td>\n",
       "      <td>4.96</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90000</th>\n",
       "      <td>79</td>\n",
       "      <td>514</td>\n",
       "      <td>49</td>\n",
       "      <td>591.62</td>\n",
       "      <td>792</td>\n",
       "      <td>586.59</td>\n",
       "      <td>4.80</td>\n",
       "      <td>0.22</td>\n",
       "      <td>566.28</td>\n",
       "      <td>812</td>\n",
       "      <td>561.10</td>\n",
       "      <td>5.01</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95000</th>\n",
       "      <td>79</td>\n",
       "      <td>542</td>\n",
       "      <td>149</td>\n",
       "      <td>591.43</td>\n",
       "      <td>790</td>\n",
       "      <td>586.39</td>\n",
       "      <td>4.82</td>\n",
       "      <td>0.22</td>\n",
       "      <td>566.32</td>\n",
       "      <td>814</td>\n",
       "      <td>561.22</td>\n",
       "      <td>4.93</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>79</td>\n",
       "      <td>571</td>\n",
       "      <td>74</td>\n",
       "      <td>591.28</td>\n",
       "      <td>789</td>\n",
       "      <td>586.22</td>\n",
       "      <td>4.84</td>\n",
       "      <td>0.22</td>\n",
       "      <td>566.34</td>\n",
       "      <td>812</td>\n",
       "      <td>561.20</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105000</th>\n",
       "      <td>83</td>\n",
       "      <td>599</td>\n",
       "      <td>174</td>\n",
       "      <td>591.24</td>\n",
       "      <td>788</td>\n",
       "      <td>586.18</td>\n",
       "      <td>4.85</td>\n",
       "      <td>0.22</td>\n",
       "      <td>566.20</td>\n",
       "      <td>812</td>\n",
       "      <td>561.04</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110000</th>\n",
       "      <td>80</td>\n",
       "      <td>628</td>\n",
       "      <td>99</td>\n",
       "      <td>591.02</td>\n",
       "      <td>786</td>\n",
       "      <td>585.94</td>\n",
       "      <td>4.86</td>\n",
       "      <td>0.21</td>\n",
       "      <td>566.10</td>\n",
       "      <td>810</td>\n",
       "      <td>560.96</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115000</th>\n",
       "      <td>105</td>\n",
       "      <td>657</td>\n",
       "      <td>24</td>\n",
       "      <td>590.99</td>\n",
       "      <td>785</td>\n",
       "      <td>585.90</td>\n",
       "      <td>4.88</td>\n",
       "      <td>0.21</td>\n",
       "      <td>566.18</td>\n",
       "      <td>809</td>\n",
       "      <td>561.01</td>\n",
       "      <td>5.02</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120000</th>\n",
       "      <td>82</td>\n",
       "      <td>685</td>\n",
       "      <td>124</td>\n",
       "      <td>590.81</td>\n",
       "      <td>784</td>\n",
       "      <td>585.70</td>\n",
       "      <td>4.89</td>\n",
       "      <td>0.21</td>\n",
       "      <td>566.14</td>\n",
       "      <td>810</td>\n",
       "      <td>560.99</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125000</th>\n",
       "      <td>87</td>\n",
       "      <td>714</td>\n",
       "      <td>49</td>\n",
       "      <td>590.78</td>\n",
       "      <td>783</td>\n",
       "      <td>585.66</td>\n",
       "      <td>4.90</td>\n",
       "      <td>0.21</td>\n",
       "      <td>566.19</td>\n",
       "      <td>809</td>\n",
       "      <td>560.99</td>\n",
       "      <td>5.04</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130000</th>\n",
       "      <td>83</td>\n",
       "      <td>742</td>\n",
       "      <td>149</td>\n",
       "      <td>590.66</td>\n",
       "      <td>782</td>\n",
       "      <td>585.53</td>\n",
       "      <td>4.91</td>\n",
       "      <td>0.20</td>\n",
       "      <td>565.98</td>\n",
       "      <td>807</td>\n",
       "      <td>560.80</td>\n",
       "      <td>5.03</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135000</th>\n",
       "      <td>85</td>\n",
       "      <td>771</td>\n",
       "      <td>74</td>\n",
       "      <td>590.56</td>\n",
       "      <td>782</td>\n",
       "      <td>585.42</td>\n",
       "      <td>4.92</td>\n",
       "      <td>0.20</td>\n",
       "      <td>566.27</td>\n",
       "      <td>808</td>\n",
       "      <td>561.08</td>\n",
       "      <td>5.04</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140000</th>\n",
       "      <td>85</td>\n",
       "      <td>799</td>\n",
       "      <td>174</td>\n",
       "      <td>590.55</td>\n",
       "      <td>781</td>\n",
       "      <td>585.40</td>\n",
       "      <td>4.93</td>\n",
       "      <td>0.20</td>\n",
       "      <td>566.15</td>\n",
       "      <td>811</td>\n",
       "      <td>560.98</td>\n",
       "      <td>5.02</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145000</th>\n",
       "      <td>91</td>\n",
       "      <td>828</td>\n",
       "      <td>99</td>\n",
       "      <td>590.39</td>\n",
       "      <td>780</td>\n",
       "      <td>585.24</td>\n",
       "      <td>4.94</td>\n",
       "      <td>0.20</td>\n",
       "      <td>566.03</td>\n",
       "      <td>806</td>\n",
       "      <td>560.84</td>\n",
       "      <td>5.05</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150000</th>\n",
       "      <td>106</td>\n",
       "      <td>857</td>\n",
       "      <td>24</td>\n",
       "      <td>590.38</td>\n",
       "      <td>779</td>\n",
       "      <td>585.21</td>\n",
       "      <td>4.95</td>\n",
       "      <td>0.20</td>\n",
       "      <td>566.06</td>\n",
       "      <td>806</td>\n",
       "      <td>560.85</td>\n",
       "      <td>5.08</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155000</th>\n",
       "      <td>83</td>\n",
       "      <td>885</td>\n",
       "      <td>124</td>\n",
       "      <td>590.25</td>\n",
       "      <td>778</td>\n",
       "      <td>585.07</td>\n",
       "      <td>4.96</td>\n",
       "      <td>0.19</td>\n",
       "      <td>566.15</td>\n",
       "      <td>808</td>\n",
       "      <td>560.99</td>\n",
       "      <td>5.03</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160000</th>\n",
       "      <td>82</td>\n",
       "      <td>914</td>\n",
       "      <td>49</td>\n",
       "      <td>590.23</td>\n",
       "      <td>778</td>\n",
       "      <td>585.05</td>\n",
       "      <td>4.97</td>\n",
       "      <td>0.19</td>\n",
       "      <td>566.12</td>\n",
       "      <td>807</td>\n",
       "      <td>560.87</td>\n",
       "      <td>5.11</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165000</th>\n",
       "      <td>83</td>\n",
       "      <td>942</td>\n",
       "      <td>149</td>\n",
       "      <td>590.14</td>\n",
       "      <td>777</td>\n",
       "      <td>584.95</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.19</td>\n",
       "      <td>565.98</td>\n",
       "      <td>805</td>\n",
       "      <td>560.79</td>\n",
       "      <td>5.05</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170000</th>\n",
       "      <td>86</td>\n",
       "      <td>971</td>\n",
       "      <td>74</td>\n",
       "      <td>590.07</td>\n",
       "      <td>776</td>\n",
       "      <td>584.87</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0.19</td>\n",
       "      <td>566.08</td>\n",
       "      <td>806</td>\n",
       "      <td>560.88</td>\n",
       "      <td>5.07</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175000</th>\n",
       "      <td>83</td>\n",
       "      <td>999</td>\n",
       "      <td>174</td>\n",
       "      <td>590.06</td>\n",
       "      <td>776</td>\n",
       "      <td>584.86</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0.19</td>\n",
       "      <td>565.98</td>\n",
       "      <td>808</td>\n",
       "      <td>560.78</td>\n",
       "      <td>5.07</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       TRAIN:   TM                      VALID:    TM          \\\n",
       "       Time   Ep   Ct    LOSS  PPL     NLL    KL   REG    LOSS   PPL     NLL   \n",
       "5000     75   28   99  600.61  888  597.80  2.38  0.43  607.18  1250  603.64   \n",
       "10000    67   57   24  600.17  871  596.76  3.05  0.36  576.08   935  571.90   \n",
       "15000    78   85  124  598.01  857  594.12  3.55  0.34  571.98   870  567.37   \n",
       "20000    95  114   49  597.31  846  593.12  3.87  0.32  569.33   856  564.50   \n",
       "25000    76  142  149  596.19  836  591.82  4.07  0.31  568.00   837  563.03   \n",
       "30000    79  171   74  595.39  829  590.88  4.22  0.30  567.52   828  562.47   \n",
       "35000    78  199  174  595.03  823  590.43  4.33  0.29  567.15   829  562.09   \n",
       "40000    86  228   99  594.20  818  589.52  4.42  0.28  567.02   821  561.92   \n",
       "45000    77  257   24  593.96  813  589.21  4.49  0.27  566.93   823  561.77   \n",
       "50000    77  285  124  593.36  810  588.56  4.55  0.26  566.87   818  561.82   \n",
       "55000    86  314   49  593.16  806  588.32  4.60  0.25  566.73   818  561.61   \n",
       "60000    93  342  149  592.80  804  587.92  4.64  0.25  566.56   816  561.47   \n",
       "65000    77  371   74  592.52  801  587.59  4.68  0.24  566.64   818  561.55   \n",
       "70000    77  399  174  592.42  799  587.47  4.71  0.24  566.41   817  561.26   \n",
       "75000    78  428   99  592.06  797  587.08  4.74  0.23  566.43   812  561.28   \n",
       "80000    80  457   24  591.98  795  586.99  4.76  0.23  566.22   812  561.03   \n",
       "85000    74  485  124  591.69  793  586.68  4.78  0.23  566.31   812  561.18   \n",
       "90000    79  514   49  591.62  792  586.59  4.80  0.22  566.28   812  561.10   \n",
       "95000    79  542  149  591.43  790  586.39  4.82  0.22  566.32   814  561.22   \n",
       "100000   79  571   74  591.28  789  586.22  4.84  0.22  566.34   812  561.20   \n",
       "105000   83  599  174  591.24  788  586.18  4.85  0.22  566.20   812  561.04   \n",
       "110000   80  628   99  591.02  786  585.94  4.86  0.21  566.10   810  560.96   \n",
       "115000  105  657   24  590.99  785  585.90  4.88  0.21  566.18   809  561.01   \n",
       "120000   82  685  124  590.81  784  585.70  4.89  0.21  566.14   810  560.99   \n",
       "125000   87  714   49  590.78  783  585.66  4.90  0.21  566.19   809  560.99   \n",
       "130000   83  742  149  590.66  782  585.53  4.91  0.20  565.98   807  560.80   \n",
       "135000   85  771   74  590.56  782  585.42  4.92  0.20  566.27   808  561.08   \n",
       "140000   85  799  174  590.55  781  585.40  4.93  0.20  566.15   811  560.98   \n",
       "145000   91  828   99  590.39  780  585.24  4.94  0.20  566.03   806  560.84   \n",
       "150000  106  857   24  590.38  779  585.21  4.95  0.20  566.06   806  560.85   \n",
       "155000   83  885  124  590.25  778  585.07  4.96  0.19  566.15   808  560.99   \n",
       "160000   82  914   49  590.23  778  585.05  4.97  0.19  566.12   807  560.87   \n",
       "165000   83  942  149  590.14  777  584.95  4.98  0.19  565.98   805  560.79   \n",
       "170000   86  971   74  590.07  776  584.87  4.99  0.19  566.08   806  560.88   \n",
       "175000   83  999  174  590.06  776  584.86  4.99  0.19  565.98   808  560.78   \n",
       "\n",
       "                    \n",
       "          KL   REG  \n",
       "5000    3.09  0.46  \n",
       "10000   3.91  0.27  \n",
       "15000   4.35  0.26  \n",
       "20000   4.58  0.25  \n",
       "25000   4.74  0.23  \n",
       "30000   4.80  0.24  \n",
       "35000   4.84  0.21  \n",
       "40000   4.90  0.20  \n",
       "45000   4.96  0.20  \n",
       "50000   4.86  0.19  \n",
       "55000   4.92  0.20  \n",
       "60000   4.90  0.19  \n",
       "65000   4.91  0.18  \n",
       "70000   4.97  0.18  \n",
       "75000   4.97  0.18  \n",
       "80000   5.01  0.18  \n",
       "85000   4.96  0.17  \n",
       "90000   5.01  0.17  \n",
       "95000   4.93  0.17  \n",
       "100000  4.98  0.16  \n",
       "105000  5.00  0.17  \n",
       "110000  4.98  0.16  \n",
       "115000  5.02  0.15  \n",
       "120000  5.00  0.15  \n",
       "125000  5.04  0.15  \n",
       "130000  5.03  0.15  \n",
       "135000  5.04  0.14  \n",
       "140000  5.02  0.14  \n",
       "145000  5.05  0.14  \n",
       "150000  5.08  0.14  \n",
       "155000  5.03  0.13  \n",
       "160000  5.11  0.13  \n",
       "165000  5.05  0.13  \n",
       "170000  5.07  0.13  \n",
       "175000  5.07  0.13  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 R: 1.000 P: 0.149 use key write one know post get article please anyone\n",
      "   1 R: 0.28 P: 0.076 government people say law make go think president state gun\n",
      "     11 R: 0.08 P: 0.079 write car get article one like use go good bike\n",
      "     13 R: 0.05 P: 0.052 game team play year player win season hockey go write\n",
      "     14 R: 0.06 P: 0.064 israel turkish armenian people israeli war jews armenians government attack\n",
      "     12 R: 0.01 P: 0.010 write article post like please get opinion one make know\n",
      "   2 R: 0.32 P: 0.077 write article use people gun child drug one health study\n",
      "     21 R: 0.09 P: 0.094 say go know one people see get think come take\n",
      "     22 R: 0.08 P: 0.082 god jesus christian say one church bible people believe christ\n",
      "     25 R: 0.06 P: 0.059 god say one think believe people question make write exist\n",
      "     23 R: 0.00 P: 0.002 offer dos shipping cd sale game include price good excellent\n",
      "   4 R: 0.08 P: 0.034 space launch nasa program satellite system orbit center project mission\n",
      "     41 R: 0.03 P: 0.033 write get article like think go one know work thing\n",
      "     42 R: 0.01 P: 0.013 book science one theory point appear time new universe find\n",
      "   5 R: 0.17 P: 0.078 file use window program image server available version display get\n",
      "     51 R: 0.10 P: 0.096 use drive card get problem one system work disk mac\n",
      "{0: [1, 2, 5, 3], 1: [11, 13, 14, 15], 2: [21, 22, 25, 24], 5: [51, 52], 3: [31]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>VALID:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>75</td>\n",
       "      <td>28</td>\n",
       "      <td>99</td>\n",
       "      <td>600.61</td>\n",
       "      <td>888</td>\n",
       "      <td>597.80</td>\n",
       "      <td>2.38</td>\n",
       "      <td>0.43</td>\n",
       "      <td>607.18</td>\n",
       "      <td>1250</td>\n",
       "      <td>603.64</td>\n",
       "      <td>3.09</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>67</td>\n",
       "      <td>57</td>\n",
       "      <td>24</td>\n",
       "      <td>600.17</td>\n",
       "      <td>871</td>\n",
       "      <td>596.76</td>\n",
       "      <td>3.05</td>\n",
       "      <td>0.36</td>\n",
       "      <td>576.08</td>\n",
       "      <td>935</td>\n",
       "      <td>571.90</td>\n",
       "      <td>3.91</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>78</td>\n",
       "      <td>85</td>\n",
       "      <td>124</td>\n",
       "      <td>598.01</td>\n",
       "      <td>857</td>\n",
       "      <td>594.12</td>\n",
       "      <td>3.55</td>\n",
       "      <td>0.34</td>\n",
       "      <td>571.98</td>\n",
       "      <td>870</td>\n",
       "      <td>567.37</td>\n",
       "      <td>4.35</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>95</td>\n",
       "      <td>114</td>\n",
       "      <td>49</td>\n",
       "      <td>597.31</td>\n",
       "      <td>846</td>\n",
       "      <td>593.12</td>\n",
       "      <td>3.87</td>\n",
       "      <td>0.32</td>\n",
       "      <td>569.33</td>\n",
       "      <td>856</td>\n",
       "      <td>564.50</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>76</td>\n",
       "      <td>142</td>\n",
       "      <td>149</td>\n",
       "      <td>596.19</td>\n",
       "      <td>836</td>\n",
       "      <td>591.82</td>\n",
       "      <td>4.07</td>\n",
       "      <td>0.31</td>\n",
       "      <td>568.00</td>\n",
       "      <td>837</td>\n",
       "      <td>563.03</td>\n",
       "      <td>4.74</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>79</td>\n",
       "      <td>171</td>\n",
       "      <td>74</td>\n",
       "      <td>595.39</td>\n",
       "      <td>829</td>\n",
       "      <td>590.88</td>\n",
       "      <td>4.22</td>\n",
       "      <td>0.30</td>\n",
       "      <td>567.52</td>\n",
       "      <td>828</td>\n",
       "      <td>562.47</td>\n",
       "      <td>4.80</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35000</th>\n",
       "      <td>78</td>\n",
       "      <td>199</td>\n",
       "      <td>174</td>\n",
       "      <td>595.03</td>\n",
       "      <td>823</td>\n",
       "      <td>590.43</td>\n",
       "      <td>4.33</td>\n",
       "      <td>0.29</td>\n",
       "      <td>567.15</td>\n",
       "      <td>829</td>\n",
       "      <td>562.09</td>\n",
       "      <td>4.84</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40000</th>\n",
       "      <td>86</td>\n",
       "      <td>228</td>\n",
       "      <td>99</td>\n",
       "      <td>594.20</td>\n",
       "      <td>818</td>\n",
       "      <td>589.52</td>\n",
       "      <td>4.42</td>\n",
       "      <td>0.28</td>\n",
       "      <td>567.02</td>\n",
       "      <td>821</td>\n",
       "      <td>561.92</td>\n",
       "      <td>4.90</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45000</th>\n",
       "      <td>77</td>\n",
       "      <td>257</td>\n",
       "      <td>24</td>\n",
       "      <td>593.96</td>\n",
       "      <td>813</td>\n",
       "      <td>589.21</td>\n",
       "      <td>4.49</td>\n",
       "      <td>0.27</td>\n",
       "      <td>566.93</td>\n",
       "      <td>823</td>\n",
       "      <td>561.77</td>\n",
       "      <td>4.96</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000</th>\n",
       "      <td>77</td>\n",
       "      <td>285</td>\n",
       "      <td>124</td>\n",
       "      <td>593.36</td>\n",
       "      <td>810</td>\n",
       "      <td>588.56</td>\n",
       "      <td>4.55</td>\n",
       "      <td>0.26</td>\n",
       "      <td>566.87</td>\n",
       "      <td>818</td>\n",
       "      <td>561.82</td>\n",
       "      <td>4.86</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55000</th>\n",
       "      <td>86</td>\n",
       "      <td>314</td>\n",
       "      <td>49</td>\n",
       "      <td>593.16</td>\n",
       "      <td>806</td>\n",
       "      <td>588.32</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0.25</td>\n",
       "      <td>566.73</td>\n",
       "      <td>818</td>\n",
       "      <td>561.61</td>\n",
       "      <td>4.92</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60000</th>\n",
       "      <td>93</td>\n",
       "      <td>342</td>\n",
       "      <td>149</td>\n",
       "      <td>592.80</td>\n",
       "      <td>804</td>\n",
       "      <td>587.92</td>\n",
       "      <td>4.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>566.56</td>\n",
       "      <td>816</td>\n",
       "      <td>561.47</td>\n",
       "      <td>4.90</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65000</th>\n",
       "      <td>77</td>\n",
       "      <td>371</td>\n",
       "      <td>74</td>\n",
       "      <td>592.52</td>\n",
       "      <td>801</td>\n",
       "      <td>587.59</td>\n",
       "      <td>4.68</td>\n",
       "      <td>0.24</td>\n",
       "      <td>566.64</td>\n",
       "      <td>818</td>\n",
       "      <td>561.55</td>\n",
       "      <td>4.91</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70000</th>\n",
       "      <td>77</td>\n",
       "      <td>399</td>\n",
       "      <td>174</td>\n",
       "      <td>592.42</td>\n",
       "      <td>799</td>\n",
       "      <td>587.47</td>\n",
       "      <td>4.71</td>\n",
       "      <td>0.24</td>\n",
       "      <td>566.41</td>\n",
       "      <td>817</td>\n",
       "      <td>561.26</td>\n",
       "      <td>4.97</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75000</th>\n",
       "      <td>78</td>\n",
       "      <td>428</td>\n",
       "      <td>99</td>\n",
       "      <td>592.06</td>\n",
       "      <td>797</td>\n",
       "      <td>587.08</td>\n",
       "      <td>4.74</td>\n",
       "      <td>0.23</td>\n",
       "      <td>566.43</td>\n",
       "      <td>812</td>\n",
       "      <td>561.28</td>\n",
       "      <td>4.97</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80000</th>\n",
       "      <td>80</td>\n",
       "      <td>457</td>\n",
       "      <td>24</td>\n",
       "      <td>591.98</td>\n",
       "      <td>795</td>\n",
       "      <td>586.99</td>\n",
       "      <td>4.76</td>\n",
       "      <td>0.23</td>\n",
       "      <td>566.22</td>\n",
       "      <td>812</td>\n",
       "      <td>561.03</td>\n",
       "      <td>5.01</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85000</th>\n",
       "      <td>74</td>\n",
       "      <td>485</td>\n",
       "      <td>124</td>\n",
       "      <td>591.69</td>\n",
       "      <td>793</td>\n",
       "      <td>586.68</td>\n",
       "      <td>4.78</td>\n",
       "      <td>0.23</td>\n",
       "      <td>566.31</td>\n",
       "      <td>812</td>\n",
       "      <td>561.18</td>\n",
       "      <td>4.96</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90000</th>\n",
       "      <td>79</td>\n",
       "      <td>514</td>\n",
       "      <td>49</td>\n",
       "      <td>591.62</td>\n",
       "      <td>792</td>\n",
       "      <td>586.59</td>\n",
       "      <td>4.80</td>\n",
       "      <td>0.22</td>\n",
       "      <td>566.28</td>\n",
       "      <td>812</td>\n",
       "      <td>561.10</td>\n",
       "      <td>5.01</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95000</th>\n",
       "      <td>79</td>\n",
       "      <td>542</td>\n",
       "      <td>149</td>\n",
       "      <td>591.43</td>\n",
       "      <td>790</td>\n",
       "      <td>586.39</td>\n",
       "      <td>4.82</td>\n",
       "      <td>0.22</td>\n",
       "      <td>566.32</td>\n",
       "      <td>814</td>\n",
       "      <td>561.22</td>\n",
       "      <td>4.93</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>79</td>\n",
       "      <td>571</td>\n",
       "      <td>74</td>\n",
       "      <td>591.28</td>\n",
       "      <td>789</td>\n",
       "      <td>586.22</td>\n",
       "      <td>4.84</td>\n",
       "      <td>0.22</td>\n",
       "      <td>566.34</td>\n",
       "      <td>812</td>\n",
       "      <td>561.20</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105000</th>\n",
       "      <td>83</td>\n",
       "      <td>599</td>\n",
       "      <td>174</td>\n",
       "      <td>591.24</td>\n",
       "      <td>788</td>\n",
       "      <td>586.18</td>\n",
       "      <td>4.85</td>\n",
       "      <td>0.22</td>\n",
       "      <td>566.20</td>\n",
       "      <td>812</td>\n",
       "      <td>561.04</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110000</th>\n",
       "      <td>80</td>\n",
       "      <td>628</td>\n",
       "      <td>99</td>\n",
       "      <td>591.02</td>\n",
       "      <td>786</td>\n",
       "      <td>585.94</td>\n",
       "      <td>4.86</td>\n",
       "      <td>0.21</td>\n",
       "      <td>566.10</td>\n",
       "      <td>810</td>\n",
       "      <td>560.96</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115000</th>\n",
       "      <td>105</td>\n",
       "      <td>657</td>\n",
       "      <td>24</td>\n",
       "      <td>590.99</td>\n",
       "      <td>785</td>\n",
       "      <td>585.90</td>\n",
       "      <td>4.88</td>\n",
       "      <td>0.21</td>\n",
       "      <td>566.18</td>\n",
       "      <td>809</td>\n",
       "      <td>561.01</td>\n",
       "      <td>5.02</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120000</th>\n",
       "      <td>82</td>\n",
       "      <td>685</td>\n",
       "      <td>124</td>\n",
       "      <td>590.81</td>\n",
       "      <td>784</td>\n",
       "      <td>585.70</td>\n",
       "      <td>4.89</td>\n",
       "      <td>0.21</td>\n",
       "      <td>566.14</td>\n",
       "      <td>810</td>\n",
       "      <td>560.99</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125000</th>\n",
       "      <td>87</td>\n",
       "      <td>714</td>\n",
       "      <td>49</td>\n",
       "      <td>590.78</td>\n",
       "      <td>783</td>\n",
       "      <td>585.66</td>\n",
       "      <td>4.90</td>\n",
       "      <td>0.21</td>\n",
       "      <td>566.19</td>\n",
       "      <td>809</td>\n",
       "      <td>560.99</td>\n",
       "      <td>5.04</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130000</th>\n",
       "      <td>83</td>\n",
       "      <td>742</td>\n",
       "      <td>149</td>\n",
       "      <td>590.66</td>\n",
       "      <td>782</td>\n",
       "      <td>585.53</td>\n",
       "      <td>4.91</td>\n",
       "      <td>0.20</td>\n",
       "      <td>565.98</td>\n",
       "      <td>807</td>\n",
       "      <td>560.80</td>\n",
       "      <td>5.03</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135000</th>\n",
       "      <td>85</td>\n",
       "      <td>771</td>\n",
       "      <td>74</td>\n",
       "      <td>590.56</td>\n",
       "      <td>782</td>\n",
       "      <td>585.42</td>\n",
       "      <td>4.92</td>\n",
       "      <td>0.20</td>\n",
       "      <td>566.27</td>\n",
       "      <td>808</td>\n",
       "      <td>561.08</td>\n",
       "      <td>5.04</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140000</th>\n",
       "      <td>85</td>\n",
       "      <td>799</td>\n",
       "      <td>174</td>\n",
       "      <td>590.55</td>\n",
       "      <td>781</td>\n",
       "      <td>585.40</td>\n",
       "      <td>4.93</td>\n",
       "      <td>0.20</td>\n",
       "      <td>566.15</td>\n",
       "      <td>811</td>\n",
       "      <td>560.98</td>\n",
       "      <td>5.02</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145000</th>\n",
       "      <td>91</td>\n",
       "      <td>828</td>\n",
       "      <td>99</td>\n",
       "      <td>590.39</td>\n",
       "      <td>780</td>\n",
       "      <td>585.24</td>\n",
       "      <td>4.94</td>\n",
       "      <td>0.20</td>\n",
       "      <td>566.03</td>\n",
       "      <td>806</td>\n",
       "      <td>560.84</td>\n",
       "      <td>5.05</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150000</th>\n",
       "      <td>106</td>\n",
       "      <td>857</td>\n",
       "      <td>24</td>\n",
       "      <td>590.38</td>\n",
       "      <td>779</td>\n",
       "      <td>585.21</td>\n",
       "      <td>4.95</td>\n",
       "      <td>0.20</td>\n",
       "      <td>566.06</td>\n",
       "      <td>806</td>\n",
       "      <td>560.85</td>\n",
       "      <td>5.08</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155000</th>\n",
       "      <td>83</td>\n",
       "      <td>885</td>\n",
       "      <td>124</td>\n",
       "      <td>590.25</td>\n",
       "      <td>778</td>\n",
       "      <td>585.07</td>\n",
       "      <td>4.96</td>\n",
       "      <td>0.19</td>\n",
       "      <td>566.15</td>\n",
       "      <td>808</td>\n",
       "      <td>560.99</td>\n",
       "      <td>5.03</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160000</th>\n",
       "      <td>82</td>\n",
       "      <td>914</td>\n",
       "      <td>49</td>\n",
       "      <td>590.23</td>\n",
       "      <td>778</td>\n",
       "      <td>585.05</td>\n",
       "      <td>4.97</td>\n",
       "      <td>0.19</td>\n",
       "      <td>566.12</td>\n",
       "      <td>807</td>\n",
       "      <td>560.87</td>\n",
       "      <td>5.11</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165000</th>\n",
       "      <td>83</td>\n",
       "      <td>942</td>\n",
       "      <td>149</td>\n",
       "      <td>590.14</td>\n",
       "      <td>777</td>\n",
       "      <td>584.95</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.19</td>\n",
       "      <td>565.98</td>\n",
       "      <td>805</td>\n",
       "      <td>560.79</td>\n",
       "      <td>5.05</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170000</th>\n",
       "      <td>86</td>\n",
       "      <td>971</td>\n",
       "      <td>74</td>\n",
       "      <td>590.07</td>\n",
       "      <td>776</td>\n",
       "      <td>584.87</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0.19</td>\n",
       "      <td>566.08</td>\n",
       "      <td>806</td>\n",
       "      <td>560.88</td>\n",
       "      <td>5.07</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175000</th>\n",
       "      <td>83</td>\n",
       "      <td>999</td>\n",
       "      <td>174</td>\n",
       "      <td>590.06</td>\n",
       "      <td>776</td>\n",
       "      <td>584.86</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0.19</td>\n",
       "      <td>565.98</td>\n",
       "      <td>808</td>\n",
       "      <td>560.78</td>\n",
       "      <td>5.07</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       TRAIN:   TM                      VALID:    TM          \\\n",
       "       Time   Ep   Ct    LOSS  PPL     NLL    KL   REG    LOSS   PPL     NLL   \n",
       "5000     75   28   99  600.61  888  597.80  2.38  0.43  607.18  1250  603.64   \n",
       "10000    67   57   24  600.17  871  596.76  3.05  0.36  576.08   935  571.90   \n",
       "15000    78   85  124  598.01  857  594.12  3.55  0.34  571.98   870  567.37   \n",
       "20000    95  114   49  597.31  846  593.12  3.87  0.32  569.33   856  564.50   \n",
       "25000    76  142  149  596.19  836  591.82  4.07  0.31  568.00   837  563.03   \n",
       "30000    79  171   74  595.39  829  590.88  4.22  0.30  567.52   828  562.47   \n",
       "35000    78  199  174  595.03  823  590.43  4.33  0.29  567.15   829  562.09   \n",
       "40000    86  228   99  594.20  818  589.52  4.42  0.28  567.02   821  561.92   \n",
       "45000    77  257   24  593.96  813  589.21  4.49  0.27  566.93   823  561.77   \n",
       "50000    77  285  124  593.36  810  588.56  4.55  0.26  566.87   818  561.82   \n",
       "55000    86  314   49  593.16  806  588.32  4.60  0.25  566.73   818  561.61   \n",
       "60000    93  342  149  592.80  804  587.92  4.64  0.25  566.56   816  561.47   \n",
       "65000    77  371   74  592.52  801  587.59  4.68  0.24  566.64   818  561.55   \n",
       "70000    77  399  174  592.42  799  587.47  4.71  0.24  566.41   817  561.26   \n",
       "75000    78  428   99  592.06  797  587.08  4.74  0.23  566.43   812  561.28   \n",
       "80000    80  457   24  591.98  795  586.99  4.76  0.23  566.22   812  561.03   \n",
       "85000    74  485  124  591.69  793  586.68  4.78  0.23  566.31   812  561.18   \n",
       "90000    79  514   49  591.62  792  586.59  4.80  0.22  566.28   812  561.10   \n",
       "95000    79  542  149  591.43  790  586.39  4.82  0.22  566.32   814  561.22   \n",
       "100000   79  571   74  591.28  789  586.22  4.84  0.22  566.34   812  561.20   \n",
       "105000   83  599  174  591.24  788  586.18  4.85  0.22  566.20   812  561.04   \n",
       "110000   80  628   99  591.02  786  585.94  4.86  0.21  566.10   810  560.96   \n",
       "115000  105  657   24  590.99  785  585.90  4.88  0.21  566.18   809  561.01   \n",
       "120000   82  685  124  590.81  784  585.70  4.89  0.21  566.14   810  560.99   \n",
       "125000   87  714   49  590.78  783  585.66  4.90  0.21  566.19   809  560.99   \n",
       "130000   83  742  149  590.66  782  585.53  4.91  0.20  565.98   807  560.80   \n",
       "135000   85  771   74  590.56  782  585.42  4.92  0.20  566.27   808  561.08   \n",
       "140000   85  799  174  590.55  781  585.40  4.93  0.20  566.15   811  560.98   \n",
       "145000   91  828   99  590.39  780  585.24  4.94  0.20  566.03   806  560.84   \n",
       "150000  106  857   24  590.38  779  585.21  4.95  0.20  566.06   806  560.85   \n",
       "155000   83  885  124  590.25  778  585.07  4.96  0.19  566.15   808  560.99   \n",
       "160000   82  914   49  590.23  778  585.05  4.97  0.19  566.12   807  560.87   \n",
       "165000   83  942  149  590.14  777  584.95  4.98  0.19  565.98   805  560.79   \n",
       "170000   86  971   74  590.07  776  584.87  4.99  0.19  566.08   806  560.88   \n",
       "175000   83  999  174  590.06  776  584.86  4.99  0.19  565.98   808  560.78   \n",
       "\n",
       "                    \n",
       "          KL   REG  \n",
       "5000    3.09  0.46  \n",
       "10000   3.91  0.27  \n",
       "15000   4.35  0.26  \n",
       "20000   4.58  0.25  \n",
       "25000   4.74  0.23  \n",
       "30000   4.80  0.24  \n",
       "35000   4.84  0.21  \n",
       "40000   4.90  0.20  \n",
       "45000   4.96  0.20  \n",
       "50000   4.86  0.19  \n",
       "55000   4.92  0.20  \n",
       "60000   4.90  0.19  \n",
       "65000   4.91  0.18  \n",
       "70000   4.97  0.18  \n",
       "75000   4.97  0.18  \n",
       "80000   5.01  0.18  \n",
       "85000   4.96  0.17  \n",
       "90000   5.01  0.17  \n",
       "95000   4.93  0.17  \n",
       "100000  4.98  0.16  \n",
       "105000  5.00  0.17  \n",
       "110000  4.98  0.16  \n",
       "115000  5.02  0.15  \n",
       "120000  5.00  0.15  \n",
       "125000  5.04  0.15  \n",
       "130000  5.03  0.15  \n",
       "135000  5.04  0.14  \n",
       "140000  5.02  0.14  \n",
       "145000  5.05  0.14  \n",
       "150000  5.08  0.14  \n",
       "155000  5.03  0.13  \n",
       "160000  5.11  0.13  \n",
       "165000  5.05  0.13  \n",
       "170000  5.07  0.13  \n",
       "175000  5.07  0.13  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 R: 1.000 P: 0.152 use key write one know post get article please anyone\n",
      "   1 R: 0.28 P: 0.077 government people say law make go think president state gun\n",
      "     11 R: 0.08 P: 0.077 write car get article one like use go good bike\n",
      "     13 R: 0.05 P: 0.053 game team play year player win season hockey go write\n",
      "     14 R: 0.06 P: 0.062 israel turkish armenian people israeli war jews armenians government attack\n",
      "     15 R: 0.01 P: 0.011 write article post like please get opinion one make know\n",
      "   2 R: 0.32 P: 0.079 write article use people gun child drug one health study\n",
      "     21 R: 0.09 P: 0.094 say go know one people see get think come take\n",
      "     22 R: 0.08 P: 0.084 god jesus christian say one church bible people believe christ\n",
      "     25 R: 0.06 P: 0.059 god say one think believe people question make write exist\n",
      "     24 R: 0.00 P: 0.002 offer dos shipping cd sale game include price good excellent\n",
      "   5 R: 0.08 P: 0.033 space launch nasa program satellite system orbit center project mission\n",
      "     51 R: 0.03 P: 0.032 write get article like think go one know work thing\n",
      "     52 R: 0.01 P: 0.014 book science one theory point appear time new universe find\n",
      "   3 R: 0.17 P: 0.078 file use window program image server available version display get\n",
      "     31 R: 0.09 P: 0.094 use drive card get problem one system work disk mac\n"
     ]
    }
   ],
   "source": [
    "if len(log_df) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "while epoch < config.epochs:    \n",
    "    # train\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = model.get_feed_dict(batch)\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, ppls_batch, global_step_log = \\\n",
    "        sess.run([model.opt, model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_reg, model.topic_ppls, tf.train.get_global_step()], feed_dict = feed_dict)\n",
    "\n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        # validate\n",
    "#         if global_step_log % config.log_period == 0:\n",
    "        if global_step_log % 5000 == 0:            \n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, rads_bow_dev, probs_topic_dev = get_loss(sess, dev_batches, model)\n",
    "\n",
    "            # log\n",
    "            clear_output()\n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_reg_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_reg_dev],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "\n",
    "            # visualize topic\n",
    "#             topic_rad_bow = {topic_idx: rad_bow for topic_idx, rad_bow in zip(model.topic_idxs, rads_bow_dev)}\n",
    "            topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "            \n",
    "            recur_topic_idxs = {parent_idx: get_recur_topic_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "            recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in recur_topic_idxs.items()}\n",
    "            \n",
    "            print_topic_sample(tree_idxs, sess, model, topic_prob_topic=topic_prob_topic, topic_rad_bow=recur_prob_topic)\n",
    "            time_start = time.time()\n",
    "\n",
    "            # update tree\n",
    "#             tree_idxs, update_tree_flg = update_tree(topic_rad_bow, topic_prob_topic, model, add_threshold=0.2, remove_threshold=0.05)\n",
    "            tree_idxs, update_tree_flg = update_tree(recur_prob_topic, topic_prob_topic, model, add_threshold=0.05, remove_threshold=0.05)\n",
    "            if update_tree_flg:\n",
    "                print(tree_idxs)\n",
    "                name_variables = {tensor.name: variable for tensor, variable in zip(tf.global_variables(), sess.run(tf.global_variables()))} # store paremeters\n",
    "                if 'sess' in globals(): sess.close()\n",
    "                model = Model(config, tree_idxs)\n",
    "                sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "                name_tensors = {tensor.name: tensor for tensor in tf.global_variables()}\n",
    "                sess.run([name_tensors[name].assign(variable) for name, variable in name_variables.items()]) # restore parameters\n",
    "\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "    epoch += 1\n",
    "\n",
    "loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, rads_bow_dev, probs_topic_dev = get_loss(sess, dev_batches, model)\n",
    "topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "\n",
    "recur_topic_idxs = {parent_idx: get_recur_topic_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in recur_topic_idxs.items()}\n",
    "display(log_df)\n",
    "print_topic_sample(tree_idxs, sess, model, topic_prob_topic=topic_prob_topic, topic_rad_bow=recur_prob_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_bow = sess.run(model.topic_bow)\n",
    "norm_bow = np.sum([instance.bow for instance in instances_train], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_vec = topics_bow / np.linalg.norm(topics_bow, axis=1, keepdims=True)\n",
    "norm_vec = norm_bow / np.linalg.norm(norm_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_spec = 1 - topics_vec.dot(norm_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_topic_idxs = defaultdict(list)\n",
    "for topic_idx, depth in model.tree_depth.items():\n",
    "    depth_topic_idxs[depth].append(topic_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.1985454189063376\n",
      "2 0.4160374942875843\n",
      "3 0.41167937757780354\n"
     ]
    }
   ],
   "source": [
    "for depth, topic_idxs in depth_topic_idxs.items():\n",
    "    topic_indices = np.array([model.topic_idxs.index(topic_idx) for topic_idx in topic_idxs])\n",
    "    depth_spec = np.mean(topics_spec[topic_indices])\n",
    "    print(depth, depth_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
