{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from data_structure import get_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "from topic_model import TopicModel\n",
    "\n",
    "from topic_beam_search_decoder import BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '2', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/apnews/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/tglm_vae', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'apnews', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 1000, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 500, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.1, 'lr')\n",
    "flags.DEFINE_float('reg', 0.1, 'regularization term')\n",
    "flags.DEFINE_float('beta', 0.001, 'initial value of beta')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_integer('warmup', 5000, 'warmup period for KL')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 10, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_topic', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('PAD_IDX', word_to_idx[PAD], 'PAD_IDX')\n",
    "flags.DEFINE_integer('UNK_IDX', word_to_idx[UNK], 'UNK_IDX')\n",
    "flags.DEFINE_integer('BOS_IDX', word_to_idx[BOS], 'BOS_IDX')\n",
    "flags.DEFINE_integer('EOS_IDX', word_to_idx[EOS], 'EOS_IDX')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "maximum_iterations = max([max([instance.max_sent_l for instance in batch]) for ct, batch in dev_batches])\n",
    "flags.DEFINE_integer('maximum_iterations', maximum_iterations, 'maximum_iterations')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build language model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['bow'] = tf.placeholder(tf.float32, [None, config.dim_bow], name='bow')\n",
    "t_variables['input_token_idxs'] = tf.placeholder(tf.int32, [None, None], name='input_token_idxs')\n",
    "t_variables['dec_input_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_input_idxs')\n",
    "t_variables['dec_target_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_target_idxs')\n",
    "t_variables['batch_l'] = tf.placeholder(tf.int32, name='batch_l')\n",
    "t_variables['doc_l'] = tf.placeholder(tf.int32, [None], name='doc_l')\n",
    "t_variables['sent_l'] = tf.placeholder(tf.int32, [None], name='sent_l')\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train', assertion=False):\n",
    "    def token_dropout(sent_idxs):\n",
    "        sent_idxs_dropout = np.asarray(sent_idxs)\n",
    "        sent_idxs_dropout[np.random.rand(len(sent_idxs)) > config.word_keep_prob] = config.UNK_IDX\n",
    "        return list(sent_idxs_dropout)\n",
    "\n",
    "    bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "    \n",
    "    doc_l = np.array([len(instance.token_idxs) for instance in batch])\n",
    "    \n",
    "    feed_input_token_idxs_list = [sent_idxs for instance in batch for sent_idxs in instance.token_idxs]\n",
    "    feed_dec_input_idxs_list = [[config.BOS_IDX] + token_dropout(sent_idxs) for sent_idxs in feed_input_token_idxs_list]\n",
    "    feed_dec_target_idxs_list = [sent_idxs + [config.EOS_IDX]  for sent_idxs in feed_input_token_idxs_list]\n",
    "        \n",
    "    sent_l = np.array([len(sent_idxs) for sent_idxs in feed_input_token_idxs_list], np.int32)\n",
    "    batch_l = len(sent_l)\n",
    "    \n",
    "    feed_input_token_idxs = pad_sequences(feed_input_token_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_input_idxs = pad_sequences(feed_dec_input_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_target_idxs = pad_sequences(feed_dec_target_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    \n",
    "    if assertion:\n",
    "        index = 0\n",
    "        for instance in batch:\n",
    "            for line_idxs in instance.token_idxs:\n",
    "                assert feed_input_token_idxs_list[index] == line_idxs\n",
    "                index += 1\n",
    "        assert feed_input_token_idxs.shape[1] == np.max(sent_l)\n",
    "        assert feed_dec_input_idxs.shape[1] == np.max(sent_l) + 1\n",
    "        assert feed_dec_target_idxs.shape[1] == np.max(sent_l) + 1\n",
    "    \n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "\n",
    "    feed_dict = {\n",
    "                t_variables['bow']: bow, \n",
    "                t_variables['batch_l']: batch_l, t_variables['doc_l']: doc_l, t_variables['sent_l']: sent_l, \n",
    "                t_variables['input_token_idxs']: feed_input_token_idxs, t_variables['dec_input_idxs']: feed_dec_input_idxs, t_variables['dec_target_idxs']: feed_dec_target_idxs, \n",
    "                t_variables['keep_prob']: keep_prob\n",
    "    }\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "def check_shape(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encode bow\n",
    "with tf.variable_scope('topic/enc', reuse=False):\n",
    "    hidden_bow_ = tf.layers.Dense(units=config.dim_hidden_bow, activation=tf.nn.relu, name='hidden_bow')(t_variables['bow'])\n",
    "    hidden_bow = tf.layers.Dropout(t_variables['keep_prob'])(hidden_bow_)\n",
    "    means_bow = tf.layers.Dense(units=config.dim_latent_topic, name='mean_bow')(hidden_bow)\n",
    "    logvars_bow = tf.layers.Dense(units=config.dim_latent_topic, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_bow)\n",
    "    latents_bow = sample_latents(means_bow, logvars_bow) # sample latent vectors\n",
    "\n",
    "    prob_topic = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob')(latents_bow) # inference of topic probabilities\n",
    "\n",
    "# decode bow\n",
    "with tf.variable_scope('shared', reuse=False):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "bow_embeddings = tf.nn.embedding_lookup(embeddings, bow_idxs) # embeddings of each bow features\n",
    "\n",
    "with tf.variable_scope('topic/dec', reuse=False):\n",
    "    topic_embeddings = tf.get_variable('topic_emb', [config.n_topic, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of topics\n",
    "\n",
    "    topic_bow = tf.nn.softmax(tf.matmul(topic_embeddings, bow_embeddings, transpose_b=True), 1) # bow vectors for each topic\n",
    "    logits_bow = tf_log(tf.matmul(prob_topic, topic_bow)) # predicted bow distribution\n",
    "\n",
    "# prior of each gaussian distribution (computed for each topic)\n",
    "    means_topic = tf.layers.Dense(units=config.dim_latent, name='mean_topic')(topic_bow)\n",
    "    logvars_topic = tf.layers.Dense(units=config.dim_latent, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(topic_bow)\n",
    "sigma_topic = tf.exp(0.5 * logvars_topic)\n",
    "gauss_topic = tfd.Normal(loc=means_topic, scale=sigma_topic)    \n",
    "    \n",
    "# define losses\n",
    "topic_losses_recon = -tf.reduce_sum(tf.multiply(t_variables['bow'], logits_bow), 1)\n",
    "topic_loss_recon = tf.reduce_mean(topic_losses_recon) # negative log likelihood of each words\n",
    "\n",
    "topic_loss_kl = compute_kl_loss(means_bow, logvars_bow) # KL divergence b/w latent dist & gaussian std\n",
    "\n",
    "topic_embeddings_norm = topic_embeddings / tf.norm(topic_embeddings, axis=1, keepdims=True)\n",
    "topic_angles = tf.matmul(topic_embeddings_norm, tf.transpose(topic_embeddings_norm))\n",
    "topic_angles_mean = tf.reduce_mean(topic_angles, keepdims=True)\n",
    "topic_angles_vars = tf.reduce_mean(tf.square(topic_angles - topic_angles_mean))\n",
    "topic_loss_reg = topic_angles_vars - tf.squeeze(topic_angles_mean)\n",
    "\n",
    "# monitor\n",
    "n_bow = tf.reduce_sum(t_variables['bow'], 1)\n",
    "topic_ppls = tf.divide(topic_losses_recon, n_bow)\n",
    "topics_freq_bow_indices = tf.nn.top_k(topic_bow, 10, name='topic_freq_bow').indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "input_token_idxs = t_variables['input_token_idxs']\n",
    "batch_l = t_variables['batch_l']\n",
    "sent_l = t_variables['sent_l']\n",
    "max_sent_l = tf.reduce_max(sent_l)\n",
    "\n",
    "with tf.variable_scope('sent/enc', reuse=False):\n",
    "    # get word embedding\n",
    "    enc_input = tf.nn.embedding_lookup(embeddings, input_token_idxs)\n",
    "\n",
    "    # get sentence embedding\n",
    "    _, enc_state = dynamic_rnn(enc_input, sent_l, config.dim_hidden, t_variables['keep_prob'])\n",
    "\n",
    "    # TODO House Holder flow\n",
    "    prob_topic_infer = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax)(enc_state)\n",
    "    \n",
    "    # inference of each gaussian dist. parameter\n",
    "    enc_state_infer = tf.tile(tf.expand_dims(enc_state, 1), [1, config.n_topic, 1]) # tile over topics\n",
    "    means_topic_infer = tf.layers.Dense(units=config.dim_latent, name='mean_topic_infer')(enc_state_infer)\n",
    "    logvars_topic_infer = tf.layers.Dense(units=config.dim_latent, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic_infer')(enc_state_infer)\n",
    "    sigma_topic_infer = tf.exp(0.5 * logvars_topic_infer)\n",
    "    gauss_topic_infer = tfd.Normal(loc=means_topic_infer, scale=sigma_topic_infer)\n",
    "\n",
    "    # latent vectors from each gaussian dist.\n",
    "    latents_topic_infer = sample_latents(means_topic_infer, logvars_topic_infer) \n",
    "    # latent vector from gaussian mixture    \n",
    "    latents_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), latents_topic_infer, transpose_a=True)\n",
    "    \n",
    "    # for beam search\n",
    "    means_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), means_topic_infer, transpose_a=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for decoding\n",
    "dec_sent_l = tf.add(sent_l, 1)\n",
    "dec_input_idxs = t_variables['dec_input_idxs']\n",
    "dec_input = tf.nn.embedding_lookup(embeddings, dec_input_idxs)\n",
    "\n",
    "dec_latents_input = tf.tile(latents_input, [1, tf.shape(dec_input)[1], 1])\n",
    "dec_concat_input = tf.concat([dec_input, dec_latents_input], -1)\n",
    "\n",
    "# decode for training\n",
    "with tf.variable_scope('sent/dec/rnn', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32, reuse=False):\n",
    "    dec_cell = tf.contrib.rnn.GRUCell(config.dim_hidden)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob = t_variables['keep_prob'])\n",
    "\n",
    "    dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(latents_input, 1))\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_concat_input, sequence_length=dec_sent_l)\n",
    "\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=dec_cell,\n",
    "        helper=helper,\n",
    "        initial_state=dec_initial_state)\n",
    "\n",
    "    dec_outputs, _, output_sent_l = tf.contrib.seq2seq.dynamic_decode(train_decoder)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(config.n_vocab, use_bias=False, name='out')\n",
    "    output_logits = output_layer(dec_outputs.rnn_output)\n",
    "    \n",
    "    output_token_idxs = tf.argmax(output_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tokens = tf.fill([batch_l], config.BOS_IDX)\n",
    "end_token = config.EOS_IDX\n",
    "\n",
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    infer_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(means_input, 1))\n",
    "    beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(infer_dec_initial_state, multiplier=config.beam_width)\n",
    "    beam_latents_input = tf.contrib.seq2seq.tile_batch(tf.squeeze(means_input, 1), multiplier=config.beam_width) # added\n",
    "    \n",
    "    beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=beam_latents_input)\n",
    "\n",
    "    beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    beam_output_token_idxs = beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    inter_means_input = tf.placeholder(tf.float32, [None, config.dim_latent])\n",
    "    \n",
    "    inter_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(inter_means_input)\n",
    "    inter_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(inter_dec_initial_state, multiplier=config.beam_width)\n",
    "    inter_beam_latents_input = tf.contrib.seq2seq.tile_batch(inter_means_input, multiplier=config.beam_width) # added\n",
    "    \n",
    "    inter_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=inter_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=inter_beam_latents_input)\n",
    "\n",
    "    inter_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inter_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    inter_beam_output_token_idxs = inter_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language modeling cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target and mask\n",
    "dec_target_idxs = t_variables['dec_target_idxs']\n",
    "dec_mask_tokens = tf.sequence_mask(dec_sent_l, maxlen=max_sent_l+1, dtype=tf.float32)\n",
    "\n",
    "# nll for each token (averaged over batch & sentence)\n",
    "sent_loss_recon = tf.contrib.seq2seq.sequence_loss(output_logits, dec_target_idxs, dec_mask_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_l = t_variables['doc_l']\n",
    "mask_sents = tf.sequence_mask(doc_l)\n",
    "mask_sents_flatten = tf.reshape(mask_sents, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1]])\n",
    "\n",
    "prob_topic_tiled = tf.tile(tf.expand_dims(prob_topic, 1), [1, tf.shape(mask_sents)[1], 1])\n",
    "prob_topic_flatten = tf.reshape(prob_topic_tiled, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1], config.n_topic])\n",
    "prob_topic_sents = tf.boolean_mask(prob_topic_flatten, mask_sents_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferred mixture probabilities (computed for each sentence)\n",
    "categ_topic_infer = tfd.Categorical(probs=prob_topic_infer)\n",
    "\n",
    "# prior of mixture probabilities (computed for each document, tiled for each sentence)\n",
    "categ_topic = tfd.Categorical(probs=prob_topic_sents)\n",
    "\n",
    "sent_loss_kl_categ = tf.reduce_mean(tfd.kl_divergence(categ_topic_infer, categ_topic))\n",
    "\n",
    "# inference of each gaussian gaussribution (computed for each sentence)\n",
    "\n",
    "sent_loss_kl_gauss = tf.reduce_sum(tfd.kl_divergence(gauss_topic_infer, gauss_topic), -1)\n",
    "sent_loss_kl_gmm = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss), -1))\n",
    "\n",
    "sent_loss_kl = sent_loss_kl_categ + sent_loss_kl_gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "beta = tf.Variable(config.beta, name='beta', trainable=False) if config.warmup > 0 else tf.constant(1., name='beta')\n",
    "update_beta = tf.assign_add(beta, 1./(config.warmup*len(train_batches)))\n",
    "sent_loss = sent_loss_recon + beta * sent_loss_kl\n",
    "\n",
    "topic_loss = topic_loss_recon + topic_loss_kl + config.reg * topic_loss_reg\n",
    "loss = topic_loss + sent_loss\n",
    "\n",
    "# define optimizer\n",
    "if config.opt == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif config.opt == 'Adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "    \n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_sents(token_idxs, config, idx_to_word):\n",
    "    sents = []\n",
    "    for sent_idxs in token_idxs:\n",
    "        tokens = []\n",
    "        for idx in sent_idxs:\n",
    "            if idx == config.EOS_IDX: break\n",
    "            tokens.append(idx_to_word[idx])\n",
    "        sent = ' '.join(tokens)\n",
    "        sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_batch, sent_loss_batch, ppls_batch = sess.run([loss, topic_loss, sent_loss, topic_ppls], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_batch, sent_loss_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "    loss_mean, topic_loss_mean, sent_loss_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    return loss_mean, topic_loss_mean, sent_loss_mean, ppl_mean\n",
    "\n",
    "def get_all_losses(sess, batches):\n",
    "    losses = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch = \\\n",
    "        sess.run([loss, topic_loss_recon, topic_loss_kl, sent_loss_recon, sent_loss_kl], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "    print('LOSS %.2f | TM NLL: %.2f, KL: %.4f | LM NLL: %.2f, KL: %.4f' %  np.mean(losses, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_batch):\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    pred_token_idxs = sess.run(output_token_idxs, feed_dict = feed_dict)\n",
    "    true_token_idxs = [sent_idxs for instance in sample_batch for sent_idxs in instance.token_idxs]\n",
    "    \n",
    "    assert len(pred_token_idxs) == len(true_token_idxs)\n",
    "    \n",
    "    pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "    true_sents = idxs_to_sents(true_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    for true_sent, pred_sent in zip(true_sents, pred_sents):        \n",
    "        print('True: %s' % true_sent)\n",
    "        print('Pred: %s' % pred_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "logs = []\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "saver = tf.train.Saver(max_to_keep=10)\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "009[s], 08[s], Ep: 00, Ct: 00000|TR LOSS: 343, PPL: 2661|TM NLL: 332, KL: 0.96 | LM NLL: 10.34, KL: 0.71|DE LOSS: 347, PPL: 2655, TM: 337, LM: 10.34|BETA: 0.001000\n",
      "077[s], 07[s], Ep: 00, Ct: 00500|TR LOSS: 326, PPL: 1604|TM NLL: 317, KL: 1.34 | LM NLL: 7.39, KL: 20.82|DE LOSS: 319, PPL: 1439, TM: 312, LM: 6.86|BETA: 0.001035\n",
      "077[s], 07[s], Ep: 00, Ct: 01000|TR LOSS: 323, PPL: 1489|TM NLL: 314, KL: 1.74 | LM NLL: 7.12, KL: 17.07|DE LOSS: 316, PPL: 1345, TM: 309, LM: 6.72|BETA: 0.001069\n",
      "077[s], 07[s], Ep: 00, Ct: 01500|TR LOSS: 322, PPL: 1428|TM NLL: 313, KL: 1.93 | LM NLL: 7.00, KL: 14.74|DE LOSS: 314, PPL: 1272, TM: 307, LM: 6.65|BETA: 0.001104\n",
      "077[s], 08[s], Ep: 00, Ct: 02000|TR LOSS: 321, PPL: 1380|TM NLL: 312, KL: 2.13 | LM NLL: 6.92, KL: 13.37|DE LOSS: 312, PPL: 1216, TM: 305, LM: 6.57|BETA: 0.001139\n",
      "076[s], 07[s], Ep: 00, Ct: 02500|TR LOSS: 320, PPL: 1343|TM NLL: 311, KL: 2.31 | LM NLL: 6.86, KL: 12.83|DE LOSS: 311, PPL: 1170, TM: 304, LM: 6.50|BETA: 0.001173\n",
      "060[s], 07[s], Ep: 01, Ct: 00000|TR LOSS: 319, PPL: 1320|TM NLL: 310, KL: 2.41 | LM NLL: 6.81, KL: 12.55|DE LOSS: 310, PPL: 1167, TM: 304, LM: 6.41|BETA: 0.001200\n",
      "076[s], 07[s], Ep: 01, Ct: 00500|TR LOSS: 318, PPL: 1298|TM NLL: 309, KL: 2.50 | LM NLL: 6.75, KL: 12.46|DE LOSS: 310, PPL: 1144, TM: 303, LM: 6.32|BETA: 0.001235\n",
      "078[s], 07[s], Ep: 01, Ct: 01000|TR LOSS: 318, PPL: 1277|TM NLL: 308, KL: 2.58 | LM NLL: 6.70, KL: 12.47|DE LOSS: 309, PPL: 1138, TM: 303, LM: 6.21|BETA: 0.001269\n",
      "078[s], 07[s], Ep: 01, Ct: 01500|TR LOSS: 317, PPL: 1259|TM NLL: 308, KL: 2.64 | LM NLL: 6.65, KL: 12.56|DE LOSS: 309, PPL: 1123, TM: 303, LM: 6.13|BETA: 0.001304\n",
      "076[s], 07[s], Ep: 01, Ct: 02000|TR LOSS: 317, PPL: 1244|TM NLL: 307, KL: 2.69 | LM NLL: 6.60, KL: 12.77|DE LOSS: 308, PPL: 1117, TM: 302, LM: 6.06|BETA: 0.001339\n",
      "076[s], 07[s], Ep: 01, Ct: 02500|TR LOSS: 316, PPL: 1230|TM NLL: 307, KL: 2.74 | LM NLL: 6.54, KL: 13.15|DE LOSS: 308, PPL: 1106, TM: 302, LM: 5.92|BETA: 0.001374\n",
      "057[s], 07[s], Ep: 02, Ct: 00000|TR LOSS: 316, PPL: 1221|TM NLL: 307, KL: 2.78 | LM NLL: 6.50, KL: 13.54|DE LOSS: 308, PPL: 1099, TM: 302, LM: 5.83|BETA: 0.001400\n",
      "072[s], 07[s], Ep: 02, Ct: 00500|TR LOSS: 316, PPL: 1212|TM NLL: 306, KL: 2.82 | LM NLL: 6.45, KL: 14.13|DE LOSS: 307, PPL: 1090, TM: 301, LM: 5.72|BETA: 0.001435\n",
      "073[s], 07[s], Ep: 02, Ct: 01000|TR LOSS: 315, PPL: 1202|TM NLL: 306, KL: 2.86 | LM NLL: 6.40, KL: 14.75|DE LOSS: 307, PPL: 1094, TM: 301, LM: 5.61|BETA: 0.001470\n",
      "072[s], 07[s], Ep: 02, Ct: 01500|TR LOSS: 315, PPL: 1193|TM NLL: 306, KL: 2.89 | LM NLL: 6.34, KL: 15.41|DE LOSS: 307, PPL: 1084, TM: 301, LM: 5.50|BETA: 0.001504\n",
      "072[s], 07[s], Ep: 02, Ct: 02000|TR LOSS: 315, PPL: 1185|TM NLL: 305, KL: 2.92 | LM NLL: 6.29, KL: 16.07|DE LOSS: 307, PPL: 1088, TM: 301, LM: 5.35|BETA: 0.001539\n",
      "073[s], 07[s], Ep: 02, Ct: 02500|TR LOSS: 314, PPL: 1178|TM NLL: 305, KL: 2.94 | LM NLL: 6.23, KL: 16.75|DE LOSS: 306, PPL: 1073, TM: 301, LM: 5.28|BETA: 0.001574\n",
      "057[s], 07[s], Ep: 03, Ct: 00000|TR LOSS: 314, PPL: 1173|TM NLL: 305, KL: 3.15 | LM NLL: 6.19, KL: 17.27|DE LOSS: 306, PPL: 1068, TM: 301, LM: 5.20|BETA: 0.001600\n",
      "073[s], 07[s], Ep: 03, Ct: 00500|TR LOSS: 314, PPL: 1167|TM NLL: 305, KL: 3.16 | LM NLL: 6.14, KL: 17.93|DE LOSS: 306, PPL: 1069, TM: 301, LM: 5.20|BETA: 0.001635\n",
      "073[s], 07[s], Ep: 03, Ct: 01000|TR LOSS: 314, PPL: 1161|TM NLL: 304, KL: 3.17 | LM NLL: 6.09, KL: 18.59|DE LOSS: 306, PPL: 1063, TM: 301, LM: 5.11|BETA: 0.001670\n",
      "073[s], 07[s], Ep: 03, Ct: 01500|TR LOSS: 313, PPL: 1156|TM NLL: 304, KL: 3.18 | LM NLL: 6.04, KL: 19.26|DE LOSS: 305, PPL: 1058, TM: 300, LM: 4.96|BETA: 0.001704\n",
      "072[s], 07[s], Ep: 03, Ct: 02000|TR LOSS: 313, PPL: 1151|TM NLL: 304, KL: 3.19 | LM NLL: 5.99, KL: 19.91|DE LOSS: 305, PPL: 1062, TM: 300, LM: 4.86|BETA: 0.001739\n",
      "077[s], 08[s], Ep: 03, Ct: 02500|TR LOSS: 313, PPL: 1146|TM NLL: 304, KL: 3.20 | LM NLL: 5.94, KL: 20.56|DE LOSS: 305, PPL: 1052, TM: 300, LM: 4.76|BETA: 0.001774\n",
      "061[s], 07[s], Ep: 04, Ct: 00000|TR LOSS: 313, PPL: 1143|TM NLL: 304, KL: 3.21 | LM NLL: 5.90, KL: 21.05|DE LOSS: 305, PPL: 1053, TM: 300, LM: 4.77|BETA: 0.001800\n",
      "078[s], 07[s], Ep: 04, Ct: 00500|TR LOSS: 313, PPL: 1139|TM NLL: 304, KL: 3.22 | LM NLL: 5.85, KL: 21.66|DE LOSS: 305, PPL: 1046, TM: 300, LM: 4.74|BETA: 0.001835\n",
      "077[s], 07[s], Ep: 04, Ct: 01000|TR LOSS: 313, PPL: 1135|TM NLL: 303, KL: 3.24 | LM NLL: 5.81, KL: 22.27|DE LOSS: 305, PPL: 1045, TM: 300, LM: 4.64|BETA: 0.001870\n",
      "077[s], 07[s], Ep: 04, Ct: 01500|TR LOSS: 312, PPL: 1130|TM NLL: 303, KL: 3.25 | LM NLL: 5.76, KL: 22.87|DE LOSS: 304, PPL: 1038, TM: 300, LM: 4.55|BETA: 0.001904\n",
      "076[s], 07[s], Ep: 04, Ct: 02000|TR LOSS: 312, PPL: 1126|TM NLL: 303, KL: 3.26 | LM NLL: 5.72, KL: 23.45|DE LOSS: 304, PPL: 1032, TM: 300, LM: 4.46|BETA: 0.001939\n",
      "077[s], 07[s], Ep: 04, Ct: 02500|TR LOSS: 312, PPL: 1123|TM NLL: 303, KL: 3.27 | LM NLL: 5.67, KL: 24.00|DE LOSS: 304, PPL: 1040, TM: 300, LM: 4.40|BETA: 0.001974\n",
      "060[s], 07[s], Ep: 05, Ct: 00000|TR LOSS: 312, PPL: 1120|TM NLL: 303, KL: 3.28 | LM NLL: 5.64, KL: 24.42|DE LOSS: 304, PPL: 1031, TM: 299, LM: 4.38|BETA: 0.002001\n",
      "077[s], 07[s], Ep: 05, Ct: 00500|TR LOSS: 312, PPL: 1117|TM NLL: 303, KL: 3.29 | LM NLL: 5.60, KL: 24.95|DE LOSS: 304, PPL: 1029, TM: 299, LM: 4.33|BETA: 0.002035\n",
      "077[s], 07[s], Ep: 05, Ct: 01000|TR LOSS: 312, PPL: 1113|TM NLL: 303, KL: 3.30 | LM NLL: 5.56, KL: 25.46|DE LOSS: 304, PPL: 1029, TM: 299, LM: 4.42|BETA: 0.002070\n",
      "077[s], 07[s], Ep: 05, Ct: 01500|TR LOSS: 311, PPL: 1110|TM NLL: 303, KL: 3.31 | LM NLL: 5.52, KL: 25.96|DE LOSS: 303, PPL: 1020, TM: 299, LM: 4.25|BETA: 0.002105\n",
      "027[s], 27[s], Ep: 00, Ct: 00000|TR LOSS: 311, PPL: 1108|TM NLL: 302, KL: 3.31 | LM NLL: 5.49, KL: 26.32|DE LOSS: 303, PPL: 1024, TM: 299, LM: 4.27|BETA: 0.002131\n",
      "109[s], 39[s], Ep: 00, Ct: 00500|TR LOSS: 311, PPL: 1105|TM NLL: 302, KL: 3.32 | LM NLL: 5.45, KL: 26.78|DE LOSS: 303, PPL: 1021, TM: 299, LM: 4.21|BETA: 0.002165\n",
      "077[s], 07[s], Ep: 00, Ct: 01000|TR LOSS: 311, PPL: 1102|TM NLL: 302, KL: 3.33 | LM NLL: 5.42, KL: 27.23|DE LOSS: 303, PPL: 1022, TM: 299, LM: 4.37|BETA: 0.002200\n",
      "104[s], 34[s], Ep: 00, Ct: 01500|TR LOSS: 311, PPL: 1099|TM NLL: 302, KL: 3.34 | LM NLL: 5.38, KL: 27.66|DE LOSS: 303, PPL: 1020, TM: 299, LM: 4.12|BETA: 0.002235\n",
      "095[s], 26[s], Ep: 00, Ct: 02000|TR LOSS: 311, PPL: 1096|TM NLL: 302, KL: 3.35 | LM NLL: 5.35, KL: 28.08|DE LOSS: 303, PPL: 1021, TM: 299, LM: 4.08|BETA: 0.002270\n",
      "097[s], 27[s], Ep: 00, Ct: 02500|TR LOSS: 311, PPL: 1094|TM NLL: 302, KL: 3.36 | LM NLL: 5.31, KL: 28.46|DE LOSS: 303, PPL: 1014, TM: 299, LM: 4.07|BETA: 0.002304\n",
      "061[s], 07[s], Ep: 01, Ct: 00000|TR LOSS: 311, PPL: 1092|TM NLL: 302, KL: 3.36 | LM NLL: 5.29, KL: 28.76|DE LOSS: 303, PPL: 1018, TM: 299, LM: 4.05|BETA: 0.002331\n",
      "077[s], 07[s], Ep: 01, Ct: 00500|TR LOSS: 310, PPL: 1090|TM NLL: 302, KL: 3.37 | LM NLL: 5.26, KL: 29.12|DE LOSS: 303, PPL: 1009, TM: 299, LM: 4.06|BETA: 0.002366\n",
      "074[s], 07[s], Ep: 01, Ct: 01000|TR LOSS: 310, PPL: 1087|TM NLL: 302, KL: 3.38 | LM NLL: 5.23, KL: 29.48|DE LOSS: 303, PPL: 1011, TM: 299, LM: 4.00|BETA: 0.002400\n",
      "096[s], 26[s], Ep: 01, Ct: 01500|TR LOSS: 310, PPL: 1085|TM NLL: 302, KL: 3.39 | LM NLL: 5.20, KL: 29.82|DE LOSS: 303, PPL: 1010, TM: 299, LM: 4.04|BETA: 0.002435\n",
      "095[s], 26[s], Ep: 01, Ct: 02000|TR LOSS: 310, PPL: 1083|TM NLL: 301, KL: 3.39 | LM NLL: 5.17, KL: 30.16|DE LOSS: 303, PPL: 1008, TM: 299, LM: 3.96|BETA: 0.002470\n",
      "077[s], 07[s], Ep: 01, Ct: 02500|TR LOSS: 310, PPL: 1081|TM NLL: 301, KL: 3.40 | LM NLL: 5.14, KL: 30.47|DE LOSS: 303, PPL: 1013, TM: 299, LM: 3.94|BETA: 0.002504\n",
      "007[s], 07[s], Ep: 00, Ct: 00000|TR LOSS: 310, PPL: 1081|TM NLL: 301, KL: 3.40 | LM NLL: 5.14, KL: 30.47|DE LOSS: 321, PPL: 1091, TM: 309, LM: 11.84|BETA: 0.002505\n",
      "073[s], 07[s], Ep: 00, Ct: 00500|TR LOSS: 310, PPL: 1079|TM NLL: 301, KL: 3.41 | LM NLL: 5.16, KL: 30.22|DE LOSS: 305, PPL: 1014, TM: 299, LM: 5.94|BETA: 0.002540\n",
      "True: money manager legg mason inc. on tuesday named four people to its executive committee , saying that the moves will help align its executive team with its growth objectives\n",
      "Pred: the says to the of are the to the , , the $\n",
      "True: joseph sullivan , who became the baltimore company 's permanent ceo last month , said legg mason will focus on expanding investment products , working with its affiliate partners and strengthening its global distribution platforms\n",
      "Pred: the say says says says the state of of the <unk> of week\n",
      "True: the new executive committee members include chief financial officer pete <unk> and terry johnson , its head of global distribution\n",
      "Pred: the state state department is the says that the , the the the the , the # of the the of and\n",
      "True: the company also promoted jennifer murphy , who previously served as president and ceo of legg mason capital management , to the role of chief administrative officer\n",
      "Pred: the state 's are the the , the are the of the , the of the ,\n",
      "True: and it promoted thomas merchant , its corporate general counsel , to the role of general counsel\n",
      "Pred: the the is the the , the the the , <unk> says the state 's the #\n",
      "True: both murphy and merchant also were named to the executive committee\n",
      "Pred: the officials , the , will released to the state of of of and of\n",
      "True: thomas <unk> , the company 's general counsel and head of governance , and ronald dewhurst , former head of global investment managers , will both leave the company\n",
      "Pred: the says says the company of office has that the the the and the # , , is <unk> , the # and\n",
      "True: dewhurst had recently been one of two internal candidates to become the company 's ceo , legg mason said\n",
      "Pred: the is been the the of the and #\n",
      "True: legg mason shares added # cents to $ # in morning trading\n",
      "Pred: the says , the says the , the # million the ,\n",
      "True: fire crews say a wildfire is much smaller than they thought , but they are still working to contain the blaze in southwestern walla walla county near <unk>\n",
      "Pred: the say say the fire of still to the # and the the the have to\n",
      "True: fire spokesman tom reilly says the fire has burned nearly # square miles , or about # acres\n",
      "Pred: the ( ( <unk> says the <unk> 's been to the percent and , and the #\n",
      "True: crews had estimated the fire at more than twice that size on tuesday morning\n",
      "Pred: the says the the # of the # the to the was the\n",
      "True: the fire is about # percent contained\n",
      "Pred: the says is the to percent of the and and and and and ,\n",
      "True: reilly says crews hope to have it fully contained by thursday\n",
      "Pred: the 's the are to have the was the to the\n",
      "True: no structures have been threatened and no injuries have been reported\n",
      "Pred: the <unk> is been the to the state and the found in\n",
      "True: the fire started sunday afternoon\n",
      "Pred: the say the the , and and and and and and were , , and , and and and and\n",
      "True: the cause is under investigation , but a person is believed to have started it\n",
      "Pred: the say of the the # of the , 's the to the a the 's in and\n",
      "True: health officials say one new case of west nile virus has been reported in mississippi , bringing the state total to four so far this year\n",
      "Pred: the 's say the of the of the , <unk> , been released in the of and the state of the the years the\n",
      "True: the mississippi department of health said in a news release that the new case occurred in an adult in scott county\n",
      "Pred: the . says says the has the http , the of the company was the the the # <unk>\n",
      "True: this year 's other cases have been reported in leflore and coahoma counties\n",
      "Pred: the the old the <unk> says the been to the , , of\n",
      "True: one human case of lacrosse encephalitis has been reported in montgomery county\n",
      "Pred: the says the has the , of been sentenced to the\n",
      "True: that mosquito borne illness is similar to west nile virus\n",
      "Pred: the 's is is of the <unk> # the of , and and and and and and and the\n",
      "True: symptoms of west nile virus may include fever , headache , nausea , vomiting , a rash , muscle weakness or swollen <unk> <unk>\n",
      "Pred: the says the , <unk> , the the of but , the , , and and <unk>\n",
      "True: in a small number of cases , infection can result in encephalitis or meningitis , which can lead to paralysis , coma and possibly death\n",
      "Pred: in , year # of the , the , the , the , the , who was be the the\n",
      "True: the salvation army got a welcome surprise this week in topeka when volunteers found two gold coins one of the kettles\n",
      "Pred: the says , says <unk> to , , week to the and the and the years , , of the state of\n",
      "True: topeka kettle drive coordinator tim hall told the topeka capital journal ( http : //bit.ly/ # <unk> # ) that volunteers found the two gold coins when they were counting the collections thursday\n",
      "Pred: the also , , , <unk> , the company of , the is , <unk> #\n",
      "True: he says have no idea who made the donation\n",
      "Pred: he says the 's attorney of was the state of of\n",
      "True: the coins are dated # and are about the size of dimes\n",
      "Pred: the company is n't to percent # the\n",
      "True: they were wrapped in paper , which kept volunteers from accidentally mixing them with less valuable coins\n",
      "Pred: the say arrested in the , but was the and the and\n",
      "True: hall says the value of the coins is unclear , but an internet search found similar coins selling for about $ #\n",
      "Pred: the says the company is the <unk> 's the to the he <unk> ,\n",
      "True: the salvation army says collections from the kettle campaign this year have fallen far short of goals , so the extra boost from the gold coins helps\n",
      "Pred: the company is has the is the the says and week old # to to to the , and the state ,\n",
      "True: germany 's foreign minister has said during a rare visit to hamas ruled gaza that the israeli egyptian blockade of # million palestinians living there is unacceptable and must end\n",
      "Pred: the says office , , been the the # of of the the\n",
      "True: guido westerwelle also said after monday 's tour of a u.n. school and a german funded sewage treatment plant that the border closure is strengthening extremists at the expense of moderates\n",
      "Pred: the , , says the the to <unk> of the # of\n",
      "True: israel and egypt closed gaza 's borders after hamas seized the territory more than three years ago\n",
      "Pred: the says the of the of office 's the\n",
      "True: westerwelle is one of just a few senior western diplomats to have visited gaza since #\n",
      "Pred: the says the to the and and year\n",
      "True: israel eased the blockade over the summer , but still bans the import of crucial raw materials and virtually all exports\n",
      "Pred: the is the # of the # of the the the to company of the\n",
      "True: westerwelle did not meet with gaza 's hamas rulers\n",
      "Pred: the 's n't been the the\n",
      "True: construction is beginning on a $ # million child care center in aberdeen\n",
      "Pred: the 's the to the new # million of\n",
      "True: the # square foot ymca youth development center will provide child care services for children from infancy through the sixth grade\n",
      "Pred: the company of of , says <unk> is , the the <unk> the the the of the and the # of of\n",
      "True: aberdeen family ymca executive director steve graf tells the american news ( http : <unk> ) that it should be ready in about a year\n",
      "Pred: the says , the says the <unk> of the # 's , http : <unk> # <unk> the was the been to the # #\n",
      "True: the building will have a gymnasium , an indoor playground and in floor heating\n",
      "Pred: the department is be been new of the <unk> area of <unk> the and and\n",
      "True: it will be able to serve about # children\n",
      "Pred: the was be a to the the the percent\n",
      "True: officials have raised about $ # million so far through donations and grants\n",
      "Pred: the are the the # # of , , , the , the , ,\n",
      "True: the new director of the fbi is emphasizing the agency 's commitment to combat terrorism and fostering strong partnerships with local law enforcement\n",
      "Pred: the spokesman york ) <unk> state of the to state of # of the the and the and and\n",
      "True: director james comey stopped by the baltimore fbi field office monday on his tour of the # fbi field offices in the united states\n",
      "Pred: the says <unk> says the the <unk> of the , , the the <unk> of the # year of\n",
      "True: comey said the bureau 's top priority remains national security and counterterrorism , citing enduring threats from al qaida in iraq and syria\n",
      "Pred: the company he company , office <unk> , in , , in and the the , , the the , the and other\n",
      "True: but comey also stressed the bureau 's dedication to supporting city and state law enforcement agencies as they tackle violent crime gang activity and public corruption\n",
      "Pred: the the of are the the of office ,\n",
      "True: comey is the seventh director of the federal bureau of investigation\n",
      "Pred: the is the to of , the state 's of the\n",
      "True: he succeeded robert mueller , who served as fbi director for # years\n",
      "Pred: he said the the , the was the the and\n",
      "True: springfield mayor domenic sarno says the property tax rate he is proposing would lower tax bills for about # percent of single family homeowners in the next fiscal year\n",
      "Pred: the 's , <unk> says the company of <unk> of is n't to the and\n",
      "True: sarno on friday recommended a residential tax rate of $ # per $ # valuation , and a rate of $ # for commercial and industrial properties\n",
      "Pred: the 's the , <unk> # of , , the , , # # million\n",
      "True: the proposal is expected to be considered by the city council on monday\n",
      "Pred: the 's , expected to be the the the state 's , the\n",
      "True: the mayor says preliminary data shows that under his plan , the average tax bill for a single family homeowner would fall by $ # to $ # , the lowest rate since #\n",
      "Pred: the state of the says says the the the office of the company of the to the <unk> #\n",
      "True: he says most business property owners would also see a lower bill and that the tax rate strikes a balance between helping homeowners and promoting a positive business climate for the largest city in western massachusetts\n",
      "Pred: he was been and the the of be be the <unk>\n",
      "True: an online auction of museum quality art works will benefit a maine museum 's $ # million endowment campaign to sustain the artistic legacy of the late andrew wyeth\n",
      "Pred: the new # , says of , and says be the new of of <unk> to million of in\n",
      "True: the farnsworth art museum in rockland says the auction begins monday and continues through nov. #\n",
      "Pred: the company is is the of the the 's , to the the to the # and ,\n",
      "True: adelson galleries in new york city is hosting the auction on its <unk> website , with works donated by artists , collectors and philanthropists who are interested in sustaining the museum 's mission\n",
      "Pred: the says says the york 's ) the the of of the state of of but the of and the and and and the and the a to the\n",
      "True: proceeds from the auction will support of operation of the farnsworth facilities devoted to the maine related work of andrew wyeth , as well as that of his father , n.c. , and son jamie\n",
      "Pred: the of the state of be the of , , # of of\n",
      "True: wyeth , whose work focused on coastal maine and pennsylvania 's brandywine valley , died in january # at the age of #\n",
      "Pred: the says says officials says of the says the the of # ,\n",
      "True: authorities in fargo are working to identify the body that was pulled from the red river in downtown fargo thursday\n",
      "Pred: in in the county a in the in , in the sentenced in the scene , of the\n",
      "True: three fishermen called # after spotting the body near the first avenue bridge\n",
      "Pred: the year , the , the # state of the , year of the\n",
      "True: personnel from the fargo fire department , as well as emergency and rescue crews were dispatched to the scene\n",
      "Pred: the , the state 's , of the says the the , the , is not\n",
      "True: lt. joel <unk> says an autopsy will be conducted to determine the person 's cause of death\n",
      "Pred: the 's , says the says and be a to the the state of of\n",
      "0 : school officers reports department http died student told reported high\n",
      "1 : department company million health workers federal plant government people agency\n",
      "2 : court federal u.s. judge attorney law lawsuit office district case\n",
      "3 : park national http center years event museum reports day open\n",
      "4 : fire water area power service miles crews river tuesday weather\n",
      "5 : charged charges guilty court pleaded years prosecutors arrested murder prison\n",
      "6 : fire authorities found hospital home car died woman crash dead\n",
      "7 : percent million company billion fell cents average share rate revenue\n",
      "8 : bill senate house republican gov vote budget election tax democratic\n",
      "9 : school university students president board schools program college education district\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-29f6607b93a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_recon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_kl_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_recon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_kl_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppls_batch\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_recon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_kl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_recon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_kl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_ppls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msent_loss_kl_batch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if len(logs) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "while epoch < config.epochs\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = get_feed_dict(batch)\n",
    "        if config.warmup > 0 and beta_eval < 1.0: sess.run(update_beta)\n",
    "\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch, ppls_batch = \\\n",
    "        sess.run([opt, loss, topic_loss_recon, topic_loss_kl, sent_loss_recon, sent_loss_kl, topic_ppls], feed_dict = feed_dict)\n",
    "        \n",
    "        if sent_loss_kl_batch == np.inf:\n",
    "            pdb.set_trace()\n",
    "\n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if ct%config.log_period==0:\n",
    "            time_dev = time.time()\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, sent_loss_recon_train, sent_loss_kl_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_dev, sent_loss_dev, ppl_dev = get_loss(sess, dev_batches)\n",
    "            \n",
    "            if loss_dev < loss_min:\n",
    "                loss_min = loss_dev\n",
    "                saver.save(sess, config.modelpath, global_step=epoch*10000+ct)\n",
    "\n",
    "            if config.warmup > 0: beta_eval = beta.eval(session=sess)\n",
    "\n",
    "            clear_output()\n",
    "            time_finish = time.time()\n",
    "            time_log = int(time_finish - time_start)\n",
    "            time_log_dev = int(time_finish - time_dev)\n",
    "            logs += [(time_log, time_log_dev, epoch, ct, loss_train, ppl_train, topic_loss_recon_train, topic_loss_kl_train, sent_loss_recon_train, sent_loss_kl_train, loss_dev, ppl_dev, topic_loss_dev, sent_loss_dev, beta_eval)]\n",
    "            for log in logs:\n",
    "                print('%03d[s], %02d[s], Ep: %02d, Ct: %05d|TR LOSS: %.0f, PPL: %.0f|TM NLL: %.0f, KL: %.2f | LM NLL: %.2f, KL: %.2f|DE LOSS: %.0f, PPL: %.0f, TM: %.0f, LM: %.2f|BETA: %.6f' %  log)\n",
    "\n",
    "            print_sample(batch)\n",
    "\n",
    "            time_start = time.time()\n",
    "            \n",
    "            # visualize topic\n",
    "            topics_freq_bow_idxs = bow_idxs[sess.run(topics_freq_bow_indices)]\n",
    "            for topic, topic_freq_bow_idxs in enumerate(topics_freq_bow_idxs):\n",
    "                print(topic, ':', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "                \n",
    "    epoch += 1\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2883"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([i for i, batch in ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-24bb357ae5d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while next(train_iter):\n",
    "    i += 1\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2884"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "007[s], 06[s], Ep: 00, Ct: 00000|TR LOSS: 398, PPL: 2663|TM NLL: 387, KL: 0.75 | LM NLL: 10.34, KL: 0.60|DE LOSS: 347, PPL: 2656, TM: 337, LM: 10.34|BETA: 0.001000\n",
      "065[s], 06[s], Ep: 00, Ct: 00500|TR LOSS: 326, PPL: 1675|TM NLL: 318, KL: 0.60 | LM NLL: 7.41, KL: 19.54|DE LOSS: 319, PPL: 1471, TM: 312, LM: 6.83|BETA: 0.001035\n",
      "065[s], 06[s], Ep: 00, Ct: 01000|TR LOSS: 324, PPL: 1530|TM NLL: 315, KL: 1.43 | LM NLL: 7.12, KL: 16.75|DE LOSS: 316, PPL: 1349, TM: 309, LM: 6.76|BETA: 0.001069\n",
      "066[s], 06[s], Ep: 00, Ct: 01500|TR LOSS: 323, PPL: 1468|TM NLL: 314, KL: 1.79 | LM NLL: 7.01, KL: 14.90|DE LOSS: 315, PPL: 1321, TM: 309, LM: 6.67|BETA: 0.001104\n",
      "066[s], 06[s], Ep: 00, Ct: 02000|TR LOSS: 322, PPL: 1421|TM NLL: 313, KL: 2.04 | LM NLL: 6.93, KL: 13.92|DE LOSS: 313, PPL: 1252, TM: 307, LM: 6.58|BETA: 0.001139\n",
      "066[s], 06[s], Ep: 00, Ct: 02500|TR LOSS: inf, PPL: 1387|TM NLL: 312, KL: 2.29 | LM NLL: 6.89, KL: inf|DE LOSS: 313, PPL: 1246, TM: 306, LM: 6.66|BETA: 0.001173\n",
      "052[s], 06[s], Ep: 01, Ct: 00000|TR LOSS: inf, PPL: 1361|TM NLL: 311, KL: 2.42 | LM NLL: 6.86, KL: inf|DE LOSS: 312, PPL: 1203, TM: 305, LM: 6.65|BETA: 0.001200\n",
      "066[s], 06[s], Ep: 01, Ct: 00500|TR LOSS: inf, PPL: 1333|TM NLL: 310, KL: 2.56 | LM NLL: 6.84, KL: inf|DE LOSS: 311, PPL: 1159, TM: 304, LM: 6.60|BETA: 0.001235\n",
      "066[s], 06[s], Ep: 01, Ct: 01000|TR LOSS: inf, PPL: 1308|TM NLL: 309, KL: 2.68 | LM NLL: 6.81, KL: inf|DE LOSS: 310, PPL: 1141, TM: 303, LM: 6.57|BETA: 0.001269\n",
      "066[s], 06[s], Ep: 01, Ct: 01500|TR LOSS: inf, PPL: 1288|TM NLL: 309, KL: 2.76 | LM NLL: 6.79, KL: inf|DE LOSS: 309, PPL: 1121, TM: 302, LM: 6.51|BETA: 0.001304\n",
      "066[s], 06[s], Ep: 01, Ct: 02000|TR LOSS: inf, PPL: 1269|TM NLL: 308, KL: 2.82 | LM NLL: 6.76, KL: inf|DE LOSS: 308, PPL: 1104, TM: 302, LM: 6.46|BETA: 0.001339\n",
      "066[s], 06[s], Ep: 01, Ct: 02500|TR LOSS: inf, PPL: 1251|TM NLL: 307, KL: 2.90 | LM NLL: 6.74, KL: inf|DE LOSS: 308, PPL: 1090, TM: 301, LM: 6.42|BETA: 0.001374\n",
      "052[s], 06[s], Ep: 02, Ct: 00000|TR LOSS: inf, PPL: 1239|TM NLL: 307, KL: 2.93 | LM NLL: 6.72, KL: inf|DE LOSS: 308, PPL: 1099, TM: 301, LM: 6.37|BETA: 0.001400\n",
      "066[s], 06[s], Ep: 02, Ct: 00500|TR LOSS: inf, PPL: 1226|TM NLL: 307, KL: 2.97 | LM NLL: 6.69, KL: inf|DE LOSS: 307, PPL: 1071, TM: 301, LM: 6.34|BETA: 0.001435\n",
      "066[s], 06[s], Ep: 02, Ct: 01000|TR LOSS: inf, PPL: 1214|TM NLL: 306, KL: 3.00 | LM NLL: 6.67, KL: inf|DE LOSS: 307, PPL: 1077, TM: 301, LM: 6.29|BETA: 0.001470\n",
      "066[s], 06[s], Ep: 02, Ct: 01500|TR LOSS: inf, PPL: 1205|TM NLL: 306, KL: 3.03 | LM NLL: 6.65, KL: inf|DE LOSS: 307, PPL: 1070, TM: 300, LM: 6.24|BETA: 0.001504\n",
      "066[s], 06[s], Ep: 02, Ct: 02000|TR LOSS: inf, PPL: 1196|TM NLL: 306, KL: 3.05 | LM NLL: 6.62, KL: inf|DE LOSS: 307, PPL: 1063, TM: 300, LM: 6.20|BETA: 0.001539\n",
      "066[s], 06[s], Ep: 02, Ct: 02500|TR LOSS: inf, PPL: 1186|TM NLL: 305, KL: 3.08 | LM NLL: 6.60, KL: inf|DE LOSS: 306, PPL: 1064, TM: 300, LM: 6.16|BETA: 0.001574\n",
      "052[s], 06[s], Ep: 03, Ct: 00000|TR LOSS: inf, PPL: 1180|TM NLL: 305, KL: 3.09 | LM NLL: 6.58, KL: inf|DE LOSS: 306, PPL: 1062, TM: 300, LM: 6.14|BETA: 0.001600\n",
      "066[s], 06[s], Ep: 03, Ct: 00500|TR LOSS: inf, PPL: 1173|TM NLL: 305, KL: 3.11 | LM NLL: 6.56, KL: inf|DE LOSS: 306, PPL: 1051, TM: 300, LM: 6.11|BETA: 0.001635\n",
      "066[s], 06[s], Ep: 03, Ct: 01000|TR LOSS: inf, PPL: 1166|TM NLL: 304, KL: 3.13 | LM NLL: 6.54, KL: inf|DE LOSS: 306, PPL: 1056, TM: 300, LM: 6.10|BETA: 0.001670\n",
      "066[s], 06[s], Ep: 03, Ct: 01500|TR LOSS: inf, PPL: 1161|TM NLL: 304, KL: 3.15 | LM NLL: 6.52, KL: inf|DE LOSS: 306, PPL: 1044, TM: 300, LM: 6.03|BETA: 0.001704\n",
      "066[s], 06[s], Ep: 03, Ct: 02000|TR LOSS: inf, PPL: 1155|TM NLL: 304, KL: 3.17 | LM NLL: 6.50, KL: inf|DE LOSS: 306, PPL: 1050, TM: 300, LM: 6.00|BETA: 0.001739\n",
      "066[s], 06[s], Ep: 03, Ct: 02500|TR LOSS: inf, PPL: 1149|TM NLL: 304, KL: 3.19 | LM NLL: 6.48, KL: inf|DE LOSS: 306, PPL: 1045, TM: 300, LM: 5.97|BETA: 0.001774\n",
      "052[s], 06[s], Ep: 04, Ct: 00000|TR LOSS: inf, PPL: 1145|TM NLL: 304, KL: 3.20 | LM NLL: 6.46, KL: inf|DE LOSS: 306, PPL: 1044, TM: 300, LM: 5.95|BETA: 0.001800\n",
      "067[s], 06[s], Ep: 04, Ct: 00500|TR LOSS: inf, PPL: 1141|TM NLL: 304, KL: 3.21 | LM NLL: 6.44, KL: inf|DE LOSS: 306, PPL: 1041, TM: 300, LM: 5.92|BETA: 0.001835\n",
      "066[s], 06[s], Ep: 04, Ct: 01000|TR LOSS: inf, PPL: 1136|TM NLL: 303, KL: 3.23 | LM NLL: 6.42, KL: inf|DE LOSS: 305, PPL: 1043, TM: 300, LM: 5.90|BETA: 0.001870\n",
      "068[s], 06[s], Ep: 04, Ct: 01500|TR LOSS: inf, PPL: 1133|TM NLL: 303, KL: 3.24 | LM NLL: 6.41, KL: inf|DE LOSS: 305, PPL: 1038, TM: 299, LM: 5.88|BETA: 0.001904\n",
      "070[s], 07[s], Ep: 04, Ct: 02000|TR LOSS: inf, PPL: 1129|TM NLL: 303, KL: 3.25 | LM NLL: 6.39, KL: inf|DE LOSS: 305, PPL: 1037, TM: 299, LM: 5.85|BETA: 0.001939\n",
      "070[s], 07[s], Ep: 04, Ct: 02500|TR LOSS: inf, PPL: 1125|TM NLL: 303, KL: 3.27 | LM NLL: 6.37, KL: inf|DE LOSS: 305, PPL: 1036, TM: 299, LM: 5.82|BETA: 0.001974\n",
      "055[s], 07[s], Ep: 05, Ct: 00000|TR LOSS: inf, PPL: 1122|TM NLL: 303, KL: 3.28 | LM NLL: 6.36, KL: inf|DE LOSS: 305, PPL: 1038, TM: 300, LM: 5.81|BETA: 0.002001\n",
      "072[s], 07[s], Ep: 05, Ct: 00500|TR LOSS: inf, PPL: 1119|TM NLL: 303, KL: 3.29 | LM NLL: 6.34, KL: inf|DE LOSS: 305, PPL: 1032, TM: 299, LM: 5.81|BETA: 0.002035\n",
      "071[s], 07[s], Ep: 05, Ct: 01000|TR LOSS: inf, PPL: 1115|TM NLL: 303, KL: 3.30 | LM NLL: 6.32, KL: inf|DE LOSS: 305, PPL: 1033, TM: 299, LM: 5.79|BETA: 0.002070\n",
      "072[s], 07[s], Ep: 05, Ct: 01500|TR LOSS: inf, PPL: 1113|TM NLL: 303, KL: 3.31 | LM NLL: 6.31, KL: inf|DE LOSS: 305, PPL: 1031, TM: 299, LM: 5.74|BETA: 0.002105\n",
      "071[s], 06[s], Ep: 05, Ct: 02000|TR LOSS: inf, PPL: 1110|TM NLL: 302, KL: 3.32 | LM NLL: 6.29, KL: inf|DE LOSS: 305, PPL: 1026, TM: 299, LM: 5.73|BETA: 0.002139\n",
      "072[s], 07[s], Ep: 05, Ct: 02500|TR LOSS: inf, PPL: 1107|TM NLL: 302, KL: 3.33 | LM NLL: 6.28, KL: inf|DE LOSS: 305, PPL: 1031, TM: 299, LM: 5.71|BETA: 0.002174\n",
      "057[s], 07[s], Ep: 06, Ct: 00000|TR LOSS: inf, PPL: 1105|TM NLL: 302, KL: 3.34 | LM NLL: 6.27, KL: inf|DE LOSS: 305, PPL: 1032, TM: 299, LM: 5.69|BETA: 0.002201\n",
      "076[s], 07[s], Ep: 06, Ct: 00500|TR LOSS: inf, PPL: 1102|TM NLL: 302, KL: 3.35 | LM NLL: 6.25, KL: inf|DE LOSS: 305, PPL: 1023, TM: 299, LM: 5.67|BETA: 0.002235\n",
      "077[s], 07[s], Ep: 06, Ct: 01000|TR LOSS: inf, PPL: 1100|TM NLL: 302, KL: 3.36 | LM NLL: 6.24, KL: inf|DE LOSS: 305, PPL: 1026, TM: 299, LM: 5.66|BETA: 0.002270\n",
      "077[s], 07[s], Ep: 06, Ct: 01500|TR LOSS: inf, PPL: 1098|TM NLL: 302, KL: 3.37 | LM NLL: 6.22, KL: inf|DE LOSS: 305, PPL: 1020, TM: 299, LM: 5.64|BETA: 0.002305\n",
      "077[s], 07[s], Ep: 06, Ct: 02000|TR LOSS: inf, PPL: 1096|TM NLL: 302, KL: 3.38 | LM NLL: 6.21, KL: inf|DE LOSS: 305, PPL: 1018, TM: 299, LM: 5.62|BETA: 0.002339\n",
      "076[s], 08[s], Ep: 06, Ct: 02500|TR LOSS: inf, PPL: 1093|TM NLL: 302, KL: 3.38 | LM NLL: 6.20, KL: inf|DE LOSS: 304, PPL: 1021, TM: 299, LM: 5.61|BETA: 0.002374\n",
      "061[s], 07[s], Ep: 07, Ct: 00000|TR LOSS: inf, PPL: 1091|TM NLL: 302, KL: 3.39 | LM NLL: 6.19, KL: inf|DE LOSS: 305, PPL: 1024, TM: 299, LM: 5.60|BETA: 0.002401\n",
      "077[s], 07[s], Ep: 07, Ct: 00500|TR LOSS: inf, PPL: 1089|TM NLL: 302, KL: 3.40 | LM NLL: 6.17, KL: inf|DE LOSS: 305, PPL: 1019, TM: 299, LM: 5.59|BETA: 0.002435\n",
      "077[s], 07[s], Ep: 07, Ct: 01000|TR LOSS: inf, PPL: 1087|TM NLL: 302, KL: 3.41 | LM NLL: 6.16, KL: inf|DE LOSS: 304, PPL: 1017, TM: 299, LM: 5.57|BETA: 0.002470\n",
      "077[s], 07[s], Ep: 07, Ct: 01500|TR LOSS: inf, PPL: 1086|TM NLL: 302, KL: 3.42 | LM NLL: 6.15, KL: inf|DE LOSS: 304, PPL: 1014, TM: 299, LM: 5.56|BETA: 0.002505\n",
      "077[s], 07[s], Ep: 07, Ct: 02000|TR LOSS: inf, PPL: 1084|TM NLL: 301, KL: 3.42 | LM NLL: 6.14, KL: inf|DE LOSS: 304, PPL: 1017, TM: 299, LM: 5.54|BETA: 0.002539\n",
      "077[s], 07[s], Ep: 07, Ct: 02500|TR LOSS: inf, PPL: 1082|TM NLL: 301, KL: 3.43 | LM NLL: 6.12, KL: inf|DE LOSS: 304, PPL: 1017, TM: 299, LM: 5.54|BETA: 0.002574\n",
      "060[s], 07[s], Ep: 08, Ct: 00000|TR LOSS: inf, PPL: 1081|TM NLL: 301, KL: 3.44 | LM NLL: 6.12, KL: inf|DE LOSS: 304, PPL: 1021, TM: 299, LM: 5.53|BETA: 0.002601\n",
      "077[s], 07[s], Ep: 08, Ct: 00500|TR LOSS: inf, PPL: 1079|TM NLL: 301, KL: 3.44 | LM NLL: 6.10, KL: inf|DE LOSS: 304, PPL: 1011, TM: 299, LM: 5.51|BETA: 0.002636\n",
      "077[s], 07[s], Ep: 08, Ct: 01000|TR LOSS: inf, PPL: 1077|TM NLL: 301, KL: 3.45 | LM NLL: 6.09, KL: inf|DE LOSS: 304, PPL: 1015, TM: 299, LM: 5.51|BETA: 0.002670\n",
      "077[s], 07[s], Ep: 08, Ct: 01500|TR LOSS: inf, PPL: 1076|TM NLL: 301, KL: 3.46 | LM NLL: 6.08, KL: inf|DE LOSS: 304, PPL: 1015, TM: 299, LM: 5.49|BETA: 0.002705\n",
      "077[s], 07[s], Ep: 08, Ct: 02000|TR LOSS: inf, PPL: 1075|TM NLL: 301, KL: 3.46 | LM NLL: 6.07, KL: inf|DE LOSS: 304, PPL: 1015, TM: 299, LM: 5.48|BETA: 0.002740\n",
      "077[s], 07[s], Ep: 08, Ct: 02500|TR LOSS: inf, PPL: 1073|TM NLL: 301, KL: 3.47 | LM NLL: 6.06, KL: inf|DE LOSS: 304, PPL: 1014, TM: 299, LM: 5.47|BETA: 0.002774\n",
      "061[s], 07[s], Ep: 09, Ct: 00000|TR LOSS: inf, PPL: 1072|TM NLL: 301, KL: 3.47 | LM NLL: 6.05, KL: inf|DE LOSS: 304, PPL: 1013, TM: 299, LM: 5.46|BETA: 0.002801\n",
      "077[s], 07[s], Ep: 09, Ct: 00500|TR LOSS: inf, PPL: 1070|TM NLL: 301, KL: 3.48 | LM NLL: 6.04, KL: inf|DE LOSS: 304, PPL: 1013, TM: 299, LM: 5.45|BETA: 0.002836\n",
      "077[s], 07[s], Ep: 09, Ct: 01000|TR LOSS: inf, PPL: 1069|TM NLL: 301, KL: 3.49 | LM NLL: 6.03, KL: inf|DE LOSS: 304, PPL: 1011, TM: 299, LM: 5.44|BETA: 0.002870\n",
      "077[s], 07[s], Ep: 09, Ct: 01500|TR LOSS: inf, PPL: 1068|TM NLL: 301, KL: 3.49 | LM NLL: 6.02, KL: inf|DE LOSS: 304, PPL: 1012, TM: 299, LM: 5.43|BETA: 0.002905\n",
      "077[s], 07[s], Ep: 09, Ct: 02000|TR LOSS: inf, PPL: 1067|TM NLL: 301, KL: 3.50 | LM NLL: 6.01, KL: inf|DE LOSS: 304, PPL: 1011, TM: 299, LM: 5.42|BETA: 0.002940\n",
      "076[s], 07[s], Ep: 09, Ct: 02500|TR LOSS: inf, PPL: 1065|TM NLL: 301, KL: 3.50 | LM NLL: 6.00, KL: inf|DE LOSS: 304, PPL: 1009, TM: 299, LM: 5.42|BETA: 0.002974\n",
      "061[s], 07[s], Ep: 10, Ct: 00000|TR LOSS: inf, PPL: 1064|TM NLL: 301, KL: 3.51 | LM NLL: 6.00, KL: inf|DE LOSS: 304, PPL: 1009, TM: 299, LM: 5.41|BETA: 0.003001\n",
      "077[s], 07[s], Ep: 10, Ct: 00500|TR LOSS: inf, PPL: 1063|TM NLL: 301, KL: 3.51 | LM NLL: 5.99, KL: inf|DE LOSS: 304, PPL: 1012, TM: 299, LM: 5.40|BETA: 0.003036\n",
      "077[s], 07[s], Ep: 10, Ct: 01000|TR LOSS: inf, PPL: 1062|TM NLL: 301, KL: 3.52 | LM NLL: 5.98, KL: inf|DE LOSS: 304, PPL: 1008, TM: 299, LM: 5.39|BETA: 0.003070\n",
      "077[s], 07[s], Ep: 10, Ct: 01500|TR LOSS: inf, PPL: 1061|TM NLL: 301, KL: 3.52 | LM NLL: 5.97, KL: inf|DE LOSS: 304, PPL: 1010, TM: 299, LM: 5.38|BETA: 0.003105\n",
      "077[s], 07[s], Ep: 10, Ct: 02000|TR LOSS: inf, PPL: 1060|TM NLL: 301, KL: 3.53 | LM NLL: 5.96, KL: inf|DE LOSS: 304, PPL: 1006, TM: 298, LM: 5.37|BETA: 0.003140\n",
      "077[s], 07[s], Ep: 10, Ct: 02500|TR LOSS: inf, PPL: 1059|TM NLL: 300, KL: 3.53 | LM NLL: 5.95, KL: inf|DE LOSS: 304, PPL: 1011, TM: 299, LM: 5.37|BETA: 0.003174\n",
      "061[s], 08[s], Ep: 11, Ct: 00000|TR LOSS: inf, PPL: 1058|TM NLL: 300, KL: 3.54 | LM NLL: 5.94, KL: inf|DE LOSS: 304, PPL: 1010, TM: 299, LM: 5.37|BETA: 0.003201\n",
      "077[s], 07[s], Ep: 11, Ct: 00500|TR LOSS: inf, PPL: 1057|TM NLL: 300, KL: 3.54 | LM NLL: 5.94, KL: inf|DE LOSS: 304, PPL: 1004, TM: 299, LM: 5.36|BETA: 0.003236\n",
      "077[s], 07[s], Ep: 11, Ct: 01000|TR LOSS: inf, PPL: 1056|TM NLL: 300, KL: 3.54 | LM NLL: 5.93, KL: inf|DE LOSS: 304, PPL: 1007, TM: 299, LM: 5.35|BETA: 0.003271\n",
      "077[s], 07[s], Ep: 11, Ct: 01500|TR LOSS: inf, PPL: 1056|TM NLL: 300, KL: 3.55 | LM NLL: 5.92, KL: inf|DE LOSS: 304, PPL: 1005, TM: 298, LM: 5.34|BETA: 0.003305\n",
      "077[s], 07[s], Ep: 11, Ct: 02000|TR LOSS: inf, PPL: 1055|TM NLL: 300, KL: 3.55 | LM NLL: 5.91, KL: inf|DE LOSS: 304, PPL: 1008, TM: 299, LM: 5.33|BETA: 0.003340\n",
      "077[s], 07[s], Ep: 11, Ct: 02500|TR LOSS: inf, PPL: 1054|TM NLL: 300, KL: 3.56 | LM NLL: 5.90, KL: inf|DE LOSS: 304, PPL: 1006, TM: 298, LM: 5.33|BETA: 0.003375\n",
      "060[s], 07[s], Ep: 12, Ct: 00000|TR LOSS: inf, PPL: 1053|TM NLL: 300, KL: 3.56 | LM NLL: 5.90, KL: inf|DE LOSS: 304, PPL: 1011, TM: 299, LM: 5.33|BETA: 0.003401\n",
      "077[s], 07[s], Ep: 12, Ct: 00500|TR LOSS: inf, PPL: 1052|TM NLL: 300, KL: 3.56 | LM NLL: 5.89, KL: inf|DE LOSS: 304, PPL: 1006, TM: 298, LM: 5.31|BETA: 0.003436\n",
      "077[s], 07[s], Ep: 12, Ct: 01000|TR LOSS: inf, PPL: 1051|TM NLL: 300, KL: 3.57 | LM NLL: 5.88, KL: inf|DE LOSS: 304, PPL: 1004, TM: 298, LM: 5.31|BETA: 0.003471\n",
      "077[s], 07[s], Ep: 12, Ct: 01500|TR LOSS: inf, PPL: 1050|TM NLL: 300, KL: 3.57 | LM NLL: 5.87, KL: inf|DE LOSS: 304, PPL: 1008, TM: 299, LM: 5.31|BETA: 0.003505\n",
      "077[s], 07[s], Ep: 12, Ct: 02000|TR LOSS: inf, PPL: 1050|TM NLL: 300, KL: 3.57 | LM NLL: 5.87, KL: inf|DE LOSS: 304, PPL: 1007, TM: 298, LM: 5.30|BETA: 0.003540\n",
      "077[s], 07[s], Ep: 12, Ct: 02500|TR LOSS: inf, PPL: 1049|TM NLL: 300, KL: 3.58 | LM NLL: 5.86, KL: inf|DE LOSS: 304, PPL: 1007, TM: 298, LM: 5.29|BETA: 0.003575\n",
      "061[s], 07[s], Ep: 13, Ct: 00000|TR LOSS: inf, PPL: 1048|TM NLL: 300, KL: 3.58 | LM NLL: 5.85, KL: inf|DE LOSS: 304, PPL: 1005, TM: 298, LM: 5.29|BETA: 0.003601\n",
      "077[s], 07[s], Ep: 13, Ct: 00500|TR LOSS: inf, PPL: 1047|TM NLL: 300, KL: 3.58 | LM NLL: 5.85, KL: inf|DE LOSS: 304, PPL: 1004, TM: 298, LM: 5.29|BETA: 0.003636\n",
      "077[s], 07[s], Ep: 13, Ct: 01000|TR LOSS: inf, PPL: 1047|TM NLL: 300, KL: 3.59 | LM NLL: 5.84, KL: inf|DE LOSS: 304, PPL: 1004, TM: 298, LM: 5.28|BETA: 0.003671\n",
      "077[s], 07[s], Ep: 13, Ct: 01500|TR LOSS: inf, PPL: 1046|TM NLL: 300, KL: 3.59 | LM NLL: 5.83, KL: inf|DE LOSS: 304, PPL: 1008, TM: 298, LM: 5.27|BETA: 0.003705\n",
      "077[s], 07[s], Ep: 13, Ct: 02000|TR LOSS: inf, PPL: 1045|TM NLL: 300, KL: 3.59 | LM NLL: 5.83, KL: inf|DE LOSS: 304, PPL: 1007, TM: 298, LM: 5.26|BETA: 0.003740\n",
      "077[s], 08[s], Ep: 13, Ct: 02500|TR LOSS: inf, PPL: 1044|TM NLL: 300, KL: 3.60 | LM NLL: 5.82, KL: inf|DE LOSS: 304, PPL: 1006, TM: 298, LM: 5.26|BETA: 0.003775\n",
      "061[s], 08[s], Ep: 14, Ct: 00000|TR LOSS: inf, PPL: 1044|TM NLL: 300, KL: 3.60 | LM NLL: 5.81, KL: inf|DE LOSS: 304, PPL: 1008, TM: 299, LM: 5.25|BETA: 0.003801\n",
      "077[s], 07[s], Ep: 14, Ct: 00500|TR LOSS: inf, PPL: 1043|TM NLL: 300, KL: 3.60 | LM NLL: 5.81, KL: inf|DE LOSS: 304, PPL: 1002, TM: 298, LM: 5.24|BETA: 0.003836\n",
      "077[s], 07[s], Ep: 14, Ct: 01000|TR LOSS: inf, PPL: 1043|TM NLL: 300, KL: 3.60 | LM NLL: 5.80, KL: inf|DE LOSS: 304, PPL: 1003, TM: 298, LM: 5.24|BETA: 0.003871\n",
      "077[s], 07[s], Ep: 14, Ct: 01500|TR LOSS: inf, PPL: 1042|TM NLL: 300, KL: 3.61 | LM NLL: 5.80, KL: inf|DE LOSS: 304, PPL: 1003, TM: 298, LM: 5.24|BETA: 0.003906\n",
      "077[s], 07[s], Ep: 14, Ct: 02000|TR LOSS: inf, PPL: 1042|TM NLL: 300, KL: 3.61 | LM NLL: 5.79, KL: inf|DE LOSS: 304, PPL: 1001, TM: 298, LM: 5.23|BETA: 0.003940\n",
      "077[s], 08[s], Ep: 14, Ct: 02500|TR LOSS: inf, PPL: 1041|TM NLL: 300, KL: 3.61 | LM NLL: 5.78, KL: inf|DE LOSS: 304, PPL: 1007, TM: 299, LM: 5.23|BETA: 0.003975\n",
      "061[s], 07[s], Ep: 15, Ct: 00000|TR LOSS: inf, PPL: 1040|TM NLL: 300, KL: 3.61 | LM NLL: 5.78, KL: inf|DE LOSS: 304, PPL: 1009, TM: 299, LM: 5.23|BETA: 0.004002\n",
      "077[s], 07[s], Ep: 15, Ct: 00500|TR LOSS: inf, PPL: 1040|TM NLL: 300, KL: 3.62 | LM NLL: 5.77, KL: inf|DE LOSS: 304, PPL: 1003, TM: 298, LM: 5.22|BETA: 0.004036\n",
      "077[s], 07[s], Ep: 15, Ct: 01000|TR LOSS: inf, PPL: 1039|TM NLL: 300, KL: 3.62 | LM NLL: 5.77, KL: inf|DE LOSS: 304, PPL: 1005, TM: 298, LM: 5.22|BETA: 0.004071\n",
      "077[s], 08[s], Ep: 15, Ct: 01500|TR LOSS: inf, PPL: 1039|TM NLL: 300, KL: 3.62 | LM NLL: 5.76, KL: inf|DE LOSS: 304, PPL: 1002, TM: 298, LM: 5.20|BETA: 0.004106\n",
      "077[s], 07[s], Ep: 15, Ct: 02000|TR LOSS: inf, PPL: 1038|TM NLL: 300, KL: 3.62 | LM NLL: 5.75, KL: inf|DE LOSS: 304, PPL: 1004, TM: 298, LM: 5.21|BETA: 0.004140\n",
      "075[s], 07[s], Ep: 15, Ct: 02500|TR LOSS: inf, PPL: 1037|TM NLL: 300, KL: 3.63 | LM NLL: 5.75, KL: inf|DE LOSS: 304, PPL: 1005, TM: 298, LM: 5.20|BETA: 0.004175\n",
      "056[s], 07[s], Ep: 16, Ct: 00000|TR LOSS: inf, PPL: 1037|TM NLL: 300, KL: 3.63 | LM NLL: 5.74, KL: inf|DE LOSS: 304, PPL: 1006, TM: 298, LM: 5.20|BETA: 0.004202\n",
      "072[s], 07[s], Ep: 16, Ct: 00500|TR LOSS: inf, PPL: 1036|TM NLL: 300, KL: 3.63 | LM NLL: 5.74, KL: inf|DE LOSS: 303, PPL: 1001, TM: 298, LM: 5.19|BETA: 0.004236\n",
      "071[s], 07[s], Ep: 16, Ct: 01000|TR LOSS: inf, PPL: 1036|TM NLL: 300, KL: 3.63 | LM NLL: 5.73, KL: inf|DE LOSS: 303, PPL: 999, TM: 298, LM: 5.19|BETA: 0.004271\n",
      "072[s], 07[s], Ep: 16, Ct: 01500|TR LOSS: inf, PPL: 1035|TM NLL: 300, KL: 3.64 | LM NLL: 5.73, KL: inf|DE LOSS: 304, PPL: 1004, TM: 298, LM: 5.19|BETA: 0.004306\n",
      "072[s], 07[s], Ep: 16, Ct: 02000|TR LOSS: inf, PPL: 1035|TM NLL: 300, KL: 3.64 | LM NLL: 5.72, KL: inf|DE LOSS: 304, PPL: 1005, TM: 298, LM: 5.18|BETA: 0.004340\n",
      "072[s], 07[s], Ep: 16, Ct: 02500|TR LOSS: inf, PPL: 1034|TM NLL: 299, KL: 3.64 | LM NLL: 5.72, KL: inf|DE LOSS: 304, PPL: 1002, TM: 298, LM: 5.18|BETA: 0.004375\n",
      "057[s], 07[s], Ep: 17, Ct: 00000|TR LOSS: inf, PPL: 1034|TM NLL: 299, KL: 3.64 | LM NLL: 5.71, KL: inf|DE LOSS: 304, PPL: 1005, TM: 298, LM: 5.18|BETA: 0.004402\n",
      "072[s], 07[s], Ep: 17, Ct: 00500|TR LOSS: inf, PPL: 1033|TM NLL: 299, KL: 3.64 | LM NLL: 5.71, KL: inf|DE LOSS: 303, PPL: 1000, TM: 298, LM: 5.17|BETA: 0.004436\n",
      "072[s], 07[s], Ep: 17, Ct: 01000|TR LOSS: inf, PPL: 1033|TM NLL: 299, KL: 3.65 | LM NLL: 5.70, KL: inf|DE LOSS: 303, PPL: 1002, TM: 298, LM: 5.16|BETA: 0.004471\n",
      "072[s], 07[s], Ep: 17, Ct: 01500|TR LOSS: inf, PPL: 1033|TM NLL: 299, KL: 3.65 | LM NLL: 5.70, KL: inf|DE LOSS: 304, PPL: 1004, TM: 298, LM: 5.16|BETA: 0.004506\n",
      "072[s], 07[s], Ep: 17, Ct: 02000|TR LOSS: inf, PPL: 1032|TM NLL: 299, KL: 3.65 | LM NLL: 5.69, KL: inf|DE LOSS: 303, PPL: 1000, TM: 298, LM: 5.16|BETA: 0.004541\n",
      "072[s], 07[s], Ep: 17, Ct: 02500|TR LOSS: inf, PPL: 1032|TM NLL: 299, KL: 3.65 | LM NLL: 5.69, KL: inf|DE LOSS: 303, PPL: 1002, TM: 298, LM: 5.16|BETA: 0.004575\n",
      "057[s], 07[s], Ep: 18, Ct: 00000|TR LOSS: inf, PPL: 1031|TM NLL: 299, KL: 3.65 | LM NLL: 5.68, KL: inf|DE LOSS: 304, PPL: 1004, TM: 298, LM: 5.16|BETA: 0.004602\n",
      "072[s], 07[s], Ep: 18, Ct: 00500|TR LOSS: inf, PPL: 1031|TM NLL: 299, KL: 3.66 | LM NLL: 5.68, KL: inf|DE LOSS: 304, PPL: 1003, TM: 298, LM: 5.15|BETA: 0.004637\n",
      "072[s], 07[s], Ep: 18, Ct: 01000|TR LOSS: inf, PPL: 1030|TM NLL: 299, KL: 3.66 | LM NLL: 5.67, KL: inf|DE LOSS: 303, PPL: 1001, TM: 298, LM: 5.14|BETA: 0.004671\n",
      "071[s], 07[s], Ep: 18, Ct: 01500|TR LOSS: inf, PPL: 1030|TM NLL: 299, KL: 3.66 | LM NLL: 5.67, KL: inf|DE LOSS: 303, PPL: 1001, TM: 298, LM: 5.14|BETA: 0.004706\n",
      "072[s], 07[s], Ep: 18, Ct: 02000|TR LOSS: inf, PPL: 1030|TM NLL: 299, KL: 3.66 | LM NLL: 5.66, KL: inf|DE LOSS: 303, PPL: 1002, TM: 298, LM: 5.13|BETA: 0.004741\n",
      "072[s], 07[s], Ep: 18, Ct: 02500|TR LOSS: inf, PPL: 1029|TM NLL: 299, KL: 3.66 | LM NLL: 5.66, KL: inf|DE LOSS: 304, PPL: 1002, TM: 298, LM: 5.14|BETA: 0.004775\n",
      "056[s], 07[s], Ep: 19, Ct: 00000|TR LOSS: inf, PPL: 1029|TM NLL: 299, KL: 3.66 | LM NLL: 5.65, KL: inf|DE LOSS: 303, PPL: 1002, TM: 298, LM: 5.13|BETA: 0.004802\n",
      "072[s], 07[s], Ep: 19, Ct: 00500|TR LOSS: inf, PPL: 1028|TM NLL: 299, KL: 3.67 | LM NLL: 5.65, KL: inf|DE LOSS: 303, PPL: 1000, TM: 298, LM: 5.12|BETA: 0.004837\n",
      "072[s], 07[s], Ep: 19, Ct: 01000|TR LOSS: inf, PPL: 1028|TM NLL: 299, KL: 3.67 | LM NLL: 5.64, KL: inf|DE LOSS: 303, PPL: 1003, TM: 298, LM: 5.12|BETA: 0.004871\n",
      "072[s], 07[s], Ep: 19, Ct: 01500|TR LOSS: inf, PPL: 1028|TM NLL: 299, KL: 3.67 | LM NLL: 5.64, KL: inf|DE LOSS: 303, PPL: 999, TM: 298, LM: 5.12|BETA: 0.004906\n",
      "073[s], 07[s], Ep: 19, Ct: 02000|TR LOSS: inf, PPL: 1027|TM NLL: 299, KL: 3.67 | LM NLL: 5.63, KL: inf|DE LOSS: 303, PPL: 1003, TM: 298, LM: 5.11|BETA: 0.004941\n",
      "076[s], 07[s], Ep: 19, Ct: 02500|TR LOSS: inf, PPL: 1027|TM NLL: 299, KL: 3.67 | LM NLL: 5.63, KL: inf|DE LOSS: 303, PPL: 1001, TM: 298, LM: 5.11|BETA: 0.004975\n",
      "060[s], 07[s], Ep: 20, Ct: 00000|TR LOSS: inf, PPL: 1026|TM NLL: 299, KL: 3.67 | LM NLL: 5.63, KL: inf|DE LOSS: 303, PPL: 1001, TM: 298, LM: 5.11|BETA: 0.005002\n",
      "076[s], 07[s], Ep: 20, Ct: 00500|TR LOSS: inf, PPL: 1026|TM NLL: 299, KL: 3.68 | LM NLL: 5.62, KL: inf|DE LOSS: 303, PPL: 999, TM: 298, LM: 5.11|BETA: 0.005037\n",
      "075[s], 07[s], Ep: 20, Ct: 01000|TR LOSS: inf, PPL: 1026|TM NLL: 299, KL: 3.68 | LM NLL: 5.62, KL: inf|DE LOSS: 303, PPL: 1003, TM: 298, LM: 5.10|BETA: 0.005071\n",
      "076[s], 07[s], Ep: 20, Ct: 01500|TR LOSS: inf, PPL: 1025|TM NLL: 299, KL: 3.68 | LM NLL: 5.61, KL: inf|DE LOSS: 303, PPL: 997, TM: 298, LM: 5.10|BETA: 0.005106\n",
      "075[s], 07[s], Ep: 20, Ct: 02000|TR LOSS: inf, PPL: 1025|TM NLL: 299, KL: 3.68 | LM NLL: 5.61, KL: inf|DE LOSS: 303, PPL: 1001, TM: 298, LM: 5.10|BETA: 0.005141\n",
      "072[s], 07[s], Ep: 20, Ct: 02500|TR LOSS: inf, PPL: 1025|TM NLL: 299, KL: 3.68 | LM NLL: 5.60, KL: inf|DE LOSS: 303, PPL: 1002, TM: 298, LM: 5.09|BETA: 0.005176\n",
      "057[s], 07[s], Ep: 21, Ct: 00000|TR LOSS: inf, PPL: 1024|TM NLL: 299, KL: 3.68 | LM NLL: 5.60, KL: inf|DE LOSS: 303, PPL: 1000, TM: 298, LM: 5.09|BETA: 0.005202\n",
      "072[s], 07[s], Ep: 21, Ct: 00500|TR LOSS: inf, PPL: 1024|TM NLL: 299, KL: 3.69 | LM NLL: 5.60, KL: inf|DE LOSS: 303, PPL: 995, TM: 298, LM: 5.09|BETA: 0.005237\n",
      "074[s], 07[s], Ep: 21, Ct: 01000|TR LOSS: inf, PPL: 1024|TM NLL: 299, KL: 3.69 | LM NLL: 5.59, KL: inf|DE LOSS: 303, PPL: 1000, TM: 298, LM: 5.09|BETA: 0.005272\n",
      "076[s], 07[s], Ep: 21, Ct: 01500|TR LOSS: inf, PPL: 1023|TM NLL: 299, KL: 3.69 | LM NLL: 5.59, KL: inf|DE LOSS: 303, PPL: 1000, TM: 298, LM: 5.08|BETA: 0.005306\n",
      "076[s], 07[s], Ep: 21, Ct: 02000|TR LOSS: inf, PPL: 1023|TM NLL: 299, KL: 3.69 | LM NLL: 5.58, KL: inf|DE LOSS: 303, PPL: 1001, TM: 298, LM: 5.07|BETA: 0.005341\n",
      "073[s], 07[s], Ep: 21, Ct: 02500|TR LOSS: inf, PPL: 1023|TM NLL: 299, KL: 3.69 | LM NLL: 5.58, KL: inf|DE LOSS: 303, PPL: 1002, TM: 298, LM: 5.07|BETA: 0.005376\n",
      "057[s], 07[s], Ep: 22, Ct: 00000|TR LOSS: inf, PPL: 1022|TM NLL: 299, KL: 3.69 | LM NLL: 5.58, KL: inf|DE LOSS: 303, PPL: 997, TM: 298, LM: 5.08|BETA: 0.005402\n",
      "073[s], 07[s], Ep: 22, Ct: 00500|TR LOSS: inf, PPL: 1022|TM NLL: 299, KL: 3.69 | LM NLL: 5.57, KL: inf|DE LOSS: 303, PPL: 999, TM: 298, LM: 5.07|BETA: 0.005437\n",
      "073[s], 07[s], Ep: 22, Ct: 01000|TR LOSS: inf, PPL: 1022|TM NLL: 299, KL: 3.70 | LM NLL: 5.57, KL: inf|DE LOSS: 303, PPL: 999, TM: 298, LM: 5.06|BETA: 0.005472\n",
      "073[s], 07[s], Ep: 22, Ct: 01500|TR LOSS: inf, PPL: 1021|TM NLL: 299, KL: 3.70 | LM NLL: 5.56, KL: inf|DE LOSS: 303, PPL: 1002, TM: 298, LM: 5.06|BETA: 0.005506\n",
      "072[s], 07[s], Ep: 22, Ct: 02000|TR LOSS: inf, PPL: 1021|TM NLL: 299, KL: 3.70 | LM NLL: 5.56, KL: inf|DE LOSS: 303, PPL: 997, TM: 298, LM: 5.06|BETA: 0.005541\n",
      "071[s], 07[s], Ep: 22, Ct: 02500|TR LOSS: inf, PPL: 1021|TM NLL: 299, KL: 3.70 | LM NLL: 5.56, KL: inf|DE LOSS: 303, PPL: 998, TM: 298, LM: 5.06|BETA: 0.005576\n",
      "057[s], 07[s], Ep: 23, Ct: 00000|TR LOSS: inf, PPL: 1021|TM NLL: 299, KL: 3.70 | LM NLL: 5.55, KL: inf|DE LOSS: 303, PPL: 1000, TM: 298, LM: 5.06|BETA: 0.005602\n",
      "072[s], 07[s], Ep: 23, Ct: 00500|TR LOSS: inf, PPL: 1020|TM NLL: 299, KL: 3.70 | LM NLL: 5.55, KL: inf|DE LOSS: 303, PPL: 998, TM: 298, LM: 5.05|BETA: 0.005637\n",
      "072[s], 07[s], Ep: 23, Ct: 01000|TR LOSS: inf, PPL: 1020|TM NLL: 299, KL: 3.70 | LM NLL: 5.55, KL: inf|DE LOSS: 303, PPL: 997, TM: 298, LM: 5.05|BETA: 0.005672\n",
      "072[s], 07[s], Ep: 23, Ct: 01500|TR LOSS: inf, PPL: 1020|TM NLL: 299, KL: 3.71 | LM NLL: 5.54, KL: inf|DE LOSS: 303, PPL: 997, TM: 298, LM: 5.05|BETA: 0.005706\n",
      "072[s], 07[s], Ep: 23, Ct: 02000|TR LOSS: inf, PPL: 1019|TM NLL: 299, KL: 3.71 | LM NLL: 5.54, KL: inf|DE LOSS: 303, PPL: 1001, TM: 298, LM: 5.04|BETA: 0.005741\n",
      "072[s], 07[s], Ep: 23, Ct: 02500|TR LOSS: inf, PPL: 1019|TM NLL: 299, KL: 3.71 | LM NLL: 5.53, KL: inf|DE LOSS: 303, PPL: 1001, TM: 298, LM: 5.05|BETA: 0.005776\n",
      "057[s], 07[s], Ep: 24, Ct: 00000|TR LOSS: inf, PPL: 1019|TM NLL: 299, KL: 3.71 | LM NLL: 5.53, KL: inf|DE LOSS: 303, PPL: 998, TM: 298, LM: 5.04|BETA: 0.005802\n",
      "072[s], 07[s], Ep: 24, Ct: 00500|TR LOSS: inf, PPL: 1019|TM NLL: 299, KL: 3.71 | LM NLL: 5.53, KL: inf|DE LOSS: 303, PPL: 994, TM: 298, LM: 5.04|BETA: 0.005837\n",
      "072[s], 07[s], Ep: 24, Ct: 01000|TR LOSS: inf, PPL: 1018|TM NLL: 299, KL: 3.71 | LM NLL: 5.52, KL: inf|DE LOSS: 303, PPL: 999, TM: 298, LM: 5.04|BETA: 0.005872\n",
      "071[s], 06[s], Ep: 24, Ct: 01500|TR LOSS: inf, PPL: 1018|TM NLL: 299, KL: 3.71 | LM NLL: 5.52, KL: inf|DE LOSS: 303, PPL: 997, TM: 298, LM: 5.04|BETA: 0.005907\n",
      "073[s], 07[s], Ep: 24, Ct: 02000|TR LOSS: inf, PPL: 1018|TM NLL: 299, KL: 3.71 | LM NLL: 5.52, KL: inf|DE LOSS: 303, PPL: 998, TM: 298, LM: 5.03|BETA: 0.005941\n",
      "071[s], 07[s], Ep: 24, Ct: 02500|TR LOSS: inf, PPL: 1017|TM NLL: 299, KL: 3.72 | LM NLL: 5.51, KL: inf|DE LOSS: 303, PPL: 1000, TM: 298, LM: 5.03|BETA: 0.005976\n",
      "056[s], 07[s], Ep: 25, Ct: 00000|TR LOSS: inf, PPL: 1017|TM NLL: 299, KL: 3.72 | LM NLL: 5.51, KL: inf|DE LOSS: 303, PPL: 997, TM: 298, LM: 5.03|BETA: 0.006003\n",
      "071[s], 06[s], Ep: 25, Ct: 00500|TR LOSS: inf, PPL: 1017|TM NLL: 299, KL: 3.72 | LM NLL: 5.51, KL: inf|DE LOSS: 303, PPL: 996, TM: 298, LM: 5.03|BETA: 0.006037\n",
      "072[s], 07[s], Ep: 25, Ct: 01000|TR LOSS: inf, PPL: 1017|TM NLL: 299, KL: 3.72 | LM NLL: 5.50, KL: inf|DE LOSS: 303, PPL: 997, TM: 298, LM: 5.02|BETA: 0.006072\n",
      "072[s], 07[s], Ep: 25, Ct: 01500|TR LOSS: inf, PPL: 1017|TM NLL: 299, KL: 3.72 | LM NLL: 5.50, KL: inf|DE LOSS: 303, PPL: 998, TM: 298, LM: 5.02|BETA: 0.006107\n",
      "072[s], 07[s], Ep: 25, Ct: 02000|TR LOSS: inf, PPL: 1016|TM NLL: 299, KL: 3.72 | LM NLL: 5.50, KL: inf|DE LOSS: 303, PPL: 995, TM: 298, LM: 5.02|BETA: 0.006141\n",
      "071[s], 07[s], Ep: 25, Ct: 02500|TR LOSS: inf, PPL: 1016|TM NLL: 299, KL: 3.72 | LM NLL: 5.49, KL: inf|DE LOSS: 303, PPL: 999, TM: 298, LM: 5.01|BETA: 0.006176\n",
      "057[s], 07[s], Ep: 26, Ct: 00000|TR LOSS: inf, PPL: 1016|TM NLL: 299, KL: 3.72 | LM NLL: 5.49, KL: inf|DE LOSS: 303, PPL: 995, TM: 298, LM: 5.02|BETA: 0.006203\n",
      "072[s], 07[s], Ep: 26, Ct: 00500|TR LOSS: inf, PPL: 1015|TM NLL: 299, KL: 3.73 | LM NLL: 5.49, KL: inf|DE LOSS: 303, PPL: 997, TM: 298, LM: 5.02|BETA: 0.006237\n",
      "071[s], 07[s], Ep: 26, Ct: 01000|TR LOSS: inf, PPL: 1015|TM NLL: 299, KL: 3.73 | LM NLL: 5.48, KL: inf|DE LOSS: 303, PPL: 995, TM: 298, LM: 5.01|BETA: 0.006272\n",
      "073[s], 07[s], Ep: 26, Ct: 01500|TR LOSS: inf, PPL: 1015|TM NLL: 299, KL: 3.73 | LM NLL: 5.48, KL: inf|DE LOSS: 303, PPL: 999, TM: 298, LM: 5.01|BETA: 0.006307\n",
      "072[s], 07[s], Ep: 26, Ct: 02000|TR LOSS: inf, PPL: 1015|TM NLL: 299, KL: 3.73 | LM NLL: 5.48, KL: inf|DE LOSS: 303, PPL: 996, TM: 298, LM: 5.01|BETA: 0.006341\n",
      "072[s], 07[s], Ep: 26, Ct: 02500|TR LOSS: inf, PPL: 1015|TM NLL: 299, KL: 3.73 | LM NLL: 5.47, KL: inf|DE LOSS: 303, PPL: 999, TM: 298, LM: 5.00|BETA: 0.006376\n",
      "057[s], 07[s], Ep: 27, Ct: 00000|TR LOSS: inf, PPL: 1014|TM NLL: 299, KL: 3.73 | LM NLL: 5.47, KL: inf|DE LOSS: 303, PPL: 994, TM: 298, LM: 5.00|BETA: 0.006403\n",
      "072[s], 07[s], Ep: 27, Ct: 00500|TR LOSS: inf, PPL: 1014|TM NLL: 299, KL: 3.73 | LM NLL: 5.47, KL: inf|DE LOSS: 303, PPL: 996, TM: 298, LM: 5.00|BETA: 0.006437\n",
      "072[s], 07[s], Ep: 27, Ct: 01000|TR LOSS: inf, PPL: 1014|TM NLL: 299, KL: 3.73 | LM NLL: 5.46, KL: inf|DE LOSS: 303, PPL: 996, TM: 298, LM: 5.00|BETA: 0.006472\n",
      "072[s], 07[s], Ep: 27, Ct: 01500|TR LOSS: inf, PPL: 1014|TM NLL: 299, KL: 3.73 | LM NLL: 5.46, KL: inf|DE LOSS: 303, PPL: 999, TM: 298, LM: 4.99|BETA: 0.006507\n",
      "072[s], 07[s], Ep: 27, Ct: 02000|TR LOSS: inf, PPL: 1014|TM NLL: 299, KL: 3.74 | LM NLL: 5.46, KL: inf|DE LOSS: 303, PPL: 993, TM: 298, LM: 4.99|BETA: 0.006542\n",
      "071[s], 07[s], Ep: 27, Ct: 02500|TR LOSS: inf, PPL: 1013|TM NLL: 299, KL: 3.74 | LM NLL: 5.45, KL: inf|DE LOSS: 303, PPL: 999, TM: 298, LM: 4.99|BETA: 0.006576\n",
      "055[s], 07[s], Ep: 28, Ct: 00000|TR LOSS: inf, PPL: 1013|TM NLL: 299, KL: 3.74 | LM NLL: 5.45, KL: inf|DE LOSS: 303, PPL: 998, TM: 298, LM: 4.99|BETA: 0.006603\n",
      "070[s], 07[s], Ep: 28, Ct: 00500|TR LOSS: inf, PPL: 1013|TM NLL: 299, KL: 3.74 | LM NLL: 5.45, KL: inf|DE LOSS: 303, PPL: 995, TM: 298, LM: 4.98|BETA: 0.006638\n",
      "071[s], 07[s], Ep: 28, Ct: 01000|TR LOSS: inf, PPL: 1013|TM NLL: 299, KL: 3.74 | LM NLL: 5.45, KL: inf|DE LOSS: 303, PPL: 995, TM: 298, LM: 4.99|BETA: 0.006672\n",
      "073[s], 07[s], Ep: 28, Ct: 01500|TR LOSS: inf, PPL: 1012|TM NLL: 299, KL: 3.74 | LM NLL: 5.44, KL: inf|DE LOSS: 303, PPL: 996, TM: 298, LM: 4.98|BETA: 0.006707\n",
      "073[s], 07[s], Ep: 28, Ct: 02000|TR LOSS: inf, PPL: 1012|TM NLL: 299, KL: 3.74 | LM NLL: 5.44, KL: inf|DE LOSS: 303, PPL: 997, TM: 298, LM: 4.98|BETA: 0.006742\n"
     ]
    }
   ],
   "source": [
    "for log in logs:\n",
    "    print('%03d[s], %02d[s], Ep: %02d, Ct: %05d|TR LOSS: %.0f, PPL: %.0f|TM NLL: %.0f, KL: %.2f | LM NLL: %.2f, KL: %.2f|DE LOSS: %.0f, PPL: %.0f, TM: %.0f, LM: %.2f|BETA: %.6f' %  log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : republican senate democratic house election party campaign governor republicans gov\n",
      "1 : court judge federal u.s. attorney lawsuit filed case district law\n",
      "2 : million company http water project reports federal department u.s. plant\n",
      "3 : vehicle car crash driver truck died driving killed hit struck\n",
      "4 : department http people reports health office law officers week security\n",
      "5 : percent million company cents share revenue billion rate shares average\n",
      "6 : national saturday event museum died day years u.s. center president\n",
      "7 : bill tax budget million health lawmakers house measure gov approved\n",
      "8 : school university students board education program president college schools district\n",
      "9 : fire firefighters area reported authorities home people sunday miles blaze\n",
      "10 : found home officers authorities shot woman sheriff shooting arrested body\n",
      "11 : charged guilty prosecutors charges court years prison pleaded attorney murder\n",
      "12 : service weather power national storm snow tuesday expected rain customers\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
