{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pdb\n",
    "\n",
    "from math import log\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "\n",
    "import tensorflow as tf\n",
    "import _pickle as cPickle\n",
    "\n",
    "from hlda import HierarchicalLDA\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/bags/instances.pkl', 'path of data')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "config = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus_raw = [[[bow_idxs[bow_index]]*int(instance.bow[bow_index]) for bow_index in np.where(instance.bow > 0)[0]] for instance in instances_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = [[idx for idxs in doc for idx in idxs] for doc in train_corpus_raw]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCRPNode(object):\n",
    "\n",
    "    # class variable to keep track of total nodes created so far\n",
    "    total_nodes = 0\n",
    "    last_node_id = 0\n",
    "\n",
    "    def __init__(self, num_levels, vocab, parent=None, level=0,\n",
    "                 random_state=None):\n",
    "\n",
    "        self.node_id = NCRPNode.last_node_id\n",
    "        NCRPNode.last_node_id += 1\n",
    "\n",
    "        self.customers = 0\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.level = level\n",
    "        self.total_words = 0\n",
    "        self.num_levels = num_levels\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.word_counts = np.zeros(len(vocab))\n",
    "\n",
    "        if random_state is None:\n",
    "            self.random_state = RandomState()\n",
    "        else:\n",
    "            self.random_state = random_state\n",
    "\n",
    "    def __repr__(self):\n",
    "        parent_id = None\n",
    "        if self.parent is not None:\n",
    "            parent_id = self.parent.node_id\n",
    "        return 'Node=%d level=%d customers=%d total_words=%d parent=%s' % (self.node_id,\n",
    "            self.level, self.customers, self.total_words, parent_id)\n",
    "\n",
    "    def add_child(self):\n",
    "        ''' Adds a child to the next level of this node '''\n",
    "        node = NCRPNode(self.num_levels, self.vocab, parent=self, level=self.level+1)\n",
    "        self.children.append(node)\n",
    "        NCRPNode.total_nodes += 1\n",
    "        return node\n",
    "\n",
    "    def is_leaf(self):\n",
    "        ''' Check if this node is a leaf node '''\n",
    "        return self.level == self.num_levels-1\n",
    "\n",
    "    def get_new_leaf(self):\n",
    "        ''' Keeps adding nodes along the path until a leaf node is generated'''\n",
    "        node = self\n",
    "        for l in range(self.level, self.num_levels-1):\n",
    "            node = node.add_child()\n",
    "        return node\n",
    "\n",
    "    def drop_path(self):\n",
    "        ''' Removes a document from a path starting from this node '''\n",
    "        node = self\n",
    "        node.customers -= 1\n",
    "        if node.customers == 0:\n",
    "            node.parent.remove(node)\n",
    "        for level in range(1, self.num_levels): # skip the root\n",
    "            node = node.parent\n",
    "            node.customers -= 1\n",
    "            if node.customers == 0:\n",
    "                node.parent.remove(node)\n",
    "\n",
    "    def remove(self, node):\n",
    "        ''' Removes a child node '''\n",
    "        self.children.remove(node)\n",
    "        NCRPNode.total_nodes -= 1\n",
    "\n",
    "    def add_path(self):\n",
    "        ''' Adds a document to a path starting from this node '''\n",
    "        node = self\n",
    "        node.customers += 1\n",
    "        for level in range(1, self.num_levels):\n",
    "            node = node.parent\n",
    "            node.customers += 1\n",
    "\n",
    "    def select(self, gamma):\n",
    "        ''' Selects an existing child or create a new one according to the CRP '''\n",
    "\n",
    "        weights = np.zeros(len(self.children)+1)\n",
    "        weights[0] = float(gamma) / (gamma+self.customers)\n",
    "        i = 1\n",
    "        for child in self.children:\n",
    "            weights[i] = float(child.customers) / (gamma + self.customers)\n",
    "            i += 1\n",
    "\n",
    "        choice = self.random_state.multinomial(1, weights).argmax()\n",
    "        if choice == 0:\n",
    "            return self.add_child()\n",
    "        else:\n",
    "            return self.children[choice-1]\n",
    "\n",
    "    def get_top_words(self, n_words, with_weight):\n",
    "        ''' Get the top n words in this node '''\n",
    "\n",
    "        pos = np.argsort(self.word_counts)[::-1]distributions\n",
    "\n",
    "        sorted_vocab = np.array(list(self.vocab.values()))[pos]\n",
    "        sorted_vocab = sorted_vocab[:n_words]\n",
    "        sorted_weights = self.word_counts[pos]\n",
    "        sorted_weights = sorted_weights[:n_words]\n",
    "\n",
    "        output = ''\n",
    "        for word, weight in zip(sorted_vocab, sorted_weights):\n",
    "            if with_weight:\n",
    "                output += '%s (%d), ' % (word, weight)\n",
    "            else:\n",
    "                output += '%s, ' % word\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalLDA(object):\n",
    "\n",
    "    def __init__(self, corpus, vocab,\n",
    "                 alpha=10.0, gamma=1.0, eta=0.1,\n",
    "                 seed=0, verbose=True, num_levels=3):\n",
    "\n",
    "        NCRPNode.total_nodes = 0\n",
    "        NCRPNode.last_node_id = 0\n",
    "\n",
    "        self.corpus = corpus\n",
    "        self.vocab = vocab\n",
    "        self.alpha = alpha  # smoothing on doc-topic distributions\n",
    "        self.gamma = gamma  # \"imaginary\" customers at the next, as yet unused table\n",
    "        self.eta = eta      # smoothing on topic-word distributions\n",
    "\n",
    "        self.seed = seed\n",
    "        self.random_state = RandomState(seed)\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.num_levels = num_levels\n",
    "        self.num_documents = len(corpus)\n",
    "        self.num_types = len(vocab)\n",
    "        self.eta_sum = eta * self.num_types\n",
    "\n",
    "        # if self.verbose:\n",
    "        #     for d in range(len(self.corpus)):\n",
    "        #         doc = self.corpus[d]\n",
    "        #         words = ' '.join([self.vocab[n] for n in doc])\n",
    "        #         print 'doc_%d = %s' % (d, words)\n",
    "\n",
    "        # initialise a single path\n",
    "        path = np.zeros(self.num_levels, dtype=np.object)\n",
    "\n",
    "        # initialize and fill the topic pointer arrays for\n",
    "        # every document. Set everything to the single path that\n",
    "        # we added earlier.\n",
    "        self.root_node = NCRPNode(self.num_levels, self.vocab)\n",
    "        self.document_leaves = {}                                   # currently selected path (ie leaf node) through the NCRP tree\n",
    "        self.levels = np.zeros(self.num_documents, dtype=np.object) # indexed < doc, token >\n",
    "        for d in range(len(self.corpus)):\n",
    "\n",
    "            # populate nodes into the path of this document\n",
    "            doc = self.corpus[d]\n",
    "            doc_len = len(doc)\n",
    "            path[0] = self.root_node\n",
    "            self.root_node.customers += 1 # always add to the root node first\n",
    "            for level in range(1, self.num_levels):\n",
    "                # at each level, a node is selected by its parent node based on the CRP prior\n",
    "                parent_node = path[level-1]\n",
    "                level_node = parent_node.select(self.gamma)\n",
    "                level_node.customers += 1\n",
    "                path[level] = level_node\n",
    "\n",
    "            # set the leaf node for this document\n",
    "            leaf_node = path[self.num_levels-1]\n",
    "            self.document_leaves[d] = leaf_node\n",
    "\n",
    "            # randomly assign each word in the document to a level (node) along the path\n",
    "            self.levels[d] = np.zeros(doc_len, dtype=np.int)\n",
    "            for n in range(doc_len):\n",
    "                w = doc[n]\n",
    "                random_level = self.random_state.randint(self.num_levels)\n",
    "                random_node = path[random_level]\n",
    "                random_node.word_counts[w] += 1\n",
    "                random_node.total_words += 1\n",
    "                self.levels[d][n] = random_level\n",
    "\n",
    "    def estimate(self, num_samples, display_topics=50, n_words=5, with_weights=True):\n",
    "\n",
    "        print('HierarchicalLDA sampling\\n')\n",
    "        for s in range(num_samples):\n",
    "\n",
    "            sys.stdout.write('.')\n",
    "\n",
    "            for d in range(len(self.corpus)):\n",
    "                self.sample_path(d)\n",
    "\n",
    "            for d in range(len(self.corpus)):\n",
    "                self.sample_topics(d)\n",
    "\n",
    "            if (s > 0) and ((s+1) % display_topics == 0):\n",
    "                print(\" %d\" % (s+1))\n",
    "                self.print_nodes(n_words, with_weights)\n",
    "                print('')\n",
    "\n",
    "    def sample_path(self, d):\n",
    "\n",
    "        # define a path starting from the leaf node of this doc\n",
    "        path = np.zeros(self.num_levels, dtype=np.object)\n",
    "        node = self.document_leaves[d]\n",
    "        for level in range(self.num_levels-1, -1, -1): # e.g. [3, 2, 1, 0] for num_levels = 4\n",
    "            path[level] = node\n",
    "            node = node.parent\n",
    "\n",
    "        # remove this document from the path, deleting empty nodes if necessary\n",
    "        self.document_leaves[d].drop_path()\n",
    "\n",
    "        ############################################################\n",
    "        # calculates the prior p(c_d | c_{-d}) in eq. (4)\n",
    "        ############################################################\n",
    "\n",
    "        node_weights = {}\n",
    "        self.calculate_ncrp_prior(node_weights, self.root_node, 0.0)\n",
    "        pdb.set_trace()\n",
    "\n",
    "        ############################################################\n",
    "        # calculates the likelihood p(w_d | c, w_{-d}, z) in eq. (4)\n",
    "        ############################################################\n",
    "\n",
    "        level_word_counts = {level: defaultdict(int) for level in range(self.num_levels)}\n",
    "        doc_levels = self.levels[d]\n",
    "        doc = self.corpus[d]\n",
    "\n",
    "        # remove words in doc from path\n",
    "        for n in range(len(doc)): # for each word in the doc\n",
    "            # count the word at each level\n",
    "            level = doc_levels[n]\n",
    "            w = doc[n]\n",
    "            level_word_counts[level][w] += 1\n",
    "\n",
    "            # remove word count from the node at that level\n",
    "            level_node = path[level]\n",
    "            level_node.word_counts[w] -= 1\n",
    "            level_node.total_words -= 1\n",
    "            assert level_node.word_counts[w] >= 0\n",
    "            assert level_node.total_words >= 0\n",
    "\n",
    "        self.calculate_doc_likelihood(node_weights, level_word_counts)\n",
    "\n",
    "        ############################################################\n",
    "        # pick a new path\n",
    "        ############################################################\n",
    "\n",
    "        nodes = np.array(list(node_weights.keys()))\n",
    "        weights = np.array([node_weights[node] for node in nodes])\n",
    "        weights = np.exp(weights - np.max(weights)) # normalise so the largest weight is 1\n",
    "        weights = weights / np.sum(weights)\n",
    "\n",
    "        choice = self.random_state.multinomial(1, weights).argmax()\n",
    "        node = nodes[choice]\n",
    "\n",
    "        # if we picked an internal node, we need to add a new path to the leaf\n",
    "        if not node.is_leaf():\n",
    "            node = node.get_new_leaf()\n",
    "\n",
    "        # add the doc back to the path\n",
    "        node.add_path()                     # add a customer to the path\n",
    "        self.document_leaves[d] = node      # store the leaf node for this doc\n",
    "\n",
    "        # add the words\n",
    "        for level in range(self.num_levels-1, -1, -1): # e.g. [3, 2, 1, 0] for num_levels = 4\n",
    "            word_counts = level_word_counts[level]\n",
    "            for w in word_counts:\n",
    "                node.word_counts[w] += word_counts[w]\n",
    "                node.total_words += word_counts[w]\n",
    "            node = node.parent\n",
    "\n",
    "    def calculate_ncrp_prior(self, node_weights, node, weight):\n",
    "        ''' Calculates the prior on the path according to the nested CRP '''\n",
    "\n",
    "        for child in node.children:\n",
    "            child_weight = log( float(child.customers) / (node.customers + self.gamma) )\n",
    "            self.calculate_ncrp_prior(node_weights, child, weight + child_weight)\n",
    "\n",
    "        node_weights[node] = weight + log( self.gamma / (node.customers + self.gamma)) ## Why?\n",
    "\n",
    "    def calculate_doc_likelihood(self, node_weights, level_word_counts):\n",
    "\n",
    "        # calculate the weight for a new path at a given level\n",
    "        new_topic_weights = np.zeros(self.num_levels)\n",
    "        for level in range(1, self.num_levels):  # skip the root\n",
    "\n",
    "            word_counts = level_word_counts[level]\n",
    "            total_tokens = 0\n",
    "\n",
    "            for w in word_counts:\n",
    "                count = word_counts[w]\n",
    "                for i in range(count):  # why ?????????\n",
    "                    new_topic_weights[level] += log((self.eta + i) / (self.eta_sum + total_tokens))\n",
    "                    total_tokens += 1\n",
    "\n",
    "        self.calculate_word_likelihood(node_weights, self.root_node, 0.0, level_word_counts, new_topic_weights, 0)\n",
    "\n",
    "    def calculate_word_likelihood(self, node_weights, node, weight, level_word_counts, new_topic_weights, level):\n",
    "\n",
    "        # first calculate the likelihood of the words at this level, given this topic\n",
    "        node_weight = 0.0\n",
    "        word_counts = level_word_counts[level]\n",
    "        total_words = 0\n",
    "\n",
    "        for w in word_counts:\n",
    "            count = word_counts[w]\n",
    "            for i in range(count): # why ?????????\n",
    "                node_weight += log( (self.eta + node.word_counts[w] + i) /\n",
    "                                    (self.eta_sum + node.total_words + total_words) )\n",
    "                total_words += 1\n",
    "\n",
    "        # propagate that weight to the child nodes\n",
    "        for child in node.children:\n",
    "            self.calculate_word_likelihood(node_weights, child, weight + node_weight,\n",
    "                                           level_word_counts, new_topic_weights, level+1)\n",
    "\n",
    "        # finally if this is an internal node, add the weight of a new path\n",
    "        level += 1\n",
    "        while level < self.num_levels:\n",
    "            node_weight += new_topic_weights[level]\n",
    "            level += 1\n",
    "\n",
    "        node_weights[node] += node_weight\n",
    "\n",
    "    def sample_topics(self, d):\n",
    "\n",
    "        doc = self.corpus[d]\n",
    "\n",
    "        # initialise level counts\n",
    "        doc_levels = self.levels[d]\n",
    "        level_counts = np.zeros(self.num_levels, dtype=np.int)\n",
    "        for c in doc_levels:\n",
    "            level_counts[c] += 1\n",
    "\n",
    "        # get the leaf node and populate the path\n",
    "        path = np.zeros(self.num_levels, dtype=np.object)\n",
    "        node = self.document_leaves[d]\n",
    "        for level in range(self.num_levels-1, -1, -1): # e.g. [3, 2, 1, 0] for num_levels = 4\n",
    "            path[level] = node\n",
    "            node = node.parent\n",
    "\n",
    "        # sample a new level for each word\n",
    "        level_weights = np.zeros(self.num_levels)\n",
    "        for n in range(len(doc)):\n",
    "\n",
    "            w = doc[n]\n",
    "            word_level = doc_levels[n]\n",
    "\n",
    "            # remove from model\n",
    "            level_counts[word_level] -= 1\n",
    "            node = path[word_level]\n",
    "            node.word_counts[w] -= 1\n",
    "            node.total_words -= 1\n",
    "\n",
    "            # pick new level\n",
    "            for level in range(self.num_levels):\n",
    "                level_weights[level] = (self.alpha + level_counts[level]) *                     \\\n",
    "                    (self.eta + path[level].word_counts[w]) /                                   \\\n",
    "                    (self.eta_sum + path[level].total_words)\n",
    "            level_weights = level_weights / np.sum(level_weights)\n",
    "            level = self.random_state.multinomial(1, level_weights).argmax()\n",
    "\n",
    "            # put the word back into the model\n",
    "            doc_levels[n] = level\n",
    "            level_counts[level] += 1\n",
    "            node = path[level]\n",
    "            node.word_counts[w] += 1\n",
    "            node.total_words += 1\n",
    "\n",
    "    def print_nodes(self, n_words, with_weights):\n",
    "        self.print_node(self.root_node, 0, n_words, with_weights)\n",
    "\n",
    "    def print_node(self, node, indent, n_words, with_weights):\n",
    "        out = '    ' * indent\n",
    "        out += 'topic=%d level=%d (documents=%d): ' % (node.node_id, node.level, node.customers)\n",
    "        out += node.get_top_words(n_words, with_weights)\n",
    "        print(out)\n",
    "        for child in node.children:\n",
    "            self.print_node(child, indent+1, n_words, with_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hlda = HierarchicalLDA(train_corpus, vocab=idx_to_word, alpha=1, gamma=1.0, eta=1.0, num_levels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HierarchicalLDA sampling\n",
      "\n",
      ".> <ipython-input-7-ff63e12b6f01>(110)sample_path()\n",
      "-> level_word_counts = {}\n",
      "(Pdb) l\n",
      "105  \t\n",
      "106  \t        ############################################################\n",
      "107  \t        # calculates the likelihood p(w_d | c, w_{-d}, z) in eq. (4)\n",
      "108  \t        ############################################################\n",
      "109  \t\n",
      "110  ->\t        level_word_counts = {}\n",
      "111  \t        for level in range(self.num_levels):\n",
      "112  \t            level_word_counts[level] = {}\n",
      "113  \t        doc_levels = self.levels[d]\n",
      "114  \t        doc = self.corpus[d]\n",
      "115  \t\n",
      "(Pdb) node_weights\n",
      "{Node=2 level=2 customers=9855 total_words=58465 parent=1: -10.371847285999223, Node=3 level=2 customers=12945 total_words=76517 parent=1: -10.371823066732562, Node=4 level=2 customers=3745 total_words=22347 parent=1: -10.37201280686546, Node=34 level=2 customers=138 total_words=836 parent=1: -10.37896606778623, Node=1 level=1 customers=26683 total_words=158218 parent=0: -10.371745819812743, Node=6 level=2 customers=101 total_words=603 parent=5: -10.385257501794019, Node=21 level=2 customers=19 total_words=144 parent=5: -10.426698499738558, Node=22 level=2 customers=98 total_words=582 parent=5: -10.385557576815025, Node=30 level=2 customers=47 total_words=256 parent=5: -10.39645861454884, Node=47 level=2 customers=5 total_words=50 parent=5: -10.557726762144963, Node=5 level=1 customers=270 total_words=1538 parent=0: -10.375405205351008, Node=8 level=2 customers=102 total_words=629 parent=7: -10.38211197716217, Node=13 level=2 customers=1134 total_words=6726 parent=7: -10.373237247844612, Node=19 level=2 customers=200 total_words=1193 parent=7: -10.377343343727844, Node=24 level=2 customers=57 total_words=325 parent=7: -10.389747544928674, Node=25 level=2 customers=40 total_words=270 parent=7: -10.397048414807177, Node=45 level=2 customers=10 total_words=70 parent=7: -10.467665982021131, Node=50 level=2 customers=1 total_words=3 parent=7: -11.065502982776751, Node=7 level=1 customers=1544 total_words=9053 parent=0: -10.372355802216806, Node=10 level=2 customers=52 total_words=295 parent=9: -10.391776426341123, Node=14 level=2 customers=650 total_words=3814 parent=9: -10.374265510689312, Node=18 level=2 customers=61 total_words=363 parent=9: -10.388988752242206, Node=20 level=2 customers=189 total_words=1012 parent=9: -10.378005288471272, Node=23 level=2 customers=5 total_words=32 parent=9: -10.555049788164382, Node=38 level=2 customers=17 total_words=112 parent=9: -10.429886645210376, Node=43 level=2 customers=4 total_words=21 parent=9: -10.595871782684636, Node=49 level=2 customers=2 total_words=11 parent=9: -10.778193339478591, Node=9 level=1 customers=980 total_words=5844 parent=0: -10.372728231370427, Node=12 level=2 customers=705 total_words=4079 parent=11: -10.374026273483905, Node=15 level=2 customers=351 total_words=1941 parent=11: -10.375453790935163, Node=36 level=2 customers=49 total_words=243 parent=11: -10.392811546120452, Node=40 level=2 customers=2 total_words=7 parent=11: -10.778073946911096, Node=44 level=2 customers=1 total_words=11 parent=11: -11.065756019362878, Node=46 level=2 customers=2 total_words=16 parent=11: -10.778073946911096, Node=11 level=1 customers=1110 total_words=6440 parent=0: -10.372608838802932, Node=17 level=2 customers=924 total_words=5484 parent=16: -10.373554243233178, Node=28 level=2 customers=202 total_words=1184 parent=16: -10.377410859003021, Node=31 level=2 customers=7 total_words=46 parent=16: -10.50600396998696, Node=32 level=2 customers=95 total_words=588 parent=16: -10.382943877229732, Node=33 level=2 customers=12 total_words=51 parent=16: -10.452515285035973, Node=35 level=2 customers=40 total_words=248 parent=16: -10.397165189952808, Node=39 level=2 customers=28 total_words=143 parent=16: -10.407563897173707, Node=16 level=1 customers=1308 total_words=7766 parent=0: -10.372472577362437, Node=27 level=2 customers=11 total_words=64 parent=26: -10.483412333049683, Node=29 level=2 customers=14 total_words=91 parent=26: -10.465393827547004, Node=37 level=2 customers=15 total_words=62 parent=26: -10.460939477197623, Node=26 level=1 customers=40 total_words=232 parent=0: -10.396400956060052, Node=42 level=2 customers=6 total_words=38 parent=41: -10.659390415921463, Node=48 level=2 customers=1 total_words=4 parent=41: -11.19838691665415, Node=41 level=1 customers=7 total_words=40 parent=0: -10.505239736094204, Node=0 level=0 customers=31942 total_words=190294 parent=None: -10.371708343469681}\n",
      "(Pdb) qa\n",
      "*** NameError: name 'qa' is not defined\n",
      "(Pdb) q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2664f00a57fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-ff63e12b6f01>\u001b[0m in \u001b[0;36mestimate\u001b[0;34m(self, num_samples, display_topics, n_words, with_weights)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-ff63e12b6f01>\u001b[0m in \u001b[0;36msample_path\u001b[0;34m(self, d)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m############################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mlevel_word_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_levels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mlevel_word_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-ff63e12b6f01>\u001b[0m in \u001b[0;36msample_path\u001b[0;34m(self, d)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m############################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mlevel_word_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_levels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mlevel_word_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hlda.estimate(num_samples=100, display_topics=10, n_words=5, with_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
