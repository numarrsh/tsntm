{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from data_structure import get_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "from topic_model import TopicModel\n",
    "\n",
    "from topic_beam_search_decoder import BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '2', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/apnews/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/tglm_vae', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'apnews', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 20, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 500, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.1, 'lr')\n",
    "flags.DEFINE_float('reg', 1., 'regularization term')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_bool('warmup', True, 'flg of warming up')\n",
    "flags.DEFINE_integer('epochs_cycle', 5, 'number of epochs within a cycle')\n",
    "flags.DEFINE_float('r_cycle', 0.5, 'proportion used to increase beta within a cycle')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 10, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_bow', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_hidden_topic', 512, 'dim_hidden_topic')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "flags.DEFINE_bool('bidirectional', True, 'flg of bidirectional encoding')\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('PAD_IDX', word_to_idx[PAD], 'PAD_IDX')\n",
    "flags.DEFINE_integer('UNK_IDX', word_to_idx[UNK], 'UNK_IDX')\n",
    "flags.DEFINE_integer('BOS_IDX', word_to_idx[BOS], 'BOS_IDX')\n",
    "flags.DEFINE_integer('EOS_IDX', word_to_idx[EOS], 'EOS_IDX')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "maximum_iterations = max([max([instance.max_sent_l for instance in batch]) for ct, batch in dev_batches])\n",
    "flags.DEFINE_integer('maximum_iterations', maximum_iterations, 'maximum_iterations')\n",
    "\n",
    "flags.DEFINE_integer('cycle_steps', len(train_batches)*config.epochs_cycle, 'number of steps for each cycle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build language model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['bow'] = tf.placeholder(tf.float32, [None, config.dim_bow], name='bow')\n",
    "t_variables['input_token_idxs'] = tf.placeholder(tf.int32, [None, None], name='input_token_idxs')\n",
    "t_variables['dec_input_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_input_idxs')\n",
    "t_variables['dec_target_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_target_idxs')\n",
    "t_variables['batch_l'] = tf.placeholder(tf.int32, name='batch_l')\n",
    "t_variables['doc_l'] = tf.placeholder(tf.int32, [None], name='doc_l')\n",
    "t_variables['sent_l'] = tf.placeholder(tf.int32, [None], name='sent_l')\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train', assertion=False):\n",
    "    def token_dropout(sent_idxs):\n",
    "        sent_idxs_dropout = np.asarray(sent_idxs)\n",
    "        sent_idxs_dropout[np.random.rand(len(sent_idxs)) > config.word_keep_prob] = config.UNK_IDX\n",
    "        return list(sent_idxs_dropout)\n",
    "\n",
    "    bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "    \n",
    "    doc_l = np.array([len(instance.token_idxs) for instance in batch])\n",
    "    \n",
    "    feed_input_token_idxs_list = [sent_idxs for instance in batch for sent_idxs in instance.token_idxs]\n",
    "    feed_dec_input_idxs_list = [[config.BOS_IDX] + token_dropout(sent_idxs) for sent_idxs in feed_input_token_idxs_list]\n",
    "    feed_dec_target_idxs_list = [sent_idxs + [config.EOS_IDX]  for sent_idxs in feed_input_token_idxs_list]\n",
    "        \n",
    "    sent_l = np.array([len(sent_idxs) for sent_idxs in feed_input_token_idxs_list], np.int32)\n",
    "    batch_l = len(sent_l)\n",
    "    \n",
    "    feed_input_token_idxs = pad_sequences(feed_input_token_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_input_idxs = pad_sequences(feed_dec_input_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_target_idxs = pad_sequences(feed_dec_target_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    \n",
    "    if assertion:\n",
    "        index = 0\n",
    "        for instance in batch:\n",
    "            for line_idxs in instance.token_idxs:\n",
    "                assert feed_input_token_idxs_list[index] == line_idxs\n",
    "                index += 1\n",
    "        assert feed_input_token_idxs.shape[1] == np.max(sent_l)\n",
    "        assert feed_dec_input_idxs.shape[1] == np.max(sent_l) + 1\n",
    "        assert feed_dec_target_idxs.shape[1] == np.max(sent_l) + 1\n",
    "    \n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "\n",
    "    feed_dict = {\n",
    "                t_variables['bow']: bow, \n",
    "                t_variables['batch_l']: batch_l, t_variables['doc_l']: doc_l, t_variables['sent_l']: sent_l, \n",
    "                t_variables['input_token_idxs']: feed_input_token_idxs, t_variables['dec_input_idxs']: feed_dec_input_idxs, t_variables['dec_target_idxs']: feed_dec_target_idxs, \n",
    "                t_variables['keep_prob']: keep_prob\n",
    "    }\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     10,
     24,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "def check_shape(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()\n",
    "    \n",
    "def check_value(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()    \n",
    "    \n",
    "# sent_loss_kl_categ_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, tf_log(prob_topic_infer/prob_topic_sents)), 1))\n",
    "# debug_value([sent_loss_kl_categ, sent_loss_kl_categ_tmp])\n",
    "# sent_loss_kl_gauss_tmp = 0.5 * tf.reduce_sum(tf.exp(logvars_topic_infer-logvars_topic) + tf.square(means_topic - means_topic_infer) / tf.exp(logvars_topic) - 1 + (logvars_topic - logvars_topic_infer), -1)\n",
    "# sent_loss_kl_gmm_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss_tmp), -1))\n",
    "# debug_value([sent_loss_kl_gmm_tmp, sent_loss_kl_gmm])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encode bow\n",
    "with tf.variable_scope('topic/enc', reuse=False):\n",
    "    hidden_bow_ = tf.layers.Dense(units=config.dim_hidden_bow, activation=tf.nn.relu, name='hidden_bow')(t_variables['bow'])\n",
    "    hidden_bow = tf.layers.Dropout(t_variables['keep_prob'])(hidden_bow_)\n",
    "    means_bow = tf.layers.Dense(units=config.dim_latent_bow, name='mean_bow')(hidden_bow)\n",
    "    logvars_bow = tf.layers.Dense(units=config.dim_latent_bow, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_bow)\n",
    "    latents_bow = sample_latents(means_bow, logvars_bow) # sample latent vectors\n",
    "\n",
    "    prob_topic = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic')(latents_bow) # inference of topic probabilities\n",
    "\n",
    "# decode bow\n",
    "with tf.variable_scope('shared', reuse=False):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "bow_embeddings = tf.nn.embedding_lookup(embeddings, bow_idxs) # embeddings of each bow features\n",
    "\n",
    "with tf.variable_scope('topic/dec', reuse=False):\n",
    "    topic_embeddings = tf.get_variable('topic_emb', [config.n_topic, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of topics\n",
    "\n",
    "    topic_bow = tf.nn.softmax(tf.matmul(topic_embeddings, bow_embeddings, transpose_b=True), 1) # bow vectors for each topic\n",
    "    logits_bow = tf_log(tf.matmul(prob_topic, topic_bow)) # predicted bow distribution\n",
    "\n",
    "    # prior of each gaussian distribution (computed for each topic)\n",
    "    hidden_topic = tf.layers.Dense(units=config.dim_hidden_topic, activation=tf.nn.relu, name='hidden_topic')(topic_bow)\n",
    "    means_topic = tf.layers.Dense(units=config.dim_latent, name='mean_topic')(hidden_topic)\n",
    "    logvars_topic = tf.layers.Dense(units=config.dim_latent, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_topic)\n",
    "    sigma_topic = tf.exp(0.5 * logvars_topic)\n",
    "    gauss_topic = tfd.Normal(loc=means_topic, scale=sigma_topic)    \n",
    "    \n",
    "# define losses\n",
    "topic_losses_recon = -tf.reduce_sum(tf.multiply(t_variables['bow'], logits_bow), 1)\n",
    "topic_loss_recon = tf.reduce_mean(topic_losses_recon) # negative log likelihood of each words\n",
    "\n",
    "topic_loss_kl = compute_kl_loss(means_bow, logvars_bow) # KL divergence b/w latent dist & gaussian std\n",
    "\n",
    "topic_bow_norm = topic_bow / tf.norm(topic_bow, axis=1, keepdims=True)\n",
    "topic_dots = tf.clip_by_value(tf.matmul(topic_bow_norm, tf.transpose(topic_bow_norm)), -1., 1.)\n",
    "topic_loss_reg = tf.reduce_mean(tf.square(topic_dots - tf.eye(config.n_topic)))\n",
    "# topic_angles = tf.acos(topic_dots)\n",
    "# topic_angles_mean = tf.reduce_mean(topic_angles)\n",
    "# topic_angles_vars = tf.reduce_mean(tf.square(topic_angles - topic_angles_mean))\n",
    "# topic_loss_reg = tf.exp(topic_angles_vars - topic_angles_mean)\n",
    "\n",
    "# monitor\n",
    "n_bow = tf.reduce_sum(t_variables['bow'], 1)\n",
    "topic_ppls = tf.divide(topic_losses_recon, n_bow)\n",
    "topics_freq_bow_indices = tf.nn.top_k(topic_bow, 10, name='topic_freq_bow').indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "input_token_idxs = t_variables['input_token_idxs']\n",
    "batch_l = t_variables['batch_l']\n",
    "sent_l = t_variables['sent_l']\n",
    "max_sent_l = tf.reduce_max(sent_l)\n",
    "\n",
    "with tf.variable_scope('sent/enc', reuse=False):\n",
    "    # get word embedding\n",
    "    enc_input = tf.nn.embedding_lookup(embeddings, input_token_idxs)\n",
    "\n",
    "    # get sentence embedding\n",
    "    _, enc_state = dynamic_bi_rnn(enc_input, sent_l, config.dim_hidden, t_variables['keep_prob'])\n",
    "\n",
    "    # TODO House Holder flow\n",
    "    hidden_topic_infer =  tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='hidden_topic_infer')(enc_state)\n",
    "    prob_topic_infer = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic_infer')(hidden_topic_infer)\n",
    "\n",
    "    w_mean_topic_infer = tf.get_variable('mean_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32)\n",
    "    b_mean_topic_infer = tf.get_variable('mean_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32)\n",
    "    means_topic_infer = tf.tensordot(enc_state, w_mean_topic_infer, axes=[[1], [1]]) + b_mean_topic_infer\n",
    "    \n",
    "    w_logvar_topic_infer = tf.get_variable('logvar_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    b_logvar_topic_infer = tf.get_variable('logvar_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    logvars_topic_infer = tf.tensordot(enc_state, w_logvar_topic_infer, axes=[[1], [1]]) + b_logvar_topic_infer\n",
    "    sigma_topic_infer = tf.exp(0.5 * logvars_topic_infer)\n",
    "    gauss_topic_infer = tfd.Normal(loc=means_topic_infer, scale=sigma_topic_infer)\n",
    "    \n",
    "    # latent vectors from each gaussian dist.\n",
    "    latents_topic_infer = sample_latents(means_topic_infer, logvars_topic_infer) \n",
    "    # latent vector from gaussian mixture\n",
    "    latents_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), latents_topic_infer, transpose_a=True)\n",
    "    \n",
    "    # for beam search\n",
    "    means_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), means_topic_infer, transpose_a=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for decoding\n",
    "dec_sent_l = tf.add(sent_l, 1)\n",
    "dec_input_idxs = t_variables['dec_input_idxs']\n",
    "dec_input = tf.nn.embedding_lookup(embeddings, dec_input_idxs)\n",
    "\n",
    "dec_latents_input = tf.tile(latents_input, [1, tf.shape(dec_input)[1], 1])\n",
    "dec_concat_input = tf.concat([dec_input, dec_latents_input], -1)\n",
    "\n",
    "# decode for training\n",
    "with tf.variable_scope('sent/dec/rnn', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32, reuse=False):\n",
    "    dec_cell = tf.contrib.rnn.GRUCell(config.dim_hidden)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob = t_variables['keep_prob'])\n",
    "\n",
    "    dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(latents_input, 1))\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_concat_input, sequence_length=dec_sent_l)\n",
    "\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=dec_cell,\n",
    "        helper=helper,\n",
    "        initial_state=dec_initial_state)\n",
    "\n",
    "    dec_outputs, _, output_sent_l = tf.contrib.seq2seq.dynamic_decode(train_decoder)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(config.n_vocab, use_bias=False, name='out')\n",
    "    output_logits = output_layer(dec_outputs.rnn_output)\n",
    "    \n",
    "    output_token_idxs = tf.argmax(output_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tokens = tf.fill([batch_l], config.BOS_IDX)\n",
    "end_token = config.EOS_IDX\n",
    "\n",
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    infer_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(means_input, 1))\n",
    "    beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(infer_dec_initial_state, multiplier=config.beam_width)\n",
    "    beam_latents_input = tf.contrib.seq2seq.tile_batch(tf.squeeze(means_input, 1), multiplier=config.beam_width) # added\n",
    "    \n",
    "    beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=beam_latents_input)\n",
    "\n",
    "    beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    beam_output_token_idxs = beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    inter_means_input = tf.placeholder(tf.float32, [None, config.dim_latent])\n",
    "    \n",
    "    inter_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(inter_means_input)\n",
    "    inter_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(inter_dec_initial_state, multiplier=config.beam_width)\n",
    "    inter_beam_latents_input = tf.contrib.seq2seq.tile_batch(inter_means_input, multiplier=config.beam_width) # added\n",
    "    \n",
    "    inter_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=inter_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=inter_beam_latents_input)\n",
    "\n",
    "    inter_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inter_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    inter_beam_output_token_idxs = inter_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    topic_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic)\n",
    "    topic_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(topic_dec_initial_state, multiplier=config.beam_width)\n",
    "    topic_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic, multiplier=config.beam_width) # added\n",
    "    \n",
    "    topic_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=topic_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=topic_beam_latents_input)\n",
    "\n",
    "    topic_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        topic_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    topic_beam_output_token_idxs = topic_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language modeling cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target and mask\n",
    "dec_target_idxs = t_variables['dec_target_idxs']\n",
    "dec_mask_tokens = tf.sequence_mask(dec_sent_l, maxlen=max_sent_l+1, dtype=tf.float32)\n",
    "\n",
    "# nll for each token (averaged over batch & sentence)\n",
    "sent_loss_recon = tf.contrib.seq2seq.sequence_loss(output_logits, dec_target_idxs, dec_mask_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_l = t_variables['doc_l']\n",
    "mask_sents = tf.sequence_mask(doc_l)\n",
    "mask_sents_flatten = tf.reshape(mask_sents, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1]])\n",
    "\n",
    "prob_topic_tiled = tf.tile(tf.expand_dims(prob_topic, 1), [1, tf.shape(mask_sents)[1], 1])\n",
    "prob_topic_flatten = tf.reshape(prob_topic_tiled, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1], config.n_topic])\n",
    "prob_topic_sents = tf.boolean_mask(prob_topic_flatten, mask_sents_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferred mixture probabilities (computed for each sentence)\n",
    "categ_topic_infer = tfd.Categorical(probs=prob_topic_infer)\n",
    "\n",
    "# prior of mixture probabilities (computed for each document, tiled for each sentence)\n",
    "categ_topic = tfd.Categorical(probs=prob_topic_sents)\n",
    "\n",
    "sent_loss_kl_categ = tf.reduce_mean(tfd.kl_divergence(categ_topic_infer, categ_topic))\n",
    "\n",
    "# inference of each gaussian gaussribution (computed for each sentence)\n",
    "\n",
    "sent_loss_kl_gauss = tf.reduce_sum(tfd.kl_divergence(gauss_topic_infer, gauss_topic), -1)\n",
    "sent_loss_kl_gmm = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss), -1))\n",
    "\n",
    "sent_loss_kl = sent_loss_kl_categ + sent_loss_kl_gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "tau = tf.cast(tf.divide(tf.mod(global_step, tf.constant(config.cycle_steps)), tf.constant(config.cycle_steps)), dtype=tf.float32)\n",
    "beta = tf.minimum(1., tau/config.r_cycle)\n",
    "\n",
    "sent_loss = sent_loss_recon + beta * sent_loss_kl\n",
    "\n",
    "topic_loss = topic_loss_recon + topic_loss_kl + config.reg * topic_loss_reg\n",
    "loss = topic_loss + sent_loss\n",
    "\n",
    "# define optimizer\n",
    "if config.opt == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif config.opt == 'Adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "    \n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_sents(token_idxs, config, idx_to_word):\n",
    "    sents = []\n",
    "    for sent_idxs in token_idxs:\n",
    "        tokens = []\n",
    "        for idx in sent_idxs:\n",
    "            if idx == config.EOS_IDX: break\n",
    "            tokens.append(idx_to_word[idx])\n",
    "        sent = ' '.join(tokens)\n",
    "        sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_batch, sent_loss_batch, ppls_batch = sess.run([loss, topic_loss, sent_loss, topic_ppls], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_batch, sent_loss_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "    loss_mean, topic_loss_mean, sent_loss_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    return loss_mean, topic_loss_mean, sent_loss_mean, ppl_mean\n",
    "\n",
    "def get_all_losses(sess, batches):\n",
    "    losses = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch = \\\n",
    "        sess.run([loss, topic_loss_recon, topic_loss_kl, sent_loss_recon, sent_loss_kl], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "    print('LOSS %.2f | TM NLL: %.2f, KL: %.4f | LM NLL: %.2f, KL: %.4f' %  np.mean(losses, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_batch):\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    pred_token_idxs = sess.run(output_token_idxs, feed_dict = feed_dict)\n",
    "    true_token_idxs = [sent_idxs for instance in sample_batch for sent_idxs in instance.token_idxs]\n",
    "    \n",
    "    assert len(pred_token_idxs) == len(true_token_idxs)\n",
    "    \n",
    "    pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "    true_sents = idxs_to_sents(true_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    for i, (true_sent, pred_sent) in enumerate(zip(true_sents, pred_sents)):        \n",
    "        print(i, 'True: %s' % true_sent)\n",
    "        print(i, 'Pred: %s' % pred_sent)\n",
    "\n",
    "def print_topic_sample():\n",
    "    pred_topics_freq_bow_indices, pred_topic_token_idxs = sess.run([topics_freq_bow_indices, topic_beam_output_token_idxs], \n",
    "                                                                                                           feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "    pred_topic_sents = idxs_to_sents(pred_topic_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Topic Samples-----------')\n",
    "    for i, (topic_freq_bow_idxs, pred_topic_sent) in enumerate(zip(topics_freq_bow_idxs, pred_topic_sents)):\n",
    "        print(i, ' bow:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' sent:', pred_topic_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "logs = []\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "saver = tf.train.Saver(max_to_keep=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "011[s], Ep: 00, Ct: 00000|TR LOSS: 376, PPL: 2660|TM NLL: 364, KL: 0.89, REG:0.90 | LM NLL: 10.34, KL: 1.47|DE LOSS: 348, PPL: 2656, TM: 338, LM: 10.34|BETA: 0.000139\n",
      "0 True: a federal judge is expected to <unk> a jury and hear opening arguments in the elizabeth smart kidnapping trial\n",
      "0 Pred: stop compel midafternoon darla carolinas turnover asbury asbury turnover asbury guidelines sandra asbury archivist cabe asbury turnover livable blackfoot asbury jessop priority asbury asbury nonpartisan rattner asbury asbury sandra asbury asbury turnover sandra turnover asbury turnover kxmc earned riskier asbury asbury\n",
      "1 True: brian david mitchell 's trial on kidnapping and unlawful transportation of a minor charges began monday in salt lake city 's u.s. district court\n",
      "1 Pred: tavarez ik auxier grice eufaula abrams june hofmeister revoking june dunlap harvest holt viacom harvest dunlap harvest prawdzik eli grice harvest ensign harvest harvest harvest harvest revoking harvest harvest prawdzik dunlap grice harvest harvest deductibles dunlap investigative harvest prawdzik harvest eufaula\n",
      "2 True: the trial is expected to last at least five weeks\n",
      "2 Pred: kaneohe kaneohe shorelines shorelines thrill occupancy contrasts lur blows uri shorelines uconn hhs kccr alpacas uri gosch kccr leyton thrill kccr constructed hhs uri uri uri uri sole uri uri leyton outdated uri supermajorities uri leyton leyton gosch leyton hhs silverstein\n",
      "3 True: if convicted , mitchell could spend the rest of his life in prison\n",
      "3 Pred: mum rescuers rescuers savoy bistro fist fist rescuers fist burbine minnick fist minnick encana minnick canfor fist vial pressing pressing insurgent fist hungry hungry fist benson fist fist fist studer fist nature mandalay condon hungry marred fist nature petroleum canfor fist\n",
      "4 True: u.s. district judge dale kimball has narrowed the jury pool to # people\n",
      "4 Pred: jos kema kema great kema great great great great anchorage great great great great businessman great loaned strategy metrics ranked great great accreditation ranked great great great barba great great great great great stagger great varnado great valleys tennant ranked roadblock\n",
      "5 True: he 's expected to seat # , plus two alternates to hear the case thursday morning\n",
      "5 Pred: noticeable clues clues laune consisted laune replacing necessary replacing helmet suitland defaulted o'toole wigs harkins confrontational abc suitland esso cartagena wigs defaulted yelled wigs mckinley newberry chimpanzees suitland abc wigs yelled yelled defaulted abc esso advanced overrun minneapolis suitland yelled yelled\n",
      "6 True: opening arguments will begin once a jury is sworn in\n",
      "6 Pred: wearing spotted dcf spurred duecker fahrenholtz mills unnecessarily breach breach duecker duecker chattahoochee mills duecker missteps ellwood missteps declined breach dumb duecker missteps breach sight breach permitting missteps breach bowles missteps duecker duecker wild missteps breach q duecker dobson thorough mcadams\n",
      "7 True: the first witness for the prosecution could also take the stand thursday\n",
      "7 Pred: teh soo maintains doomed soo doomed doomed doomed unesco doomed doomed doomed doomed doomed doomed doomed doomed maintains doomed wallet dighton doomed doomed doomed doomed doomed doomed doomed doomed doomed doomed doomed doomed doomed doomed doomed doomed doomed doomed solace doomed\n",
      "8 True: smart was # when she was kidnapped from her home\n",
      "8 Pred: ultimate valentin drisdel fong wimberly override neuroscience wimberly point notch wimberly alto notch wimberly wimberly huawei override kenny wimberly leno wimberly leno insufficient wimberly mcclendon insufficient kenny wimberly insufficient wimberly wimberly point kenny insufficient hometowns insufficient wimberly notch notch insufficient newton\n",
      "9 True: a jury has convicted a retired rhinelander police officer of beating and sexually assaulting a woman\n",
      "9 Pred: engel struggling sail sank shells abrasions shells abrasions abrasions waterman rousseff rousseff waterman rousseff rousseff overloaded rousseff rousseff lutz suicide dedeaux validates platforms abrasions celesio olympian elko rousseff waterman platforms tina lutz oakdale excellent shells rousseff rousseff merz goop hoem rousseff\n",
      "10 True: wisconsin attorney general j.b. van hollen says # year old greg derosier was convicted thursday of third degree sexual assault , battery , felony intimidation of a witness , two counts of felony bail jumping , and threatening a witness\n",
      "10 Pred: salinas asher komo westlands westlands westlands westlands westlands convict westlands westlands westlands westlands westlands westlands minot westlands westlands westlands westlands westlands westlands westlands westlands ywca westlands minot westlands westlands westlands westlands westlands westlands westlands westlands westlands westlands westlands westlands westlands minot\n",
      "11 True: the daily news ( http : <unk> # d ) reports derosier was acquitted on a second charge of sexual assault and two counts of intimidation of a victim\n",
      "11 Pred: loberg poised infinity debit debit debit debit debit debit debit debit debit debit debit debit debit debit debit debit listeners bedroom debit debit debit rhetoric debit reaches debit debit clairton freight discretionary bedroom reaches debit reaches debit duct debit paulina freight\n",
      "12 True: the jury deliberated for about # hours over two days\n",
      "12 Pred: breaches presided hall hall hall northern hall northern hall castle northern withstand alco teenagers hall hall spector northern analyzed northern northern castle alco castle joyce seawall northern seawall digit farris hall seawall hall hall hall revenues hall escondido withstand northern castle\n",
      "13 True: the woman told investigators derosier was at her house oct. # , # , became angry and beat her\n",
      "13 Pred: sandra garth wboy lumberton delegates montee interchange diplomacy carved unveil unveil lockers reinhart within darbeau lawyers stranded mecham beechcraft aquariums lockers lawyers sisters aquariums aquariums jars lawyers manageable manageable loancare lawyers mecham sudden friends mecham scottsboro claimed haynes knowles deborah battered\n",
      "14 True: she says derosier then ordered her to the bedroom where he sexually assaulted her\n",
      "14 Pred: winners audit ankara patented wsu heather ankara public kampachi rely dah shipley investmaryland shipley shipley erased dah santos recognition erased termites princeton ashford ludescher investmaryland dah scharf englewood ankara recognition erased conservatism tad dah ashford erased investmaryland frankfurt perennial ankara conservatism\n",
      "15 True: after the verdict , derosier 's bond was increased to $ # cash\n",
      "15 Pred: center traffic center traffic across exit essential exit humm exit dyson brandishing poizner essential highly faiella rotating greene essential rotating highly katahdin rotating greene greene pequot essential essential basilica yearly essential rotating marshfield rotating rotating brandishing keyser spokespeople yearly greene essential\n",
      "16 True: sentencing is set for jan. #\n",
      "16 Pred: distillery disease disease disease lawns baseless provoked least shrank room least trenton landslides shrank fidelity shrank trenton westlands morgan morgan morgan shrank reveals morgan fidelity shortly aib coli fidelity shrank travels practitioner reveals aib fidelity travels coli improbable practitioner fidelity landslides\n",
      "17 True: the surry county board of supervisors has approved permits that could bring the state 's largest coal fueled power plant there\n",
      "17 Pred: mix retained simulator brownville brownville simulator designed kern mahwah simulator mahwah discussing simulator manhattan simulator insurer kern lyric waldo waldo waldo waldo round liner underscore vibrant helmand simulator waldo susceptible knifed waldo mcallen simulator exams // waldo simulator waldo akin //\n",
      "18 True: the board thursday unanimously approved permits for about # acres outside dendron , where old dominion electric cooperative plans to build the $ # billion , # megawatt cypress creek power station\n",
      "18 Pred: hazelwood jamaican libyan danny termination termination baristas god termination first god termination hc bancshares termination spirits wisconsin adviser older collecting artificial random dealings unesco joanna references awoke termination termination evacuate references prevent callaway shadow clinging termination callaway bellied recognizing encrypted references\n",
      "19 True: while most of the project will be in dendron , a fly ash landfill and water intake facility would be built in the county\n",
      "19 Pred: thom erks shyler latitude startup reitz startup startup shotspotter boucher rehabilitating rehabilitating boucher restriction markets biotronik shotspotter crystal boucher markets memorial independent startup markets handing yearlong markets restriction markets enhanced markets markets confessed undersheriff lessard boucher yearlong markets undersheriff improprieties reitz\n",
      "20 True: cooperative spokesman jeb <unk> says the vote allows federal and state environmental agencies to start testing the site\n",
      "20 Pred: nobel raped noticed pesticides technicians couric scorching hostile ferry citations prussia towerstream lyons pankey ferry krueger couric inflict ferry krueger jurist sprinkled bolted //bit.ly/ couric tuscumbia couric bolted tuscumbia transform towerstream towerstream towerstream jurist ferry golan couric servitude towerstream rentals swahili\n",
      "21 True: dendron 's town council approved the project monday\n",
      "21 Pred: farming truckloads outskirts owing pickup monumental suspended certainty frankfurt frankfurt impairments frankfurt skinner nor'easter josephine poachers rosary sorghum legendary exported frankfurt frankfurt poachers frankfurt kmart frankfurt exported cords breslin importantly redesigned josephine expires subpoenaed exported josephine sorghum frankfurt frankfurt poachers sorghum\n",
      "22 True: opponents say the plant 's emissions would harm the environment and people 's health\n",
      "22 Pred: filmmaker rejoin rejoin ehlers rup bocelli jerald public dontae maloney bocelli public confessed leesburg public public mccallum resumption mccallum sprint ceglia restraining eli rollins eli rollins eli outperform rollins barriers mccallum confessed evacuate eli mccallum rats eli rollins mccallum stacey confessed\n",
      "23 True: supporters say the area needs new jobs and tax revenue\n",
      "23 Pred: chapter revived mahwah mahwah dismissed dismissed mahwah dismissed evolution mahwah dismissed dismissed silencer poudre mahwah roadless retreated dismissed tuhn mahwah dismissed chukchi attleboro ranked margarita manual distributed distributed distributed teeters dismissed cmdr dismissed homes chukchi thruway manual pike chukchi distributed margarita\n",
      "24 True: a former bank teller convicted for masterminding a series of bank robberies in suburbs west of phoenix is going to federal prison\n",
      "24 Pred: indefinitely events events playland shelton hartsfield solyndra events handicap hartsfield schubert hartsfield ennis anticline developmentally hawes balked roofing anticline anticline visionary clemente anticline grad eldest hartsfield grad accompanied eldest grad eldest eldest anticline hartsfield eldest hartsfield debolt geo roofing expenses roofing\n",
      "25 True: the u.s. attorney 's office in arizona says # year old joel leon thomas , jr. , of el mirage , was sentenced wednesday to #\n",
      "25 Pred: metrodome silva dalles lang crocs spray lang lang canyons deductions overloaded deductions deductions deductions filed breeder deductions breeder overpopulation filed deductions mideast deductions deductions breeder breeder filed breeder filed breeder breeder christenson filed mideast filed deductions misguided filed filed canyons month\n",
      "26 True: years\n",
      "26 Pred: bullying ongoing fielding architect rear idol murtha misapplication marital sighted testimony awesome sims wondering headlined grady mark mark brianna mark samia johnnie prosecutors brianna murtha elko had schmoll sims testimony mark sims raping governmental mark headlined renault michoacan neo lewd mark\n",
      "27 True: a jury found thomas guilty of eight charges , including three counts of conspiracy to commit bank robbery\n",
      "27 Pred: ousted hearing medically garvin garvin kristen wyly illness blanketed roland garvin au garvin garvin tabech jobs disbanded roland stilts throws cites yun folded cortes perceptions jobs yun kalin tabech roland roland jobs tabech wept hospital shim cites yun represented bushnell jobs\n",
      "28 True: thomas was a bank teller at three banks that were robbed between january and february # in peoria , surprise and sun city\n",
      "28 Pred: cody caledonia ackerman handbags veterinary named veterinary eisenhower named eisenhower veterinary schuster schuster schuster named shattered solans schuster named schuster schuster wagner schuster cullen named schuster carr schuster downward afford maury baar schuster schuster named schuster schuster eisenhower recalls schuster schuster\n",
      "29 True: prosecutors say evidence showed that thomas gave inside information to five others who have since pleaded guilty in the conspiracy\n",
      "29 Pred: wheelers attachment plc michel hearing donnelly mourdock mourdock mourdock upshaw hoku upshaw mourdock latches eastman mourdock mourdock mourdock strategically upshaw cranley upshaw mourdock upshaw upshaw mourdock doj mourdock upshaw mourdock cranley mourdock mourdock upshaw upshaw upshaw mourdock upshaw upshaw upshaw upshaw\n",
      "30 True: they say police seized $ # in stolen bank money , <unk> of the banks and other evidence from thomas ' home and car\n",
      "30 Pred: chapter parkland apiece decide bargained links haneke lena turnpike transgender falashmura picket symbolize season season philip mainly symbolize springleaf johansen season season brochures rabobank johansen argenta horton colclough turnpike motorists motorists motorists springleaf brochures proclaimed motorists season brochures rabobank rabobank minnick\n",
      "31 True: the fbi plans to announce dozens of arrests from a major four year investigation into corruption in eastern arkansas\n",
      "31 Pred: roofing arabia false jack tailgate folic ia jack minnesotans folic sang jack ramped vulcan homeward folic longmont month folic month folic month ogilvie ia kmov folic folic folic ia folic abbey envision ia hacked month clovis ogilvie minnesotans composition ameridose folic\n",
      "32 True: authorities have called a # p.m. press conference in helena to announce the results of the investigation\n",
      "32 Pred: antero twist notable notable wzzm ysidro morvant tiffin possibility possibility img possibility possibility methamphetamine morvant montoya morvant methamphetamine fragments ek raytheon fragments possibility fragments fragments fragments ek morvant hancock morvant fragments qc blower control morvant ek morvant possibility fragments nanoscience possibility\n",
      "33 True: fbi supervisory special agent jason pack tells the associated press that dozens of people were arrested tuesday morning , with more indicted\n",
      "33 Pred: forbes yankee kenny barreled kenny agrees ada kenny agrees oracle kenny oracle named pagosa agrees copyright lucas scalera mariah denish oracle flaws manos agrees named kenny rescues oracle agrees agrees agrees jewel denish agrees agrees copyright agrees agrees scalera agrees named\n",
      "34 True: five law enforcement officers were among those arrested\n",
      "34 Pred: kills herdman advance escribano escribano centreville escribano mcintyre devon centreville kopp caltrain coerce spelled montreal montreal z. booker montreal spelled dedicated devon z. orchards booker hiding staggs koa booker spelled kopp reopen mulling orchards quotes booker z. demaio z. spelled z.\n",
      "35 True: those indicted in the investigation are expected to appear in federal court in little rock on thursday\n",
      "35 Pred: wexford iced remain asu remain water doughnuts molloy dealt windham doughnuts hollenbach ontario christy baa , traminette dramatically dealt doughnuts sb normandy martinez christy dramatically doughnuts texans dramatically named kingsford ontario walnut dramatically walnut examines walnut ontario kingsford equipment modernizing biting\n",
      "36 True: peck says authorities continue to conduct search warrants in eastern arkansas\n",
      "36 Pred: nanticoke hungarian locate southdown esso saltzman coffin turlock snowmobile southdown romantically lights southdown discussing southdown gazprom gazprom wilbur implied southdown southdown southdown southdown wilbur turlock esso southdown southdown implied cornman implied company southdown southdown implied romantically esso southdown southdown deselms southdown\n",
      "37 True: he declined to specify what the nature of the charges was in advance of the press conference\n",
      "37 Pred: suitcase duey jamaican jamaican greenbrier formulas rested bundle paintings saranac basis barela ernaut ordained bundle paintings rested pony makarowsky revoking bundle bundle glennon makarowsky views bies aubin rested barefoot bundle paintings alongside makarowsky begin bies makarowsky alongside bundle graphite paintings katahdin\n",
      "38 True: yellowstone national park is gradually closing down after the busiest summer season on record\n",
      "38 Pred: bothell lennar kindergartners jd portsmouth kenny notations notations kenny portsmouth notations notations kenny notations mario cialis profane confederacy kenny frightened notations frightened conlon gardens frightened crimean skywheel consumers biologic villarreal notations ojibwe horseback ojibwe villarreal smothered kenny crimean cataquet mclaughlin snapping\n",
      "39 True: all facilities at lake , fishing bridge , <unk> and grant village have closed for the season except for pay at the pump gas stations\n",
      "39 Pred: ow hutchins hutchins huge republican notifications notifications ran published hagarty hagarty rohrer notifications notifications notifications published ty hagarty kcrg notifications kow content veteran barbra notifications veteran veteran elective hagarty content salibi published notifications bent hagarty veteran notifications hagarty notifications firearms notifications\n",
      "40 True: the hotel at mammoth hot springs closes monday\n",
      "40 Pred: carvings madeley consortium breeder welcomed rindge rindge exclusive daneen scottsboro smalls welcomed rindge exclusive tri consortium tri unveil welcomed welcomed severely welcomed underutilized scottsboro welcomed tangible daneen circle tri circle tangible scottsboro pam tangible smalls tri circle daneen rindge daneen circle\n",
      "41 True: the old faithful inn closes oct. # , followed by the old faithful snow lodge on oct. #\n",
      "41 Pred: thief channel disagrees elca shadowy elca margarine derry burroughs elca reformer elca barriers circumstantial loeffelholz elca d.c. reformer hardie d.c. sailor spew resolute loeffelholz sunday laden rouse hardie bradford lockers reformer wesson deny spew elca loeffelholz bet hardie footprint wesson resolute\n",
      "42 True: remaining facilities close for the season on nov. #\n",
      "42 Pred: replicate replicate cinematic trustees killing killing trustees menzel favor favor landry favor musicals favor favor favor nutt snooping cautious musicals produced favor escobar favor favor musicals favor favor favor macho macho favor favor favor favor favor favor favor favor favor favor\n",
      "43 True: yellowstone national park has had # million visitors so far this year , already making it a record year for park visitation\n",
      "43 Pred: mandarin hearing flashpoint passer pfizer linamar pfizer cooportunity pfizer every rodrigues henson kurnik gold kurtz discussing cooportunity henson pfizer uss corral pfizer pfizer corral pfizer pfizer uss corral knospler corral pfizer buyers cooportunity circumstances pfizer pfizer corral pfizer cooportunity pockets rodrigues\n",
      "44 True: first time claims for unemployment insurance in louisiana for the week ending oct. # decreased from the previous week 's total\n",
      "44 Pred: kristian uniforms uniforms dapolito dapolito ute hawes rates suspiciously nie realignment sort suspiciously realignment pardon hoyas chad runways suspiciously espinola hawes suspiciously hoyas suspiciously hoyas pardon reducing frisby suspiciously runways suspiciously rates clicking frisby suspiciously suspiciously pardon processes mckoon mcpherson hawes\n",
      "45 True: state labor department figures released friday show initial claims decreased to # from the previous week 's total of #\n",
      "45 Pred: tell center keon unveil hits scattered placements suggestion mann placements placements moss indirectly placements moss bonne moss bonne moss isaacs isaacs placements bonne keyser varnado mann chapel bonne scattered chapel placements bonne katahdin apnea keyser bonne placements isaacs placements chapel bonne\n",
      "46 True: the figure was lower than it was for the comparable week a year ago , when there were # claims\n",
      "46 Pred: festival mayor giraffes improves illarramendi ankle expenses expenses expenses mclaughlin expenses disgusted holzer expenses boring expenses pox brewing ankle boring expenses actors factor expenses actors boas actors ridicule hashish actors expenses wilbanks ankle employers christenson administration mark hashish boring hashish gogh\n",
      "47 True: the four week moving average , which is a less volatile measure of claims , decreased to # from the previous week 's average of #\n",
      "47 Pred: dynamic sarno mountains orly warwick bused lankan gondolas lunas lunas holt itself volusia any ordering scarce spousal beachfront auxier eliminates fate built ordering volusia kush built eliminates katahdin comb comb beachfront fate bused jaclyn jaclyn addition fate beachfront jaclyn lankan dennis\n",
      "48 True: continued unemployment claims claimed for the week ending oct. # totaled # , an increase from # the previous week\n",
      "48 Pred: bumped t.f operator downgrading excitement operator bridal approvals kuster paralyzed groggins approvals circumference motivational circumference holocaust approvals approvals handbags groggins measuring placid trenton statue outweigh component groggins motivational experiment groggins experiment approvals groggins motivational approvals measuring windmill motivational guardrails ashford president\n",
      "49 True: the four week moving average for such claims fell to # from the previous week 's average of #\n",
      "49 Pred: blas unorganized unorganized spaulding desmond discounts dontae dontae copyright flood stow valleys cease olivier olivier valleys copyright demco flood pagosa flood copyright dontae dontae olivier urgently exclusively dontae chipping apps valleys insurer olivier spaulding adjourn olivier incubation flood duh tort flood\n",
      "50 True: maine 's first lady is taking part in a national effort to break the world record for the largest shared reading experience\n",
      "50 Pred: zuckerberg mobile clive ankara urquhart particular baucus preventive unloading truman preventive contingent contingent contingent vending traps traps traps merle traps wrought extramarital perce vending excavated labor traps traps roxbury vending kctv gooch feud refused kctv vending vending kctv contingent kctv kctv\n",
      "51 True: ann lepage will read to more than # second graders at the <unk> elementary school in lewiston and then more than # second graders at <unk> j. gilbert school in augusta on thursday\n",
      "51 Pred: retiring jp savers sarno jp sisak sarno jp delauro sisak sisak sisak dcs pike sisak sisak fierce xbox xbox sisak sisak sisak sisak pike sisak wilking pike packet overcorrected latterner packet trenton sisak trenton sisak jp sisak sisak causevic causevic jp\n",
      "52 True: lepage is participating in jumpstart 's `` read for the record , `` an annual campaign that seeks to promote literacy and early childhood education\n",
      "52 Pred: bangor instrument instrument instrument ashford silverstein ehl underutilized underutilized kaibab underutilized loose underutilized loose greed exam underutilized underutilized tangible greed impacting carla aliayah tangible seize underutilized underutilized pies underutilized loose bartow underutilized loose brownville greed underutilized underutilized multistate underutilized newhouse saranac\n",
      "53 True: the group seeks to break the world record for the most people nationwide reading the same book on the same day\n",
      "53 Pred: housed housed robotic aldinger aldinger robotic aldinger stenger integrity ariz. aspirin robotic integrity tape prior torgerson chesapeake chesapeake tape scrapped chesapeake lamotte chesapeake tape cornyn tape lamotte dove goop detain clerk anyway dimasi lamotte anyway dove bogalusa robotic moo lamotte tape\n",
      "54 True: this year , people will read `` otis `` by loren long\n",
      "54 Pred: patchwork naylor worries fighting keene marigny stays keene unaccounted keci baltierra options keci trevisan severe pig sohler keene extinct imprisoned trevisan imprisoned germans keene american trevisan keene customary trevisan baltierra roe trevisan presided trevisan trevisan trevisan redraws keene keene trevisan bald\n",
      "55 True: lepage says if children learn to read , they can accomplish anything and become a life long learner\n",
      "55 Pred: stickney nstar nstar hone blues gumby blues decreases mills vacated donate donate drummer mistreatment donate mistreatment dubbed necessary donate drummer baltimore trim drummer donate drummer drummer hopewell jordanian trim donate cue donate donate mills drummer donate drummer worlds drummer donate drummer\n",
      "56 True: a line of women wrapped around a loveland bridal store on sunday for its annual wedding dress giveaway\n",
      "56 Pred: cricco bedlion willman during unmarked betsy penton forgiveness betsy leesburg collings leesburg leesburg leesburg collings pei collings collings collings collings unmarked clifton collings collings collings leesburg collings leesburg vial hal collings collings collings breaks collings collings collings collings leesburg collings ma\n",
      "57 True: the loveland wedding center is continuing a tradition to help <unk> as part of a longstanding effort to help others\n",
      "57 Pred: cuomo barriers cuomo reaped playland intensified playland aspirin intensified perroni aspirin aspirin aspirin slowly stagger perroni dongor aspirin aspirin intensified kfvs ramifications voluntarily devito aspirin intensified photographer devito tyson intensified boozman devito advisor absentia potsdam perroni boozman dongor cornyn potsdam absentia\n",
      "58 True: the owners got the idea from another store\n",
      "58 Pred: f jammed coating lemaster coating huawei kenny kenny traci coating perspective groped vogtle almond seaton informed hunger eakin luff kenny groped pump groped gardena groped seaton differs eakin groped static packer eakin expected huawei groped hoosiers wbns eakin cp wbns rigged\n",
      "59 True: according to the loveland reporter herald ( http : <unk> # z ) , the dresses are given out on a first come , first served basis with no proof of financial need required\n",
      "59 Pred: statesville cyprus ryburn winfrey winfrey weed roof morrison storefront weed janetski janetski hollyhock predatory weed leventis roof predatory cargile weed expand heiman janetski janetski predatory roof finally hatchet weed janetski heiman pathways heiman heiman zappala janetski weed janetski janetski janetski janetski\n",
      "60 True: any <unk> to the dresses will be handled at cost\n",
      "60 Pred: disc experiencing cebrun equestrian iced iced iced iced greenleaf ct iced swarm equestrian morning iced iced jointly jointly hov equestrian jointly kuna iced avenues iced classmate iced equestrian kuna iced kuna loose iced loose memorial iced pathologist equestrian dennis jointly iced\n",
      "61 True: regulators have shut down a small bank in georgia , lifting to # the number of u.s. banks to fail this year as the industry has struggled to cope with mounting loan defaults and recession\n",
      "61 Pred: saves neutralize succeeds replicate defeated household seitz household defeated greatly defeated household household skowhegan saturation seitz kisby household double kisby household forging seitz offerings saturation defeated household lyn offerings seitz healthiest seitz defeated richard richard household household richard richard saturation offerings\n",
      "62 True: the federal deposit insurance corp. on friday took over northwest bank and trust , based in acworth , ga. , with $ # million in assets\n",
      "62 Pred: crashing cashing passmore cashing griswold capshaw accomplish piringer memo winding accomplish introduce vibrant idled juror pumpkin traps albert cottonwood tape traps traps traps sandbar enforcer caldera traps cedric labor traps stalsby traps rae stalsby overspending barfield traps labor cottonwood hampstead highland\n",
      "63 True: state bank and trust co. , based in macon , ga. , agreed to assume the assets and deposits of the failed bank\n",
      "63 Pred: lie concept hollande reindeer corzine stockpiles sipapu rohatinski rohatinski rendition tarleton tarleton rohatinski niki attributes cornelius heels kelly tarleton rohatinski tarleton jars edith imminent kelly rohatinski platforms jeh helen attributes upgrading attributes edith tarleton attributes attributes misled rohatinski attributes rohatinski rohatinski\n",
      "64 True: the failure of northwest bank and trust is expected to cost the deposit insurance fund $ # million\n",
      "64 Pred: //automatedinsights.com/ap j.l emmer statesville spot j.l rawlings j.l accessible j.l alcoholic j.l j.l j.l j.l j.l nut holt projected j.l railings j.l anton j.l alcoholic j.l alcoholic j.l earl projected projected bathon nut accessible rawlings chihuahua projected j.l projected railings j.l\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Topic Samples-----------\n",
      "0  bow: investigation press arkansas indicted authorities dozens arrested eastern conference announce\n",
      "0  sent: continually gash calvert nbc nbc crapser rikers kiefer ultra ultra ultra handmade clearly clearly clearly clearly clearly clearly clearly clearly tides smriga tides smithsonian belanger belanger belanger heckler heckler landowners landowners jonas jonas jonas rehab purdon space space lavish torgerson\n",
      "1  bow: bank million insurance jury counts attempting sun markets shortly providing\n",
      "1  sent: continually gash calvert nbc nbc crapser rikers kiefer ultra ultra ultra handmade clearly clearly clearly clearly clearly clearly clearly clearly tides smriga tides smithsonian belanger belanger belanger heckler heckler landowners landowners jonas jonas jonas rehab purdon space space lavish torgerson\n",
      "2  bow: record jury press facilities investigation conference arrested expected oct. thomas\n",
      "2  sent: continually gash calvert nbc nbc crapser rikers kiefer ultra ultra ultra handmade clearly clearly clearly clearly clearly clearly clearly clearly tides smriga tides smithsonian belanger belanger belanger heckler heckler landowners landowners jonas jonas jonas rehab purdon space space lavish torgerson\n",
      "3  bow: bank thomas read conspiracy baltimore jury speech february commit federal\n",
      "3  sent: continually gash calvert nbc nbc crapser rikers kiefer ultra ultra ultra handmade clearly clearly clearly clearly clearly clearly clearly clearly tides smriga tides smithsonian belanger belanger belanger heckler heckler landowners landowners jonas jonas jonas rehab purdon space space lavish torgerson\n",
      "4  bow: week claims previous bank average park thomas friday moving ending\n",
      "4  sent: continually gash calvert nbc nbc crapser rikers kiefer ultra ultra ultra handmade clearly clearly clearly clearly clearly clearly clearly clearly tides smriga tides smithsonian belanger belanger belanger heckler heckler landowners landowners jonas jonas jonas rehab purdon space space lavish torgerson\n",
      "5  bow: bank week expected agriculture project wide conference reduced young order\n",
      "5  sent: continually gash calvert nbc nbc crapser rikers kiefer ultra ultra ultra handmade clearly clearly clearly clearly clearly clearly clearly clearly tides smriga tides smithsonian belanger belanger belanger heckler heckler landowners landowners jonas jonas jonas rehab purdon space space lavish torgerson\n",
      "6  bow: read bank record people break world reading seeks trust early\n",
      "6  sent: continually gash calvert nbc nbc crapser rikers kiefer ultra ultra ultra handmade clearly clearly clearly clearly clearly clearly clearly clearly tides smriga tides smithsonian belanger belanger belanger heckler heckler landowners landowners jonas jonas jonas rehab purdon space space lavish torgerson\n",
      "7  bow: week jury thursday people federal expected approved power trial board\n",
      "7  sent: continually gash calvert nbc nbc crapser rikers kiefer ultra ultra ultra handmade clearly clearly clearly clearly clearly clearly clearly clearly tides smriga tides smithsonian belanger belanger belanger heckler heckler landowners landowners jonas jonas jonas rehab purdon space space lavish torgerson\n",
      "8  bow: week oct. claims mobile wayne agreement kenneth threw continued twenty\n",
      "8  sent: continually gash calvert nbc nbc crapser rikers kiefer ultra ultra ultra handmade clearly clearly clearly clearly clearly clearly clearly clearly tides smriga tides smithsonian belanger belanger belanger heckler heckler landowners landowners jonas jonas jonas rehab purdon space space lavish torgerson\n",
      "9  bow: bank thomas counts guilty found jury banks evidence robbed years\n",
      "9  sent: continually gash calvert nbc nbc crapser rikers kiefer ultra ultra ultra handmade clearly clearly clearly clearly clearly clearly clearly clearly tides smriga tides smithsonian belanger belanger belanger heckler heckler landowners landowners jonas jonas jonas rehab purdon space space lavish torgerson\n"
     ]
    }
   ],
   "source": [
    "if len(logs) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "while epoch < config.epochs:\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = get_feed_dict(batch)\n",
    "\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, sent_loss_kl_categ_batch, sent_loss_kl_gmm_batch, ppls_batch = \\\n",
    "        sess.run([opt, loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, sent_loss_kl_categ, sent_loss_kl_gmm, topic_ppls], feed_dict = feed_dict)\n",
    "   \n",
    "        if sent_loss_kl_batch == np.inf:\n",
    "            print('Nan occured')\n",
    "            ckpt = tf.train.get_checkpoint_state(config.modeldir)\n",
    "            model_checkpoint_path = ckpt.all_model_checkpoint_paths[-1]\n",
    "            saver.restore(sess, model_checkpoint_path)            \n",
    "            break\n",
    "            \n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if ct%config.log_period==0:\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train, sent_loss_recon_train, sent_loss_kl_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_dev, sent_loss_dev, ppl_dev = get_loss(sess, dev_batches)\n",
    "\n",
    "            if config.warmup: beta_eval = beta.eval(session=sess)\n",
    "            global_step_log = sess.run(tf.train.get_global_step())            \n",
    "            \n",
    "#             if loss_dev < loss_min:\n",
    "#                 loss_min = loss_dev\n",
    "#                 saver.save(sess, config.modelpath, global_step=global_step_log)\n",
    "\n",
    "            clear_output()\n",
    "\n",
    "            time_finish = time.time()\n",
    "            time_log = int(time_finish - time_start)\n",
    "            logs += [(time_log, epoch, ct, loss_train, ppl_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train, sent_loss_recon_train, sent_loss_kl_train, loss_dev, ppl_dev, topic_loss_dev, sent_loss_dev, beta_eval)]\n",
    "            for log in logs:\n",
    "                print('%03d[s], Ep: %02d, Ct: %05d|TR LOSS: %.0f, PPL: %.0f|TM NLL: %.0f, KL: %.2f, REG:%.2f | LM NLL: %.2f, KL: %.2f|DE LOSS: %.0f, PPL: %.0f, TM: %.0f, LM: %.2f|BETA: %.6f' %  log)\n",
    "\n",
    "            print_sample(batch)\n",
    "\n",
    "            time_start = time.time()\n",
    "            \n",
    "            print_topic_sample()\n",
    "                \n",
    "    epoch += 1\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "_prob_topic, _prob_topic_sents, _prob_topic_infer, _means_topic_infer = debug_value([prob_topic, prob_topic_sents, prob_topic_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.2281889e-02, 6.7835855e-03, 2.0132679e-01, 1.1674699e-02,\n",
       "        6.5264981e-03, 1.1411838e-02, 6.9189700e-03, 2.8612482e-04,\n",
       "        7.4042326e-01, 2.3663496e-03], dtype=float32),\n",
       " array([0.09889801, 0.11335436, 0.12141278, 0.10027827, 0.13508864,\n",
       "        0.09455997, 0.0860846 , 0.08042429, 0.09635323, 0.07354581],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_i = 4\n",
    "_prob_topic_sents[batch_i], _prob_topic_infer[batch_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02824139, -0.22793637,  0.3788033 , -0.02814816],\n",
       "       [ 0.06249218, -0.08085692,  0.06756439,  0.2259818 ],\n",
       "       [-0.1377574 ,  0.28185233,  0.23656711, -0.3201025 ],\n",
       "       [-0.03711405,  0.16953555,  0.16389738, -0.16461828],\n",
       "       [ 0.27302447, -0.22462857,  0.37200606, -0.00549014],\n",
       "       [-0.16755986,  0.11964113,  0.17052333,  0.18083367],\n",
       "       [ 0.26351342, -0.33828875,  0.3920058 , -0.17029142],\n",
       "       [ 0.21033517, -0.1955087 ,  0.20963705, -0.24809986],\n",
       "       [ 0.01922028, -0.03185754,  0.28022933, -0.13928899],\n",
       "       [-0.26817918, -0.3364467 ,  0.36924574,  0.15818927]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_means_topic_infer[0][:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_means_topic, b_means_topic = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"topic/dec/mean_topic\")\n",
    "\n",
    "pred_topic_embeddings, pred_topic_bow, pred_means_topic, pred_logvars_topic, pred_token_idxs, _w_means_topic, _b_means_topic, _w_mean_topic_infer = \\\n",
    "                                sess.run([topic_embeddings, topic_bow, means_topic, logvars_topic, topic_beam_output_token_idxs, w_means_topic, b_means_topic, w_mean_topic_infer], \n",
    "                                         feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "\n",
    "pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "\n",
    "pred_topics_freq_bow_indices = np.argsort(pred_topic_bow, 1)[:, ::-1][:, :10]\n",
    "pred_topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['million', 'http', 'years', 'tuesday', 'federal', 'u.s.', 'month', 'office', 'people', 'monday']\n",
      "['reports', 'department', 'thursday', 'court', 'time', 'u.s.', 'years', 'monday', 'tuesday', \"'\"]\n",
      "['school', 'public', 'u.s.', 'board', 'university', 'federal', 'president', 'program', 'health', 'students']\n",
      "['fire', 'found', 'authorities', 'hospital', 'home', 'a.m.', 'dead', 'investigation', 'chief', 'morning']\n",
      "['service', 'national', 'department', 'http', 'water', 'park', 'area', 'center', 'wednesday', 'weather']\n",
      "['percent', 'company', 'million', 'billion', 'shares', 'inc.', 'revenue', 'average', 'cents', 'price']\n",
      "['design', 'researchers', 'donations', 'prize', 'encourage', 'peak', 'awards', 'feature', 'chapter', 'affect']\n",
      "['court', 'arrested', 'charged', 'attorney', 'death', 'authorities', 'found', 'prison', 'shooting', 'woman']\n",
      "['bill', 'senate', 'house', 'republican', 'committee', 'public', 'board', 'approved', 'u.s.', 'law']\n",
      "['todd', 'rear', 'drowned', 'confronted', 'foul', 'transported', 'assaulted', 'loaded', 'defender', 'unidentified']\n"
     ]
    }
   ],
   "source": [
    "for idxs in pred_topics_freq_bow_idxs:\n",
    "    print([idx_to_word[idx] for idx in idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09207956,  0.09077395,  0.06120839, -0.22176096,  0.19921999,\n",
       "        -0.05485373, -0.23069587,  0.05650459,  0.142239  , -0.17410392],\n",
       "       [ 0.29974467, -0.03281972,  0.25064805, -0.2893645 , -0.1312135 ,\n",
       "        -0.11136644,  0.02060958, -0.13598587,  0.24592783, -0.18463418],\n",
       "       [ 0.22819577,  0.14566512,  0.05597903, -0.1109414 , -0.00879419,\n",
       "        -0.1605561 ,  0.11445126,  0.00916692, -0.04847115, -0.0603605 ],\n",
       "       [ 0.3365141 , -0.06439152,  0.3631938 ,  0.14203475,  0.10052318,\n",
       "        -0.09119357, -0.08768138, -0.28562906,  0.3186355 , -0.08452626],\n",
       "       [ 0.26727033,  0.03540339,  0.28315246, -0.1286456 , -0.02444943,\n",
       "        -0.163507  , -0.02146725, -0.21729966,  0.26542294, -0.1589102 ],\n",
       "       [ 0.28826687, -0.18438952,  0.2760168 , -0.12692557,  0.05485712,\n",
       "         0.17983724,  0.14970489, -0.36168167,  0.30563408, -0.13128197],\n",
       "       [-0.32988435, -0.11005852, -0.27013943,  0.26166642, -0.3013388 ,\n",
       "        -0.2234488 ,  0.02287463,  0.20320751, -0.10510623,  0.28636736],\n",
       "       [ 0.37211177, -0.00585993,  0.2853141 , -0.05771174,  0.15170154,\n",
       "         0.05411884, -0.24091715, -0.01475959, -0.0408047 , -0.17167823],\n",
       "       [ 0.20512433,  0.18697639, -0.08326819,  0.03792854,  0.03214281,\n",
       "         0.05618311,  0.09544107,  0.03913466, -0.06514693, -0.04000437],\n",
       "       [-0.3745221 , -0.06385167, -0.29924905,  0.08158813, -0.28344202,\n",
       "        -0.01799835,  0.02471407,  0.408938  , -0.169172  ,  0.37273586]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_topic_embeddings[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0327697e-03, 4.6811462e-03, 1.7024261e-04, ..., 3.3795869e-04,\n",
       "        1.5871278e-04, 6.6570101e-05],\n",
       "       [5.9443718e-04, 7.4376534e-03, 2.0111131e-04, ..., 7.4048399e-04,\n",
       "        1.5028552e-04, 8.9953668e-05],\n",
       "       [8.5580017e-04, 2.8901210e-03, 1.6464434e-04, ..., 3.1770146e-04,\n",
       "        1.9349743e-04, 3.0012103e-04],\n",
       "       ...,\n",
       "       [1.3188391e-04, 2.6697975e-03, 2.3009101e-04, ..., 8.1575691e-04,\n",
       "        1.5104198e-04, 3.2140684e-05],\n",
       "       [3.8558390e-04, 4.0810513e-03, 1.4347673e-04, ..., 1.4157601e-04,\n",
       "        1.8342295e-04, 4.7484271e-05],\n",
       "       [7.1679087e-06, 2.9757638e-07, 1.8662996e-04, ..., 5.9841594e-05,\n",
       "        1.7972868e-04, 5.0798093e-04]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_topic_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09637171, -0.09069463,  0.09532599, ..., -0.02989542,\n",
       "        -0.02514938,  0.01415634],\n",
       "       [-0.02764424,  0.01254364,  0.03703775, ..., -0.01512817,\n",
       "         0.02590305,  0.01404281],\n",
       "       [-0.08408152, -0.00924051, -0.08365072, ...,  0.05623364,\n",
       "        -0.05008389,  0.0209127 ],\n",
       "       ...,\n",
       "       [-0.00629492, -0.05131948, -0.05206006, ..., -0.0955721 ,\n",
       "        -0.01980347, -0.02646225],\n",
       "       [-0.00839109, -0.05062511, -0.05731497, ...,  0.07618995,\n",
       "         0.03893617,  0.06835916],\n",
       "       [ 0.07883421, -0.02804748,  0.09326512, ..., -0.0734218 ,\n",
       "         0.0045918 , -0.10043538]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_w_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03207342, -0.08436021,  0.36105958, -0.02925379, -0.26779142,\n",
       "       -0.4809294 , -0.04713235, -0.6384421 ,  0.49655244, -0.18943405,\n",
       "        0.32139724,  0.00376519, -0.02733823,  0.06968141,  0.12723993,\n",
       "        0.17086434,  0.4321125 ,  0.2930171 ,  0.43750855, -0.32728267,\n",
       "       -0.29320472,  0.46633536,  0.03467208, -0.11661331,  0.02106199,\n",
       "        0.10828511,  0.14413024,  0.00801178, -0.16259423,  0.31461757,\n",
       "       -0.25328413,  0.11320477], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03563995, -0.3379975 ,  1.6953684 , -0.3394102 , -1.3313038 ,\n",
       "        -1.9561913 , -0.34439525, -3.1041431 ,  2.302101  , -0.80340064,\n",
       "         1.374892  ,  0.10505254,  0.13142577,  0.36230466,  0.5036737 ,\n",
       "         0.6154421 ,  1.8879794 ,  1.263777  ,  2.0281937 , -1.4289141 ,\n",
       "        -1.3110695 ,  2.0390706 ,  0.32288784, -0.39520237,  0.10320839,\n",
       "         0.35608286,  0.61754334, -0.1438482 , -0.74413097,  1.4731948 ,\n",
       "        -1.1249324 ,  0.33894244],\n",
       "       [ 0.03510073, -0.3374708 ,  1.6888778 , -0.3363807 , -1.3308815 ,\n",
       "        -1.9608293 , -0.34732953, -3.1081154 ,  2.3005512 , -0.80602133,\n",
       "         1.3695524 ,  0.10256523,  0.13283278,  0.36182955,  0.49822897,\n",
       "         0.6213894 ,  1.8883276 ,  1.266403  ,  2.023539  , -1.426166  ,\n",
       "        -1.3108103 ,  2.0448976 ,  0.32430837, -0.38903922,  0.10560946,\n",
       "         0.3596735 ,  0.6160856 , -0.14840832, -0.74532497,  1.4738762 ,\n",
       "        -1.1249926 ,  0.33331037],\n",
       "       [ 0.03490497, -0.33676392,  1.6840277 , -0.335459  , -1.3295547 ,\n",
       "        -1.95664   , -0.3462094 , -3.1003447 ,  2.296549  , -0.80408335,\n",
       "         1.367739  ,  0.10281808,  0.13425133,  0.36240244,  0.4972452 ,\n",
       "         0.6206323 ,  1.885982  ,  1.2645416 ,  2.0215812 , -1.4212072 ,\n",
       "        -1.3081043 ,  2.0386996 ,  0.32136053, -0.39039296,  0.1050597 ,\n",
       "         0.36023104,  0.6137113 , -0.15039542, -0.7433981 ,  1.4694595 ,\n",
       "        -1.1245916 ,  0.3360805 ],\n",
       "       [ 0.03728028, -0.33768234,  1.6832604 , -0.3353232 , -1.325423  ,\n",
       "        -1.9593027 , -0.34599763, -3.1006794 ,  2.2962196 , -0.80693865,\n",
       "         1.3678529 ,  0.10114851,  0.13371727,  0.36420822,  0.49783063,\n",
       "         0.62049437,  1.8832903 ,  1.2625886 ,  2.0200832 , -1.421174  ,\n",
       "        -1.3108637 ,  2.0388248 ,  0.32396334, -0.38813725,  0.10552703,\n",
       "         0.3586173 ,  0.6122677 , -0.15264365, -0.74335086,  1.468328  ,\n",
       "        -1.1214948 ,  0.337562  ],\n",
       "       [ 0.0355639 , -0.33673748,  1.6850188 , -0.334822  , -1.3297123 ,\n",
       "        -1.9579692 , -0.34517902, -3.0999303 ,  2.296747  , -0.8048171 ,\n",
       "         1.3683693 ,  0.10311563,  0.13507861,  0.36239594,  0.4968614 ,\n",
       "         0.61993766,  1.8848993 ,  1.2634866 ,  2.0213115 , -1.4220002 ,\n",
       "        -1.3081868 ,  2.0382977 ,  0.32180363, -0.39071375,  0.10513556,\n",
       "         0.35903966,  0.6132862 , -0.15032193, -0.74355686,  1.4703372 ,\n",
       "        -1.1240762 ,  0.3361809 ],\n",
       "       [ 0.03776056, -0.33906677,  1.6886759 , -0.33578348, -1.3277831 ,\n",
       "        -1.9574624 , -0.3511446 , -3.1008887 ,  2.298576  , -0.80058026,\n",
       "         1.3695161 ,  0.10116885,  0.13310117,  0.36374724,  0.49994904,\n",
       "         0.62002206,  1.8863395 ,  1.2639412 ,  2.0218256 , -1.4257591 ,\n",
       "        -1.3068638 ,  2.038707  ,  0.3251842 , -0.38946715,  0.10602362,\n",
       "         0.3615039 ,  0.6127277 , -0.15162012, -0.739843  ,  1.4696462 ,\n",
       "        -1.1235677 ,  0.34125343],\n",
       "       [ 0.03675649, -0.33748797,  1.6818695 , -0.3343631 , -1.3270382 ,\n",
       "        -1.9566132 , -0.34484407, -3.0962553 ,  2.2958052 , -0.8025343 ,\n",
       "         1.365838  ,  0.10129849,  0.13410446,  0.36187994,  0.49527365,\n",
       "         0.62073433,  1.8827194 ,  1.264018  ,  2.019868  , -1.4191431 ,\n",
       "        -1.3083122 ,  2.036423  ,  0.32277   , -0.38972613,  0.10532874,\n",
       "         0.36029965,  0.6137044 , -0.14923215, -0.7424054 ,  1.4679198 ,\n",
       "        -1.1228634 ,  0.33640632],\n",
       "       [ 0.03632384, -0.33631054,  1.6842166 , -0.33446336, -1.3275865 ,\n",
       "        -1.959459  , -0.34500512, -3.1014085 ,  2.2958436 , -0.804686  ,\n",
       "         1.3688817 ,  0.10245646,  0.13472772,  0.3634492 ,  0.49877107,\n",
       "         0.61891234,  1.8855013 ,  1.263787  ,  2.0223496 , -1.4211094 ,\n",
       "        -1.309351  ,  2.0364125 ,  0.32266062, -0.3891954 ,  0.10458721,\n",
       "         0.35922444,  0.61319816, -0.14912874, -0.7429457 ,  1.469351  ,\n",
       "        -1.1232082 ,  0.3378038 ],\n",
       "       [ 0.03574873, -0.3368752 ,  1.6819855 , -0.33521634, -1.3290966 ,\n",
       "        -1.9556532 , -0.34607896, -3.0987115 ,  2.2963123 , -0.80486095,\n",
       "         1.3669474 ,  0.10132913,  0.13473387,  0.36283588,  0.49758494,\n",
       "         0.6216788 ,  1.8858868 ,  1.2627757 ,  2.0219636 , -1.4208897 ,\n",
       "        -1.3082187 ,  2.0389626 ,  0.32159996, -0.39116064,  0.1062526 ,\n",
       "         0.35957128,  0.6122661 , -0.14942539, -0.74453473,  1.468337  ,\n",
       "        -1.123489  ,  0.33671775],\n",
       "       [ 0.03600421, -0.33810538,  1.6826388 , -0.33433414, -1.3267329 ,\n",
       "        -1.9560882 , -0.34396413, -3.096003  ,  2.2952938 , -0.8032101 ,\n",
       "         1.366725  ,  0.10274363,  0.13443008,  0.36315334,  0.4956261 ,\n",
       "         0.6213716 ,  1.882305  ,  1.2623034 ,  2.0204127 , -1.4200761 ,\n",
       "        -1.3082595 ,  2.034914  ,  0.32189733, -0.38998067,  0.10444504,\n",
       "         0.35928738,  0.6142209 , -0.1489011 , -0.74237704,  1.4684169 ,\n",
       "        -1.1216142 ,  0.3379858 ]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02438005, -0.02036242, -0.01039507,  0.00277665,  0.02350294,\n",
       "         0.01473409, -0.02157389,  0.02251465,  0.00814133,  0.01785252],\n",
       "       [ 0.00074128, -0.001453  , -0.01162554, -0.0014644 , -0.02097618,\n",
       "         0.00570552,  0.00674119, -0.01879225,  0.02161046,  0.01941887],\n",
       "       [ 0.01402811, -0.00613734, -0.0027943 , -0.01052572, -0.00041091,\n",
       "         0.0045091 , -0.01547336,  0.0194314 ,  0.00784631,  0.00505763],\n",
       "       [ 0.01655601, -0.02403533,  0.01607199, -0.01643238,  0.00686195,\n",
       "        -0.01788632,  0.01109112,  0.00087933,  0.00448566, -0.00823843],\n",
       "       [-0.00695892,  0.01631877,  0.01657149,  0.01772625,  0.02026216,\n",
       "         0.01811663,  0.02153757, -0.00020743, -0.02066918, -0.00025119],\n",
       "       [ 0.02195978,  0.00867016, -0.02278856,  0.0122384 , -0.02183604,\n",
       "         0.00798479,  0.00545774,  0.00923075,  0.01814621,  0.02018008],\n",
       "       [ 0.02242495, -0.00673729,  0.01447002,  0.02163557,  0.02312511,\n",
       "        -0.00920323, -0.01116639,  0.01708085, -0.01181559, -0.00763743],\n",
       "       [-0.01978129,  0.00916991, -0.00304236,  0.00106503,  0.00690803,\n",
       "        -0.01416942,  0.0063372 ,  0.00018093,  0.00393429, -0.00307162],\n",
       "       [-0.01821756,  0.01890057,  0.00228031, -0.01240117, -0.01108933,\n",
       "        -0.01674686, -0.01161283, -0.01693372,  0.00370204, -0.00271433],\n",
       "       [-0.0158878 ,  0.01951969, -0.01606018, -0.00457847,  0.00044555,\n",
       "        -0.0050344 , -0.0013686 ,  0.00539651,  0.02015766, -0.00510509]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_w_mean_topic_infer[:, :10, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03922943,  0.46112132, -0.08166478, -0.06917454, -0.01617851,\n",
       "        0.18225916, -0.28025913, -0.06620797,  0.03231472, -0.02211766,\n",
       "       -0.38082743,  0.11382294,  0.39363608, -0.4168986 ,  1.1677127 ,\n",
       "        0.08979508,  0.03593519, -0.07863234,  0.12493958,  0.03175303,\n",
       "        0.13351525,  0.8176425 , -0.28750482,  0.37927672, -0.01720515,\n",
       "       -0.21228473, -0.05377329, -0.10451841, -0.24399954, -0.25936088,\n",
       "        0.0845396 ,  0.33591354], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enc_state_infer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-ee7c3cd147b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_enc_state_infer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_means_topic_infer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdebug_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menc_state_infer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans_topic_infer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'enc_state_infer' is not defined"
     ]
    }
   ],
   "source": [
    "_enc_state_infer, _means_topic_infer = debug_value([enc_state_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67, 10, 1024)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_enc_state_infer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_means_topic_infer[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
