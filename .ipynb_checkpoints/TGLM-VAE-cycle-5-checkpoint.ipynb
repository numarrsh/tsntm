{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from data_structure import get_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "from topic_model import TopicModel\n",
    "\n",
    "from topic_beam_search_decoder import BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '2', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/apnews/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/tglm_vae', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'apnews', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 30, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 10, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.1, 'lr')\n",
    "flags.DEFINE_float('reg', 1., 'regularization term')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_integer('n_cycle', 5, 'number of cycle')\n",
    "flags.DEFINE_float('r_cycle', 0.5, 'proportion used to increase beta within a cycle')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 10, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_bow', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_hidden_topic', 512, 'dim_hidden_topic')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "flags.DEFINE_bool('bidirectional', True, 'flg of bidirectional encoding')\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('PAD_IDX', word_to_idx[PAD], 'PAD_IDX')\n",
    "flags.DEFINE_integer('UNK_IDX', word_to_idx[UNK], 'UNK_IDX')\n",
    "flags.DEFINE_integer('BOS_IDX', word_to_idx[BOS], 'BOS_IDX')\n",
    "flags.DEFINE_integer('EOS_IDX', word_to_idx[EOS], 'EOS_IDX')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "maximum_iterations = max([max([instance.max_sent_l for instance in batch]) for ct, batch in dev_batches])\n",
    "flags.DEFINE_integer('maximum_iterations', maximum_iterations, 'maximum_iterations')\n",
    "\n",
    "flags.DEFINE_integer('cycle_steps', int(len(train_batches)*config.epochs/config.n_cycle), 'number of steps for each cycle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build language model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['bow'] = tf.placeholder(tf.float32, [None, config.dim_bow], name='bow')\n",
    "t_variables['input_token_idxs'] = tf.placeholder(tf.int32, [None, None], name='input_token_idxs')\n",
    "t_variables['dec_input_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_input_idxs')\n",
    "t_variables['dec_target_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_target_idxs')\n",
    "t_variables['batch_l'] = tf.placeholder(tf.int32, name='batch_l')\n",
    "t_variables['doc_l'] = tf.placeholder(tf.int32, [None], name='doc_l')\n",
    "t_variables['sent_l'] = tf.placeholder(tf.int32, [None], name='sent_l')\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train', assertion=False):\n",
    "    def token_dropout(sent_idxs):\n",
    "        sent_idxs_dropout = np.asarray(sent_idxs)\n",
    "        sent_idxs_dropout[np.random.rand(len(sent_idxs)) > config.word_keep_prob] = config.UNK_IDX\n",
    "        return list(sent_idxs_dropout)\n",
    "\n",
    "    bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "    \n",
    "    doc_l = np.array([len(instance.token_idxs) for instance in batch])\n",
    "    \n",
    "    feed_input_token_idxs_list = [sent_idxs for instance in batch for sent_idxs in instance.token_idxs]\n",
    "    feed_dec_input_idxs_list = [[config.BOS_IDX] + token_dropout(sent_idxs) for sent_idxs in feed_input_token_idxs_list]\n",
    "    feed_dec_target_idxs_list = [sent_idxs + [config.EOS_IDX]  for sent_idxs in feed_input_token_idxs_list]\n",
    "        \n",
    "    sent_l = np.array([len(sent_idxs) for sent_idxs in feed_input_token_idxs_list], np.int32)\n",
    "    batch_l = len(sent_l)\n",
    "    \n",
    "    feed_input_token_idxs = pad_sequences(feed_input_token_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_input_idxs = pad_sequences(feed_dec_input_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_target_idxs = pad_sequences(feed_dec_target_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    \n",
    "    if assertion:\n",
    "        index = 0\n",
    "        for instance in batch:\n",
    "            for line_idxs in instance.token_idxs:\n",
    "                assert feed_input_token_idxs_list[index] == line_idxs\n",
    "                index += 1\n",
    "        assert feed_input_token_idxs.shape[1] == np.max(sent_l)\n",
    "        assert feed_dec_input_idxs.shape[1] == np.max(sent_l) + 1\n",
    "        assert feed_dec_target_idxs.shape[1] == np.max(sent_l) + 1\n",
    "    \n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "\n",
    "    feed_dict = {\n",
    "                t_variables['bow']: bow, \n",
    "                t_variables['batch_l']: batch_l, t_variables['doc_l']: doc_l, t_variables['sent_l']: sent_l, \n",
    "                t_variables['input_token_idxs']: feed_input_token_idxs, t_variables['dec_input_idxs']: feed_dec_input_idxs, t_variables['dec_target_idxs']: feed_dec_target_idxs, \n",
    "                t_variables['keep_prob']: keep_prob\n",
    "    }\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "code_folding": [
     0,
     10,
     24,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "def check_shape(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()\n",
    "    \n",
    "def check_value(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()    \n",
    "    \n",
    "# sent_loss_kl_categ_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, tf_log(prob_topic_infer/prob_topic_sents)), 1))\n",
    "# debug_value([sent_loss_kl_categ, sent_loss_kl_categ_tmp])\n",
    "# sent_loss_kl_gauss_tmp = 0.5 * tf.reduce_sum(tf.exp(logvars_topic_infer-logvars_topic) + tf.square(means_topic - means_topic_infer) / tf.exp(logvars_topic) - 1 + (logvars_topic - logvars_topic_infer), -1)\n",
    "# sent_loss_kl_gmm_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss_tmp), -1))\n",
    "# debug_value([sent_loss_kl_gmm_tmp, sent_loss_kl_gmm])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encode bow\n",
    "with tf.variable_scope('topic/enc', reuse=False):\n",
    "    hidden_bow_ = tf.layers.Dense(units=config.dim_hidden_bow, activation=tf.nn.relu, name='hidden_bow')(t_variables['bow'])\n",
    "    hidden_bow = tf.layers.Dropout(t_variables['keep_prob'])(hidden_bow_)\n",
    "    means_bow = tf.layers.Dense(units=config.dim_latent_bow, name='mean_bow')(hidden_bow)\n",
    "    logvars_bow = tf.layers.Dense(units=config.dim_latent_bow, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_bow)\n",
    "    latents_bow = sample_latents(means_bow, logvars_bow) # sample latent vectors\n",
    "\n",
    "    prob_topic = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic')(latents_bow) # inference of topic probabilities\n",
    "\n",
    "# decode bow\n",
    "with tf.variable_scope('shared', reuse=False):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "bow_embeddings = tf.nn.embedding_lookup(embeddings, bow_idxs) # embeddings of each bow features\n",
    "\n",
    "with tf.variable_scope('topic/dec', reuse=False):\n",
    "    topic_embeddings = tf.get_variable('topic_emb', [config.n_topic, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of topics\n",
    "\n",
    "    topic_bow = tf.nn.softmax(tf.matmul(topic_embeddings, bow_embeddings, transpose_b=True), 1) # bow vectors for each topic\n",
    "    logits_bow = tf_log(tf.matmul(prob_topic, topic_bow)) # predicted bow distribution\n",
    "\n",
    "    # prior of each gaussian distribution (computed for each topic)\n",
    "    hidden_topic = tf.layers.Dense(units=config.dim_hidden_topic, activation=tf.nn.relu, name='hidden_topic')(topic_bow)\n",
    "    means_topic = tf.layers.Dense(units=config.dim_latent, name='mean_topic')(hidden_topic)\n",
    "    logvars_topic = tf.layers.Dense(units=config.dim_latent, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_topic)\n",
    "    sigma_topic = tf.exp(0.5 * logvars_topic)\n",
    "    gauss_topic = tfd.Normal(loc=means_topic, scale=sigma_topic)    \n",
    "    \n",
    "# define losses\n",
    "topic_losses_recon = -tf.reduce_sum(tf.multiply(t_variables['bow'], logits_bow), 1)\n",
    "topic_loss_recon = tf.reduce_mean(topic_losses_recon) # negative log likelihood of each words\n",
    "\n",
    "topic_loss_kl = compute_kl_loss(means_bow, logvars_bow) # KL divergence b/w latent dist & gaussian std\n",
    "\n",
    "topic_bow_norm = topic_bow / tf.norm(topic_bow, axis=1, keepdims=True)\n",
    "topic_dots = tf.clip_by_value(tf.matmul(topic_bow_norm, tf.transpose(topic_bow_norm)), -1., 1.)\n",
    "topic_loss_reg = tf.reduce_mean(tf.square(topic_dots - tf.eye(config.n_topic)))\n",
    "# topic_angles = tf.acos(topic_dots)\n",
    "# topic_angles_mean = tf.reduce_mean(topic_angles)\n",
    "# topic_angles_vars = tf.reduce_mean(tf.square(topic_angles - topic_angles_mean))\n",
    "# topic_loss_reg = tf.exp(topic_angles_vars - topic_angles_mean)\n",
    "\n",
    "# monitor\n",
    "n_bow = tf.reduce_sum(t_variables['bow'], 1)\n",
    "topic_ppls = tf.divide(topic_losses_recon, n_bow)\n",
    "topics_freq_bow_indices = tf.nn.top_k(topic_bow, 10, name='topic_freq_bow').indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "input_token_idxs = t_variables['input_token_idxs']\n",
    "batch_l = t_variables['batch_l']\n",
    "sent_l = t_variables['sent_l']\n",
    "max_sent_l = tf.reduce_max(sent_l)\n",
    "\n",
    "with tf.variable_scope('sent/enc', reuse=False):\n",
    "    # get word embedding\n",
    "    enc_input = tf.nn.embedding_lookup(embeddings, input_token_idxs)\n",
    "\n",
    "    # get sentence embedding\n",
    "    _, enc_state = dynamic_bi_rnn(enc_input, sent_l, config.dim_hidden, t_variables['keep_prob'])\n",
    "\n",
    "    # TODO House Holder flow\n",
    "    hidden_topic_infer =  tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='hidden_topic_infer')(enc_state)\n",
    "    prob_topic_infer = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic_infer')(hidden_topic_infer)\n",
    "\n",
    "    w_mean_topic_infer = tf.get_variable('mean_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32)\n",
    "    b_mean_topic_infer = tf.get_variable('mean_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32)\n",
    "    means_topic_infer = tf.tensordot(enc_state, w_mean_topic_infer, axes=[[1], [1]]) + b_mean_topic_infer\n",
    "    \n",
    "    w_logvar_topic_infer = tf.get_variable('logvar_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    b_logvar_topic_infer = tf.get_variable('logvar_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    logvars_topic_infer = tf.tensordot(enc_state, w_logvar_topic_infer, axes=[[1], [1]]) + b_logvar_topic_infer\n",
    "    sigma_topic_infer = tf.exp(0.5 * logvars_topic_infer)\n",
    "    gauss_topic_infer = tfd.Normal(loc=means_topic_infer, scale=sigma_topic_infer)\n",
    "    \n",
    "    # latent vectors from each gaussian dist.\n",
    "    latents_topic_infer = sample_latents(means_topic_infer, logvars_topic_infer) \n",
    "    # latent vector from gaussian mixture\n",
    "    latents_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), latents_topic_infer, transpose_a=True)\n",
    "    \n",
    "    # for beam search\n",
    "    means_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), means_topic_infer, transpose_a=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for decoding\n",
    "dec_sent_l = tf.add(sent_l, 1)\n",
    "dec_input_idxs = t_variables['dec_input_idxs']\n",
    "dec_input = tf.nn.embedding_lookup(embeddings, dec_input_idxs)\n",
    "\n",
    "dec_latents_input = tf.tile(latents_input, [1, tf.shape(dec_input)[1], 1])\n",
    "dec_concat_input = tf.concat([dec_input, dec_latents_input], -1)\n",
    "\n",
    "# decode for training\n",
    "with tf.variable_scope('sent/dec/rnn', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32, reuse=False):\n",
    "    dec_cell = tf.contrib.rnn.GRUCell(config.dim_hidden)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob = t_variables['keep_prob'])\n",
    "\n",
    "    dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(latents_input, 1))\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_concat_input, sequence_length=dec_sent_l)\n",
    "\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=dec_cell,\n",
    "        helper=helper,\n",
    "        initial_state=dec_initial_state)\n",
    "\n",
    "    dec_outputs, _, output_sent_l = tf.contrib.seq2seq.dynamic_decode(train_decoder)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(config.n_vocab, use_bias=False, name='out')\n",
    "    output_logits = output_layer(dec_outputs.rnn_output)\n",
    "    \n",
    "    output_token_idxs = tf.argmax(output_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tokens = tf.fill([batch_l], config.BOS_IDX)\n",
    "end_token = config.EOS_IDX\n",
    "\n",
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    infer_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(means_input, 1))\n",
    "    beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(infer_dec_initial_state, multiplier=config.beam_width)\n",
    "    beam_latents_input = tf.contrib.seq2seq.tile_batch(tf.squeeze(means_input, 1), multiplier=config.beam_width) # added\n",
    "    \n",
    "    beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=beam_latents_input)\n",
    "\n",
    "    beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    beam_output_token_idxs = beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    inter_means_input = tf.placeholder(tf.float32, [None, config.dim_latent])\n",
    "    \n",
    "    inter_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(inter_means_input)\n",
    "    inter_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(inter_dec_initial_state, multiplier=config.beam_width)\n",
    "    inter_beam_latents_input = tf.contrib.seq2seq.tile_batch(inter_means_input, multiplier=config.beam_width) # added\n",
    "    \n",
    "    inter_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=inter_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=inter_beam_latents_input)\n",
    "\n",
    "    inter_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inter_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    inter_beam_output_token_idxs = inter_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    topic_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic)\n",
    "    topic_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(topic_dec_initial_state, multiplier=config.beam_width)\n",
    "    topic_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic, multiplier=config.beam_width) # added\n",
    "    \n",
    "    topic_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=topic_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=topic_beam_latents_input)\n",
    "\n",
    "    topic_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        topic_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    topic_beam_output_token_idxs = topic_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language modeling cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target and mask\n",
    "dec_target_idxs = t_variables['dec_target_idxs']\n",
    "dec_mask_tokens = tf.sequence_mask(dec_sent_l, maxlen=max_sent_l+1, dtype=tf.float32)\n",
    "\n",
    "# nll for each token (averaged over batch & sentence)\n",
    "sent_loss_recon = tf.contrib.seq2seq.sequence_loss(output_logits, dec_target_idxs, dec_mask_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_l = t_variables['doc_l']\n",
    "mask_sents = tf.sequence_mask(doc_l)\n",
    "mask_sents_flatten = tf.reshape(mask_sents, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1]])\n",
    "\n",
    "prob_topic_tiled = tf.tile(tf.expand_dims(prob_topic, 1), [1, tf.shape(mask_sents)[1], 1])\n",
    "prob_topic_flatten = tf.reshape(prob_topic_tiled, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1], config.n_topic])\n",
    "prob_topic_sents = tf.boolean_mask(prob_topic_flatten, mask_sents_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferred mixture probabilities (computed for each sentence)\n",
    "categ_topic_infer = tfd.Categorical(probs=prob_topic_infer)\n",
    "\n",
    "# prior of mixture probabilities (computed for each document, tiled for each sentence)\n",
    "categ_topic = tfd.Categorical(probs=prob_topic_sents)\n",
    "\n",
    "sent_loss_kl_categ = tf.reduce_mean(tfd.kl_divergence(categ_topic_infer, categ_topic))\n",
    "\n",
    "# inference of each gaussian gaussribution (computed for each sentence)\n",
    "\n",
    "sent_loss_kl_gauss = tf.reduce_sum(tfd.kl_divergence(gauss_topic_infer, gauss_topic), -1)\n",
    "sent_loss_kl_gmm = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss), -1))\n",
    "\n",
    "sent_loss_kl = sent_loss_kl_categ + sent_loss_kl_gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "tau = tf.divide(tf.mod(global_step, tf.constant(config.cycle_steps)), tf.constant(config.cycle_steps))\n",
    "beta = tf.minimum(1., tau/config.r_cycle)\n",
    "\n",
    "sent_loss = sent_loss_recon + beta * sent_loss_kl\n",
    "\n",
    "topic_loss = topic_loss_recon + topic_loss_kl + config.reg * topic_loss_reg\n",
    "loss = topic_loss + sent_loss\n",
    "\n",
    "# define optimizer\n",
    "if config.opt == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif config.opt == 'Adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "    \n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_sents(token_idxs, config, idx_to_word):\n",
    "    sents = []\n",
    "    for sent_idxs in token_idxs:\n",
    "        tokens = []\n",
    "        for idx in sent_idxs:\n",
    "            if idx == config.EOS_IDX: break\n",
    "            tokens.append(idx_to_word[idx])\n",
    "        sent = ' '.join(tokens)\n",
    "        sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_batch, sent_loss_batch, ppls_batch = sess.run([loss, topic_loss, sent_loss, topic_ppls], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_batch, sent_loss_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "    loss_mean, topic_loss_mean, sent_loss_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    return loss_mean, topic_loss_mean, sent_loss_mean, ppl_mean\n",
    "\n",
    "def get_all_losses(sess, batches):\n",
    "    losses = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch = \\\n",
    "        sess.run([loss, topic_loss_recon, topic_loss_kl, sent_loss_recon, sent_loss_kl], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "    print('LOSS %.2f | TM NLL: %.2f, KL: %.4f | LM NLL: %.2f, KL: %.4f' %  np.mean(losses, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_batch):\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    pred_token_idxs = sess.run(output_token_idxs, feed_dict = feed_dict)\n",
    "    true_token_idxs = [sent_idxs for instance in sample_batch for sent_idxs in instance.token_idxs]\n",
    "    \n",
    "    assert len(pred_token_idxs) == len(true_token_idxs)\n",
    "    \n",
    "    pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "    true_sents = idxs_to_sents(true_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    for i, (true_sent, pred_sent) in enumerate(zip(true_sents, pred_sents)):        \n",
    "        print(i, 'True: %s' % true_sent)\n",
    "        print(i, 'Pred: %s' % pred_sent)\n",
    "\n",
    "def print_topic_sample():\n",
    "    pred_topics_freq_bow_indices, pred_topic_token_idxs = sess.run([topics_freq_bow_indices, topic_beam_output_token_idxs], \n",
    "                                                                                                           feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "    pred_topic_sents = idxs_to_sents(pred_topic_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Topic Samples-----------')\n",
    "    for i, (topic_freq_bow_idxs, pred_topic_sent) in enumerate(zip(topics_freq_bow_idxs, pred_topic_sents)):\n",
    "        print(i, ' bow:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' sent:', pred_topic_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "logs = []\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "saver = tf.train.Saver(max_to_keep=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "033[s], 30[s], Ep: 00, Ct: 00000|TR LOSS: 406, PPL: 2661|TM NLL: 394, KL: 0.75, REG:0.90 | LM NLL: 10.34, KL: 1.45|DE LOSS: 348, PPL: 2656, TM: 338, LM: 10.34|BETA: 0.001000\n",
      "032[s], 30[s], Ep: 00, Ct: 00020|TR LOSS: 352, PPL: 2520|TM NLL: 341, KL: 0.31, REG:0.90 | LM NLL: 10.33, KL: 1.44|DE LOSS: 337, PPL: 2042, TM: 326, LM: 10.31|BETA: 0.001001\n",
      "010[s], 08[s], Ep: 00, Ct: 00040|TR LOSS: 345, PPL: 2184|TM NLL: 334, KL: 0.32, REG:0.84 | LM NLL: 10.10, KL: 1.65|DE LOSS: 330, PPL: 1809, TM: 321, LM: 8.81|BETA: 0.001003\n",
      "010[s], 08[s], Ep: 00, Ct: 00050|TR LOSS: 343, PPL: 2089|TM NLL: 332, KL: 0.30, REG:0.83 | LM NLL: 9.82, KL: 1.68|DE LOSS: 329, PPL: 1792, TM: 320, LM: 8.37|BETA: 0.001003\n",
      "010[s], 08[s], Ep: 00, Ct: 00060|TR LOSS: 339, PPL: 2024|TM NLL: 328, KL: 0.29, REG:0.82 | LM NLL: 9.58, KL: 1.71|DE LOSS: 328, PPL: 1758, TM: 320, LM: 8.11|BETA: 0.001004\n",
      "0 True: a judge has dismissed a lawsuit against tax commissioner cory fong and several legislators about a proposal to get rid of north dakota property taxes\n",
      "0 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "1 True: supporters of abolishing property taxes say fong and other public officials have been illegally using public money to campaign against the idea\n",
      "1 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "2 True: they filed a lawsuit , asking a judge to intervene\n",
      "2 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "3 True: south central district judge bruce romanick threw out the request thursday\n",
      "3 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "4 True: romanick says the law against using public funds for campaigning does n't allow private citizens to file lawsuits to try to enforce it\n",
      "4 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "5 True: he says the law can only be enforced by prosecutors\n",
      "5 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "6 True: the argument is about a proposed constitutional amendment called measure #\n",
      "6 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "7 True: it would abolish property taxes\n",
      "7 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "8 True: north dakota voters will decide in june whether to approve it\n",
      "8 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "9 True: @ <unk> tv , washington , d.c. washington a us airways baggage handler rescued from the belly of a jet before a flight is back on the job\n",
      "9 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "10 True: the worker , whose name was not made available , was rescued monday after a co pilot on the flight between washington 's reagan national airport and hartford , conn. , heard pounding and shouting through the floor\n",
      "10 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "11 True: the baggage handler had been locked in a space just # inches high inside an embraer # jet\n",
      "11 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "12 True: he was rescued before the plane 's engines were started while the aircraft was still at the gate\n",
      "12 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "13 True: us airways officials say the airline is investigating the incident\n",
      "13 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "14 True: in # , a baggage handler survived a flight between new york and boston after being accidentally locked in the hold of a jet blue flight\n",
      "14 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "15 True: the recent incident did not delay the aircraft 's departure\n",
      "15 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "16 True: in fact , the rescued baggage handler drove the tug used to push the aircraft back\n",
      "16 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "17 True: a monkey that has eluded capture for months has again escaped from wildlife officials in st. petersburg\n",
      "17 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "18 True: authorities were called to the area on wednesday afternoon after the monkey was spotted\n",
      "18 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "19 True: it was twice hit with tranquilizer darts , but still got away\n",
      "19 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "20 True: wildlife rehabilitator vernon yates says the tranquilizer do n't appear to affect the animal , though officials have increased the dosage each time they 've used the drug on the monkey\n",
      "20 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "21 True: yates says the monkey is smart , even stopping to check traffic before crossing a busy street\n",
      "21 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "22 True: the monkey has been spotted several times in hillsborough county and pinellas county\n",
      "22 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "23 True: it is n't considered a threat to humans\n",
      "23 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "24 True: county supervisors in hancock county are taking no pay increase for fiscal year # , the second time in two years they 've chosen no raise\n",
      "24 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "25 True: the globe gazette in mason city ( http : <unk> ) reports that supervisors also cut the recommended pay increases for other county officials on monday\n",
      "25 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "26 True: the county 's compensation board recommended pay raises of # percent for the treasurer , recorder , auditor and the three supervisors\n",
      "26 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "27 True: it suggested pay hikes of # / # percent for the county attorney and sheriff\n",
      "27 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "28 True: the supervisors approved pay raises of # percent for the treasurer , recorder and auditor and # percent for the sheriff and county attorney\n",
      "28 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "29 True: in the current year supervisors in # of iowa 's # counties froze their pay\n",
      "29 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "30 True: the average increase was # percent , according to the iowa state association of counties\n",
      "30 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "31 True: gov . deval patrick is predicting `` consequences `` for what he calls a `` breakdown in oversight `` at a now closed state crime lab\n",
      "31 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "32 True: the governor ordered the boston lab shut down last month after authorities discovered that a chemist had not followed proper protocols in the testing of drug samples\n",
      "32 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "33 True: the finding could bring legal challenges from people who have been convicted or are awaiting trial\n",
      "33 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "34 True: the chemist resigned in march and two supervisors have been suspended\n",
      "34 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "35 True: a criminal investigation and administrative review are under way\n",
      "35 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "36 True: the operation of the lab was transferred from the state department of public health to state police in march\n",
      "36 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "37 True: patrick said monday he was limited in what he could say because of the ongoing probes , but added that his office was still trying to understand the full scope of the problem\n",
      "37 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "38 True: gov . chris christie legal battle with the federal government over whether the state owes $ # million for abandoning a train tunnel under the hudson has so far cost the state more than a $ # million\n",
      "38 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "39 True: the gop governor says the state will fight a finding it must repay the federal government the entire $ # million spent on early design and construction work for a now scrapped new jersey new york train tunnel\n",
      "39 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "40 True: christie abandoned the project last fall , citing escalating costs\n",
      "40 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "41 True: but federal transportation officials say new jersey only got the federal funds because it committed to finishing the project\n",
      "41 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "42 True: invoices obtained by the star ledger show the state owes $ # a month in interest on the money\n",
      "42 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "43 True: the state has also been paying the patton boggs <unk> an average of $ # a month in legal bills\n",
      "43 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "44 True: gov . rick snyder has signed legislation aimed at making sure union dues are n't collected from certain home health care workers\n",
      "44 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "45 True: the bill that the republican governor announced signing tuesday would exclude those who receive a government subsidy for private employment from the definition of a public employee\n",
      "45 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "46 True: snyder said the legislation clarifies michigan law to its original intent\n",
      "46 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "47 True: republicans who control the michigan legislature have been critical of what they consider stealth , unilaterally imposed union dues collection from those who serve as care providers through a program called the michigan quality community care council\n",
      "47 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "48 True: unions are attempting to counteract the legislation through a ballot campaign aimed at getting features of the program <unk> in the state constitution\n",
      "48 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "49 True: they 'd have to collect nearly # voter signatures to make the november ballot\n",
      "49 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "50 True: the residents of a cedar falls house and their dog fled from their home after a fire broke out in an attached garage\n",
      "50 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "51 True: according to the waterloo cedar falls courier ( http : <unk> ) , fire officials say thursday morning 's fire apparently started after an electrical problem occurred in an automobile engine block heater\n",
      "51 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "52 True: the heater was plugged into a truck in the garage\n",
      "52 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "53 True: the flames spread to garage cabinets\n",
      "53 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "54 True: fire chief john schilling says little damage was done to the house\n",
      "54 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "55 True: documents posted saturday by u.s. safety regulators say gm is recalling more than # vehicles because the side air bag <unk> could rupture and send shrapnel into drivers and passengers\n",
      "55 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "56 True: the recall means that the air bag problems are growing to more newer models and into side air bags\n",
      "56 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "57 True: previously the problem was limited mainly to older vehicles and front air bags\n",
      "57 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "58 True: eight people have been killed worldwide because of the faulty <unk> and more than # have been hurt\n",
      "58 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "59 True: the latest recall covers certain # chevrolet equinox , malibu and camaro vehicles as well as the buick lacrosse , cadillac <unk> and gmc terrain\n",
      "59 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "60 True: independent petroleum producer stone energy corp. has raised its # capital expenditure budget from $ # million to a range of $ # million to $ # million\n",
      "60 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "61 True: lafayette based stone said wednesday that the move was made in the face of projected production increases , oil prices and estimated cash flow , combined with an inventory of attractive projects\n",
      "61 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "62 True: stone also increased its production guidance from a range of # million to # million cubic feet of gas equivalent per day to a range of # million to # million per day\n",
      "62 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "63 True: stone says it expects to fund substantially all of the increased number of projects with operating cash flow\n",
      "63 Pred: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Topic Samples-----------\n",
      "0  bow: million http federal monday wednesday percent authorities public fire reports\n",
      "0  sent: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "1  bow: million federal http people authorities monday fire public percent wednesday\n",
      "1  sent: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "2  bow: million wednesday federal public authorities reports monday http percent fire\n",
      "2  sent: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "3  bow: million percent wednesday federal http authorities department reports fire home\n",
      "3  sent: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "4  bow: percent million thursday reports public federal http monday wednesday people\n",
      "4  sent: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "5  bow: million federal house wednesday percent http fire monday authorities public\n",
      "5  sent: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "6  bow: million fire http home authorities federal wednesday monday house department\n",
      "6  sent: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "7  bow: percent million http federal public monday reports wednesday people company\n",
      "7  sent: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "8  bow: million http wednesday federal public percent people reports authorities monday\n",
      "8  sent: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "9  bow: million fire federal authorities wednesday http monday percent house reports\n",
      "9  sent: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-f9f039945c7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_recon_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_kl_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_reg_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_recon_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_kl_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mppl_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mppls_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mloss_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppl_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#             if loss_dev < loss_min:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-7cabbf2ac912>\u001b[0m in \u001b[0;36mget_loss\u001b[0;34m(sess, batches)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mloss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppls_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_ppls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mppl_list\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mppls_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if len(logs) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "while epoch < config.epochs:\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = get_feed_dict(batch)\n",
    "\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, sent_loss_kl_categ_batch, sent_loss_kl_gmm_batch, ppls_batch = \\\n",
    "        sess.run([opt, loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, sent_loss_kl_categ, sent_loss_kl_gmm, topic_ppls], feed_dict = feed_dict)\n",
    "   \n",
    "        if sent_loss_kl_batch == np.inf:\n",
    "            print('Nan occured')\n",
    "            ckpt = tf.train.get_checkpoint_state(config.modeldir)\n",
    "            model_checkpoint_path = ckpt.all_model_checkpoint_paths[-1]\n",
    "            saver.restore(sess, model_checkpoint_path)            \n",
    "            break\n",
    "            \n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if ct%config.log_period==0:\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train, sent_loss_recon_train, sent_loss_kl_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_dev, sent_loss_dev, ppl_dev = get_loss(sess, dev_batches)\n",
    "\n",
    "            if config.warmup > 0: beta_eval = beta.eval(session=sess)\n",
    "            global_step_log = sess.run(tf.train.get_global_step())            \n",
    "            \n",
    "            if loss_dev < loss_min:\n",
    "                loss_min = loss_dev\n",
    "                saver.save(sess, config.modelpath, global_step=global_step_log)\n",
    "\n",
    "            clear_output()\n",
    "\n",
    "            time_finish = time.time()\n",
    "            time_log = int(time_finish - time_start)\n",
    "            logs += [(time_log, epoch, ct, loss_train, ppl_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train, sent_loss_recon_train, sent_loss_kl_train, loss_dev, ppl_dev, topic_loss_dev, sent_loss_dev, beta_eval)]\n",
    "            for log in logs:\n",
    "                print('%03d[s], Ep: %02d, Ct: %05d|TR LOSS: %.0f, PPL: %.0f|TM NLL: %.0f, KL: %.2f, REG:%.2f | LM NLL: %.2f, KL: %.2f|DE LOSS: %.0f, PPL: %.0f, TM: %.0f, LM: %.2f|BETA: %.6f' %  log)\n",
    "\n",
    "            print_sample(batch)\n",
    "\n",
    "            time_start = time.time()\n",
    "            \n",
    "            print_topic_sample()\n",
    "                \n",
    "    epoch += 1\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "_prob_topic, _prob_topic_sents, _prob_topic_infer, _means_topic_infer = debug_value([prob_topic, prob_topic_sents, prob_topic_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.2281889e-02, 6.7835855e-03, 2.0132679e-01, 1.1674699e-02,\n",
       "        6.5264981e-03, 1.1411838e-02, 6.9189700e-03, 2.8612482e-04,\n",
       "        7.4042326e-01, 2.3663496e-03], dtype=float32),\n",
       " array([0.09889801, 0.11335436, 0.12141278, 0.10027827, 0.13508864,\n",
       "        0.09455997, 0.0860846 , 0.08042429, 0.09635323, 0.07354581],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_i = 4\n",
    "_prob_topic_sents[batch_i], _prob_topic_infer[batch_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02824139, -0.22793637,  0.3788033 , -0.02814816],\n",
       "       [ 0.06249218, -0.08085692,  0.06756439,  0.2259818 ],\n",
       "       [-0.1377574 ,  0.28185233,  0.23656711, -0.3201025 ],\n",
       "       [-0.03711405,  0.16953555,  0.16389738, -0.16461828],\n",
       "       [ 0.27302447, -0.22462857,  0.37200606, -0.00549014],\n",
       "       [-0.16755986,  0.11964113,  0.17052333,  0.18083367],\n",
       "       [ 0.26351342, -0.33828875,  0.3920058 , -0.17029142],\n",
       "       [ 0.21033517, -0.1955087 ,  0.20963705, -0.24809986],\n",
       "       [ 0.01922028, -0.03185754,  0.28022933, -0.13928899],\n",
       "       [-0.26817918, -0.3364467 ,  0.36924574,  0.15818927]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_means_topic_infer[0][:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_means_topic, b_means_topic = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"topic/dec/mean_topic\")\n",
    "\n",
    "pred_topic_embeddings, pred_topic_bow, pred_means_topic, pred_logvars_topic, pred_token_idxs, _w_means_topic, _b_means_topic, _w_mean_topic_infer = \\\n",
    "                                sess.run([topic_embeddings, topic_bow, means_topic, logvars_topic, topic_beam_output_token_idxs, w_means_topic, b_means_topic, w_mean_topic_infer], \n",
    "                                         feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "\n",
    "pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "\n",
    "pred_topics_freq_bow_indices = np.argsort(pred_topic_bow, 1)[:, ::-1][:, :10]\n",
    "pred_topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['million', 'http', 'years', 'tuesday', 'federal', 'u.s.', 'month', 'office', 'people', 'monday']\n",
      "['reports', 'department', 'thursday', 'court', 'time', 'u.s.', 'years', 'monday', 'tuesday', \"'\"]\n",
      "['school', 'public', 'u.s.', 'board', 'university', 'federal', 'president', 'program', 'health', 'students']\n",
      "['fire', 'found', 'authorities', 'hospital', 'home', 'a.m.', 'dead', 'investigation', 'chief', 'morning']\n",
      "['service', 'national', 'department', 'http', 'water', 'park', 'area', 'center', 'wednesday', 'weather']\n",
      "['percent', 'company', 'million', 'billion', 'shares', 'inc.', 'revenue', 'average', 'cents', 'price']\n",
      "['design', 'researchers', 'donations', 'prize', 'encourage', 'peak', 'awards', 'feature', 'chapter', 'affect']\n",
      "['court', 'arrested', 'charged', 'attorney', 'death', 'authorities', 'found', 'prison', 'shooting', 'woman']\n",
      "['bill', 'senate', 'house', 'republican', 'committee', 'public', 'board', 'approved', 'u.s.', 'law']\n",
      "['todd', 'rear', 'drowned', 'confronted', 'foul', 'transported', 'assaulted', 'loaded', 'defender', 'unidentified']\n"
     ]
    }
   ],
   "source": [
    "for idxs in pred_topics_freq_bow_idxs:\n",
    "    print([idx_to_word[idx] for idx in idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09207956,  0.09077395,  0.06120839, -0.22176096,  0.19921999,\n",
       "        -0.05485373, -0.23069587,  0.05650459,  0.142239  , -0.17410392],\n",
       "       [ 0.29974467, -0.03281972,  0.25064805, -0.2893645 , -0.1312135 ,\n",
       "        -0.11136644,  0.02060958, -0.13598587,  0.24592783, -0.18463418],\n",
       "       [ 0.22819577,  0.14566512,  0.05597903, -0.1109414 , -0.00879419,\n",
       "        -0.1605561 ,  0.11445126,  0.00916692, -0.04847115, -0.0603605 ],\n",
       "       [ 0.3365141 , -0.06439152,  0.3631938 ,  0.14203475,  0.10052318,\n",
       "        -0.09119357, -0.08768138, -0.28562906,  0.3186355 , -0.08452626],\n",
       "       [ 0.26727033,  0.03540339,  0.28315246, -0.1286456 , -0.02444943,\n",
       "        -0.163507  , -0.02146725, -0.21729966,  0.26542294, -0.1589102 ],\n",
       "       [ 0.28826687, -0.18438952,  0.2760168 , -0.12692557,  0.05485712,\n",
       "         0.17983724,  0.14970489, -0.36168167,  0.30563408, -0.13128197],\n",
       "       [-0.32988435, -0.11005852, -0.27013943,  0.26166642, -0.3013388 ,\n",
       "        -0.2234488 ,  0.02287463,  0.20320751, -0.10510623,  0.28636736],\n",
       "       [ 0.37211177, -0.00585993,  0.2853141 , -0.05771174,  0.15170154,\n",
       "         0.05411884, -0.24091715, -0.01475959, -0.0408047 , -0.17167823],\n",
       "       [ 0.20512433,  0.18697639, -0.08326819,  0.03792854,  0.03214281,\n",
       "         0.05618311,  0.09544107,  0.03913466, -0.06514693, -0.04000437],\n",
       "       [-0.3745221 , -0.06385167, -0.29924905,  0.08158813, -0.28344202,\n",
       "        -0.01799835,  0.02471407,  0.408938  , -0.169172  ,  0.37273586]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_topic_embeddings[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0327697e-03, 4.6811462e-03, 1.7024261e-04, ..., 3.3795869e-04,\n",
       "        1.5871278e-04, 6.6570101e-05],\n",
       "       [5.9443718e-04, 7.4376534e-03, 2.0111131e-04, ..., 7.4048399e-04,\n",
       "        1.5028552e-04, 8.9953668e-05],\n",
       "       [8.5580017e-04, 2.8901210e-03, 1.6464434e-04, ..., 3.1770146e-04,\n",
       "        1.9349743e-04, 3.0012103e-04],\n",
       "       ...,\n",
       "       [1.3188391e-04, 2.6697975e-03, 2.3009101e-04, ..., 8.1575691e-04,\n",
       "        1.5104198e-04, 3.2140684e-05],\n",
       "       [3.8558390e-04, 4.0810513e-03, 1.4347673e-04, ..., 1.4157601e-04,\n",
       "        1.8342295e-04, 4.7484271e-05],\n",
       "       [7.1679087e-06, 2.9757638e-07, 1.8662996e-04, ..., 5.9841594e-05,\n",
       "        1.7972868e-04, 5.0798093e-04]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_topic_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09637171, -0.09069463,  0.09532599, ..., -0.02989542,\n",
       "        -0.02514938,  0.01415634],\n",
       "       [-0.02764424,  0.01254364,  0.03703775, ..., -0.01512817,\n",
       "         0.02590305,  0.01404281],\n",
       "       [-0.08408152, -0.00924051, -0.08365072, ...,  0.05623364,\n",
       "        -0.05008389,  0.0209127 ],\n",
       "       ...,\n",
       "       [-0.00629492, -0.05131948, -0.05206006, ..., -0.0955721 ,\n",
       "        -0.01980347, -0.02646225],\n",
       "       [-0.00839109, -0.05062511, -0.05731497, ...,  0.07618995,\n",
       "         0.03893617,  0.06835916],\n",
       "       [ 0.07883421, -0.02804748,  0.09326512, ..., -0.0734218 ,\n",
       "         0.0045918 , -0.10043538]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_w_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03207342, -0.08436021,  0.36105958, -0.02925379, -0.26779142,\n",
       "       -0.4809294 , -0.04713235, -0.6384421 ,  0.49655244, -0.18943405,\n",
       "        0.32139724,  0.00376519, -0.02733823,  0.06968141,  0.12723993,\n",
       "        0.17086434,  0.4321125 ,  0.2930171 ,  0.43750855, -0.32728267,\n",
       "       -0.29320472,  0.46633536,  0.03467208, -0.11661331,  0.02106199,\n",
       "        0.10828511,  0.14413024,  0.00801178, -0.16259423,  0.31461757,\n",
       "       -0.25328413,  0.11320477], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03563995, -0.3379975 ,  1.6953684 , -0.3394102 , -1.3313038 ,\n",
       "        -1.9561913 , -0.34439525, -3.1041431 ,  2.302101  , -0.80340064,\n",
       "         1.374892  ,  0.10505254,  0.13142577,  0.36230466,  0.5036737 ,\n",
       "         0.6154421 ,  1.8879794 ,  1.263777  ,  2.0281937 , -1.4289141 ,\n",
       "        -1.3110695 ,  2.0390706 ,  0.32288784, -0.39520237,  0.10320839,\n",
       "         0.35608286,  0.61754334, -0.1438482 , -0.74413097,  1.4731948 ,\n",
       "        -1.1249324 ,  0.33894244],\n",
       "       [ 0.03510073, -0.3374708 ,  1.6888778 , -0.3363807 , -1.3308815 ,\n",
       "        -1.9608293 , -0.34732953, -3.1081154 ,  2.3005512 , -0.80602133,\n",
       "         1.3695524 ,  0.10256523,  0.13283278,  0.36182955,  0.49822897,\n",
       "         0.6213894 ,  1.8883276 ,  1.266403  ,  2.023539  , -1.426166  ,\n",
       "        -1.3108103 ,  2.0448976 ,  0.32430837, -0.38903922,  0.10560946,\n",
       "         0.3596735 ,  0.6160856 , -0.14840832, -0.74532497,  1.4738762 ,\n",
       "        -1.1249926 ,  0.33331037],\n",
       "       [ 0.03490497, -0.33676392,  1.6840277 , -0.335459  , -1.3295547 ,\n",
       "        -1.95664   , -0.3462094 , -3.1003447 ,  2.296549  , -0.80408335,\n",
       "         1.367739  ,  0.10281808,  0.13425133,  0.36240244,  0.4972452 ,\n",
       "         0.6206323 ,  1.885982  ,  1.2645416 ,  2.0215812 , -1.4212072 ,\n",
       "        -1.3081043 ,  2.0386996 ,  0.32136053, -0.39039296,  0.1050597 ,\n",
       "         0.36023104,  0.6137113 , -0.15039542, -0.7433981 ,  1.4694595 ,\n",
       "        -1.1245916 ,  0.3360805 ],\n",
       "       [ 0.03728028, -0.33768234,  1.6832604 , -0.3353232 , -1.325423  ,\n",
       "        -1.9593027 , -0.34599763, -3.1006794 ,  2.2962196 , -0.80693865,\n",
       "         1.3678529 ,  0.10114851,  0.13371727,  0.36420822,  0.49783063,\n",
       "         0.62049437,  1.8832903 ,  1.2625886 ,  2.0200832 , -1.421174  ,\n",
       "        -1.3108637 ,  2.0388248 ,  0.32396334, -0.38813725,  0.10552703,\n",
       "         0.3586173 ,  0.6122677 , -0.15264365, -0.74335086,  1.468328  ,\n",
       "        -1.1214948 ,  0.337562  ],\n",
       "       [ 0.0355639 , -0.33673748,  1.6850188 , -0.334822  , -1.3297123 ,\n",
       "        -1.9579692 , -0.34517902, -3.0999303 ,  2.296747  , -0.8048171 ,\n",
       "         1.3683693 ,  0.10311563,  0.13507861,  0.36239594,  0.4968614 ,\n",
       "         0.61993766,  1.8848993 ,  1.2634866 ,  2.0213115 , -1.4220002 ,\n",
       "        -1.3081868 ,  2.0382977 ,  0.32180363, -0.39071375,  0.10513556,\n",
       "         0.35903966,  0.6132862 , -0.15032193, -0.74355686,  1.4703372 ,\n",
       "        -1.1240762 ,  0.3361809 ],\n",
       "       [ 0.03776056, -0.33906677,  1.6886759 , -0.33578348, -1.3277831 ,\n",
       "        -1.9574624 , -0.3511446 , -3.1008887 ,  2.298576  , -0.80058026,\n",
       "         1.3695161 ,  0.10116885,  0.13310117,  0.36374724,  0.49994904,\n",
       "         0.62002206,  1.8863395 ,  1.2639412 ,  2.0218256 , -1.4257591 ,\n",
       "        -1.3068638 ,  2.038707  ,  0.3251842 , -0.38946715,  0.10602362,\n",
       "         0.3615039 ,  0.6127277 , -0.15162012, -0.739843  ,  1.4696462 ,\n",
       "        -1.1235677 ,  0.34125343],\n",
       "       [ 0.03675649, -0.33748797,  1.6818695 , -0.3343631 , -1.3270382 ,\n",
       "        -1.9566132 , -0.34484407, -3.0962553 ,  2.2958052 , -0.8025343 ,\n",
       "         1.365838  ,  0.10129849,  0.13410446,  0.36187994,  0.49527365,\n",
       "         0.62073433,  1.8827194 ,  1.264018  ,  2.019868  , -1.4191431 ,\n",
       "        -1.3083122 ,  2.036423  ,  0.32277   , -0.38972613,  0.10532874,\n",
       "         0.36029965,  0.6137044 , -0.14923215, -0.7424054 ,  1.4679198 ,\n",
       "        -1.1228634 ,  0.33640632],\n",
       "       [ 0.03632384, -0.33631054,  1.6842166 , -0.33446336, -1.3275865 ,\n",
       "        -1.959459  , -0.34500512, -3.1014085 ,  2.2958436 , -0.804686  ,\n",
       "         1.3688817 ,  0.10245646,  0.13472772,  0.3634492 ,  0.49877107,\n",
       "         0.61891234,  1.8855013 ,  1.263787  ,  2.0223496 , -1.4211094 ,\n",
       "        -1.309351  ,  2.0364125 ,  0.32266062, -0.3891954 ,  0.10458721,\n",
       "         0.35922444,  0.61319816, -0.14912874, -0.7429457 ,  1.469351  ,\n",
       "        -1.1232082 ,  0.3378038 ],\n",
       "       [ 0.03574873, -0.3368752 ,  1.6819855 , -0.33521634, -1.3290966 ,\n",
       "        -1.9556532 , -0.34607896, -3.0987115 ,  2.2963123 , -0.80486095,\n",
       "         1.3669474 ,  0.10132913,  0.13473387,  0.36283588,  0.49758494,\n",
       "         0.6216788 ,  1.8858868 ,  1.2627757 ,  2.0219636 , -1.4208897 ,\n",
       "        -1.3082187 ,  2.0389626 ,  0.32159996, -0.39116064,  0.1062526 ,\n",
       "         0.35957128,  0.6122661 , -0.14942539, -0.74453473,  1.468337  ,\n",
       "        -1.123489  ,  0.33671775],\n",
       "       [ 0.03600421, -0.33810538,  1.6826388 , -0.33433414, -1.3267329 ,\n",
       "        -1.9560882 , -0.34396413, -3.096003  ,  2.2952938 , -0.8032101 ,\n",
       "         1.366725  ,  0.10274363,  0.13443008,  0.36315334,  0.4956261 ,\n",
       "         0.6213716 ,  1.882305  ,  1.2623034 ,  2.0204127 , -1.4200761 ,\n",
       "        -1.3082595 ,  2.034914  ,  0.32189733, -0.38998067,  0.10444504,\n",
       "         0.35928738,  0.6142209 , -0.1489011 , -0.74237704,  1.4684169 ,\n",
       "        -1.1216142 ,  0.3379858 ]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02438005, -0.02036242, -0.01039507,  0.00277665,  0.02350294,\n",
       "         0.01473409, -0.02157389,  0.02251465,  0.00814133,  0.01785252],\n",
       "       [ 0.00074128, -0.001453  , -0.01162554, -0.0014644 , -0.02097618,\n",
       "         0.00570552,  0.00674119, -0.01879225,  0.02161046,  0.01941887],\n",
       "       [ 0.01402811, -0.00613734, -0.0027943 , -0.01052572, -0.00041091,\n",
       "         0.0045091 , -0.01547336,  0.0194314 ,  0.00784631,  0.00505763],\n",
       "       [ 0.01655601, -0.02403533,  0.01607199, -0.01643238,  0.00686195,\n",
       "        -0.01788632,  0.01109112,  0.00087933,  0.00448566, -0.00823843],\n",
       "       [-0.00695892,  0.01631877,  0.01657149,  0.01772625,  0.02026216,\n",
       "         0.01811663,  0.02153757, -0.00020743, -0.02066918, -0.00025119],\n",
       "       [ 0.02195978,  0.00867016, -0.02278856,  0.0122384 , -0.02183604,\n",
       "         0.00798479,  0.00545774,  0.00923075,  0.01814621,  0.02018008],\n",
       "       [ 0.02242495, -0.00673729,  0.01447002,  0.02163557,  0.02312511,\n",
       "        -0.00920323, -0.01116639,  0.01708085, -0.01181559, -0.00763743],\n",
       "       [-0.01978129,  0.00916991, -0.00304236,  0.00106503,  0.00690803,\n",
       "        -0.01416942,  0.0063372 ,  0.00018093,  0.00393429, -0.00307162],\n",
       "       [-0.01821756,  0.01890057,  0.00228031, -0.01240117, -0.01108933,\n",
       "        -0.01674686, -0.01161283, -0.01693372,  0.00370204, -0.00271433],\n",
       "       [-0.0158878 ,  0.01951969, -0.01606018, -0.00457847,  0.00044555,\n",
       "        -0.0050344 , -0.0013686 ,  0.00539651,  0.02015766, -0.00510509]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_w_mean_topic_infer[:, :10, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03922943,  0.46112132, -0.08166478, -0.06917454, -0.01617851,\n",
       "        0.18225916, -0.28025913, -0.06620797,  0.03231472, -0.02211766,\n",
       "       -0.38082743,  0.11382294,  0.39363608, -0.4168986 ,  1.1677127 ,\n",
       "        0.08979508,  0.03593519, -0.07863234,  0.12493958,  0.03175303,\n",
       "        0.13351525,  0.8176425 , -0.28750482,  0.37927672, -0.01720515,\n",
       "       -0.21228473, -0.05377329, -0.10451841, -0.24399954, -0.25936088,\n",
       "        0.0845396 ,  0.33591354], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enc_state_infer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-ee7c3cd147b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_enc_state_infer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_means_topic_infer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdebug_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menc_state_infer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans_topic_infer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'enc_state_infer' is not defined"
     ]
    }
   ],
   "source": [
    "_enc_state_infer, _means_topic_infer = debug_value([enc_state_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67, 10, 1024)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_enc_state_infer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_means_topic_infer[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
