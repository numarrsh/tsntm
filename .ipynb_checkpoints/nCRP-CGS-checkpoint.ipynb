{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "import _pickle as cPickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from scipy.special import gammaln\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/bags/instances.pkl', 'path of data')\n",
    "\n",
    "flags.DEFINE_integer('n_depth', 3, 'depth of tree')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "config = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <_io.FileIO name='data/bags/instances.pkl' mode='rb' closefd=True>\n",
      "ResourceWarning: unclosed file <_io.BufferedReader name='data/bags/instances.pkl'>\n"
     ]
    }
   ],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))\n",
    "docs_bow = [instance.bow for instance in instances_train]\n",
    "docs_raw = [[[bow_index]*int(doc_bow[bow_index]) for bow_index in np.where(doc_bow > 0)[0]] for doc_bow in docs_bow]\n",
    "docs_words = [[idx for idxs in doc for idx in idxs] for doc in docs_raw]\n",
    "words = [word for doc_words in docs_words for word in doc_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs_bow = [instance.bow for instance in instances_test]\n",
    "test_docs_raw = [[[bow_index]*int(test_doc_bow[bow_index]) for bow_index in np.where(test_doc_bow > 0)[0]] for test_doc_bow in test_docs_bow]\n",
    "test_docs_words = [[idx for idxs in doc for idx in idxs] for doc in test_docs_raw][:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31943, 1035, 568401)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_doc = len(docs_words)\n",
    "n_vocab = len(np.unique(words))\n",
    "n_words = len(words)\n",
    "assert n_vocab == len(bow_idxs)\n",
    "n_doc, n_vocab, n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assign docs to tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Topic:\n",
    "    def __init__(self, idx, sibling_idx, parent, depth, n_doc, n_vocab):\n",
    "        self.idx = idx\n",
    "        self.sibling_idx = sibling_idx\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.depth = depth\n",
    "        self.cnt_doc = 0\n",
    "        self.n_doc = n_doc\n",
    "        self.n_vocab = n_vocab\n",
    "        self.cnt_words = np.zeros(n_vocab) # Number of Words over Documents\n",
    "        self.set_prob_words()\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def sample_child(self, doc, train=True):\n",
    "        s_child_prior = self.get_s_child_prior(gam)\n",
    "        s_child_likelihood = self.get_s_child_likelihood(doc, eta)\n",
    "        p_child = np.array(s_child_prior * s_child_likelihood) / np.sum(s_child_prior * s_child_likelihood)\n",
    "        \n",
    "        child_index = np.random.multinomial(1, p_child).argmax()\n",
    "        if verbose: print('Depth: ', self.depth, 'p_child: ', p_child, 'selected:', child_index)\n",
    "        \n",
    "        if child_index < len(self.children):\n",
    "            child = self.children[child_index]\n",
    "        else:\n",
    "            child = self.get_new_child()\n",
    "            if train: self.children += [child]\n",
    "        return child\n",
    "    \n",
    "    def init_sample_child(self, train=True):\n",
    "        s_child_prior = self.get_s_child_prior(gam)\n",
    "        p_child = np.array(s_child_prior) / np.sum(s_child_prior)\n",
    "        \n",
    "        child_index = np.random.multinomial(1, p_child).argmax()\n",
    "        if verbose: print('Depth: ', self.depth, 'p_child: ', p_child, 'selected:', child_index)\n",
    "\n",
    "        if child_index < len(self.children):\n",
    "            child = self.children[child_index]\n",
    "        else:\n",
    "            child = self.get_new_child()\n",
    "            if train: self.children += [child]\n",
    "        return child\n",
    "    \n",
    "    def get_probs_child(self, doc):\n",
    "        s_child_prior = self.get_s_child_prior(gam)\n",
    "        s_child_likelihood = self.get_s_child_likelihood(doc, eta)\n",
    "        p_child = np.array(s_child_prior * s_child_likelihood) / np.sum(s_child_prior * s_child_likelihood)\n",
    "        return p_child\n",
    "    \n",
    "    def get_s_child_prior(self, gam):\n",
    "        s_child_prior = [child.cnt_doc for child in self.children]\n",
    "        s_child_prior += [gam]\n",
    "        return s_child_prior\n",
    "    \n",
    "    def get_s_child_likelihood(self, doc, eta):\n",
    "        if len(self.children) > 0:\n",
    "            children_cnt_words = np.concatenate([np.array([child.cnt_words for child in self.children]), np.zeros([1, self.n_vocab])], 0) # (Children+1) x Vocabulary\n",
    "        else:\n",
    "            children_cnt_words = np.zeros([1, self.n_vocab]) # (Children+1) x Vocabulary\n",
    "        \n",
    "        cnt_words_doc = doc.cnt_words[None, :] # 1 x Vocabulary\n",
    "\n",
    "        logits_likelihood = gammaln(np.sum(children_cnt_words, -1) + n_vocab*eta) \\\n",
    "                            - np.sum(gammaln(children_cnt_words + eta), -1) \\\n",
    "                            - gammaln(np.sum(children_cnt_words + cnt_words_doc, -1) + n_vocab*eta) \\\n",
    "                            + np.sum(gammaln(children_cnt_words + cnt_words_doc + eta), -1)\n",
    "        s_child_likelihood = np.exp(logits_likelihood)\n",
    "        return s_child_likelihood\n",
    "    \n",
    "    def get_new_child(self):\n",
    "        sibling_idx = max([child.sibling_idx for child in self.children]) + 1 if len(self.children) > 0 else 1\n",
    "        idx = self.idx + '-' + str(sibling_idx)\n",
    "        depth = self.depth+1\n",
    "        child = Topic(idx=idx, sibling_idx=sibling_idx, parent=self, depth=depth, n_doc=self.n_doc, n_vocab=self.n_vocab)        \n",
    "        return child\n",
    "        \n",
    "    def get_children(self):\n",
    "        child = self.get_new_child()\n",
    "        children = self.children + [child]\n",
    "        return children\n",
    "    \n",
    "    def delete_topic(self):\n",
    "        self.parent.children.remove(self)\n",
    "        \n",
    "    def set_prob_words(self):\n",
    "        cnt_words = self.cnt_words + eta\n",
    "        self.prob_words = cnt_words / np.sum(cnt_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc:\n",
    "    def __init__(self, idx, words, bow, n_depth):\n",
    "        self.idx = idx\n",
    "        self.words = words\n",
    "        self.cnt_words = bow\n",
    "        assert len(words) == np.sum(bow)\n",
    "        \n",
    "        self.topics = [] # Depth\n",
    "        self.word_depths = [] # Word Indices\n",
    "        self.depth_cnt_words = np.zeros([n_depth, n_vocab])\n",
    "                \n",
    "    def get_probs_depth(self, word_idx):\n",
    "        s_docs = np.sum(self.depth_cnt_words, -1) + alpha # Depth\n",
    "        s_words = np.array([topic.cnt_words[word_idx] for topic in self.topics]) + eta # Depth\n",
    "        z_words = np.array([np.sum(topic.cnt_words) for topic in self.topics]) + n_vocab*eta # Depth\n",
    "        assert s_docs.shape == s_words.shape == z_words.shape\n",
    "\n",
    "        s_depths = s_docs*s_words/z_words\n",
    "        p_depths = s_depths/np.sum(s_depths) # Depth\n",
    "        return p_depths\n",
    "    \n",
    "    def sample_depth(self, word_idx):\n",
    "        prob_depths = self.get_probs_depth(word_idx)\n",
    "        word_depth = np.argmax(np.random.multinomial(1, prob_depths))\n",
    "        return word_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample doc path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p({\\bf c}_{m}\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf w}, {\\bf c}_{-m}, {\\bf z})\\propto p({\\bf w}_{m}\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf c}, {\\bf w}_{-m}, {\\bf z})\\cdot p({\\bf c}_{m}\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf c}_{-m})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p({\\bf w}_{m}\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf c}, {\\bf w}_{-m}, {\\bf z})=\\prod_{\\ell=1}^{L}\\left(\\frac{\\Gamma(n_{c_{m,\\ell},-m}^{(\\cdot)}+W\\eta)}{\\prod_{w}\\Gamma(n_{c_{m,\\ell},-m}^{(w)}+\\eta)}\\frac{\\prod_{w}\\Gamma(n_{c_{m,\\ell},-m}^{(w)}+n_{c_{m,\\ell},m}^{(w)}+\\eta)}{\\Gamma(n_{c_{m,\\ell},-m}^{(\\cdot)}+n_{c_{m,\\ell},m}^{(\\cdot)}+W\\eta)}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_doc_topics(docs, topic_root, train=True):\n",
    "    for doc in docs:\n",
    "        topic = topic_root\n",
    "        doc.topics = [topic]\n",
    "        if train: topic.cnt_doc += 1 # increment count of docs\n",
    "\n",
    "        for depth in range(1, n_depth):\n",
    "            topic = topic.init_sample_child(train=train)\n",
    "            doc.topics += [topic]\n",
    "            if train: topic.cnt_doc += 1 # increment count of docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_doc_topics(docs, topic_root, train=True):\n",
    "    for doc in docs:\n",
    "        if train:\n",
    "            for depth in range(1, n_depth):\n",
    "                topic = doc.topics[depth]\n",
    "                topic.cnt_doc -= 1 # decrement count of docs\n",
    "                assert topic.cnt_doc >= 0\n",
    "                topic.cnt_words -= doc.depth_cnt_words[depth] # decrement count of words\n",
    "                assert np.min(topic.cnt_words) >= 0\n",
    "\n",
    "                if topic.cnt_doc == 0: \n",
    "                    topic.delete_topic()\n",
    "                    assert np.sum(topic.cnt_words) == 0\n",
    "\n",
    "        topic = topic_root\n",
    "        doc.topics = [topic]\n",
    "        for depth in range(1, n_depth):\n",
    "            topic = topic.sample_child(doc, train=train)\n",
    "            doc.topics += [topic]\n",
    "            if train: topic.cnt_doc += 1 # increment count of docs\n",
    "            if train: topic.cnt_words += doc.depth_cnt_words[depth] # increment count of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assign words to topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "p(z_{i}=j\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf z}_{-i},{\\bf w})\\propto (n_{-i,j}^{(d_{i})}+\\alpha)\\frac{n_{-i,j}^{(w_{i})}+\\eta}{n_{-i,j}^{(\\cdot)}+W\\eta}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_word_topics(docs, train=True):\n",
    "    for doc in docs:\n",
    "        if doc.idx % 10000 == 0: print(doc.idx, end=' ')\n",
    "        for word_index, word_idx in enumerate(doc.words):\n",
    "            # sample depth of word\n",
    "            new_depth = doc.sample_depth(word_idx)\n",
    "            new_topic = doc.topics[new_depth]\n",
    "            \n",
    "            # increment count of words\n",
    "            doc.depth_cnt_words[new_depth, word_idx] += 1\n",
    "            if train: new_topic.cnt_words[word_idx] += 1\n",
    "            doc.word_depths.append(new_depth) # for reference when sampling\n",
    "            \n",
    "        assert len(doc.word_depths) == len(doc.words) == np.sum(doc.depth_cnt_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_word_topics(docs, train=True):\n",
    "    for doc in docs:\n",
    "        if doc.idx % 10000 == 0: print(doc.idx, end=' ')\n",
    "        for word_index, word_idx in enumerate(doc.words):\n",
    "            # refer depth of word\n",
    "            old_depth = doc.word_depths[word_index]\n",
    "            old_topic = doc.topics[old_depth]\n",
    "            \n",
    "            # decrement count of words\n",
    "            doc.depth_cnt_words[old_depth, word_idx] -= 1\n",
    "            if train: old_topic.cnt_words[word_idx] -= 1            \n",
    "            \n",
    "            # sample depth of word\n",
    "            new_depth = doc.sample_depth(word_idx)\n",
    "            new_topic = doc.topics[new_depth]\n",
    "            \n",
    "            # increment count of words\n",
    "            doc.depth_cnt_words[new_depth, word_idx] += 1\n",
    "            if train: new_topic.cnt_words[word_idx] += 1\n",
    "            doc.word_depths[word_index] = new_depth # for sample\n",
    "            \n",
    "        assert len(doc.word_depths) == len(doc.words) == np.sum(doc.depth_cnt_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recur_cnt_words(topic):\n",
    "    cnt_words = np.sum(topic.cnt_words)\n",
    "    for child in topic.children:\n",
    "        cnt_words += recur_cnt_words(child)\n",
    "    return cnt_words\n",
    "    \n",
    "def assert_sum_cnt_words(topic_root):\n",
    "    sum_cnt_words = recur_cnt_words(topic_root)\n",
    "    assert sum_cnt_words == sum([len(doc.words) for doc in docs])\n",
    "    \n",
    "def nearly_equal(val, thre):\n",
    "    return (val > thre-1e-5) and (val < thre+1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.array([10., 5., 1.])\n",
    "gam = 0.01\n",
    "eta = 1.\n",
    "n_depth = 3\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_sample = 100\n",
    "docs = [Doc(idx=doc_idx, words=doc_words, bow=doc_bow, n_depth=config.n_depth) for doc_idx, (doc_words, doc_bow) in enumerate(zip(docs_words, docs_bow))]\n",
    "topic_root = Topic(idx='0', sibling_idx=0, parent=None, depth=0, n_doc=n_doc, n_vocab=n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_train(docs, topic_root):\n",
    "    init_doc_topics(docs=docs, topic_root=topic_root)\n",
    "    init_word_topics(docs=docs)\n",
    "    assert_sum_cnt_words(topic_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_train(docs, topic_root):\n",
    "    sample_doc_topics(docs=docs, topic_root=topic_root)\n",
    "    sample_word_topics(docs=docs)\n",
    "    assert_sum_cnt_words(topic_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_perplexity(docs, topic_root):\n",
    "    def set_prob_words(topic):\n",
    "        topic.set_prob_words()\n",
    "        for topic_child in topic.children:\n",
    "            set_prob_words(topic_child)\n",
    "            \n",
    "    # set Probabilty of Words\n",
    "    set_prob_words(topic_root)\n",
    "    \n",
    "    logit_docs = []\n",
    "    for doc in docs:\n",
    "        # Path Probability for each document\n",
    "        topic = topic_root\n",
    "        probs_paths= [{topic: 1.}]\n",
    "        for depth in range(1, n_depth):\n",
    "            probs_path = {}\n",
    "            for topic, prob_path in probs_paths[-1].items():\n",
    "                topics_child = topic.get_children()\n",
    "                probs_child = topic.get_probs_child(doc)\n",
    "                probs_path_child = prob_path * probs_child\n",
    "                for topic_child, prob_path_child in zip(topics_child, probs_path_child):\n",
    "                    probs_path[topic_child] = prob_path_child\n",
    "            probs_paths.append(probs_path)    \n",
    "            \n",
    "        assert nearly_equal(np.sum([sum(probs_path.values()) for probs_path in probs_paths]), n_depth)        \n",
    "\n",
    "        # Depth Probability for Each Word\n",
    "        probs_depths = []\n",
    "        for word_index, word_idx in enumerate(doc.words):\n",
    "            probs_depth = doc.get_probs_depth(word_idx)\n",
    "            probs_depths.append(probs_depth)\n",
    "            \n",
    "        assert nearly_equal(np.sum(probs_depths), len(doc.words))\n",
    "    \n",
    "        # Likelihood of Doc\n",
    "        assert len(probs_depths) == len(doc.words)\n",
    "        logit_doc = 0\n",
    "        for prob_depths, word_idx in zip(probs_depths, doc.words): # for each word\n",
    "#             prob_topics, prob_word_topics = [], []\n",
    "            prob_word = 0\n",
    "            for prob_paths, prob_depth in zip(probs_paths, prob_depths):d # for each depth\n",
    "                for topic, prob_path in prob_paths.items(): # for each topic in the depth\n",
    "                    prob_topic = prob_path * prob_depth # scalar\n",
    "                    prob_word_topic = topic.prob_words[word_idx] # scalar\n",
    "#                     prob_topics.append(prob_topic)\n",
    "#                     prob_word_topics.append(prob_word_topic)\n",
    "                    prob_word += prob_topic * prob_word_topic\n",
    "            logit_word = np.log(prob_word)\n",
    "            logit_doc += logit_word\n",
    "        logit_docs.append(logit_doc / len(doc.words))\n",
    "#         n_words += len(doc.words)\n",
    "#         assert nearly_equal(sum(prob_topics), 1.)\n",
    "        \n",
    "#     perplexity = np.exp(-logit_docs/n_words)\n",
    "    perplexity = np.exp(-np.mean(logit_docs))\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perplexity(docs, topic_root):\n",
    "    def set_prob_words(topic):\n",
    "        topic.set_prob_words()\n",
    "        for topic_child in topic.children:\n",
    "            set_prob_words(topic_child)\n",
    "            \n",
    "    # set Probabilty of Words\n",
    "    set_prob_words(topic_root)\n",
    "\n",
    "    logit_docs = []\n",
    "    for docs in docs:\n",
    "        # Path Probability for each document\n",
    "        topic = topic_root\n",
    "        probs_paths= [{topic: 1.}]\n",
    "        for depth in range(1, n_depth):\n",
    "            probs_path = {}\n",
    "            for topic, prob_path in probs_paths[-1].items():\n",
    "                topics_child = topic.get_children()\n",
    "                probs_child = topic.get_probs_child(doc)\n",
    "                probs_path_child = prob_path * probs_child\n",
    "                for topic_child, prob_path_child in zip(topics_child, probs_path_child):\n",
    "                    probs_path[topic_child] = prob_path_child\n",
    "            probs_paths.append(probs_path)    \n",
    "\n",
    "        all_topics = []\n",
    "        for probs_path in probs_paths:\n",
    "            all_topics += list(probs_path.keys())\n",
    "\n",
    "        # topic probability for each depth\n",
    "        probs_depths_topics = np.array([[probs_path[topic] if topic in probs_path else 0. for topic in all_topics] for probs_path in probs_paths])\n",
    "        # depth probability for each word\n",
    "        probs_words_depths = np.array([doc.get_probs_depth(word_idx) for word_idx in doc.words])\n",
    "        # topic probability for each depth\n",
    "        probs_words_topics = probs_words_depths.dot(probs_depths_topics) # n_doc x n_topic\n",
    "\n",
    "        probs_topics_words = np.array([topic.prob_words for topic in all_topics]) # n_topic x n_vocab\n",
    "        probs_words_bow = probs_words_topics.dot(probs_topics_words) # n_doc x n_vocab\n",
    "        logit_words = np.log(probs_words_bow[np.arange(len(doc.words)), doc.words])\n",
    "        logit_doc = np.mean(logit_words)\n",
    "        logit_docs.append(logit_doc)\n",
    "\n",
    "    perplexity = np.exp(-np.mean(logit_docs))\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_test(test_docs, topic_root):\n",
    "    init_doc_topics(docs=test_docs, topic_root=topic_root, train=False)\n",
    "    init_word_topics(docs=test_docs, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_test(test_docs, topic_root):\n",
    "    sample_doc_topics(docs=test_docs, topic_root=topic_root, train=False)\n",
    "    sample_word_topics(docs=test_docs, train=False)\n",
    "    assert_sum_cnt_words(topic_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 : ['0-1', '0-2', '0-3', '0-4', '0-5', '0-6'] 5000 53338.0 ['!', 'carry', 'sleeve', 'nice', 'price', 'back', 'bought', 'pockets', 'perfect', 'zipper']\n",
      "   0-1 : ['0-1-1', '0-1-2', '0-1-3'] 1877 11682.0 ['pocket', 'room', 'small', 'charger', 'space', 'side', 'inside', 'mouse', 'put', 'power']\n",
      "     0-1-1 : [] 464 446.0 ['netbook', 'smell', '?', 'acer', 'mini', 'functional', 'bit', 'mouse', 'read', 'bulky']\n",
      "     0-1-2 : [] 1047 1080.0 ['padded', 'pink', 'things', 'lunch', 'supply', 'netbook', 'mini', 'dell', 'small', 'nice']\n",
      "     0-1-3 : [] 366 370.0 ['zippered', 'compartment', 'gear', 'bottom', 'velcro', 'phone', 'plan', 'days', 'double', 'swiss']\n",
      "   0-2 : ['0-2-1', '0-2-2'] 1095 7903.0 ['handle', 'straps', 'wheels', 'months', 'zippers', \"'m\", 'year', 'broke', 'compartments', 'player']\n",
      "     0-2-1 : [] 641 777.0 ['player', 'trips', 'roller', 'clothes', 'leather', 'excellent', 'samsonite', 'products', 'amount', 'rest']\n",
      "     0-2-2 : [] 454 561.0 ['samsonite', 'update', 'completely', 'roomy', 'straps', 'recently', 'unit', 'wearing', 'easily', 'make']\n",
      "   0-3 : ['0-3-1', '0-3-2'] 986 6046.0 ['color', 'cover', \"'m\", 'bottom', 'scratches', 'recommend', 'easily', 'buy', 'shell', 'feet']\n",
      "     0-3-1 : [] 632 1027.0 ['cover', 'keyboard', 'mac', 'clear', 'snap', 'protecting', 'ports', 'easy', 'plastic', 'cute']\n",
      "     0-3-2 : [] 354 423.0 ['protects', 'ipearl', 'waste', 'damage', 'careful', 'absolutely', 'wanted', 'netbook', 'mention', 'length']\n",
      "   0-4 : ['0-4-1'] 590 4395.0 ['usb', 'sleeve', 'pocket', 'inside', 'power', 'pro', 'drive', 'card', 'charger', 'smell']\n",
      "     0-4-1 : [] 590 694.0 ['smell', 'perfectly', 'apple', 'install', 'cable', 'book', 'interior', 'model', 'returning', 'package']\n",
      "   0-5 : ['0-5-1', '0-5-2'] 439 2512.0 ['quality', 'travel', 'amazon', 'bags', \"'m\", '...', 'weight', 'office', 'wanted', 'purchase']\n",
      "     0-5-1 : [] 337 465.0 ['life', 'travels', 'mobile', 'appearance', 'delivered', 'quality', 'choice', 'wide', 'pictured', '...']\n",
      "     0-5-2 : [] 102 169.0 ['coming', '-inch', 'realized', 'fast', 'told', 'security', 'correct', 'corner', 'cords', 'heavy']\n",
      "   0-6 : ['0-6-1'] 13 95.0 ['mbp', ':', 'similar', '-inch', 'limited', 'retina', 'tablet', 'comments', 'love', 'attached']\n",
      "     0-6-1 : [] 13 26.0 ['feature', 'plastic', 'bulky', 'apple', 'screen', 'deal', 'convenient', 'seat', 'bring', 'move']\n"
     ]
    }
   ],
   "source": [
    "def print_child_idxs(topic):\n",
    "    topic_freq_words = [idx_to_word[bow_idxs[bow_index]] for bow_index in np.argsort(topic.cnt_words)[::-1][:10]]\n",
    "    print('  '*topic.depth, topic.idx, ':', [child.idx for child in topic.children], topic.cnt_doc, np.sum(topic.cnt_words), topic_freq_words)\n",
    "    for topic in topic.children:\n",
    "        print_child_idxs(topic)\n",
    "print_child_idxs(topic_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 100\n",
    "alpha = np.array([10., 5., 1.])\n",
    "gam = 0.01\n",
    "eta = 1.\n",
    "n_depth = 3\n",
    "verbose = False\n",
    "topic_root = Topic(idx='0', sibling_idx=0, parent=None, depth=0, n_doc=n_doc, n_vocab=n_vocab)\n",
    "docs = [Doc(idx=doc_idx, words=doc_words, bow=doc_bow, n_depth=config.n_depth) for doc_idx, (doc_words, doc_bow) in enumerate(zip(docs_words, docs_bow))]\n",
    "test_docs = [Doc(idx=doc_idx, words=doc_words, bow=doc_bow, n_depth=config.n_depth) for doc_idx, (doc_words, doc_bow) in enumerate(zip(test_docs_words, test_docs_bow))]\n",
    "\n",
    "n_sample=3\n",
    "docs = docs[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 Perplexity: 665.02\n",
      "0 0 Perplexity: 682.43\n",
      "0 0 Perplexity: 552.87\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_sample):\n",
    "    if i == 0:\n",
    "        init_train(docs, topic_root)\n",
    "        init_test(test_docs, topic_root)        \n",
    "        assert_sum_cnt_words(topic_root)\n",
    "    else:\n",
    "        sample_train(docs, topic_root)\n",
    "        sample_test(test_docs, topic_root)                \n",
    "        assert_sum_cnt_words(topic_root)\n",
    "        \n",
    "    perplexity = get_perplexity(test_docs, topic_root)\n",
    "    print('Perplexity: %.2f' % perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "552.8714071923821"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_perplexity(test_docs, topic_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_perplexity(test_docs, topic_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_topics(self):\n",
    "    topics = [self]\n",
    "    for child in self.children:\n",
    "        topics += child.get_all_topics()\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = test_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_prob_words(topic):\n",
    "    topic.set_prob_words()\n",
    "    for topic_child in topic.children:\n",
    "        set_prob_words(topic_child)\n",
    "\n",
    "# set Probabilty of Words\n",
    "set_prob_words(topic_root)\n",
    "\n",
    "# Path Probability for each document\n",
    "topic = topic_root\n",
    "probs_paths= [{topic: 1.}]\n",
    "for depth in range(1, n_depth):\n",
    "    probs_path = {}\n",
    "    for topic, prob_path in probs_paths[-1].items():\n",
    "        topics_child = topic.get_children()\n",
    "        probs_child = topic.get_probs_child(doc)\n",
    "        probs_path_child = prob_path * probs_child\n",
    "        for topic_child, prob_path_child in zip(topics_child, probs_path_child):\n",
    "            probs_path[topic_child] = prob_path_child\n",
    "    probs_paths.append(probs_path)    \n",
    "\n",
    "assert nearly_equal(np.sum([sum(probs_path.values()) for probs_path in probs_paths]), n_depth)        \n",
    "\n",
    "# Depth Probability for Each Word\n",
    "probs_depths = []\n",
    "for word_index, word_idx in enumerate(doc.words):\n",
    "    probs_depth = doc.get_probs_depth(word_idx)\n",
    "    probs_depths.append(probs_depth)\n",
    "\n",
    "assert nearly_equal(np.sum(probs_depths), len(doc.words))\n",
    "\n",
    "# Likelihood of Doc\n",
    "assert len(probs_depths) == len(doc.words)\n",
    "logit_doc = []\n",
    "for prob_depths, word_idx in zip(probs_depths, doc.words): # for each word\n",
    "#             prob_topics, prob_word_topics = [], []\n",
    "    prob_word = 0\n",
    "    for prob_paths, prob_depth in zip(probs_paths, prob_depths): # for each depth\n",
    "        for topic, prob_path in prob_paths.items(): # for each topic in the depth\n",
    "            prob_topic = prob_path * prob_depth # scalar\n",
    "            prob_word_topic = topic.prob_words[word_idx] # scalar\n",
    "    #                     prob_topics.append(prob_topic)\n",
    "    #                     prob_word_topics.append(prob_word_topic)\n",
    "            prob_word += prob_topic * prob_word_topic\n",
    "    logit_word = np.log(prob_word)\n",
    "    logit_doc.append(logit_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{<__main__.Topic at 0x7f8129452080>: 1.0},\n",
       " {<__main__.Topic at 0x7f8121f84dd8>: 4.425720882188072e-05,\n",
       "  <__main__.Topic at 0x7f8121f84e10>: 0.5071577894402334,\n",
       "  <__main__.Topic at 0x7f8121f98390>: 0.00040491175948256585,\n",
       "  <__main__.Topic at 0x7f8121f98518>: 0.00014083339921378528,\n",
       "  <__main__.Topic at 0x7f8121f98da0>: 0.4918627965187081,\n",
       "  <__main__.Topic at 0x7f8121f98940>: 0.0003886947578963892,\n",
       "  <__main__.Topic at 0x7f8121affcf8>: 7.169156437156351e-07},\n",
       " {<__main__.Topic at 0x7f8121f98080>: 2.456923151771099e-05,\n",
       "  <__main__.Topic at 0x7f8121f98128>: 7.399719831308048e-06,\n",
       "  <__main__.Topic at 0x7f8121f981d0>: 1.228747376818923e-05,\n",
       "  <__main__.Topic at 0x7f8121affeb8>: 7.837046724513962e-10,\n",
       "  <__main__.Topic at 0x7f8121f84f98>: 0.059416715720178384,\n",
       "  <__main__.Topic at 0x7f8121f986d8>: 0.44774064640483197,\n",
       "  <__main__.Topic at 0x7f8121aff240>: 4.2731522291219527e-07,\n",
       "  <__main__.Topic at 0x7f8121f98470>: 1.8899315515145085e-06,\n",
       "  <__main__.Topic at 0x7f8121f98208>: 0.00040300741273527357,\n",
       "  <__main__.Topic at 0x7f8121afffd0>: 1.4415195777752732e-08,\n",
       "  <__main__.Topic at 0x7f8121f98630>: 0.0001406351479302014,\n",
       "  <__main__.Topic at 0x7f8121affd30>: 1.9825128358384484e-07,\n",
       "  <__main__.Topic at 0x7f8121f98e48>: 0.29333266068130603,\n",
       "  <__main__.Topic at 0x7f8121f98f28>: 0.19848624981884908,\n",
       "  <__main__.Topic at 0x7f8121affdd8>: 4.388601855303737e-05,\n",
       "  <__main__.Topic at 0x7f8121f98908>: 0.0003883123487663018,\n",
       "  <__main__.Topic at 0x7f8121affa58>: 3.8240913008735733e-07,\n",
       "  <__main__.Topic at 0x7f8121aff748>: 7.169156437156351e-07}]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.94743784, 0.04813107, 0.00443109]),\n",
       " array([0.41080002, 0.54878201, 0.04041798]),\n",
       " array([0.4835144 , 0.47294491, 0.04354069]),\n",
       " array([0.48628011, 0.45755478, 0.0561651 ]),\n",
       " array([0.60715314, 0.37013003, 0.02271683]),\n",
       " array([0.57593792, 0.37769928, 0.0463628 ]),\n",
       " array([0.70134451, 0.27347828, 0.02517721]),\n",
       " array([0.93380429, 0.0559026 , 0.01029311]),\n",
       " array([0.88423688, 0.10997749, 0.00578563]),\n",
       " array([0.56486546, 0.41338731, 0.02174722])]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path Probability for each document\n",
    "topic = topic_root\n",
    "probs_paths= [{topic: 1.}]\n",
    "for depth in range(1, n_depth):\n",
    "    probs_path = {}\n",
    "    for topic, prob_path in probs_paths[-1].items():\n",
    "        topics_child = topic.get_children()\n",
    "        probs_child = topic.get_probs_child(doc)\n",
    "        probs_path_child = prob_path * probs_child\n",
    "        for topic_child, prob_path_child in zip(topics_child, probs_path_child):\n",
    "            probs_path[topic_child] = prob_path_child\n",
    "    probs_paths.append(probs_path)    \n",
    "\n",
    "all_topics = []\n",
    "for probs_path in probs_paths:\n",
    "    all_topics += list(probs_path.keys())\n",
    "\n",
    "# topic probability for each depth\n",
    "probs_depths_topics = np.array([[probs_path[topic] if topic in probs_path else 0. for topic in all_topics] for probs_path in probs_paths])\n",
    "# depth probability for each word\n",
    "probs_words_depths = np.array([doc.get_probs_depth(word_idx) for word_idx in doc.words])\n",
    "# topic probability for each depth\n",
    "probs_words_topics = probs_words_depths.dot(probs_depths_topics)\n",
    "\n",
    "probs_topics_words = np.array([topic.prob_words for topic in all_topics])\n",
    "probs_words_bow = probs_words_topics.dot(probs_topics_words)\n",
    "logit_words = np.log(probs_words_bow[np.arange(len(doc.words)), doc.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{<__main__.Topic at 0x7f8129452080>: 1.0},\n",
       " {<__main__.Topic at 0x7f8121f84dd8>: 4.425720882188072e-05,\n",
       "  <__main__.Topic at 0x7f8121f84e10>: 0.5071577894402334,\n",
       "  <__main__.Topic at 0x7f8121f98390>: 0.00040491175948256585,\n",
       "  <__main__.Topic at 0x7f8121f98518>: 0.00014083339921378528,\n",
       "  <__main__.Topic at 0x7f8121f98da0>: 0.4918627965187081,\n",
       "  <__main__.Topic at 0x7f8121f98940>: 0.0003886947578963892,\n",
       "  <__main__.Topic at 0x7f8121a61710>: 7.169156437156351e-07},\n",
       " {<__main__.Topic at 0x7f8121f98080>: 2.456923151771099e-05,\n",
       "  <__main__.Topic at 0x7f8121f98128>: 7.399719831308048e-06,\n",
       "  <__main__.Topic at 0x7f8121f981d0>: 1.228747376818923e-05,\n",
       "  <__main__.Topic at 0x7f8121a61668>: 7.837046724513962e-10,\n",
       "  <__main__.Topic at 0x7f8121f84f98>: 0.059416715720178384,\n",
       "  <__main__.Topic at 0x7f8121f986d8>: 0.44774064640483197,\n",
       "  <__main__.Topic at 0x7f8121a615f8>: 4.2731522291219527e-07,\n",
       "  <__main__.Topic at 0x7f8121f98470>: 1.8899315515145085e-06,\n",
       "  <__main__.Topic at 0x7f8121f98208>: 0.00040300741273527357,\n",
       "  <__main__.Topic at 0x7f8121a61550>: 1.4415195777752732e-08,\n",
       "  <__main__.Topic at 0x7f8121f98630>: 0.0001406351479302014,\n",
       "  <__main__.Topic at 0x7f8121a61438>: 1.9825128358384484e-07,\n",
       "  <__main__.Topic at 0x7f8121f98e48>: 0.29333266068130603,\n",
       "  <__main__.Topic at 0x7f8121f98f28>: 0.19848624981884908,\n",
       "  <__main__.Topic at 0x7f8121a61198>: 4.388601855303737e-05,\n",
       "  <__main__.Topic at 0x7f8121f98908>: 0.0003883123487663018,\n",
       "  <__main__.Topic at 0x7f8121a610b8>: 3.8240913008735733e-07,\n",
       "  <__main__.Topic at 0x7f8121d38d68>: 7.169156437156351e-07}]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 4.42572088e-05, 5.07157789e-01, 4.04911759e-04,\n",
       "        1.40833399e-04, 4.91862797e-01, 3.88694758e-04, 7.16915644e-07,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.45692315e-05, 7.39971983e-06, 1.22874738e-05, 7.83704672e-10,\n",
       "        5.94167157e-02, 4.47740646e-01, 4.27315223e-07, 1.88993155e-06,\n",
       "        4.03007413e-04, 1.44151958e-08, 1.40635148e-04, 1.98251284e-07,\n",
       "        2.93332661e-01, 1.98486250e-01, 4.38860186e-05, 3.88312349e-04,\n",
       "        3.82409130e-07, 7.16915644e-07]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_depths_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.47437839e-01, 2.13014700e-06, 2.44100492e-02, 1.94888379e-05,\n",
       "        6.77846278e-06, 2.36738847e-02, 1.87082962e-05, 3.45059200e-08,\n",
       "        1.08868402e-07, 3.27888024e-08, 5.44468653e-08, 3.47266359e-12,\n",
       "        2.63280637e-04, 1.98397776e-03, 1.89347093e-09, 8.37445113e-09,\n",
       "        1.78576091e-06, 6.38749866e-11, 6.23166576e-07, 8.78468686e-10,\n",
       "        1.29978254e-03, 8.79509842e-04, 1.94462766e-07, 1.72064580e-06,\n",
       "        1.69448813e-09, 3.17671559e-09],\n",
       "       [4.10800015e-01, 2.42875599e-05, 2.78319070e-01, 2.22208288e-04,\n",
       "        7.72868355e-05, 2.69925453e-01, 2.13308690e-04, 3.93430406e-07,\n",
       "        9.93038639e-07, 2.99081707e-07, 4.96634834e-07, 3.16757576e-11,\n",
       "        2.40150346e-03, 1.80967712e-02, 1.72712169e-08, 7.63872103e-08,\n",
       "        1.62887444e-05, 5.82633054e-10, 5.68418820e-06, 8.01291585e-09,\n",
       "        1.18559128e-02, 8.02241271e-03, 1.77378410e-06, 1.56947996e-05,\n",
       "        1.54562035e-08, 2.89762801e-08],\n",
       "       [4.83514398e-01, 2.09312218e-05, 2.39857697e-01, 1.91500957e-04,\n",
       "        6.66064399e-05, 2.32624008e-01, 1.83831209e-04, 3.39061607e-07,\n",
       "        1.06976123e-06, 3.22188889e-07, 5.35005056e-07, 3.41230403e-11,\n",
       "        2.58704465e-03, 1.94949356e-02, 1.86055986e-08, 8.22889192e-08,\n",
       "        1.75472198e-05, 6.27647535e-10, 6.12335103e-06, 8.63199720e-09,\n",
       "        1.27719057e-02, 8.64222779e-03, 1.91082742e-06, 1.69073866e-05,\n",
       "        1.66503564e-08, 3.12150000e-08],\n",
       "       [4.86280113e-01, 2.02500976e-05, 2.32052473e-01, 1.85269313e-04,\n",
       "        6.44389955e-05, 2.25054176e-01, 1.77849146e-04, 3.28028182e-07,\n",
       "        1.37993342e-06, 4.15606027e-07, 6.90127231e-07, 4.40168537e-11,\n",
       "        3.33714596e-03, 2.51473996e-02, 2.40002035e-08, 1.06148200e-07,\n",
       "        2.26349529e-05, 8.09630957e-10, 7.89878758e-06, 1.11348038e-08,\n",
       "        1.64750591e-02, 1.11480007e-02, 2.46486276e-06, 2.18096031e-05,\n",
       "        2.14780482e-08, 4.02656410e-08],\n",
       "       [6.07153140e-01, 1.63809219e-05, 1.87714327e-01, 1.49870001e-04,\n",
       "        5.21266699e-05, 1.82053190e-01, 1.43867601e-04, 2.65352007e-07,\n",
       "        5.58135127e-07, 1.68098199e-07, 2.79132488e-07, 1.78032881e-11,\n",
       "        1.34975960e-03, 1.01712495e-02, 9.70724852e-09, 4.29332593e-08,\n",
       "        9.15505206e-06, 3.27467594e-10, 3.19478516e-06, 4.50364128e-09,\n",
       "        6.66358904e-03, 4.50897897e-03, 9.96951351e-07, 8.82122674e-06,\n",
       "        8.68712431e-09, 1.62860529e-08],\n",
       "       [5.75937920e-01, 1.67159161e-05, 1.91553134e-01, 1.52934882e-04,\n",
       "        5.31926740e-05, 1.85776226e-01, 1.46809732e-04, 2.70778525e-07,\n",
       "        1.13909827e-06, 3.43071703e-07, 5.69681642e-07, 3.63347400e-11,\n",
       "        2.75472508e-03, 2.07585083e-02, 1.98115286e-08, 8.76225114e-08,\n",
       "        1.86845505e-05, 6.68328784e-10, 6.52023870e-06, 9.19148386e-09,\n",
       "        1.35997224e-02, 9.20237755e-03, 2.03467853e-06, 1.80032463e-05,\n",
       "        1.77295566e-08, 3.32382139e-08],\n",
       "       [7.01344509e-01, 1.21033855e-05, 1.38696642e-01, 1.10734573e-04,\n",
       "        3.85148763e-05, 1.34513793e-01, 1.06299575e-04, 1.96060860e-07,\n",
       "        6.18584653e-07, 1.86304286e-07, 3.09364283e-07, 1.97314956e-11,\n",
       "        1.49594701e-03, 1.12728594e-02, 1.07586043e-08, 4.75831998e-08,\n",
       "        1.01466015e-05, 3.62934383e-10, 3.54080038e-06, 4.99141381e-09,\n",
       "        7.38529742e-03, 4.99732960e-03, 1.10492742e-06, 9.77662079e-06,\n",
       "        9.62799422e-09, 1.80499343e-08],\n",
       "       [9.33804290e-01, 2.47409295e-06, 2.83514380e-02, 2.26356193e-05,\n",
       "        7.87295288e-06, 2.74964081e-02, 2.17290467e-05, 4.00774469e-08,\n",
       "        2.52893858e-07, 7.61661469e-08, 1.26476347e-07, 8.06676017e-12,\n",
       "        6.11582925e-04, 4.60864474e-03, 4.39840356e-09, 1.94532776e-08,\n",
       "        4.14820054e-06, 1.48377228e-10, 1.44757337e-06, 2.04062272e-09,\n",
       "        3.01930601e-03, 2.04304125e-03, 4.51723716e-07, 3.99694260e-06,\n",
       "        3.93618011e-09, 7.37929320e-09],\n",
       "       [8.84236882e-01, 4.86729675e-06, 5.57759409e-02, 4.45311791e-05,\n",
       "        1.54885038e-05, 5.40938359e-02, 4.27476740e-05, 7.88445833e-08,\n",
       "        1.42148425e-07, 4.28120236e-08, 7.10907479e-08, 4.53422342e-12,\n",
       "        3.43762993e-04, 2.59046066e-03, 2.47228677e-09, 1.09344402e-08,\n",
       "        2.33165083e-06, 8.34009552e-11, 8.13662599e-07, 1.14700811e-09,\n",
       "        1.69711355e-03, 1.14836753e-03, 2.53908162e-07, 2.24663066e-06,\n",
       "        2.21247683e-09, 4.14780697e-09],\n",
       "       [5.64865463e-01, 1.82953686e-05, 2.09652596e-01, 1.67385384e-04,\n",
       "        5.82187405e-05, 2.03329840e-01, 1.60681482e-04, 2.96363832e-07,\n",
       "        5.34312570e-07, 1.60923361e-07, 2.67218439e-07, 1.70434007e-11,\n",
       "        1.29214860e-03, 9.73711593e-03, 9.29291968e-09, 4.11007639e-08,\n",
       "        8.76429229e-06, 3.13490485e-10, 3.05842400e-06, 4.31141498e-09,\n",
       "        6.37917094e-03, 4.31652484e-03, 9.54399056e-07, 8.44471545e-06,\n",
       "        8.31633684e-09, 1.55909248e-08]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_words_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.94215671, -6.94215671, -6.94215671, -6.94215671, -6.94215671,\n",
       "       -6.94215671, -6.94215671, -6.94215671, -6.94215671, -6.94215671])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perplexity(docs, topic_root):\n",
    "    def set_prob_words(topic):\n",
    "        topic.set_prob_words()\n",
    "        for topic_child in topic.children:\n",
    "            set_prob_words(topic_child)\n",
    "            \n",
    "    # set Probabilty of Words\n",
    "    set_prob_words(topic_root)\n",
    "\n",
    "    logit_docs = []\n",
    "    for docs in docs:\n",
    "        # Path Probability for each document\n",
    "        probs_paths= [{topic: 1.}]\n",
    "        for depth in range(1, n_depth):\n",
    "            probs_path = {}\n",
    "            for topic, prob_path in probs_paths[-1].items():\n",
    "                topics_child = topic.get_children()\n",
    "                probs_child = topic.get_probs_child(doc)\n",
    "                probs_path_child = prob_path * probs_child\n",
    "                for topic_child, prob_path_child in zip(topics_child, probs_path_child):\n",
    "                    probs_path[topic_child] = prob_path_child\n",
    "            probs_paths.append(probs_path)    \n",
    "\n",
    "        all_topics = []\n",
    "        for probs_path in probs_paths:\n",
    "            all_topics += list(probs_path.keys())\n",
    "\n",
    "        # topic probability for each depth\n",
    "        probs_depths_topics = np.array([[probs_path[topic] if topic in probs_path else 0. for topic in all_topics] for probs_path in probs_paths])\n",
    "        # depth probability for each word\n",
    "        probs_words_depths = np.array([doc.get_probs_depth(word_idx) for word_idx in doc.words])\n",
    "        # topic probability for each depth\n",
    "        probs_words_topics = probs_words_depths.dot(probs_depths_topics)\n",
    "\n",
    "        probs_topics_words = np.array([topic.prob_words for topic in all_topics])\n",
    "        probs_words_bow = probs_words_topics.dot(probs_topics_words)\n",
    "        logit_words = np.log(probs_words_bow[np.arange(len(doc.words)), doc.words])\n",
    "        logit_doc = np.mean(logit_words)\n",
    "        logit_docs.append(logit_doc)\n",
    "\n",
    "    perplexity = np.exp(-np.mean(logit_docs))\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_freq_tokens(topic, topics_freq_tokens):\n",
    "    topic_freq_tokens = ' '.join([idx_to_word[bow_idxs[bow_index]] for bow_index in np.argsort(topic.cnt_words)[::-1][:10]])\n",
    "    topics_freq_tokens.append(topic_freq_tokens)\n",
    "    for child in topic.children:\n",
    "        add_freq_tokens(child, topics_freq_tokens)\n",
    "\n",
    "topics_freq_tokens = []\n",
    "add_freq_tokens(topic_root, topics_freq_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_coherence = 'npmi/data/bags/cgs.txt'\n",
    "with open(path_coherence, 'w') as f:\n",
    "    f.write('\\n'.join(topics_freq_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get specialization score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_bow = np.sum([instance.bow for instance in instances_train], 0)\n",
    "norm_vec = norm_bow / np.linalg.norm(norm_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spec(topic, depth_specs=None):\n",
    "    if depth_specs is None: depth_specs = defaultdict(list)\n",
    "    topic_vec = topic.prob_words / np.linalg.norm(topic.prob_words)\n",
    "    topic_spec = 1 - topic_vec.dot(norm_vec)\n",
    "    depth_specs[topic.depth].append(topic_spec)\n",
    "    for child in topic.children:\n",
    "        depth_specs = add_spec(child, depth_specs)\n",
    "    return depth_specs\n",
    "\n",
    "depth_specs = add_spec(topic_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0749372385946312]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth_specs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0749372385946312\n",
      "1 0.6226005207697187\n",
      "2 0.49158613401802365\n",
      "3 nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "for depth, specs in depth_specs.items():\n",
    "    spec = np.mean(specs)\n",
    "    print(depth, spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_arc(topic, depth_arcs=None):\n",
    "    if depth_arcs is None: depth_arcs = defaultdict(list)\n",
    "    topic_vec = topic.prob_words / np.linalg.norm(topic.prob_words)\n",
    "    topic_arc = np.arccos(topic_vec.dot(norm_vec))\n",
    "    depth_arcs[topic.depth].append(topic_arc)\n",
    "    for child in topic.children:\n",
    "        depth_arcs = add_arc(child, depth_arcs)\n",
    "    return depth_arcs\n",
    "\n",
    "depth_arcs = add_arc(topic_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0749372385946312\n",
      "1 0.6196820668592284\n",
      "2 0.4913526867429402\n"
     ]
    }
   ],
   "source": [
    "for depth, arcs in depth_arcs.items():\n",
    "    spec = 1 - np.cos(np.mean(arcs))\n",
    "    print(depth, spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
