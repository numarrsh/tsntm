{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from data_structure import get_batches, get_test_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "from topic_model import TopicModel\n",
    "\n",
    "from topic_beam_search_decoder import BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '5', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/bags/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/tglm_vae_tmp', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'bags', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 50, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 500, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.1, 'lr')\n",
    "flags.DEFINE_float('reg', 1., 'regularization term')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_bool('warmup', True, 'flg of warming up')\n",
    "flags.DEFINE_integer('epochs_cycle', 5, 'number of epochs within a cycle')\n",
    "flags.DEFINE_float('r_cycle', 0.5, 'proportion used to increase beta within a cycle')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 10, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_bow', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_hidden_topic', 512, 'dim_hidden_topic')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "flags.DEFINE_bool('bidirectional', True, 'flg of bidirectional encoding')\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_test_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('PAD_IDX', word_to_idx[PAD], 'PAD_IDX')\n",
    "flags.DEFINE_integer('UNK_IDX', word_to_idx[UNK], 'UNK_IDX')\n",
    "flags.DEFINE_integer('BOS_IDX', word_to_idx[BOS], 'BOS_IDX')\n",
    "flags.DEFINE_integer('EOS_IDX', word_to_idx[EOS], 'EOS_IDX')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "maximum_iterations = max([max([instance.max_sent_l for instance in batch]) for ct, batch in dev_batches])\n",
    "flags.DEFINE_integer('maximum_iterations', maximum_iterations, 'maximum_iterations')\n",
    "\n",
    "flags.DEFINE_integer('cycle_steps', len(train_batches)*config.epochs_cycle, 'number of steps for each cycle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build language model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['bow'] = tf.placeholder(tf.float32, [None, config.dim_bow], name='bow')\n",
    "t_variables['input_token_idxs'] = tf.placeholder(tf.int32, [None, None], name='input_token_idxs')\n",
    "t_variables['dec_input_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_input_idxs')\n",
    "t_variables['dec_target_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_target_idxs')\n",
    "t_variables['batch_l'] = tf.placeholder(tf.int32, name='batch_l')\n",
    "t_variables['doc_l'] = tf.placeholder(tf.int32, [None], name='doc_l')\n",
    "t_variables['sent_l'] = tf.placeholder(tf.int32, [None], name='sent_l')\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train', assertion=False):\n",
    "    def token_dropout(sent_idxs):\n",
    "        sent_idxs_dropout = np.asarray(sent_idxs)\n",
    "        sent_idxs_dropout[np.random.rand(len(sent_idxs)) > config.word_keep_prob] = config.UNK_IDX\n",
    "        return list(sent_idxs_dropout)\n",
    "\n",
    "    bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "    \n",
    "    doc_l = np.array([len(instance.token_idxs) for instance in batch])\n",
    "    \n",
    "    feed_input_token_idxs_list = [sent_idxs for instance in batch for sent_idxs in instance.token_idxs]\n",
    "    feed_dec_input_idxs_list = [[config.BOS_IDX] + token_dropout(sent_idxs) for sent_idxs in feed_input_token_idxs_list]\n",
    "    feed_dec_target_idxs_list = [sent_idxs + [config.EOS_IDX]  for sent_idxs in feed_input_token_idxs_list]\n",
    "        \n",
    "    sent_l = np.array([len(sent_idxs) for sent_idxs in feed_input_token_idxs_list], np.int32)\n",
    "    batch_l = len(sent_l)\n",
    "    \n",
    "    feed_input_token_idxs = pad_sequences(feed_input_token_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_input_idxs = pad_sequences(feed_dec_input_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_target_idxs = pad_sequences(feed_dec_target_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    \n",
    "    if assertion:\n",
    "        index = 0\n",
    "        for instance in batch:\n",
    "            for line_idxs in instance.token_idxs:\n",
    "                assert feed_input_token_idxs_list[index] == line_idxs\n",
    "                index += 1\n",
    "        assert feed_input_token_idxs.shape[1] == np.max(sent_l)\n",
    "        assert feed_dec_input_idxs.shape[1] == np.max(sent_l) + 1\n",
    "        assert feed_dec_target_idxs.shape[1] == np.max(sent_l) + 1\n",
    "    \n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "\n",
    "    feed_dict = {\n",
    "                t_variables['bow']: bow, \n",
    "                t_variables['batch_l']: batch_l, t_variables['doc_l']: doc_l, t_variables['sent_l']: sent_l, \n",
    "                t_variables['input_token_idxs']: feed_input_token_idxs, t_variables['dec_input_idxs']: feed_dec_input_idxs, t_variables['dec_target_idxs']: feed_dec_target_idxs, \n",
    "                t_variables['keep_prob']: keep_prob\n",
    "    }\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     10,
     24,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables):\n",
    "    sample_batch = dev_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "def check_shape(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()\n",
    "    \n",
    "def check_value(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()    \n",
    "    \n",
    "# sent_loss_kl_categ_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, tf_log(prob_topic_infer/prob_topic_sents)), 1))\n",
    "# debug_value([sent_loss_kl_categ, sent_loss_kl_categ_tmp])\n",
    "# sent_loss_kl_gauss_tmp = 0.5 * tf.reduce_sum(tf.exp(logvars_topic_infer-logvars_topic) + tf.square(means_topic - means_topic_infer) / tf.exp(logvars_topic) - 1 + (logvars_topic - logvars_topic_infer), -1)\n",
    "# sent_loss_kl_gmm_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss_tmp), -1))\n",
    "# debug_value([sent_loss_kl_gmm_tmp, sent_loss_kl_gmm])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encode bow\n",
    "with tf.variable_scope('topic/enc', reuse=False):\n",
    "    hidden_bow_ = tf.layers.Dense(units=config.dim_hidden_bow, activation=tf.nn.relu, name='hidden_bow')(t_variables['bow'])\n",
    "    hidden_bow = tf.layers.Dropout(t_variables['keep_prob'])(hidden_bow_)\n",
    "    means_bow = tf.layers.Dense(units=config.dim_latent_bow, name='mean_bow')(hidden_bow)\n",
    "    logvars_bow = tf.layers.Dense(units=config.dim_latent_bow, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_bow)\n",
    "    latents_bow = sample_latents(means_bow, logvars_bow) # sample latent vectors\n",
    "\n",
    "    prob_topic = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic')(latents_bow) # inference of topic probabilities\n",
    "\n",
    "# decode bow\n",
    "with tf.variable_scope('shared', reuse=False):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "bow_embeddings = tf.nn.embedding_lookup(embeddings, bow_idxs) # embeddings of each bow features\n",
    "\n",
    "with tf.variable_scope('topic/dec', reuse=False):\n",
    "    topic_embeddings = tf.get_variable('topic_emb', [config.n_topic, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of topics\n",
    "\n",
    "    topic_bow = tf.nn.softmax(tf.matmul(topic_embeddings, bow_embeddings, transpose_b=True), 1) # bow vectors for each topic\n",
    "    logits_bow = tf_log(tf.matmul(prob_topic, topic_bow)) # predicted bow distribution\n",
    "\n",
    "    # prior of each gaussian distribution (computed for each topic)\n",
    "    hidden_topic = tf.layers.Dense(units=config.dim_hidden_topic, activation=tf.nn.relu, name='hidden_topic')(topic_bow)\n",
    "    means_topic = tf.layers.Dense(units=config.dim_latent, name='mean_topic')(hidden_topic)\n",
    "    logvars_topic = tf.layers.Dense(units=config.dim_latent, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_topic)\n",
    "    sigma_topic = tf.exp(0.5 * logvars_topic)\n",
    "    gauss_topic = tfd.Normal(loc=means_topic, scale=sigma_topic)    \n",
    "    \n",
    "# define losses\n",
    "topic_losses_recon = -tf.reduce_sum(tf.multiply(t_variables['bow'], logits_bow), 1)\n",
    "topic_loss_recon = tf.reduce_mean(topic_losses_recon) # negative log likelihood of each words\n",
    "\n",
    "topic_loss_kl = compute_kl_loss(means_bow, logvars_bow) # KL divergence b/w latent dist & gaussian std\n",
    "\n",
    "topic_bow_norm = topic_bow / tf.norm(topic_bow, axis=1, keepdims=True)\n",
    "topic_dots = tf.clip_by_value(tf.matmul(topic_bow_norm, tf.transpose(topic_bow_norm)), -1., 1.)\n",
    "topic_loss_reg = tf.reduce_mean(tf.square(topic_dots - tf.eye(config.n_topic)))\n",
    "# topic_angles = tf.acos(topic_dots)\n",
    "# topic_angles_mean = tf.reduce_mean(topic_angles)\n",
    "# topic_angles_vars = tf.reduce_mean(tf.square(topic_angles - topic_angles_mean))\n",
    "# topic_loss_reg = tf.exp(topic_angles_vars - topic_angles_mean)\n",
    "\n",
    "# monitor\n",
    "n_bow = tf.reduce_sum(t_variables['bow'], 1)\n",
    "topic_ppls = tf.divide(topic_losses_recon, tf.maximum(1e-5, n_bow))\n",
    "topics_freq_bow_indices = tf.nn.top_k(topic_bow, 10, name='topic_freq_bow').indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "input_token_idxs = t_variables['input_token_idxs']\n",
    "batch_l = t_variables['batch_l']\n",
    "sent_l = t_variables['sent_l']\n",
    "max_sent_l = tf.reduce_max(sent_l)\n",
    "\n",
    "with tf.variable_scope('sent/enc', reuse=False):\n",
    "    # get word embedding\n",
    "    enc_input = tf.nn.embedding_lookup(embeddings, input_token_idxs)\n",
    "\n",
    "    # get sentence embedding\n",
    "    _, enc_state = dynamic_bi_rnn(enc_input, sent_l, config.dim_hidden, t_variables['keep_prob'])\n",
    "\n",
    "    # TODO House Holder flow\n",
    "    hidden_topic_infer =  tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='hidden_topic_infer')(enc_state)\n",
    "    prob_topic_infer = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic_infer')(hidden_topic_infer)\n",
    "\n",
    "    w_mean_topic_infer = tf.get_variable('mean_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32)\n",
    "    b_mean_topic_infer = tf.get_variable('mean_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32)\n",
    "    means_topic_infer = tf.tensordot(enc_state, w_mean_topic_infer, axes=[[1], [1]]) + b_mean_topic_infer\n",
    "    \n",
    "    w_logvar_topic_infer = tf.get_variable('logvar_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    b_logvar_topic_infer = tf.get_variable('logvar_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    logvars_topic_infer = tf.tensordot(enc_state, w_logvar_topic_infer, axes=[[1], [1]]) + b_logvar_topic_infer\n",
    "    sigma_topic_infer = tf.exp(0.5 * logvars_topic_infer)\n",
    "    gauss_topic_infer = tfd.Normal(loc=means_topic_infer, scale=sigma_topic_infer)\n",
    "    \n",
    "    # latent vectors from each gaussian dist.\n",
    "    latents_topic_infer = sample_latents(means_topic_infer, logvars_topic_infer) \n",
    "    # latent vector from gaussian mixture\n",
    "    latents_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), latents_topic_infer, transpose_a=True)\n",
    "    \n",
    "    # for beam search\n",
    "    means_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), means_topic_infer, transpose_a=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for decoding\n",
    "dec_sent_l = tf.add(sent_l, 1)\n",
    "dec_input_idxs = t_variables['dec_input_idxs']\n",
    "dec_input = tf.nn.embedding_lookup(embeddings, dec_input_idxs)\n",
    "\n",
    "dec_latents_input = tf.tile(latents_input, [1, tf.shape(dec_input)[1], 1])\n",
    "dec_concat_input = tf.concat([dec_input, dec_latents_input], -1)\n",
    "\n",
    "# decode for training\n",
    "with tf.variable_scope('sent/dec/rnn', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32, reuse=False):\n",
    "    dec_cell = tf.contrib.rnn.GRUCell(config.dim_hidden)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob = t_variables['keep_prob'])\n",
    "\n",
    "    dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(latents_input, 1))\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_concat_input, sequence_length=dec_sent_l)\n",
    "\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=dec_cell,\n",
    "        helper=helper,\n",
    "        initial_state=dec_initial_state)\n",
    "\n",
    "    dec_outputs, _, output_sent_l = tf.contrib.seq2seq.dynamic_decode(train_decoder)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(config.n_vocab, use_bias=False, name='out')\n",
    "    output_logits = output_layer(dec_outputs.rnn_output)\n",
    "    \n",
    "    output_token_idxs = tf.argmax(output_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tokens = tf.fill([batch_l], config.BOS_IDX)\n",
    "end_token = config.EOS_IDX\n",
    "\n",
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    infer_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(means_input, 1))\n",
    "    beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(infer_dec_initial_state, multiplier=config.beam_width)\n",
    "    beam_latents_input = tf.contrib.seq2seq.tile_batch(tf.squeeze(means_input, 1), multiplier=config.beam_width) # added\n",
    "    \n",
    "    beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=beam_latents_input)\n",
    "\n",
    "    beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    beam_output_token_idxs = beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    inter_means_input = tf.placeholder(tf.float32, [None, config.dim_latent])\n",
    "    \n",
    "    inter_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(inter_means_input)\n",
    "    inter_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(inter_dec_initial_state, multiplier=config.beam_width)\n",
    "    inter_beam_latents_input = tf.contrib.seq2seq.tile_batch(inter_means_input, multiplier=config.beam_width) # added\n",
    "    \n",
    "    inter_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=inter_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=inter_beam_latents_input)\n",
    "\n",
    "    inter_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inter_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    inter_beam_output_token_idxs = inter_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    topic_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic)\n",
    "    topic_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(topic_dec_initial_state, multiplier=config.beam_width)\n",
    "    topic_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic, multiplier=config.beam_width) # added\n",
    "    \n",
    "    topic_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=topic_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=topic_beam_latents_input)\n",
    "\n",
    "    topic_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        topic_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    topic_beam_output_token_idxs = topic_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    means_topic_summary = tf.reduce_mean(means_topic_infer, 0)\n",
    "    \n",
    "    summary_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic_summary)\n",
    "    summary_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(summary_dec_initial_state, multiplier=config.beam_width)\n",
    "    summary_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic_summary, multiplier=config.beam_width) # added\n",
    "    \n",
    "    summary_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=summary_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width,\n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=summary_beam_latents_input)\n",
    "\n",
    "    summary_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        summary_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    summary_beam_output_token_idxs = summary_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language modeling cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target and mask\n",
    "dec_target_idxs = t_variables['dec_target_idxs']\n",
    "dec_mask_tokens = tf.sequence_mask(dec_sent_l, maxlen=max_sent_l+1, dtype=tf.float32)\n",
    "\n",
    "# nll for each token (averaged over batch & sentence)\n",
    "sent_loss_recon = tf.contrib.seq2seq.sequence_loss(output_logits, dec_target_idxs, dec_mask_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_l = t_variables['doc_l']\n",
    "mask_sents = tf.sequence_mask(doc_l)\n",
    "mask_sents_flatten = tf.reshape(mask_sents, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1]])\n",
    "\n",
    "prob_topic_tiled = tf.tile(tf.expand_dims(prob_topic, 1), [1, tf.shape(mask_sents)[1], 1])\n",
    "prob_topic_flatten = tf.reshape(prob_topic_tiled, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1], config.n_topic])\n",
    "prob_topic_sents = tf.boolean_mask(prob_topic_flatten, mask_sents_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferred mixture probabilities (computed for each sentence)\n",
    "categ_topic_infer = tfd.Categorical(probs=prob_topic_infer)\n",
    "\n",
    "# prior of mixture probabilities (computed for each document, tiled for each sentence)\n",
    "categ_topic = tfd.Categorical(probs=prob_topic_sents)\n",
    "\n",
    "sent_loss_kl_categ = tf.reduce_mean(tfd.kl_divergence(categ_topic_infer, categ_topic))\n",
    "\n",
    "# inference of each gaussian gaussribution (computed for each sentence)\n",
    "\n",
    "sent_loss_kl_gauss = tf.reduce_sum(tfd.kl_divergence(gauss_topic_infer, gauss_topic), -1)\n",
    "sent_loss_kl_gmm = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss), -1))\n",
    "\n",
    "sent_loss_kl = sent_loss_kl_categ + sent_loss_kl_gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "tau = tf.cast(tf.divide(tf.mod(global_step, tf.constant(config.cycle_steps)), tf.constant(config.cycle_steps)), dtype=tf.float32)\n",
    "beta = tf.minimum(1., tau/config.r_cycle)\n",
    "\n",
    "sent_loss = sent_loss_recon + beta * sent_loss_kl\n",
    "\n",
    "topic_loss = topic_loss_recon + topic_loss_kl + config.reg * topic_loss_reg\n",
    "loss = topic_loss + sent_loss\n",
    "\n",
    "# define optimizer\n",
    "if config.opt == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif config.opt == 'Adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "    \n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_sents(token_idxs, config, idx_to_word):\n",
    "    sents = []\n",
    "    for sent_idxs in token_idxs:\n",
    "        tokens = []\n",
    "        for idx in sent_idxs:\n",
    "            if idx == config.EOS_IDX: break\n",
    "            tokens.append(idx_to_word[idx])\n",
    "        sent = ' '.join(tokens)\n",
    "        sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, ppls_batch \\\n",
    "            = sess.run([loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, topic_ppls], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "    loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, sent_loss_recon_mean, sent_loss_kl_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    return loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, sent_loss_recon_mean, sent_loss_kl_mean, ppl_mean\n",
    "\n",
    "def get_all_losses(sess, batches):\n",
    "    losses = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch = \\\n",
    "        sess.run([loss, topic_loss_recon, topic_loss_kl, sent_loss_recon, sent_loss_kl], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "    print('LOSS %.2f | TM NLL: %.2f, KL: %.4f | LM NLL: %.2f, KL: %.4f' %  np.mean(losses, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_batch):\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    pred_token_idxs = sess.run(output_token_idxs, feed_dict = feed_dict)\n",
    "    true_token_idxs = [sent_idxs for instance in sample_batch for sent_idxs in instance.token_idxs]\n",
    "    \n",
    "    assert len(pred_token_idxs) == len(true_token_idxs)\n",
    "    \n",
    "    pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "    true_sents = idxs_to_sents(true_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    for i, (true_sent, pred_sent) in enumerate(zip(true_sents, pred_sents)):        \n",
    "        print(i, 'TRUE: %s' % true_sent)\n",
    "        print(i, 'PRED: %s' % pred_sent)\n",
    "\n",
    "def print_topic_sample():\n",
    "    pred_topics_freq_bow_indices, pred_topic_token_idxs = sess.run([topics_freq_bow_indices, topic_beam_output_token_idxs], \n",
    "                                                                                                           feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "    pred_topic_sents = idxs_to_sents(pred_topic_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Topic Samples-----------')\n",
    "    for i, (topic_freq_bow_idxs, pred_topic_sent) in enumerate(zip(topics_freq_bow_idxs, pred_topic_sents)):\n",
    "        print(i, ' BOW:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' SENTENCE:', pred_topic_sent)\n",
    "        \n",
    "def print_summary(test_batch):\n",
    "    feed_dict = get_feed_dict(test_batch)\n",
    "    feed_dict[t_variables['batch_l']] = config.n_topic\n",
    "    feed_dict[t_variables['keep_prob']] = 1.\n",
    "    pred_topics_freq_bow_indices, pred_summary_token_idxs = sess.run([topics_freq_bow_indices, summary_beam_output_token_idxs], feed_dict=feed_dict)\n",
    "    pred_summary_sents = idxs_to_sents(pred_summary_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Output sentences for each topic-----------')\n",
    "    print('Item idx:', test_batch[0].item_idx)\n",
    "    for i, (topic_freq_bow_idxs, pred_summary_sent) in enumerate(zip(topics_freq_bow_idxs, pred_summary_sents)):\n",
    "        print(i, ' BOW:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' SENTENCE:', pred_summary_sent)\n",
    "        \n",
    "    print('-----------Summaries-----------')\n",
    "    for i, summary in enumerate(test_batch[0].summaries):\n",
    "        print('SUMMARY %i :'%i, '\\n', summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "saver = tf.train.Saver(max_to_keep=10)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','TM','','','','LM','','VALID:','TM','','','','LM','', ''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL','REG','NLL','KL','LOSS','PPL','NLL','KL','REG','NLL','KL', 'Beta']]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>LM</th>\n",
       "      <th></th>\n",
       "      <th>VALID:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>LM</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>134.57</td>\n",
       "      <td>1036</td>\n",
       "      <td>123.99</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.90</td>\n",
       "      <td>9.13</td>\n",
       "      <td>1.41</td>\n",
       "      <td>126.47</td>\n",
       "      <td>1033</td>\n",
       "      <td>116.10</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.90</td>\n",
       "      <td>9.12</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>121.15</td>\n",
       "      <td>579</td>\n",
       "      <td>114.19</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.42</td>\n",
       "      <td>6.21</td>\n",
       "      <td>1.39</td>\n",
       "      <td>111.25</td>\n",
       "      <td>519</td>\n",
       "      <td>104.75</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.72</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>119.81</td>\n",
       "      <td>550</td>\n",
       "      <td>112.97</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.27</td>\n",
       "      <td>5.99</td>\n",
       "      <td>1.10</td>\n",
       "      <td>110.67</td>\n",
       "      <td>499</td>\n",
       "      <td>104.05</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.68</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>1500</td>\n",
       "      <td>119.03</td>\n",
       "      <td>535</td>\n",
       "      <td>112.18</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.21</td>\n",
       "      <td>5.90</td>\n",
       "      <td>0.92</td>\n",
       "      <td>110.16</td>\n",
       "      <td>484</td>\n",
       "      <td>103.51</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.62</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>118.52</td>\n",
       "      <td>523</td>\n",
       "      <td>111.67</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.84</td>\n",
       "      <td>0.80</td>\n",
       "      <td>109.78</td>\n",
       "      <td>471</td>\n",
       "      <td>103.02</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.49</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>118.30</td>\n",
       "      <td>518</td>\n",
       "      <td>111.44</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.16</td>\n",
       "      <td>5.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>109.79</td>\n",
       "      <td>472</td>\n",
       "      <td>103.11</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.03</td>\n",
       "      <td>5.41</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2776</th>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>118.03</td>\n",
       "      <td>510</td>\n",
       "      <td>111.17</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.13</td>\n",
       "      <td>5.73</td>\n",
       "      <td>0.68</td>\n",
       "      <td>109.33</td>\n",
       "      <td>466</td>\n",
       "      <td>102.84</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.25</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3276</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>117.77</td>\n",
       "      <td>504</td>\n",
       "      <td>110.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.12</td>\n",
       "      <td>5.67</td>\n",
       "      <td>0.64</td>\n",
       "      <td>109.43</td>\n",
       "      <td>468</td>\n",
       "      <td>102.92</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.16</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3776</th>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>117.66</td>\n",
       "      <td>499</td>\n",
       "      <td>110.83</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>109.23</td>\n",
       "      <td>463</td>\n",
       "      <td>102.71</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.07</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4276</th>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>117.49</td>\n",
       "      <td>494</td>\n",
       "      <td>110.67</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>5.55</td>\n",
       "      <td>0.57</td>\n",
       "      <td>109.13</td>\n",
       "      <td>461</td>\n",
       "      <td>102.68</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4551</th>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>117.39</td>\n",
       "      <td>492</td>\n",
       "      <td>110.57</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>5.52</td>\n",
       "      <td>0.56</td>\n",
       "      <td>109.05</td>\n",
       "      <td>458</td>\n",
       "      <td>102.54</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.97</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5051</th>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>117.19</td>\n",
       "      <td>489</td>\n",
       "      <td>110.38</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.48</td>\n",
       "      <td>0.54</td>\n",
       "      <td>108.90</td>\n",
       "      <td>453</td>\n",
       "      <td>102.37</td>\n",
       "      <td>1.27</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.92</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5551</th>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>117.08</td>\n",
       "      <td>485</td>\n",
       "      <td>110.27</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.43</td>\n",
       "      <td>0.52</td>\n",
       "      <td>108.95</td>\n",
       "      <td>452</td>\n",
       "      <td>102.36</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.87</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6051</th>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>1500</td>\n",
       "      <td>116.98</td>\n",
       "      <td>483</td>\n",
       "      <td>110.17</td>\n",
       "      <td>1.15</td>\n",
       "      <td>0.07</td>\n",
       "      <td>5.39</td>\n",
       "      <td>0.50</td>\n",
       "      <td>108.74</td>\n",
       "      <td>445</td>\n",
       "      <td>102.19</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.82</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6551</th>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>116.96</td>\n",
       "      <td>480</td>\n",
       "      <td>110.14</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.07</td>\n",
       "      <td>5.36</td>\n",
       "      <td>0.49</td>\n",
       "      <td>108.72</td>\n",
       "      <td>448</td>\n",
       "      <td>102.23</td>\n",
       "      <td>1.37</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.78</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6826</th>\n",
       "      <td>43</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>116.91</td>\n",
       "      <td>479</td>\n",
       "      <td>110.10</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.07</td>\n",
       "      <td>5.34</td>\n",
       "      <td>0.48</td>\n",
       "      <td>108.61</td>\n",
       "      <td>447</td>\n",
       "      <td>102.12</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.75</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7326</th>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>116.85</td>\n",
       "      <td>477</td>\n",
       "      <td>110.03</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.06</td>\n",
       "      <td>5.31</td>\n",
       "      <td>0.47</td>\n",
       "      <td>108.67</td>\n",
       "      <td>440</td>\n",
       "      <td>101.95</td>\n",
       "      <td>1.63</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.74</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7826</th>\n",
       "      <td>69</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>116.84</td>\n",
       "      <td>475</td>\n",
       "      <td>110.02</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.06</td>\n",
       "      <td>5.27</td>\n",
       "      <td>0.46</td>\n",
       "      <td>108.36</td>\n",
       "      <td>434</td>\n",
       "      <td>101.77</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.69</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8326</th>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>1500</td>\n",
       "      <td>116.74</td>\n",
       "      <td>472</td>\n",
       "      <td>109.92</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.06</td>\n",
       "      <td>5.25</td>\n",
       "      <td>0.45</td>\n",
       "      <td>108.45</td>\n",
       "      <td>438</td>\n",
       "      <td>101.88</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.67</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8826</th>\n",
       "      <td>59</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>116.61</td>\n",
       "      <td>470</td>\n",
       "      <td>109.79</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.22</td>\n",
       "      <td>0.45</td>\n",
       "      <td>108.36</td>\n",
       "      <td>437</td>\n",
       "      <td>101.82</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.65</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9101</th>\n",
       "      <td>43</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>116.57</td>\n",
       "      <td>469</td>\n",
       "      <td>109.75</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.21</td>\n",
       "      <td>0.45</td>\n",
       "      <td>108.33</td>\n",
       "      <td>434</td>\n",
       "      <td>101.71</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.64</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9601</th>\n",
       "      <td>59</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "      <td>116.50</td>\n",
       "      <td>467</td>\n",
       "      <td>109.68</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.18</td>\n",
       "      <td>0.44</td>\n",
       "      <td>108.44</td>\n",
       "      <td>439</td>\n",
       "      <td>101.84</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.61</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10101</th>\n",
       "      <td>74</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>116.40</td>\n",
       "      <td>466</td>\n",
       "      <td>109.58</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.16</td>\n",
       "      <td>0.44</td>\n",
       "      <td>108.31</td>\n",
       "      <td>438</td>\n",
       "      <td>101.78</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.59</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10601</th>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>1500</td>\n",
       "      <td>116.39</td>\n",
       "      <td>464</td>\n",
       "      <td>109.56</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.14</td>\n",
       "      <td>0.43</td>\n",
       "      <td>108.40</td>\n",
       "      <td>438</td>\n",
       "      <td>101.87</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.57</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11101</th>\n",
       "      <td>63</td>\n",
       "      <td>4</td>\n",
       "      <td>2000</td>\n",
       "      <td>116.35</td>\n",
       "      <td>463</td>\n",
       "      <td>109.52</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.12</td>\n",
       "      <td>0.43</td>\n",
       "      <td>108.33</td>\n",
       "      <td>435</td>\n",
       "      <td>101.77</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.55</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11376</th>\n",
       "      <td>46</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>116.31</td>\n",
       "      <td>462</td>\n",
       "      <td>109.48</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.10</td>\n",
       "      <td>0.43</td>\n",
       "      <td>107.76</td>\n",
       "      <td>430</td>\n",
       "      <td>101.57</td>\n",
       "      <td>1.63</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.55</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11876</th>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>116.26</td>\n",
       "      <td>460</td>\n",
       "      <td>109.44</td>\n",
       "      <td>1.43</td>\n",
       "      <td>0.04</td>\n",
       "      <td>5.08</td>\n",
       "      <td>0.43</td>\n",
       "      <td>107.78</td>\n",
       "      <td>433</td>\n",
       "      <td>101.62</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.50</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12376</th>\n",
       "      <td>73</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>116.21</td>\n",
       "      <td>459</td>\n",
       "      <td>109.40</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.04</td>\n",
       "      <td>5.07</td>\n",
       "      <td>0.43</td>\n",
       "      <td>107.67</td>\n",
       "      <td>423</td>\n",
       "      <td>101.32</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.49</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12876</th>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>1500</td>\n",
       "      <td>116.17</td>\n",
       "      <td>458</td>\n",
       "      <td>109.37</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.04</td>\n",
       "      <td>5.05</td>\n",
       "      <td>0.43</td>\n",
       "      <td>107.93</td>\n",
       "      <td>430</td>\n",
       "      <td>101.55</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.49</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13376</th>\n",
       "      <td>63</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>116.09</td>\n",
       "      <td>457</td>\n",
       "      <td>109.29</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.04</td>\n",
       "      <td>5.03</td>\n",
       "      <td>0.43</td>\n",
       "      <td>107.75</td>\n",
       "      <td>425</td>\n",
       "      <td>101.39</td>\n",
       "      <td>1.72</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.46</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102903</th>\n",
       "      <td>40</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>114.23</td>\n",
       "      <td>414</td>\n",
       "      <td>107.65</td>\n",
       "      <td>2.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.17</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.27</td>\n",
       "      <td>403</td>\n",
       "      <td>100.58</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103403</th>\n",
       "      <td>54</td>\n",
       "      <td>44</td>\n",
       "      <td>500</td>\n",
       "      <td>114.22</td>\n",
       "      <td>414</td>\n",
       "      <td>107.65</td>\n",
       "      <td>2.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.17</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.30</td>\n",
       "      <td>399</td>\n",
       "      <td>100.50</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.59</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103903</th>\n",
       "      <td>55</td>\n",
       "      <td>44</td>\n",
       "      <td>1000</td>\n",
       "      <td>114.22</td>\n",
       "      <td>414</td>\n",
       "      <td>107.65</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.16</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.31</td>\n",
       "      <td>401</td>\n",
       "      <td>100.48</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.58</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104403</th>\n",
       "      <td>55</td>\n",
       "      <td>44</td>\n",
       "      <td>1500</td>\n",
       "      <td>114.22</td>\n",
       "      <td>414</td>\n",
       "      <td>107.65</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.16</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.43</td>\n",
       "      <td>403</td>\n",
       "      <td>100.54</td>\n",
       "      <td>2.12</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.59</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104903</th>\n",
       "      <td>54</td>\n",
       "      <td>44</td>\n",
       "      <td>2000</td>\n",
       "      <td>114.21</td>\n",
       "      <td>414</td>\n",
       "      <td>107.64</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.16</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.28</td>\n",
       "      <td>397</td>\n",
       "      <td>100.40</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105178</th>\n",
       "      <td>31</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>114.21</td>\n",
       "      <td>414</td>\n",
       "      <td>107.64</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.16</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.38</td>\n",
       "      <td>399</td>\n",
       "      <td>100.44</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.59</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105678</th>\n",
       "      <td>55</td>\n",
       "      <td>45</td>\n",
       "      <td>500</td>\n",
       "      <td>114.21</td>\n",
       "      <td>414</td>\n",
       "      <td>107.64</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.16</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.55</td>\n",
       "      <td>403</td>\n",
       "      <td>100.55</td>\n",
       "      <td>2.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106178</th>\n",
       "      <td>54</td>\n",
       "      <td>45</td>\n",
       "      <td>1000</td>\n",
       "      <td>114.20</td>\n",
       "      <td>414</td>\n",
       "      <td>107.64</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.15</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.66</td>\n",
       "      <td>406</td>\n",
       "      <td>100.66</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.58</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106678</th>\n",
       "      <td>52</td>\n",
       "      <td>45</td>\n",
       "      <td>1500</td>\n",
       "      <td>114.20</td>\n",
       "      <td>414</td>\n",
       "      <td>107.64</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.15</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.69</td>\n",
       "      <td>405</td>\n",
       "      <td>100.67</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107178</th>\n",
       "      <td>53</td>\n",
       "      <td>45</td>\n",
       "      <td>2000</td>\n",
       "      <td>114.20</td>\n",
       "      <td>414</td>\n",
       "      <td>107.63</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.15</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.62</td>\n",
       "      <td>403</td>\n",
       "      <td>100.51</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107453</th>\n",
       "      <td>29</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>114.20</td>\n",
       "      <td>414</td>\n",
       "      <td>107.63</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.15</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.64</td>\n",
       "      <td>402</td>\n",
       "      <td>100.51</td>\n",
       "      <td>2.12</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.59</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107953</th>\n",
       "      <td>53</td>\n",
       "      <td>46</td>\n",
       "      <td>500</td>\n",
       "      <td>114.19</td>\n",
       "      <td>414</td>\n",
       "      <td>107.63</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.15</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.67</td>\n",
       "      <td>400</td>\n",
       "      <td>100.54</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108453</th>\n",
       "      <td>52</td>\n",
       "      <td>46</td>\n",
       "      <td>1000</td>\n",
       "      <td>114.19</td>\n",
       "      <td>414</td>\n",
       "      <td>107.63</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.14</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.64</td>\n",
       "      <td>401</td>\n",
       "      <td>100.53</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.61</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108953</th>\n",
       "      <td>53</td>\n",
       "      <td>46</td>\n",
       "      <td>1500</td>\n",
       "      <td>114.19</td>\n",
       "      <td>413</td>\n",
       "      <td>107.63</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.14</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.77</td>\n",
       "      <td>403</td>\n",
       "      <td>100.61</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109453</th>\n",
       "      <td>53</td>\n",
       "      <td>46</td>\n",
       "      <td>2000</td>\n",
       "      <td>114.19</td>\n",
       "      <td>413</td>\n",
       "      <td>107.62</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.14</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.61</td>\n",
       "      <td>401</td>\n",
       "      <td>100.49</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.59</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109728</th>\n",
       "      <td>29</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>114.18</td>\n",
       "      <td>413</td>\n",
       "      <td>107.62</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.14</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.62</td>\n",
       "      <td>402</td>\n",
       "      <td>100.49</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.59</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110228</th>\n",
       "      <td>53</td>\n",
       "      <td>47</td>\n",
       "      <td>500</td>\n",
       "      <td>114.18</td>\n",
       "      <td>413</td>\n",
       "      <td>107.62</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.14</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.82</td>\n",
       "      <td>405</td>\n",
       "      <td>100.67</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.59</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110728</th>\n",
       "      <td>53</td>\n",
       "      <td>47</td>\n",
       "      <td>1000</td>\n",
       "      <td>114.18</td>\n",
       "      <td>413</td>\n",
       "      <td>107.62</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.14</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.69</td>\n",
       "      <td>402</td>\n",
       "      <td>100.53</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111228</th>\n",
       "      <td>53</td>\n",
       "      <td>47</td>\n",
       "      <td>1500</td>\n",
       "      <td>114.17</td>\n",
       "      <td>413</td>\n",
       "      <td>107.62</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.13</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.66</td>\n",
       "      <td>404</td>\n",
       "      <td>100.57</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111728</th>\n",
       "      <td>52</td>\n",
       "      <td>47</td>\n",
       "      <td>2000</td>\n",
       "      <td>114.17</td>\n",
       "      <td>413</td>\n",
       "      <td>107.62</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.13</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.79</td>\n",
       "      <td>407</td>\n",
       "      <td>100.68</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.58</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112003</th>\n",
       "      <td>29</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>114.17</td>\n",
       "      <td>413</td>\n",
       "      <td>107.61</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.13</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.69</td>\n",
       "      <td>405</td>\n",
       "      <td>100.57</td>\n",
       "      <td>2.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.59</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112503</th>\n",
       "      <td>52</td>\n",
       "      <td>48</td>\n",
       "      <td>500</td>\n",
       "      <td>114.17</td>\n",
       "      <td>413</td>\n",
       "      <td>107.61</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.13</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.63</td>\n",
       "      <td>403</td>\n",
       "      <td>100.58</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.59</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113003</th>\n",
       "      <td>52</td>\n",
       "      <td>48</td>\n",
       "      <td>1000</td>\n",
       "      <td>114.17</td>\n",
       "      <td>413</td>\n",
       "      <td>107.61</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.13</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.64</td>\n",
       "      <td>401</td>\n",
       "      <td>100.56</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113503</th>\n",
       "      <td>52</td>\n",
       "      <td>48</td>\n",
       "      <td>1500</td>\n",
       "      <td>114.16</td>\n",
       "      <td>413</td>\n",
       "      <td>107.61</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.12</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.58</td>\n",
       "      <td>399</td>\n",
       "      <td>100.44</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.58</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114003</th>\n",
       "      <td>61</td>\n",
       "      <td>48</td>\n",
       "      <td>2000</td>\n",
       "      <td>114.16</td>\n",
       "      <td>413</td>\n",
       "      <td>107.61</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.12</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.18</td>\n",
       "      <td>402</td>\n",
       "      <td>100.53</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.55</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114278</th>\n",
       "      <td>30</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>114.16</td>\n",
       "      <td>413</td>\n",
       "      <td>107.61</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.12</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.38</td>\n",
       "      <td>407</td>\n",
       "      <td>100.68</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.55</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114778</th>\n",
       "      <td>63</td>\n",
       "      <td>49</td>\n",
       "      <td>500</td>\n",
       "      <td>114.15</td>\n",
       "      <td>413</td>\n",
       "      <td>107.60</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.12</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.16</td>\n",
       "      <td>398</td>\n",
       "      <td>100.46</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.53</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115278</th>\n",
       "      <td>53</td>\n",
       "      <td>49</td>\n",
       "      <td>1000</td>\n",
       "      <td>114.15</td>\n",
       "      <td>413</td>\n",
       "      <td>107.60</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.12</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.30</td>\n",
       "      <td>400</td>\n",
       "      <td>100.52</td>\n",
       "      <td>2.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.54</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115778</th>\n",
       "      <td>53</td>\n",
       "      <td>49</td>\n",
       "      <td>1500</td>\n",
       "      <td>114.14</td>\n",
       "      <td>413</td>\n",
       "      <td>107.60</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.12</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.27</td>\n",
       "      <td>402</td>\n",
       "      <td>100.47</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.54</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116278</th>\n",
       "      <td>53</td>\n",
       "      <td>49</td>\n",
       "      <td>2000</td>\n",
       "      <td>114.14</td>\n",
       "      <td>413</td>\n",
       "      <td>107.60</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.11</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.30</td>\n",
       "      <td>399</td>\n",
       "      <td>100.42</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.54</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>257 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       TRAIN:    TM                        LM        VALID:  \\\n",
       "       Time  Ep    Ct    LOSS   PPL     NLL    KL   REG   NLL    KL    LOSS   \n",
       "1        13   0     0  134.57  1036  123.99  0.56  0.90  9.13  1.41  126.47   \n",
       "501      70   0   500  121.15   579  114.19  0.27  0.42  6.21  1.39  111.25   \n",
       "1001     70   0  1000  119.81   550  112.97  0.50  0.27  5.99  1.10  110.67   \n",
       "1501     68   0  1500  119.03   535  112.18  0.65  0.21  5.90  0.92  110.16   \n",
       "2001     70   0  2000  118.52   523  111.67  0.75  0.17  5.84  0.80  109.78   \n",
       "2276     33   1     0  118.30   518  111.44  0.79  0.16  5.80  0.75  109.79   \n",
       "2776     69   1   500  118.03   510  111.17  0.87  0.13  5.73  0.68  109.33   \n",
       "3276     61   1  1000  117.77   504  110.93  0.92  0.12  5.67  0.64  109.43   \n",
       "3776     74   1  1500  117.66   499  110.83  0.98  0.10  5.60  0.60  109.23   \n",
       "4276     69   1  2000  117.49   494  110.67  1.02  0.09  5.55  0.57  109.13   \n",
       "4551     43   2     0  117.39   492  110.57  1.04  0.09  5.52  0.56  109.05   \n",
       "5051     69   2   500  117.19   489  110.38  1.08  0.08  5.48  0.54  108.90   \n",
       "5551     60   2  1000  117.08   485  110.27  1.11  0.08  5.43  0.52  108.95   \n",
       "6051     69   2  1500  116.98   483  110.17  1.15  0.07  5.39  0.50  108.74   \n",
       "6551     70   2  2000  116.96   480  110.14  1.18  0.07  5.36  0.49  108.72   \n",
       "6826     43   3     0  116.91   479  110.10  1.20  0.07  5.34  0.48  108.61   \n",
       "7326     60   3   500  116.85   477  110.03  1.23  0.06  5.31  0.47  108.67   \n",
       "7826     69   3  1000  116.84   475  110.02  1.26  0.06  5.27  0.46  108.36   \n",
       "8326     60   3  1500  116.74   472  109.92  1.28  0.06  5.25  0.45  108.45   \n",
       "8826     59   3  2000  116.61   470  109.79  1.31  0.05  5.22  0.45  108.36   \n",
       "9101     43   4     0  116.57   469  109.75  1.32  0.05  5.21  0.45  108.33   \n",
       "9601     59   4   500  116.50   467  109.68  1.34  0.05  5.18  0.44  108.44   \n",
       "10101    74   4  1000  116.40   466  109.58  1.36  0.05  5.16  0.44  108.31   \n",
       "10601    64   4  1500  116.39   464  109.56  1.38  0.05  5.14  0.43  108.40   \n",
       "11101    63   4  2000  116.35   463  109.52  1.40  0.05  5.12  0.43  108.33   \n",
       "11376    46   5     0  116.31   462  109.48  1.41  0.05  5.10  0.43  107.76   \n",
       "11876    64   5   500  116.26   460  109.44  1.43  0.04  5.08  0.43  107.78   \n",
       "12376    73   5  1000  116.21   459  109.40  1.45  0.04  5.07  0.43  107.67   \n",
       "12876    64   5  1500  116.17   458  109.37  1.47  0.04  5.05  0.43  107.93   \n",
       "13376    63   5  2000  116.09   457  109.29  1.49  0.04  5.03  0.43  107.75   \n",
       "...     ...  ..   ...     ...   ...     ...   ...   ...   ...   ...     ...   \n",
       "102903   40  44     0  114.23   414  107.65  2.09  0.02  4.17  0.42  106.27   \n",
       "103403   54  44   500  114.22   414  107.65  2.09  0.02  4.17  0.42  106.30   \n",
       "103903   55  44  1000  114.22   414  107.65  2.10  0.02  4.16  0.42  106.31   \n",
       "104403   55  44  1500  114.22   414  107.65  2.10  0.02  4.16  0.42  106.43   \n",
       "104903   54  44  2000  114.21   414  107.64  2.10  0.02  4.16  0.42  106.28   \n",
       "105178   31  45     0  114.21   414  107.64  2.10  0.02  4.16  0.42  106.38   \n",
       "105678   55  45   500  114.21   414  107.64  2.10  0.02  4.16  0.42  106.55   \n",
       "106178   54  45  1000  114.20   414  107.64  2.10  0.02  4.15  0.42  106.66   \n",
       "106678   52  45  1500  114.20   414  107.64  2.10  0.02  4.15  0.42  106.69   \n",
       "107178   53  45  2000  114.20   414  107.63  2.10  0.02  4.15  0.42  106.62   \n",
       "107453   29  46     0  114.20   414  107.63  2.10  0.02  4.15  0.42  106.64   \n",
       "107953   53  46   500  114.19   414  107.63  2.10  0.02  4.15  0.42  106.67   \n",
       "108453   52  46  1000  114.19   414  107.63  2.10  0.02  4.14  0.42  106.64   \n",
       "108953   53  46  1500  114.19   413  107.63  2.10  0.02  4.14  0.42  106.77   \n",
       "109453   53  46  2000  114.19   413  107.62  2.10  0.02  4.14  0.42  106.61   \n",
       "109728   29  47     0  114.18   413  107.62  2.10  0.02  4.14  0.42  106.62   \n",
       "110228   53  47   500  114.18   413  107.62  2.11  0.02  4.14  0.42  106.82   \n",
       "110728   53  47  1000  114.18   413  107.62  2.11  0.02  4.14  0.42  106.69   \n",
       "111228   53  47  1500  114.17   413  107.62  2.11  0.02  4.13  0.42  106.66   \n",
       "111728   52  47  2000  114.17   413  107.62  2.11  0.02  4.13  0.42  106.79   \n",
       "112003   29  48     0  114.17   413  107.61  2.11  0.02  4.13  0.42  106.69   \n",
       "112503   52  48   500  114.17   413  107.61  2.11  0.02  4.13  0.42  106.63   \n",
       "113003   52  48  1000  114.17   413  107.61  2.11  0.02  4.13  0.42  106.64   \n",
       "113503   52  48  1500  114.16   413  107.61  2.11  0.02  4.12  0.42  106.58   \n",
       "114003   61  48  2000  114.16   413  107.61  2.11  0.02  4.12  0.42  106.18   \n",
       "114278   30  49     0  114.16   413  107.61  2.11  0.02  4.12  0.42  106.38   \n",
       "114778   63  49   500  114.15   413  107.60  2.11  0.02  4.12  0.42  106.16   \n",
       "115278   53  49  1000  114.15   413  107.60  2.11  0.02  4.12  0.42  106.30   \n",
       "115778   53  49  1500  114.14   413  107.60  2.11  0.02  4.12  0.42  106.27   \n",
       "116278   53  49  2000  114.14   413  107.60  2.11  0.02  4.11  0.42  106.30   \n",
       "\n",
       "          TM                        LM               \n",
       "         PPL     NLL    KL   REG   NLL    KL   Beta  \n",
       "1       1033  116.10  0.34  0.90  9.12  1.41  0.000  \n",
       "501      519  104.75  0.52  0.17  5.72  0.96  0.088  \n",
       "1001     499  104.05  0.74  0.10  5.68  0.57  0.176  \n",
       "1501     484  103.51  0.80  0.08  5.62  0.59  0.264  \n",
       "2001     471  103.02  1.08  0.05  5.49  0.41  0.352  \n",
       "2276     472  103.11  1.08  0.03  5.41  0.43  0.400  \n",
       "2776     466  102.84  1.04  0.02  5.25  0.37  0.488  \n",
       "3276     468  102.92  1.12  0.02  5.16  0.37  0.576  \n",
       "3776     463  102.71  1.20  0.02  5.07  0.34  0.664  \n",
       "4276     461  102.68  1.19  0.02  5.00  0.33  0.752  \n",
       "4551     458  102.54  1.23  0.02  4.97  0.36  0.800  \n",
       "5051     453  102.37  1.27  0.02  4.92  0.37  0.888  \n",
       "5551     452  102.36  1.39  0.02  4.87  0.33  0.976  \n",
       "6051     445  102.19  1.35  0.02  4.82  0.36  1.000  \n",
       "6551     448  102.23  1.37  0.02  4.78  0.33  1.000  \n",
       "6826     447  102.12  1.40  0.02  4.75  0.33  1.000  \n",
       "7326     440  101.95  1.63  0.02  4.74  0.33  1.000  \n",
       "7826     434  101.77  1.54  0.02  4.69  0.35  1.000  \n",
       "8326     438  101.88  1.51  0.02  4.67  0.37  1.000  \n",
       "8826     437  101.82  1.54  0.01  4.65  0.34  1.000  \n",
       "9101     434  101.71  1.61  0.02  4.64  0.36  1.000  \n",
       "9601     439  101.84  1.61  0.02  4.61  0.36  1.000  \n",
       "10101    438  101.78  1.57  0.01  4.59  0.35  1.000  \n",
       "10601    438  101.87  1.59  0.02  4.57  0.36  1.000  \n",
       "11101    435  101.77  1.61  0.01  4.55  0.39  1.000  \n",
       "11376    430  101.57  1.63  0.01  4.55  0.36  0.000  \n",
       "11876    433  101.62  1.59  0.02  4.50  0.52  0.088  \n",
       "12376    423  101.32  1.76  0.02  4.49  0.47  0.176  \n",
       "12876    430  101.55  1.75  0.02  4.49  0.47  0.264  \n",
       "13376    425  101.39  1.72  0.02  4.46  0.46  0.352  \n",
       "...      ...     ...   ...   ...   ...   ...    ...  \n",
       "102903   403  100.58  2.03  0.01  3.60  0.48  0.093  \n",
       "103403   399  100.50  2.10  0.01  3.59  0.51  0.181  \n",
       "103903   401  100.48  2.10  0.01  3.58  0.50  0.269  \n",
       "104403   403  100.54  2.12  0.01  3.59  0.47  0.357  \n",
       "104903   397  100.40  2.07  0.01  3.60  0.45  0.444  \n",
       "105178   399  100.44  2.07  0.01  3.59  0.52  0.493  \n",
       "105678   403  100.55  2.09  0.01  3.60  0.52  0.581  \n",
       "106178   406  100.66  2.07  0.01  3.58  0.51  0.669  \n",
       "106678   405  100.67  2.06  0.01  3.60  0.47  0.757  \n",
       "107178   403  100.51  2.10  0.01  3.60  0.47  0.844  \n",
       "107453   402  100.51  2.12  0.01  3.59  0.45  0.893  \n",
       "107953   400  100.54  2.07  0.01  3.60  0.45  0.981  \n",
       "108453   401  100.53  2.05  0.01  3.61  0.43  1.000  \n",
       "108953   403  100.61  2.11  0.01  3.60  0.44  1.000  \n",
       "109453   401  100.49  2.10  0.01  3.59  0.41  1.000  \n",
       "109728   402  100.49  2.08  0.01  3.59  0.45  1.000  \n",
       "110228   405  100.67  2.10  0.01  3.59  0.44  1.000  \n",
       "110728   402  100.53  2.11  0.01  3.60  0.44  1.000  \n",
       "111228   404  100.57  2.07  0.01  3.60  0.41  1.000  \n",
       "111728   407  100.68  2.08  0.01  3.58  0.44  1.000  \n",
       "112003   405  100.57  2.09  0.01  3.59  0.42  1.000  \n",
       "112503   403  100.58  2.05  0.01  3.59  0.40  1.000  \n",
       "113003   401  100.56  2.07  0.01  3.57  0.43  1.000  \n",
       "113503   399  100.44  2.10  0.01  3.58  0.46  1.000  \n",
       "114003   402  100.53  2.06  0.01  3.55  0.52  0.044  \n",
       "114278   407  100.68  2.08  0.01  3.55  0.49  0.093  \n",
       "114778   398  100.46  2.07  0.01  3.53  0.49  0.181  \n",
       "115278   400  100.52  2.09  0.01  3.54  0.50  0.269  \n",
       "115778   402  100.47  2.08  0.01  3.54  0.48  0.357  \n",
       "116278   399  100.42  2.10  0.01  3.54  0.53  0.444  \n",
       "\n",
       "[257 rows x 18 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Output sentences for each topic-----------\n",
      "Item idx: B000VB7EFW\n",
      "0  BOW: quality 'm price ... bought - $ made time buy\n",
      "0  SENTENCE: within the first week of use , the bottom part of the case broke off\n",
      "1  BOW: carry back strap shoulder straps compartments comfortable pack pockets books\n",
      "1  SENTENCE: i have a canon # d and # lenses , # lenses , # lenses , and a couple of lenses , and i have to carry it on the back\n",
      "2  BOW: room carry small pocket pockets inside nice hold perfect size\n",
      "2  SENTENCE: there is plenty of room for the power supply , mouse , power supply , mouse , power supply , flash drives , etc .\n",
      "3  BOW: power mouse pocket netbook cord charger cards usb drive adapter\n",
      "3  SENTENCE: there is plenty of room for the power cord , mouse , power cord , mouse , and power cord\n",
      "4  BOW: cover keyboard bottom color mac hard pro top apple scratches\n",
      "4  SENTENCE: the keyboard cover does n't fit the keys , but it does n't bother me\n",
      "5  BOW: sleeve protection air inch inside protect pro perfectly snug nice\n",
      "5  SENTENCE: the neoprene sleeve fits perfectly , the sleeve is soft , and the outer sleeve is soft and the outer lining is soft and the outer lining is soft and soft , and the sleeve is soft enough to protect\n",
      "6  BOW: months broke year started return back weeks month years ago\n",
      "6  SENTENCE: after # months of use , the stitching started to crack\n",
      "7  BOW: ! love color perfect recommend perfectly ordered price received buy\n",
      "7  SENTENCE: i love this ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "8  BOW: ; & pro perfectly big air size laptops hp retina\n",
      "8  SENTENCE: fits my # . # & # # ; laptop perfectly\n",
      "9  BOW: junk spend trust test dollars floor based positive paid caught\n",
      "9  SENTENCE: i do n't think it will last a long time\n",
      "-----------Summaries-----------\n",
      "SUMMARY 0 : \n",
      " This is a very well made bag, nice construction, lots of pockets.\n",
      "the straps are very comfortable.\n",
      "and protects everything inside.\n",
      "It says\n",
      "it fits a 17inch notebook,\n",
      "however it did not.\n",
      "after using the pack for less than a month,\n",
      "it is ripping out already.\n",
      "SUMMARY 1 : \n",
      " This is a very well made bag, nice construction, lots of pockets.\n",
      "The quality is excellent\n",
      "and it is very durable.\n",
      "the straps are very comfortable.\n",
      "and protects everything inside.\n",
      "The color is a true red\n",
      "and it fits nicely.\n",
      "it is ripping out already.\n",
      "and it doesnt fit.\n",
      "It's just not the lightest backpack\n",
      "because some zipper teeth were not aligned.\n",
      "SUMMARY 2 : \n",
      " The quality is excellent\n",
      "and I love all the pockets and compartments.\n",
      "and it is very durable.\n",
      "and can't beleive the price\n",
      "and protects everything inside.\n",
      "The laptop\n",
      "doesn't fit in it.\n",
      "it is ripping out already.\n",
      "It's just not the lightest backpack\n",
      "0 TRUE: wonderful hard case\n",
      "0 PRED: it color case\n",
      "1 TRUE: i bought one in clear , red , green , blue and purple ; for my kids and wife\n",
      "1 PRED: i ordered this for the for and and and , blue blue it\n",
      "2 TRUE: guess who i forgot to get one for , you got it , me\n",
      "2 PRED: i i will to to buy it of myself\n",
      "3 TRUE: going to change that now\n",
      "3 PRED: i to be it it\n",
      "4 TRUE: the colors are rich not pale\n",
      "4 PRED: it is are very and as\n",
      "5 TRUE: easy to put on , a little harder to take off\n",
      "5 PRED: the to put on and and little protector to type\n",
      "6 TRUE: this and the keyboard cover are a great way to protect your laptop\n",
      "6 PRED: the cover is apple cover\n",
      "7 TRUE: another plus are the legs , they elevate the back of the laptop for better cooling and give my hands a better angle at the keyboard\n",
      "7 PRED: the one cover the top that the are the bottom of the computer to the airflow\n",
      "8 TRUE: make sure you order the one for your model , in this case # . # `` a # macbook pro\n",
      "8 PRED: it case it are a case for you macbook\n",
      "9 TRUE: i believe they make one for every macbook and macbook pro model\n",
      "9 PRED: i was it case a a the macbook of i pro\n",
      "10 TRUE: we 're a timbuk # loving family\n",
      "10 PRED: i bought very little and and this\n",
      "11 TRUE: our favorite bag by far is the timbuk # command laptop messenger bag\n",
      "11 PRED: i bag bag is far\n",
      "12 TRUE: that bag is a great bag\n",
      "12 PRED: this 's is great\n",
      "13 TRUE: it 's not\n",
      "13 PRED: it is a a\n",
      "14 TRUE: it 's just shiny black canvas with a timbuk # label\n",
      "14 PRED: i is a and and bag and a <unk> bag .\n",
      "15 TRUE: no padding , no form to it , no frills , little to no reinforcement at connections which makes me question its longevity\n",
      "15 PRED: the problem on but zipper , scratch , but matter or and plastic the damage , the\n",
      "16 TRUE: this would be worth it if it were # dollars or so\n",
      "16 PRED: i was be been the money i had a bucks\n",
      "17 TRUE: at the current price , it 's definitely not worth it\n",
      "17 PRED: i first price i , i is worth worth worth the\n",
      "18 TRUE: this case is a nice fit , and has a <unk> side pocket , for flash disks , etc .\n",
      "18 PRED: it case is a nice fit for but the a room for for for the a drives , etc ,\n",
      "19 TRUE: but the handles are sewn inside the zipper , not on the outside , so that you have to have the case slightly unzipped to carry it\n",
      "19 PRED: the the you are very in and case\n",
      "20 TRUE: this is a big design flaw , in my opinion\n",
      "20 PRED: i is a very deal ,\n",
      "21 TRUE: the reason i bought a case was to make travel with my computer easier\n",
      "21 PRED: i quality i bought this was is the my it laptop my laptop\n",
      "22 TRUE: i should be able to zip my laptop into its case , and carry it\n",
      "22 PRED: i have have this to fit a # in # # in but i a in\n",
      "23 TRUE: the ipearl case does n't allow that\n",
      "23 PRED: it case is is not fit it to to to\n",
      "24 TRUE: if you just want this case as a place to store your laptop inside a backpack , this will do\n",
      "24 PRED: the you are a to to to a carry , carry your laptop in a few , then is not you\n",
      "25 TRUE: caught this on a lightning deal for $ # to replace a similar bag i had and i like it so far , probably because it is nearly identical in design of my old bag\n",
      "25 PRED: i on on amazon day deal , the # , i it # bag\n",
      "26 TRUE: i just got it so i ca n't speak on it 's durability , but it feels that it will be reasonably durable\n",
      "26 PRED: i would got it in far could n't comment to how durability durability\n",
      "27 TRUE: i expect at least a year out of it\n",
      "27 PRED: i was it to to a and of this\n",
      "28 TRUE: fits my # & # # ; macbook pro just fine as well as my ipad air and cords , chargers , flash drives , externals\n",
      "28 PRED: fits my # . # # ; dell pro perfectly fine , the padded a ipad and a power , #\n",
      "29 TRUE: this is basically my gadget bag that i can fit my laptop in\n",
      "29 PRED: this is is a great\n",
      "30 TRUE: there is a mesh slot for the laptop as well as a strap to latch it in\n",
      "30 PRED: the is a a pocket on the laptop\n",
      "31 TRUE: it works , but does not offer much protection , is cheap , and feels cheap\n",
      "31 PRED: it is fine but it n't have a protection to but it\n",
      "32 TRUE: from # feet it looks good , but any closer you will wish for more\n",
      "32 PRED: it , n't on is good , but it other to want find\n",
      "33 TRUE: the red thread is thin , and the small little buttons feel as they are going to rip off with just a bit more use\n",
      "33 PRED: it case is is n't , but i material is <unk> is\n",
      "34 TRUE: if you are wanting to spend less than $ # , and that is your basic requirement , it will do\n",
      "34 PRED: if you 're looking to buy a than # # , it this is a one case\n",
      "35 TRUE: i would recommend looking for something better , however\n",
      "35 PRED: i would definitely it for a that\n",
      "36 TRUE: the bag is a little flimsy\n",
      "36 PRED: i bag is very very more than\n",
      "37 TRUE: but considering the price it is alright\n",
      "37 PRED: it it the price of i it\n",
      "38 TRUE: the purple is quite pretty\n",
      "38 PRED: the color is is a\n",
      "39 TRUE: bag is fine if you do n't plan on carrying your laptop around often\n",
      "39 PRED: the is nice , you have a a to carrying it laptop in\n",
      "40 TRUE: i use it basically to store my laptop , and it is fine for that\n",
      "40 PRED: i have it to a my my laptop # and it is is\n",
      "41 TRUE: love the slim design , good storage and nice pockets\n",
      "41 PRED: this the bag bag and the pockets space pockets padding\n",
      "42 TRUE: i can pack it completely full and it does n't seem bulky on my back\n",
      "42 PRED: i have see it to in and it is n't hurt to\n",
      "43 TRUE: love the dark grey color as well\n",
      "43 PRED: love the color color color\n",
      "44 TRUE: it gives it a professional look , even if it is only a backpack\n",
      "44 PRED: i is me a great look\n",
      "45 TRUE: i ordered this and it came faster than i expected # days to be exact\n",
      "45 PRED: i was this case i came quickly than i expected\n",
      "46 TRUE: i absolutely love it , the case snapped right onto my macbook pro\n",
      "46 PRED: i ordered the the and and color is on on my macbook\n",
      "47 TRUE: the case and keyboard cover were just as described\n",
      "47 PRED: the color is keyboard cover is great perfect described\n",
      "48 TRUE: i would definitely order from here again\n",
      "48 PRED: i was definitely recommend this this seller again\n",
      "49 TRUE: i thought this was a great purchase at a very reasonable price\n",
      "49 PRED: i would it was a great deal\n",
      "50 TRUE: i have # digital cameras and had been storing memory cards in zip <unk> bags or a plastic case that only held #\n",
      "50 PRED: it have a # # and the <unk> a to the in the and\n",
      "51 TRUE: i found this , it stores # + cards and very compact\n",
      "51 PRED: it love this case perfect , and . # & my compact with\n",
      "52 TRUE: i have recommended it to others who take many photos\n",
      "52 PRED: i would been this for a\n",
      "53 TRUE: was a little hesitant to buy this , but i did and i 'm not <unk> it\n",
      "53 PRED: i a waste disappointed to buy this one but i 'm n't i was not it\n",
      "54 TRUE: the shipping was super fast , and it also fits well even with a speck case on my macbook\n",
      "54 PRED: i color is is fast and and the fits came my on with the free cover on\n",
      "55 TRUE: it smells kind of funny , but i 'm just hoping it 'll go away quickly\n",
      "55 PRED: it does like of cheap , but i 'm it got it\n",
      "56 TRUE: great bag that actually holds my slr camera , two lens , tri-pod , flash , camcorder , cables , <unk> , etc .\n",
      "56 PRED: it compartments , holds has my laptop , # , lenses , a , lenses , flash , a , and , etc ,\n",
      "57 TRUE: everthing fits into this one bag and have friends that paid double or more for the same functionality in another <unk> brand bag\n",
      "57 PRED: i is in this bag\n",
      "58 TRUE: wonderful bag that i just gambled would be all that it says it is and more\n",
      "58 PRED: i is is i was bought\n",
      "59 TRUE: it is not quite as big as it <unk> , i can not fit both of my # & # # ; binders in it\n",
      "59 PRED: it is a a bulky\n",
      "60 TRUE: for that kind of size , you need the far more expensive ibex\n",
      "60 PRED: i the , , the , i can to case to\n",
      "61 TRUE: however everything else is <unk> laid out and handy\n",
      "61 PRED: it , i is this in out\n",
      "62 TRUE: the bag barely fit my # . # laptop and when held by the bag 's handles it seems like the stitching is about to let the handles go\n",
      "62 PRED: the bag is is my # , # `` , i i in the handle , not\n",
      "63 TRUE: i am not sure how hard was it to make a sturdy bag but this bad is not\n",
      "63 PRED: i i n't buy it it it made for be it bag bag\n",
      "64 TRUE: bought my new laptop right before thanksgiving and needed a carrying case for it in order to travel\n",
      "64 PRED: i this laptop in bag in i\n",
      "65 TRUE: it is light weight and like that the # zipped compartments hold the cord and mouse and it has handles for easy carrying\n",
      "65 PRED: it has well and and has the it laptop . pocket for the power cord , , a is a pocket a\n"
     ]
    }
   ],
   "source": [
    "if len(log_df) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "while epoch < config.epochs:\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = get_feed_dict(batch)\n",
    "\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, sent_loss_kl_categ_batch, sent_loss_kl_gmm_batch, ppls_batch = \\\n",
    "        sess.run([opt, loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, sent_loss_kl_categ, sent_loss_kl_gmm, topic_ppls], feed_dict = feed_dict)\n",
    "   \n",
    "        if sent_loss_kl_batch == np.inf:\n",
    "            print('Nan occured')\n",
    "            ckpt = tf.train.get_checkpoint_state(config.modeldir)\n",
    "            model_checkpoint_path = ckpt.all_model_checkpoint_paths[-1]\n",
    "            saver.restore(sess, model_checkpoint_path)            \n",
    "            break\n",
    "            \n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if ct%config.log_period==0:\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train, sent_loss_recon_train, sent_loss_kl_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, sent_loss_recon_dev, sent_loss_kl_dev, ppl_dev = get_loss(sess, dev_batches)\n",
    "            global_step_log, beta_eval = sess.run([tf.train.get_global_step(), beta])\n",
    "            \n",
    "            if loss_dev < loss_min:\n",
    "                loss_min = loss_dev\n",
    "                saver.save(sess, config.modelpath, global_step=global_step_log)\n",
    "\n",
    "            clear_output()\n",
    "    \n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_reg_train, '%.2f'%sent_loss_recon_train, '%.2f'%sent_loss_kl_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_reg_dev, '%.2f'%sent_loss_recon_dev, '%.2f'%sent_loss_kl_dev,  '%.3f'%beta_eval],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "\n",
    "            print_summary(test_batches[1][1])\n",
    "            print_sample(batch)\n",
    "            \n",
    "            time_start = time.time()\n",
    "            \n",
    "    epoch += 1\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_prob_topic, _prob_topic_sents, _prob_topic_infer, _means_topic_infer = debug_value([prob_topic, prob_topic_sents, prob_topic_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_i = 4\n",
    "_prob_topic_sents[batch_i], _prob_topic_infer[batch_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_means_topic_infer[0][:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_means_topic, b_means_topic = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"topic/dec/mean_topic\")\n",
    "\n",
    "pred_topic_embeddings, pred_topic_bow, pred_means_topic, pred_logvars_topic, pred_token_idxs, _w_means_topic, _b_means_topic, _w_mean_topic_infer = \\\n",
    "                                sess.run([topic_embeddings, topic_bow, means_topic, logvars_topic, topic_beam_output_token_idxs, w_means_topic, b_means_topic, w_mean_topic_infer], \n",
    "                                         feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "\n",
    "pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "\n",
    "pred_topics_freq_bow_indices = np.argsort(pred_topic_bow, 1)[:, ::-1][:, :10]\n",
    "pred_topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idxs in pred_topics_freq_bow_idxs:\n",
    "    print([idx_to_word[idx] for idx in idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_topic_embeddings[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_topic_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_w_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_w_mean_topic_infer[:, :10, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_enc_state_infer, _means_topic_infer = debug_value([enc_state_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_enc_state_infer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_means_topic_infer[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
