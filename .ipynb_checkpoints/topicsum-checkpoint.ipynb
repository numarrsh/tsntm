{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import pdb\n",
    "from six.moves import zip_longest\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import _pickle as cPickle\n",
    "\n",
    "from data_structure import load_data\n",
    "from components import dynamic_rnn, dynamic_bi_rnn, DiagonalGaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '1', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('datadir', 'data', 'directory of input data')\n",
    "flags.DEFINE_string('dataname', 'sports_sents.pkl', 'name of data')\n",
    "flags.DEFINE_string('modeldir', 'NAS/model', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'sports', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 10, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 8, 'batch size')\n",
    "flags.DEFINE_integer('log_period', 100, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.1, 'lr')\n",
    "flags.DEFINE_float('norm', 1e-6, 'norm')\n",
    "flags.DEFINE_float('grad_clip', 10.0, 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'keep_prob')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'keep_prob')\n",
    "\n",
    "flags.DEFINE_bool('iaf', True, 'valid period')\n",
    "flags.DEFINE_bool('anneal', True, 'valid period')\n",
    "flags.DEFINE_integer('kl_rate_rise_time', 5000, 'kl rate rise time')\n",
    "flags.DEFINE_float('kl_rate_rise_factor', 1e-3, 'kl rate rise factor')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 10, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_latent')\n",
    "flags.DEFINE_integer('dim_hidden', 256, 'dim_output')\n",
    "flags.DEFINE_integer('dim_latent', 16, 'dim_latent')\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "config = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(config.datadir, config.dataname)\n",
    "data_train, data_dev, data_test, word_to_idx, idx_to_word = cPickle.load(open(data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouper(iterable, n, fillvalue=None, shorten=True, num_groups=None):\n",
    "    args = [iter(iterable)] * n\n",
    "    out = zip_longest(*args, fillvalue=fillvalue)\n",
    "    out = list(out)\n",
    "    if num_groups is not None:\n",
    "        default = (fillvalue,) * n\n",
    "        assert isinstance(num_groups, int)\n",
    "        out = list(each for each, _ in zip_longest(out, range(num_groups), fillvalue=default))\n",
    "    if shorten:\n",
    "        assert fillvalue is None\n",
    "        out = list(tuple(e for e in each if e is not None) for each in out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = grouper(data_train, config.batch_size)\n",
    "dev_batches = grouper(data_dev, config.batch_size)\n",
    "test_batches = grouper(data_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('PAD_IDX', word_to_idx[PAD], 'PAD_IDX')\n",
    "flags.DEFINE_integer('UNK_IDX', word_to_idx[UNK], 'UNK_IDX')\n",
    "flags.DEFINE_integer('BOS_IDX', word_to_idx[BOS], 'BOS_IDX')\n",
    "flags.DEFINE_integer('EOS_IDX', word_to_idx[EOS], 'EOS_IDX')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "\n",
    "maximum_iterations = max([max([len(sent_idx) for sent_idx in batch]) for batch in dev_batches])\n",
    "flags.DEFINE_integer('maximum_iterations', maximum_iterations, 'maximum_iterations')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fed variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32)\n",
    "t_variables['batch_l'] = tf.placeholder(tf.int32, [])\n",
    "t_variables['token_idxs'] = tf.placeholder(tf.int32, [None, None, None])\n",
    "t_variables['dec_input_idxs'] = tf.placeholder(tf.int32, [None, None, None])\n",
    "t_variables['dec_target_idxs'] = tf.placeholder(tf.int32, [None, None, None])\n",
    "t_variables['sent_l'] = tf.placeholder(tf.int32, [None, None])\n",
    "t_variables['dec_sent_l'] = tf.placeholder(tf.int32, [None, None])\n",
    "t_variables['max_sent_l'] = tf.placeholder(tf.int32, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trained variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = tf.float32\n",
    "\n",
    "dim_hidden = config.dim_hidden\n",
    "dim_latent = config.dim_latent\n",
    "\n",
    "with tf.variable_scope('emb', reuse=tf.AUTO_REUSE):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=dtype, initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "with tf.variable_scope('enc', reuse=tf.AUTO_REUSE):\n",
    "    w_enc = tf.get_variable('w', [dim_hidden, 2 * dim_latent], dtype=dtype)\n",
    "    b_enc = tf.get_variable('b', [2 * dim_latent], dtype=dtype)\n",
    "    \n",
    "with tf.variable_scope('dec', reuse=tf.AUTO_REUSE):\n",
    "    w_dec = tf.get_variable('w', [dim_latent, dim_hidden], dtype=dtype)\n",
    "    b_dec = tf.get_variable('b', [dim_hidden], dtype=dtype)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encode sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "batch_l = t_variables['batch_l']\n",
    "max_doc_l = t_variables['max_doc_l']\n",
    "max_sent_l = t_variables['max_sent_l']\n",
    "token_idxs = t_variables['token_idxs'][:, :max_doc_l, :max_sent_l]\n",
    "\n",
    "# get word embedding\n",
    "enc_input = tf.nn.embedding_lookup(embeddings, token_idxs)\n",
    "enc_input_do = tf.reshape(enc_input, [batch_l, max_sent_l, config.dim_emb])\n",
    "\n",
    "# get sentence embedding\n",
    "sent_l = t_variables['sent_l']\n",
    "sent_l_do = tf.reshape(sent_l, [batch_l * max_doc_l])\n",
    "\n",
    "_, enc_state = dynamic_rnn(enc_input_do, sent_l_do, dim_hidden, t_variables['keep_prob'], cell_name='Model/sent')\n",
    "\n",
    "# encode to parameter \n",
    "means_logvars = tf.nn.relu(tf.matmul(enc_state, w_enc) + b_enc)\n",
    "means, logvars = tf.split(means_logvars, 2, 1)\n",
    "\n",
    "# reparameterize\n",
    "noises = tf.random_normal(tf.shape(means))\n",
    "latents = means + tf.exp(0.5 * logvars) * noises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get latent parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# if config.iaf:\n",
    "#     with tf.variable_scope('iaf'):\n",
    "#         prior = DiagonalGaussian(tf.zeros_like(means, dtype=dtype), tf.zeros_like(logvars, dtype=dtype))\n",
    "#         posterior = DiagonalGaussian(means, logvars)\n",
    "#         z = posterior.sample\n",
    "\n",
    "#         logqs = posterior.logps(z)\n",
    "#         L = tf.get_variable(\"inverse_cholesky\", [dim_latent, dim_latent], dtype=dtype, initializer=tf.zeros_initializer)\n",
    "#         diag_one = tf.ones([dim_latent], dtype=dtype)\n",
    "#         L = tf.matrix_set_diag(L, diag_one)\n",
    "#         mask = np.tril(np.ones([dim_latent,dim_latent]))\n",
    "#         L = L * mask\n",
    "#         latents = tf.matmul(z, L)\n",
    "#         logps = prior.logps(latents)\n",
    "#         kl_losses = logqs - logps\n",
    "# else:\n",
    "#     noises = tf.random_normal(tf.shape(means))\n",
    "#     latents = means + tf.exp(0.5 * logvars) * noises\n",
    "    \n",
    "#     kl_losses = tf.reduce_sum(-0.5 * (logvars - tf.square(means) - tf.exp(logvars) + 1.0), 1) # sum over latent dimentsion\n",
    "    \n",
    "# kl_mean = tf.reduce_mean(kl_losses, [0]) #mean of kl_losses over batches\n",
    "# kl_loss = tf.reduce_sum(kl_mean)\n",
    "\n",
    "# if config.anneal:\n",
    "#     kl_rate = tf.Variable(0.0, trainable=False, dtype=dtype)\n",
    "#     kl_loss = kl_loss * kl_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decode sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for decoding\n",
    "dec_input_idxs = t_variables['dec_input_idxs']\n",
    "dec_input_idxs_do = tf.reshape(dec_input_idxs, [batch_l * max_doc_l, max_sent_l+1])\n",
    "dec_input_do = tf.nn.embedding_lookup(embeddings, dec_input_idxs_do)\n",
    "\n",
    "dec_latent_input_do = tf.tile(tf.expand_dims(latents, 1), [1, tf.shape(dec_input_do)[1], 1])\n",
    "dec_concat_input_do = tf.concat([dec_input_do, dec_latent_input_do], 2)\n",
    "\n",
    "# decode for training\n",
    "dec_sent_l = t_variables['dec_sent_l']\n",
    "dec_sent_l_do = tf.reshape(dec_sent_l, [batch_l * max_doc_l])\n",
    "\n",
    "with tf.variable_scope('Model/sent/dec', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32, reuse=tf.AUTO_REUSE):\n",
    "    dec_cell = tf.contrib.rnn.GRUCell(dim_hidden)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob = t_variables['keep_prob'])\n",
    "\n",
    "    dec_initial_state = tf.nn.relu(tf.matmul(latents, w_dec) + b_dec)\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_concat_input_do, sequence_length=dec_sent_l_do)\n",
    "\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=dec_cell,\n",
    "        helper=helper,\n",
    "        initial_state=dec_initial_state)\n",
    "\n",
    "    dec_outputs, _, output_sent_l = tf.contrib.seq2seq.dynamic_decode(train_decoder)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(config.n_vocab, use_bias=False, name=\"output_projection\")\n",
    "    output_logits = output_layer(dec_outputs.rnn_output)\n",
    "    \n",
    "    output_token_idxs_do = tf.reduce_max(output_logits, 2)\n",
    "    output_token_idxs = tf.reshape(output_token_idxs_do, [batch_l, max_doc_l, tf.shape(output_token_idxs_do)[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # decode for inferrence\n",
    "# start_tokens = tf.fill([batch_l * (max_doc_l)], config.BOS_IDX)\n",
    "# end_token = config.EOS_IDX            \n",
    "# with tf.variable_scope('Model/sent/dec', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32, reuse=tf.AUTO_REUSE):\n",
    "#     tiled_dec_initial_state = tf.contrib.seq2seq.tile_batch(dec_initial_state, multiplier=config.beam_width)\n",
    "\n",
    "#     beam_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "#         cell=dec_cell,\n",
    "#         embedding=embeddings,\n",
    "#         start_tokens=start_tokens,\n",
    "#         end_token=end_token,\n",
    "#         initial_state=tiled_dec_initial_state,\n",
    "#         beam_width=config.beam_width, \n",
    "#         output_layer=output_layer,\n",
    "#         length_penalty_weight=config.length_penalty_weight)\n",
    "\n",
    "#     beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(beam_decoder, maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "#     output_token_idxs_do = beam_dec_outputs.predicted_ids[:, :, 0]\n",
    "#     output_token_idxs = tf.reshape(output_token_idxs_do, [batch_l, max_doc_l, tf.shape(output_token_idxs_do)[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define cost & optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable Model/sent/dec/decoder/gru_cell/gates/kernel/Adagrad/ already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-20-5ccb40bc1c0b>\", line 25, in <module>\n    opt = optimizer.minimize(loss)\n  File \"/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-5cda7f197d6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Adagrad'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdagradOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n\u001b[0;32m--> 424\u001b[0;31m                                 name=name)\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m   def compute_gradients(self, loss, var_list=None,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[1;32m    598\u001b[0m                        ([str(v) for _, _, v in converted_grads_and_vars],))\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_slots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mupdate_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/adagrad.py\u001b[0m in \u001b[0;36m_create_slots\u001b[0;34m(self, var_list)\u001b[0m\n\u001b[1;32m     76\u001b[0m           \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_constant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m       self._get_or_make_slot_with_initializer(v, init, v.get_shape(), dtype,\n\u001b[0;32m---> 78\u001b[0;31m                                               \"accumulator\", self._name)\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36m_get_or_make_slot_with_initializer\u001b[0;34m(self, var, initializer, shape, dtype, slot_name, op_name)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_var_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnamed_slots\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m       new_slot_variable = slot_creator.create_slot_with_initializer(\n\u001b[0;32m-> 1129\u001b[0;31m           var, initializer, shape, dtype, op_name)\n\u001b[0m\u001b[1;32m   1130\u001b[0m       self._restore_slot_variable(\n\u001b[1;32m   1131\u001b[0m           \u001b[0mslot_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslot_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\u001b[0m in \u001b[0;36mcreate_slot_with_initializer\u001b[0;34m(primary, initializer, shape, dtype, name, colocate_with_primary)\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_vars_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         return _create_slot_var(primary, initializer, \"\", validate_shape, shape,\n\u001b[0;32m--> 155\u001b[0;31m                                 dtype)\n\u001b[0m\u001b[1;32m    156\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m       return _create_slot_var(primary, initializer, \"\", validate_shape, shape,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\u001b[0m in \u001b[0;36m_create_slot_var\u001b[0;34m(primary, val, scope, validate_shape, shape, dtype)\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresource_variable_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_resource_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m       \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m       validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m     66\u001b[0m   \u001b[0mvariable_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_partitioner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_partitioner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1315\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1318\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1319\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1077\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    423\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    392\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    731\u001b[0m                          \u001b[0;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 733\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    734\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable Model/sent/dec/decoder/gru_cell/gates/kernel/Adagrad/ already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-20-5ccb40bc1c0b>\", line 25, in <module>\n    opt = optimizer.minimize(loss)\n  File \"/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n"
     ]
    }
   ],
   "source": [
    "# target and mask\n",
    "dec_target_idxs = t_variables['dec_target_idxs']\n",
    "dec_target_idxs_do = tf.reshape(dec_target_idxs, [batch_l * max_doc_l, max_sent_l+1])                \n",
    "dec_mask_tokens_do = tf.sequence_mask(dec_sent_l_do, maxlen=max_sent_l+1, dtype=tf.float32)\n",
    "\n",
    "recon_loss = tf.contrib.seq2seq.sequence_loss(output_logits, dec_target_idxs_do, dec_mask_tokens_do) # nll for each token (averaged over batch & sentence)\n",
    "\n",
    "# define loss\n",
    "kl_losses = tf.reduce_sum(-0.5 * (logvars - tf.square(means) - tf.exp(logvars) + 1.0), 1) # sum over latent dimentsion    \n",
    "kl_loss = tf.reduce_mean(kl_losses, [0]) #mean of kl_losses over batches\n",
    "\n",
    "loss = kl_loss + recon_loss\n",
    "\n",
    "# define optimizer\n",
    "if (config.opt == 'Adam'):\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif (config.opt == 'Adagrad'):\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "opt = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train'):\n",
    "    batch_size = len(batch)\n",
    "    doc_l_matrix = np.array([instance.doc_l for instance in batch]).astype(np.int32)\n",
    "\n",
    "    max_doc_l = np.max(doc_l_matrix)\n",
    "    max_sent_l = max([instance.max_sent_l for instance in batch])\n",
    "\n",
    "    token_idxs_matrix = np.zeros([batch_size, max_doc_l, max_sent_l], np.int32)\n",
    "    dec_input_idxs_matrix = np.zeros([batch_size, max_doc_l, max_sent_l+1], np.int32)\n",
    "    dec_target_idxs_matrix = np.zeros([batch_size, max_doc_l, max_sent_l+1], np.int32)\n",
    "    sent_l_matrix = np.zeros([batch_size, max_doc_l], np.int32)\n",
    "    dec_sent_l_matrix = np.zeros([batch_size, max_doc_l], np.int32)\n",
    "\n",
    "    for i, instance in enumerate(batch):\n",
    "        for j, sent_idxs in enumerate(instance.token_idxs):\n",
    "            token_idxs_matrix[i, j, :len(sent_idxs)] = np.asarray(sent_idxs)\n",
    "            \n",
    "            sent_idxs_dropout = np.asarray(sent_idxs)\n",
    "            sent_idxs_dropout[np.random.rand(len(sent_idxs)) > config.word_keep_prob] = config.UNK_IDX\n",
    "            dec_input_idxs_matrix[i, j, :len(sent_idxs)+1] = np.concatenate([[config.BOS_IDX], sent_idxs_dropout])\n",
    "            \n",
    "            dec_target_idxs_matrix[i, j, :len(sent_idxs)+1] = np.asarray(sent_idxs + [config.EOS_IDX])\n",
    "            sent_l_matrix[i, j] = len(sent_idxs)\n",
    "            dec_sent_l_matrix[i, j] = len(sent_idxs)+1\n",
    "\n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "\n",
    "    feed_dict = {\n",
    "                t_variables['token_idxs']: token_idxs_matrix,\n",
    "                t_variables['dec_input_idxs']: dec_input_idxs_matrix, t_variables['dec_target_idxs']: dec_target_idxs_matrix, \n",
    "                t_variables['batch_l']: batch_size, t_variables['doc_l']: doc_l_matrix, t_variables['sent_l']: sent_l_matrix, t_variables['dec_sent_l']: dec_sent_l_matrix,\n",
    "                t_variables['max_doc_l']: max_doc_l, t_variables['max_sent_l']: max_sent_l, \n",
    "                t_variables['keep_prob']: keep_prob}\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_sents(token_idxs, config, idx_to_word):\n",
    "    sents = []\n",
    "    for line_idxs in token_idxs:\n",
    "        tokens = []\n",
    "        for idx in line_idxs:\n",
    "            if idx == config.EOS_IDX: break\n",
    "            tokens.append(idx_to_word[idx])\n",
    "        sent = ' '.join(tokens)\n",
    "        sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch = sess.run(loss, feed_dict = feed_dict)\n",
    "        losses += [loss_batch]        \n",
    "    loss_mean = np.mean(losses)\n",
    "    return loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_batch):\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    pred_token_idxs_batch = sess.run(output_token_idxs, feed_dict = feed_dict)\n",
    "    true_token_idxs_batch = [instance.token_idxs for instance in sample_batch]\n",
    "    \n",
    "    assert len(pred_token_idxs_batch) == len(true_token_idxs_batch)\n",
    "    \n",
    "    for true_token_idxs, pred_token_idxs in zip(true_token_idxs_batch, pred_token_idxs_batch):\n",
    "        true_sents = idxs_to_sents(true_token_idxs, config, idx_to_word)\n",
    "        pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "        assert len(true_sents) == len(pred_sents)\n",
    "        \n",
    "        for true_sent, pred_sent in zip(true_sents, pred_sents):\n",
    "            print('True: %s' % true_sent)\n",
    "            print('Pred: %s' % pred_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(embeddings.assign(embedding_matrix.astype(np.float32)));\n",
    "\n",
    "logs = []\n",
    "losses_train = []\n",
    "loss_min = np.inf\n",
    "sample_batch = test_batches[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 | LOSS TRAIN: 11.49, DEV: 11.48, TEST: 11.48 | KL: 0.00, RECON: 10.82, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 100 | LOSS TRAIN: 9.16, DEV: 7.84, TEST: 7.80 | KL: 0.00, RECON: 8.50, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 200 | LOSS TRAIN: 8.36, DEV: 7.30, TEST: 7.30 | KL: 0.00, RECON: 7.69, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 300 | LOSS TRAIN: 8.00, DEV: 7.18, TEST: 7.18 | KL: 0.00, RECON: 7.34, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 400 | LOSS TRAIN: 7.80, DEV: 7.06, TEST: 7.07 | KL: 0.00, RECON: 7.14, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 500 | LOSS TRAIN: 7.67, DEV: 6.97, TEST: 6.98 | KL: 0.00, RECON: 7.00, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 600 | LOSS TRAIN: 7.57, DEV: 6.95, TEST: 6.97 | KL: 0.00, RECON: 6.90, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 700 | LOSS TRAIN: 7.49, DEV: 6.90, TEST: 6.92 | KL: 0.00, RECON: 6.82, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 800 | LOSS TRAIN: 7.43, DEV: 6.87, TEST: 6.87 | KL: 0.00, RECON: 6.76, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 900 | LOSS TRAIN: 7.37, DEV: 6.82, TEST: 6.83 | KL: 0.00, RECON: 6.70, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 1000 | LOSS TRAIN: 7.32, DEV: 6.77, TEST: 6.78 | KL: 0.00, RECON: 6.65, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 1100 | LOSS TRAIN: 7.27, DEV: 6.75, TEST: 6.75 | KL: 0.00, RECON: 6.61, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 1200 | LOSS TRAIN: 7.23, DEV: 6.80, TEST: 6.75 | KL: 0.00, RECON: 6.56, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 1300 | LOSS TRAIN: 7.19, DEV: 6.62, TEST: 6.63 | KL: 0.00, RECON: 6.53, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 1400 | LOSS TRAIN: 7.16, DEV: 6.58, TEST: 6.60 | KL: 0.00, RECON: 6.49, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 1500 | LOSS TRAIN: 7.13, DEV: 6.60, TEST: 6.60 | KL: 0.00, RECON: 6.46, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 1600 | LOSS TRAIN: 7.09, DEV: 6.52, TEST: 6.55 | KL: 0.00, RECON: 6.43, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 1700 | LOSS TRAIN: 7.07, DEV: 6.54, TEST: 6.55 | KL: 0.00, RECON: 6.40, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 1800 | LOSS TRAIN: 7.04, DEV: 6.50, TEST: 6.52 | KL: 0.00, RECON: 6.37, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 1900 | LOSS TRAIN: 7.01, DEV: 6.48, TEST: 6.49 | KL: 0.00, RECON: 6.35, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 2000 | LOSS TRAIN: 6.99, DEV: 6.46, TEST: 6.47 | KL: 0.00, RECON: 6.32, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 2100 | LOSS TRAIN: 6.97, DEV: 6.48, TEST: 6.47 | KL: 0.00, RECON: 6.30, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 2200 | LOSS TRAIN: 6.95, DEV: 6.47, TEST: 6.47 | KL: 0.00, RECON: 6.28, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 2300 | LOSS TRAIN: 6.93, DEV: 6.41, TEST: 6.43 | KL: 0.00, RECON: 6.26, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 2400 | LOSS TRAIN: 6.91, DEV: 6.39, TEST: 6.41 | KL: 0.00, RECON: 6.24, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 2500 | LOSS TRAIN: 6.89, DEV: 6.39, TEST: 6.41 | KL: 0.00, RECON: 6.23, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 2600 | LOSS TRAIN: 6.88, DEV: 6.36, TEST: 6.38 | KL: 0.00, RECON: 6.21, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 2700 | LOSS TRAIN: 6.86, DEV: 6.39, TEST: 6.38 | KL: 0.00, RECON: 6.19, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 2800 | LOSS TRAIN: 6.85, DEV: 6.35, TEST: 6.37 | KL: 0.00, RECON: 6.18, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 2900 | LOSS TRAIN: 6.83, DEV: 6.34, TEST: 6.37 | KL: 0.00, RECON: 6.16, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 3000 | LOSS TRAIN: 6.82, DEV: 6.36, TEST: 6.37 | KL: 0.00, RECON: 6.15, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 3100 | LOSS TRAIN: 6.80, DEV: 6.53, TEST: 6.37 | KL: 0.00, RECON: 6.14, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 3200 | LOSS TRAIN: 6.79, DEV: 6.31, TEST: 6.34 | KL: 0.00, RECON: 6.12, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 3300 | LOSS TRAIN: 6.78, DEV: 6.28, TEST: 6.31 | KL: 0.00, RECON: 6.11, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 3400 | LOSS TRAIN: 6.77, DEV: 6.28, TEST: 6.31 | KL: 0.00, RECON: 6.10, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 3500 | LOSS TRAIN: 6.76, DEV: 6.28, TEST: 6.31 | KL: 0.00, RECON: 6.09, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 3600 | LOSS TRAIN: 6.75, DEV: 6.25, TEST: 6.28 | KL: 0.00, RECON: 6.08, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 3700 | LOSS TRAIN: 6.73, DEV: 6.25, TEST: 6.28 | KL: 0.00, RECON: 6.07, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 3800 | LOSS TRAIN: 6.72, DEV: 6.28, TEST: 6.28 | KL: 0.00, RECON: 6.06, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 3900 | LOSS TRAIN: 6.71, DEV: 6.23, TEST: 6.26 | KL: 0.00, RECON: 6.05, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 4000 | LOSS TRAIN: 6.70, DEV: 6.41, TEST: 6.26 | KL: 0.00, RECON: 6.04, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 4100 | LOSS TRAIN: 6.69, DEV: 6.22, TEST: 6.25 | KL: 0.00, RECON: 6.03, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 4200 | LOSS TRAIN: 6.69, DEV: 6.21, TEST: 6.24 | KL: 0.00, RECON: 6.02, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 4300 | LOSS TRAIN: 6.68, DEV: 6.26, TEST: 6.24 | KL: 0.00, RECON: 6.01, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 4400 | LOSS TRAIN: 6.67, DEV: 6.18, TEST: 6.21 | KL: 0.00, RECON: 6.00, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 4500 | LOSS TRAIN: 6.66, DEV: 6.15, TEST: 6.18 | KL: 0.00, RECON: 5.99, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 4600 | LOSS TRAIN: 6.65, DEV: 6.17, TEST: 6.18 | KL: 0.00, RECON: 5.98, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 4700 | LOSS TRAIN: 6.64, DEV: 6.14, TEST: 6.16 | KL: 0.00, RECON: 5.98, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 4800 | LOSS TRAIN: 6.63, DEV: 6.13, TEST: 6.15 | KL: 0.00, RECON: 5.97, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 4900 | LOSS TRAIN: 6.63, DEV: 6.11, TEST: 6.13 | KL: 0.00, RECON: 5.96, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 5000 | LOSS TRAIN: 6.62, DEV: 6.10, TEST: 6.14 | KL: 0.00, RECON: 5.95, NORM: 0.67 | KL rate: 0.000\n",
      "Step: 5100 | LOSS TRAIN: 6.62, DEV: 6.51, TEST: 6.14 | KL: 0.00, RECON: 5.95, NORM: 0.67 | KL rate: 0.099\n",
      "Step: 5200 | LOSS TRAIN: 6.62, DEV: 6.43, TEST: 6.14 | KL: 0.00, RECON: 5.95, NORM: 0.67 | KL rate: 0.199\n",
      "Step: 5300 | LOSS TRAIN: 6.62, DEV: 6.42, TEST: 6.14 | KL: 0.00, RECON: 5.95, NORM: 0.67 | KL rate: 0.299\n",
      "Step: 5400 | LOSS TRAIN: 6.61, DEV: 6.39, TEST: 6.14 | KL: 0.00, RECON: 5.94, NORM: 0.67 | KL rate: 0.399\n",
      "Step: 5500 | LOSS TRAIN: 6.61, DEV: 6.38, TEST: 6.14 | KL: 0.00, RECON: 5.94, NORM: 0.67 | KL rate: 0.499\n",
      "Step: 5600 | LOSS TRAIN: 6.61, DEV: 6.35, TEST: 6.14 | KL: 0.00, RECON: 5.94, NORM: 0.67 | KL rate: 0.599\n",
      "Step: 5700 | LOSS TRAIN: 6.60, DEV: 6.36, TEST: 6.14 | KL: 0.00, RECON: 5.93, NORM: 0.67 | KL rate: 0.699\n",
      "Step: 5800 | LOSS TRAIN: 6.60, DEV: 6.35, TEST: 6.14 | KL: 0.00, RECON: 5.93, NORM: 0.67 | KL rate: 0.799\n",
      "Step: 5900 | LOSS TRAIN: 6.60, DEV: 6.34, TEST: 6.14 | KL: 0.00, RECON: 5.93, NORM: 0.67 | KL rate: 0.899\n",
      "Step: 6000 | LOSS TRAIN: 6.59, DEV: 6.33, TEST: 6.14 | KL: 0.00, RECON: 5.92, NORM: 0.67 | KL rate: 0.999\n",
      "Step: 6100 | LOSS TRAIN: 6.59, DEV: 6.32, TEST: 6.14 | KL: 0.00, RECON: 5.92, NORM: 0.67 | KL rate: 1.001\n",
      "Step: 6200 | LOSS TRAIN: 6.59, DEV: 6.32, TEST: 6.14 | KL: 0.00, RECON: 5.92, NORM: 0.67 | KL rate: 1.001\n",
      "Step: 6300 | LOSS TRAIN: 6.58, DEV: 6.31, TEST: 6.14 | KL: 0.00, RECON: 5.91, NORM: 0.67 | KL rate: 1.001\n",
      "Step: 6400 | LOSS TRAIN: 6.58, DEV: 6.29, TEST: 6.14 | KL: 0.01, RECON: 5.91, NORM: 0.67 | KL rate: 1.001\n",
      "Step: 6500 | LOSS TRAIN: 6.58, DEV: 6.29, TEST: 6.14 | KL: 0.01, RECON: 5.91, NORM: 0.67 | KL rate: 1.001\n",
      "Step: 6600 | LOSS TRAIN: 6.57, DEV: 6.29, TEST: 6.14 | KL: 0.01, RECON: 5.90, NORM: 0.67 | KL rate: 1.001\n",
      "Step: 6700 | LOSS TRAIN: 6.57, DEV: 6.28, TEST: 6.14 | KL: 0.01, RECON: 5.90, NORM: 0.67 | KL rate: 1.001\n",
      "Step: 6800 | LOSS TRAIN: 6.57, DEV: 6.28, TEST: 6.14 | KL: 0.01, RECON: 5.90, NORM: 0.67 | KL rate: 1.001\n",
      "Step: 6900 | LOSS TRAIN: 6.56, DEV: 6.27, TEST: 6.14 | KL: 0.01, RECON: 5.89, NORM: 0.67 | KL rate: 1.001\n",
      "Step: 7000 | LOSS TRAIN: 6.56, DEV: 6.26, TEST: 6.14 | KL: 0.01, RECON: 5.89, NORM: 0.67 | KL rate: 1.001\n",
      "Step: 7100 | LOSS TRAIN: 6.56, DEV: 6.25, TEST: 6.14 | KL: 0.01, RECON: 5.89, NORM: 0.67 | KL rate: 1.001\n",
      "Step: 7200 | LOSS TRAIN: 6.55, DEV: 6.25, TEST: 6.14 | KL: 0.01, RECON: 5.88, NORM: 0.67 | KL rate: 1.001\n",
      "Step: 7300 | LOSS TRAIN: 6.55, DEV: 6.24, TEST: 6.14 | KL: 0.01, RECON: 5.88, NORM: 0.67 | KL rate: 1.001\n",
      "Step: 7400 | LOSS TRAIN: 6.55, DEV: 6.24, TEST: 6.14 | KL: 0.01, RECON: 5.88, NORM: 0.67 | KL rate: 1.001\n",
      "Step: 7500 | LOSS TRAIN: 6.54, DEV: 6.24, TEST: 6.14 | KL: 0.01, RECON: 5.87, NORM: 0.67 | KL rate: 1.001\n",
      "Step: 7600 | LOSS TRAIN: 6.54, DEV: 6.23, TEST: 6.14 | KL: 0.01, RECON: 5.87, NORM: 0.67 | KL rate: 1.001\n",
      "Step: 7700 | LOSS TRAIN: 6.54, DEV: 6.23, TEST: 6.14 | KL: 0.01, RECON: 5.87, NORM: 0.67 | KL rate: 1.001\n",
      "Step: 7800 | LOSS TRAIN: 6.54, DEV: 6.22, TEST: 6.14 | KL: 0.01, RECON: 5.86, NORM: 0.67 | KL rate: 1.001\n",
      "Step: 7900 | LOSS TRAIN: 6.53, DEV: 6.23, TEST: 6.14 | KL: 0.01, RECON: 5.86, NORM: 0.67 | KL rate: 1.001\n",
      "Step: 8000 | LOSS TRAIN: 6.53, DEV: 6.22, TEST: 6.14 | KL: 0.01, RECON: 5.86, NORM: 0.67 | KL rate: 1.001\n",
      "Step: 8100 | LOSS TRAIN: 6.53, DEV: 6.21, TEST: 6.14 | KL: 0.01, RECON: 5.86, NORM: 0.67 | KL rate: 1.001\n",
      "Step: 8200 | LOSS TRAIN: 6.52, DEV: 6.21, TEST: 6.14 | KL: 0.01, RECON: 5.85, NORM: 0.67 | KL rate: 1.001\n",
      "Step: 8300 | LOSS TRAIN: 6.52, DEV: 6.20, TEST: 6.14 | KL: 0.01, RECON: 5.85, NORM: 0.67 | KL rate: 1.001\n",
      "Step: 8400 | LOSS TRAIN: 6.52, DEV: 6.20, TEST: 6.14 | KL: 0.01, RECON: 5.85, NORM: 0.67 | KL rate: 1.001\n",
      "Step: 8500 | LOSS TRAIN: 6.51, DEV: 6.20, TEST: 6.14 | KL: 0.01, RECON: 5.84, NORM: 0.67 | KL rate: 1.001\n",
      "Step: 8600 | LOSS TRAIN: 6.51, DEV: 6.19, TEST: 6.14 | KL: 0.01, RECON: 5.84, NORM: 0.67 | KL rate: 1.001\n",
      "Step: 8700 | LOSS TRAIN: 6.51, DEV: 6.19, TEST: 6.14 | KL: 0.01, RECON: 5.84, NORM: 0.67 | KL rate: 1.001\n",
      "Step: 8800 | LOSS TRAIN: 6.51, DEV: 6.19, TEST: 6.14 | KL: 0.01, RECON: 5.84, NORM: 0.67 | KL rate: 1.001\n",
      "Step: 8900 | LOSS TRAIN: 6.50, DEV: 6.18, TEST: 6.14 | KL: 0.01, RECON: 5.83, NORM: 0.67 | KL rate: 1.001\n",
      "Step: 9000 | LOSS TRAIN: 6.50, DEV: 6.18, TEST: 6.14 | KL: 0.01, RECON: 5.83, NORM: 0.67 | KL rate: 1.001\n",
      "Step: 9100 | LOSS TRAIN: 6.50, DEV: 6.18, TEST: 6.14 | KL: 0.01, RECON: 5.83, NORM: 0.67 | KL rate: 1.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: this light will no doubt capture the attention of night time drivers\n",
      "Pred: this is a great product\n",
      "True: it has three functions for the led blinking strobe and solid\n",
      "Pred: i do not recommend this product\n",
      "True: the lasers project well and can be set to flash or remain solid\n",
      "Pred: i do not recommend this product\n",
      "True: awesome product\n",
      "Pred: i like this product\n",
      "True: hopefully it holds up\n",
      "Pred: i would recommend this one of these\n",
      "True: i love this light\n",
      "Pred: this is a great product\n",
      "True: i have two of these\n",
      "Pred: i do not recommend this product\n",
      "True: it is very bright\n",
      "Pred: if you are going to get it\n",
      "True: i like the different blinking modes that the light offers\n",
      "Pred: i do not recommend this product\n",
      "True: i recommend this to everyone who rides at night\n",
      "Pred: i do not buy this knife\n",
      "True: i worked out once back in <unk>\n",
      "Pred: i do not have to use it\n",
      "True: i picked up some 40 lb weights and was like 34 too easy 34\n",
      "Pred: i do not recommend this product\n",
      "True: so then i grabbed some resistance bands and almost broke them\n",
      "Pred: i do not recommend this knife\n",
      "True: now i have been recommended black mountain products because they are very durable\n",
      "Pred: i do not recommend this product\n",
      "True: i have had a set of the 25 lb red resistance bands for 3 months now and they look as good as when i first got them after an hour of use every day except sunday\n",
      "Pred: if you are looking for me\n",
      "True: works just as advertised\n",
      "Pred: it is a great product\n",
      "True: great soft but rugged spongy handles\n",
      "Pred: i have been able to use it\n",
      "True: great tension\n",
      "Pred: i do not recommend this product\n",
      "True: seems well built\n",
      "Pred: this is a great product\n",
      "True: the locking device for the door works perfectly\n",
      "Pred: i do not recommend it\n",
      "True: good for those small exercises that one can not do with freeweights\n",
      "Pred: this is a great knife\n",
      "True: i use it for slight rehab on a knee\n",
      "Pred: if you do not want to use it\n",
      "True: it is perfect for that\n",
      "Pred: i would recommend this product\n",
      "True: with the door stoppers you can get angles that really help\n",
      "Pred: if you do not want to get it\n",
      "True: of course it is all very light so i can not see doing anything heavy with these but they are good so far and seem to be made to last\n",
      "Pred: this is a great knife\n",
      "True: these work well\n",
      "Pred: i do not recommend this product\n",
      "True: it is easy to attach multiple bands for more resistance\n",
      "Pred: i do not recommend this product\n",
      "True: they are easier to use that way if you loosely secure them together in the middle\n",
      "Pred: i did not recommend this product\n",
      "True: the weight of each band is not marked on the band itself that i could find which could be a nice addition\n",
      "Pred: it is very good\n",
      "True: overall though they are durable and do what they are made to do\n",
      "Pred: i do not recommend this product\n",
      "True: i used to <unk> and from lifting heavy weight my joints started to hurt\n",
      "Pred: i do not recommend this product\n",
      "True: so i bought these resistance bands and they work great\n",
      "Pred: it is great\n",
      "True: they are not heavy but after so my <unk> you can feel the burn\n",
      "Pred: this is a great product\n",
      "True: i work out home work and on the go\n",
      "Pred: it is a great product\n",
      "True: i a gym in the bag\n",
      "Pred: i do not recommend this product\n",
      "True: my husband drives and only really walks as exercise\n",
      "Pred: i do not recommend this product\n",
      "True: his cab is cramped and does not have room for weight or other things\n",
      "Pred: i do not recommend this product\n",
      "True: so on a whim i bought this one day for him\n",
      "Pred: i would recommend this knife\n",
      "True: although he has not been 34 motivated 34 lately to exercise he did give these a try and likes them\n",
      "Pred: if you want to get it\n",
      "True: he now uses them quite often as they are so convenient and he can do it regardless of the weather\n",
      "Pred: i do not recommend this product\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ef8292676f44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_loss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon_loss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlosses_train\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_loss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon_loss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for ct, batch in train_batches:\n",
    "    feed_dict = get_feed_dict(batch)\n",
    "    _, loss_batch, kl_loss_batch, recon_loss_batch, norm_batch = sess.run([opt, loss, kl_loss, recon_loss, norm], feed_dict = feed_dict)\n",
    "    losses_train += [[loss_batch, kl_loss_batch, recon_loss_batch, norm_batch]]\n",
    "\n",
    "    kl_rate_eval = kl_rate.eval(session=sess)\n",
    "    if config.anneal and (ct > config.kl_rate_rise_time) and kl_rate_eval < 1:\n",
    "        new_kl_rate = kl_rate_eval + config.kl_rate_rise_factor\n",
    "        sess.run(kl_rate.assign(new_kl_rate))\n",
    "    \n",
    "    if ct%config.log_period==0:\n",
    "        loss_train, kl_loss_train, recon_loss_train, norm_train = np.mean(losses_train, 0)\n",
    "        loss_dev = get_loss(sess, dev_batches)\n",
    "\n",
    "        if loss_dev <= loss_min:\n",
    "            loss_min = loss_dev\n",
    "            loss_test = get_loss(sess, test_batches)\n",
    "        \n",
    "        clear_output()\n",
    "        \n",
    "        logs += [(ct, loss_train, loss_dev, loss_test, kl_loss_train, recon_loss_train, norm_train, kl_rate_eval)]\n",
    "        for log in logs:\n",
    "            print('Step: %i | LOSS TRAIN: %.2f, DEV: %.2f, TEST: %.2f | KL: %.2f, RECON: %.2f, NORM: %.2f | KL rate: %.3f' %  log)\n",
    "        \n",
    "        print_sample(sample_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = get_feed_dict(sample_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_latent_input_do = tf.tile(tf.expand_dims(latents, 1), [1, tf.shape(dec_input_do)[1], 1])\n",
    "dec_concat_input_do = tf.concat([dec_input_do, dec_latent_input_do], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "_logvars, _means, _kl_losses, _latents, _dec_latent_input_do, _dec_input_do, _dec_concat_input_do = sess.run([logvars, means, kl_losses, latents, dec_latent_input_do, dec_input_do, dec_concat_input_do], feed_dict=feed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 16),\n",
       " (120, 16),\n",
       " (120,),\n",
       " (120, 16),\n",
       " (120, 46, 300),\n",
       " (120, 46, 16),\n",
       " (120, 46, 316))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_logvars.shape, _means.shape, _kl_losses.shape, _latents.shape, _dec_input_do.shape, _dec_latent_input_do.shape, _dec_concat_input_do.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "_output_logits, _dec_target_idxs_do, _dec_mask_tokens_do, _recon_loss, _kl_losses, _ = sess.run([output_logits, dec_target_idxs_do, dec_mask_tokens_do, recon_loss, kl_losses, opt], feed_dict=feed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 46)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_max(output_logits, 2).eval(session=sess, feed_dict=feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 46, 20000), (120, 46), (120, 46))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_output_logits.shape, _dec_target_idxs_do.shape, _dec_mask_tokens_do.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "_logits = np.exp(_output_logits) / np.sum(np.exp(_output_logits), 2)[:, :, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "_idxs = _dec_target_idxs_do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "_losses = np.array([[-np.log(_logits[i, j, _idxs[i, j]]) for j in range(_idxs.shape[1])] for i in range(_idxs.shape[0])]) * _dec_mask_tokens_do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.903732"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(_losses)/np.sum(_dec_mask_tokens_do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.903732"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_recon_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_kl_losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
