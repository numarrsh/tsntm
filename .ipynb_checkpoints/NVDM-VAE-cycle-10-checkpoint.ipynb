{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from data_structure import get_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "from topic_model import TopicModel\n",
    "\n",
    "from topic_beam_search_decoder import BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '4', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/apnews/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/nvdm_vae', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'apnews', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 50, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 500, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.1, 'lr')\n",
    "flags.DEFINE_float('reg', 1., 'regularization term')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_bool('warmup', True, 'flg of warming up')\n",
    "flags.DEFINE_integer('epochs_cycle', 10, 'number of epochs within a cycle')\n",
    "flags.DEFINE_float('r_cycle', 0.5, 'proportion used to increase beta within a cycle')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 10, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_bow', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_hidden_topic', 512, 'dim_hidden_topic')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "flags.DEFINE_bool('bidirectional', True, 'flg of bidirectional encoding')\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('PAD_IDX', word_to_idx[PAD], 'PAD_IDX')\n",
    "flags.DEFINE_integer('UNK_IDX', word_to_idx[UNK], 'UNK_IDX')\n",
    "flags.DEFINE_integer('BOS_IDX', word_to_idx[BOS], 'BOS_IDX')\n",
    "flags.DEFINE_integer('EOS_IDX', word_to_idx[EOS], 'EOS_IDX')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "maximum_iterations = max([max([instance.max_sent_l for instance in batch]) for ct, batch in dev_batches])\n",
    "flags.DEFINE_integer('maximum_iterations', maximum_iterations, 'maximum_iterations')\n",
    "\n",
    "flags.DEFINE_integer('cycle_steps', len(train_batches)*config.epochs_cycle, 'number of steps for each cycle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build language model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['bow'] = tf.placeholder(tf.float32, [None, config.dim_bow], name='bow')\n",
    "t_variables['input_token_idxs'] = tf.placeholder(tf.int32, [None, None], name='input_token_idxs')\n",
    "t_variables['dec_input_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_input_idxs')\n",
    "t_variables['dec_target_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_target_idxs')\n",
    "t_variables['batch_l'] = tf.placeholder(tf.int32, name='batch_l')\n",
    "t_variables['doc_l'] = tf.placeholder(tf.int32, [None], name='doc_l')\n",
    "t_variables['sent_l'] = tf.placeholder(tf.int32, [None], name='sent_l')\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train', assertion=False):\n",
    "    def token_dropout(sent_idxs):\n",
    "        sent_idxs_dropout = np.asarray(sent_idxs)\n",
    "        sent_idxs_dropout[np.random.rand(len(sent_idxs)) > config.word_keep_prob] = config.UNK_IDX\n",
    "        return list(sent_idxs_dropout)\n",
    "\n",
    "    bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "    \n",
    "    doc_l = np.array([len(instance.token_idxs) for instance in batch])\n",
    "    \n",
    "    feed_input_token_idxs_list = [sent_idxs for instance in batch for sent_idxs in instance.token_idxs]\n",
    "    feed_dec_input_idxs_list = [[config.BOS_IDX] + token_dropout(sent_idxs) for sent_idxs in feed_input_token_idxs_list]\n",
    "    feed_dec_target_idxs_list = [sent_idxs + [config.EOS_IDX]  for sent_idxs in feed_input_token_idxs_list]\n",
    "        \n",
    "    sent_l = np.array([len(sent_idxs) for sent_idxs in feed_input_token_idxs_list], np.int32)\n",
    "    batch_l = len(sent_l)\n",
    "    \n",
    "    feed_input_token_idxs = pad_sequences(feed_input_token_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_input_idxs = pad_sequences(feed_dec_input_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_target_idxs = pad_sequences(feed_dec_target_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    \n",
    "    if assertion:\n",
    "        index = 0\n",
    "        for instance in batch:\n",
    "            for line_idxs in instance.token_idxs:\n",
    "                assert feed_input_token_idxs_list[index] == line_idxs\n",
    "                index += 1\n",
    "        assert feed_input_token_idxs.shape[1] == np.max(sent_l)\n",
    "        assert feed_dec_input_idxs.shape[1] == np.max(sent_l) + 1\n",
    "        assert feed_dec_target_idxs.shape[1] == np.max(sent_l) + 1\n",
    "    \n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "\n",
    "    feed_dict = {\n",
    "                t_variables['bow']: bow, \n",
    "                t_variables['batch_l']: batch_l, t_variables['doc_l']: doc_l, t_variables['sent_l']: sent_l, \n",
    "                t_variables['input_token_idxs']: feed_input_token_idxs, t_variables['dec_input_idxs']: feed_dec_input_idxs, t_variables['dec_target_idxs']: feed_dec_target_idxs, \n",
    "                t_variables['keep_prob']: keep_prob\n",
    "    }\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     10,
     24,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "def check_shape(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()\n",
    "    \n",
    "def check_value(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()    \n",
    "    \n",
    "# sent_loss_kl_categ_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, tf_log(prob_topic_infer/prob_topic_sents)), 1))\n",
    "# debug_value([sent_loss_kl_categ, sent_loss_kl_categ_tmp])\n",
    "# sent_loss_kl_gauss_tmp = 0.5 * tf.reduce_sum(tf.exp(logvars_topic_infer-logvars_topic) + tf.square(means_topic - means_topic_infer) / tf.exp(logvars_topic) - 1 + (logvars_topic - logvars_topic_infer), -1)\n",
    "# sent_loss_kl_gmm_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss_tmp), -1))\n",
    "# debug_value([sent_loss_kl_gmm_tmp, sent_loss_kl_gmm])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encode bow\n",
    "with tf.variable_scope('topic/enc', reuse=False):\n",
    "    hidden_bow_ = tf.layers.Dense(units=config.dim_hidden_bow, activation=tf.nn.relu, name='hidden_bow')(t_variables['bow'])\n",
    "    hidden_bow = tf.layers.Dropout(t_variables['keep_prob'])(hidden_bow_)\n",
    "    means_bow = tf.layers.Dense(units=config.dim_latent_bow, name='mean_bow')(hidden_bow)\n",
    "    logvars_bow = tf.layers.Dense(units=config.dim_latent_bow, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_bow)\n",
    "    latents_bow = sample_latents(means_bow, logvars_bow) # sample latent vectors\n",
    "\n",
    "    prob_topic = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic')(latents_bow) # inference of topic probabilities\n",
    "\n",
    "# decode bow\n",
    "with tf.variable_scope('shared', reuse=False):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "bow_embeddings = tf.nn.embedding_lookup(embeddings, bow_idxs) # embeddings of each bow features\n",
    "\n",
    "with tf.variable_scope('topic/dec', reuse=False):\n",
    "    topic_bow = tf.get_variable('topic_bow', [config.n_topic, config.dim_bow], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of topics\n",
    "    logits_bow = tf_log(tf.nn.softmax(tf.tensordot(prob_topic, topic_bow, axes=[[1], [0]]), 1))\n",
    "\n",
    "    # prior of each gaussian distribution (computed for each topic)\n",
    "    hidden_topic = tf.layers.Dense(units=config.dim_hidden_topic, activation=tf.nn.relu, name='hidden_topic')(topic_bow)\n",
    "    means_topic = tf.layers.Dense(units=config.dim_latent, name='mean_topic')(hidden_topic)\n",
    "    logvars_topic = tf.layers.Dense(units=config.dim_latent, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_topic)\n",
    "    sigma_topic = tf.exp(0.5 * logvars_topic)\n",
    "    gauss_topic = tfd.Normal(loc=means_topic, scale=sigma_topic)    \n",
    "    \n",
    "# define losses\n",
    "topic_losses_recon = -tf.reduce_sum(tf.multiply(t_variables['bow'], logits_bow), 1)\n",
    "topic_loss_recon = tf.reduce_mean(topic_losses_recon) # negative log likelihood of each words\n",
    "\n",
    "topic_loss_kl = compute_kl_loss(means_bow, logvars_bow) # KL divergence b/w latent dist & gaussian std\n",
    "\n",
    "topic_bow_norm = topic_bow / tf.norm(topic_bow, axis=1, keepdims=True)\n",
    "topic_dots = tf.clip_by_value(tf.matmul(topic_bow_norm, tf.transpose(topic_bow_norm)), -1., 1.)\n",
    "topic_loss_reg = tf.reduce_mean(tf.square(topic_dots - tf.eye(config.n_topic)))\n",
    "# topic_angles = tf.acos(topic_dots)\n",
    "# topic_angles_mean = tf.reduce_mean(topic_angles)\n",
    "# topic_angles_vars = tf.reduce_mean(tf.square(topic_angles - topic_angles_mean))\n",
    "# topic_loss_reg = tf.exp(topic_angles_vars - topic_angles_mean)\n",
    "\n",
    "# monitor\n",
    "n_bow = tf.reduce_sum(t_variables['bow'], 1)\n",
    "topic_ppls = tf.divide(topic_losses_recon, n_bow)\n",
    "topics_freq_bow_indices = tf.nn.top_k(topic_bow, 10, name='topic_freq_bow').indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "input_token_idxs = t_variables['input_token_idxs']\n",
    "batch_l = t_variables['batch_l']\n",
    "sent_l = t_variables['sent_l']\n",
    "max_sent_l = tf.reduce_max(sent_l)\n",
    "\n",
    "with tf.variable_scope('sent/enc', reuse=False):\n",
    "    # get word embedding\n",
    "    enc_input = tf.nn.embedding_lookup(embeddings, input_token_idxs)\n",
    "\n",
    "    # get sentence embedding\n",
    "    _, enc_state = dynamic_bi_rnn(enc_input, sent_l, config.dim_hidden, t_variables['keep_prob'])\n",
    "\n",
    "    # TODO House Holder flow\n",
    "    hidden_topic_infer =  tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='hidden_topic_infer')(enc_state)\n",
    "    prob_topic_infer = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic_infer')(hidden_topic_infer)\n",
    "\n",
    "    w_mean_topic_infer = tf.get_variable('mean_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32)\n",
    "    b_mean_topic_infer = tf.get_variable('mean_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32)\n",
    "    means_topic_infer = tf.tensordot(enc_state, w_mean_topic_infer, axes=[[1], [1]]) + b_mean_topic_infer\n",
    "    \n",
    "    w_logvar_topic_infer = tf.get_variable('logvar_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    b_logvar_topic_infer = tf.get_variable('logvar_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    logvars_topic_infer = tf.tensordot(enc_state, w_logvar_topic_infer, axes=[[1], [1]]) + b_logvar_topic_infer\n",
    "    sigma_topic_infer = tf.exp(0.5 * logvars_topic_infer)\n",
    "    gauss_topic_infer = tfd.Normal(loc=means_topic_infer, scale=sigma_topic_infer)\n",
    "    \n",
    "    # latent vectors from each gaussian dist.\n",
    "    latents_topic_infer = sample_latents(means_topic_infer, logvars_topic_infer) \n",
    "    # latent vector from gaussian mixture\n",
    "    latents_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), latents_topic_infer, transpose_a=True)\n",
    "    \n",
    "    # for beam search\n",
    "    means_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), means_topic_infer, transpose_a=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for decoding\n",
    "dec_sent_l = tf.add(sent_l, 1)\n",
    "dec_input_idxs = t_variables['dec_input_idxs']\n",
    "dec_input = tf.nn.embedding_lookup(embeddings, dec_input_idxs)\n",
    "\n",
    "dec_latents_input = tf.tile(latents_input, [1, tf.shape(dec_input)[1], 1])\n",
    "dec_concat_input = tf.concat([dec_input, dec_latents_input], -1)\n",
    "\n",
    "# decode for training\n",
    "with tf.variable_scope('sent/dec/rnn', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32, reuse=False):\n",
    "    dec_cell = tf.contrib.rnn.GRUCell(config.dim_hidden)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob = t_variables['keep_prob'])\n",
    "\n",
    "    dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(latents_input, 1))\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_concat_input, sequence_length=dec_sent_l)\n",
    "\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=dec_cell,\n",
    "        helper=helper,\n",
    "        initial_state=dec_initial_state)\n",
    "\n",
    "    dec_outputs, _, output_sent_l = tf.contrib.seq2seq.dynamic_decode(train_decoder)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(config.n_vocab, use_bias=False, name='out')\n",
    "    output_logits = output_layer(dec_outputs.rnn_output)\n",
    "    \n",
    "    output_token_idxs = tf.argmax(output_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tokens = tf.fill([batch_l], config.BOS_IDX)\n",
    "end_token = config.EOS_IDX\n",
    "\n",
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    infer_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(means_input, 1))\n",
    "    beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(infer_dec_initial_state, multiplier=config.beam_width)\n",
    "    beam_latents_input = tf.contrib.seq2seq.tile_batch(tf.squeeze(means_input, 1), multiplier=config.beam_width) # added\n",
    "    \n",
    "    beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=beam_latents_input)\n",
    "\n",
    "    beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    beam_output_token_idxs = beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    inter_means_input = tf.placeholder(tf.float32, [None, config.dim_latent])\n",
    "    \n",
    "    inter_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(inter_means_input)\n",
    "    inter_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(inter_dec_initial_state, multiplier=config.beam_width)\n",
    "    inter_beam_latents_input = tf.contrib.seq2seq.tile_batch(inter_means_input, multiplier=config.beam_width) # added\n",
    "    \n",
    "    inter_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=inter_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=inter_beam_latents_input)\n",
    "\n",
    "    inter_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inter_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    inter_beam_output_token_idxs = inter_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    topic_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic)\n",
    "    topic_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(topic_dec_initial_state, multiplier=config.beam_width)\n",
    "    topic_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic, multiplier=config.beam_width) # added\n",
    "    \n",
    "    topic_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=topic_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=topic_beam_latents_input)\n",
    "\n",
    "    topic_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        topic_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    topic_beam_output_token_idxs = topic_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language modeling cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target and mask\n",
    "dec_target_idxs = t_variables['dec_target_idxs']\n",
    "dec_mask_tokens = tf.sequence_mask(dec_sent_l, maxlen=max_sent_l+1, dtype=tf.float32)\n",
    "\n",
    "# nll for each token (averaged over batch & sentence)\n",
    "sent_loss_recon = tf.contrib.seq2seq.sequence_loss(output_logits, dec_target_idxs, dec_mask_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_l = t_variables['doc_l']\n",
    "mask_sents = tf.sequence_mask(doc_l)\n",
    "mask_sents_flatten = tf.reshape(mask_sents, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1]])\n",
    "\n",
    "prob_topic_tiled = tf.tile(tf.expand_dims(prob_topic, 1), [1, tf.shape(mask_sents)[1], 1])\n",
    "prob_topic_flatten = tf.reshape(prob_topic_tiled, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1], config.n_topic])\n",
    "prob_topic_sents = tf.boolean_mask(prob_topic_flatten, mask_sents_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferred mixture probabilities (computed for each sentence)\n",
    "categ_topic_infer = tfd.Categorical(probs=prob_topic_infer)\n",
    "\n",
    "# prior of mixture probabilities (computed for each document, tiled for each sentence)\n",
    "categ_topic = tfd.Categorical(probs=prob_topic_sents)\n",
    "\n",
    "sent_loss_kl_categ = tf.reduce_mean(tfd.kl_divergence(categ_topic_infer, categ_topic))\n",
    "\n",
    "# inference of each gaussian gaussribution (computed for each sentence)\n",
    "\n",
    "sent_loss_kl_gauss = tf.reduce_sum(tfd.kl_divergence(gauss_topic_infer, gauss_topic), -1)\n",
    "sent_loss_kl_gmm = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss), -1))\n",
    "\n",
    "sent_loss_kl = sent_loss_kl_categ + sent_loss_kl_gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "tau = tf.cast(tf.divide(tf.mod(global_step, tf.constant(config.cycle_steps)), tf.constant(config.cycle_steps)), dtype=tf.float32)\n",
    "beta = tf.minimum(1., tau/config.r_cycle)\n",
    "\n",
    "sent_loss = sent_loss_recon + beta * sent_loss_kl\n",
    "\n",
    "topic_loss = topic_loss_recon + topic_loss_kl + config.reg * topic_loss_reg\n",
    "loss = topic_loss + sent_loss\n",
    "\n",
    "# define optimizer\n",
    "if config.opt == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif config.opt == 'Adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "    \n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_sents(token_idxs, config, idx_to_word):\n",
    "    sents = []\n",
    "    for sent_idxs in token_idxs:\n",
    "        tokens = []\n",
    "        for idx in sent_idxs:\n",
    "            if idx == config.EOS_IDX: break\n",
    "            tokens.append(idx_to_word[idx])\n",
    "        sent = ' '.join(tokens)\n",
    "        sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_batch, sent_loss_batch, ppls_batch = sess.run([loss, topic_loss, sent_loss, topic_ppls], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_batch, sent_loss_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "    loss_mean, topic_loss_mean, sent_loss_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    return loss_mean, topic_loss_mean, sent_loss_mean, ppl_mean\n",
    "\n",
    "def get_all_losses(sess, batches):\n",
    "    losses = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch = \\\n",
    "        sess.run([loss, topic_loss_recon, topic_loss_kl, sent_loss_recon, sent_loss_kl], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "    print('LOSS %.2f | TM NLL: %.2f, KL: %.4f | LM NLL: %.2f, KL: %.4f' %  np.mean(losses, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_batch):\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    pred_token_idxs = sess.run(output_token_idxs, feed_dict = feed_dict)\n",
    "    true_token_idxs = [sent_idxs for instance in sample_batch for sent_idxs in instance.token_idxs]\n",
    "    \n",
    "    assert len(pred_token_idxs) == len(true_token_idxs)\n",
    "    \n",
    "    pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "    true_sents = idxs_to_sents(true_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    for i, (true_sent, pred_sent) in enumerate(zip(true_sents, pred_sents)):        \n",
    "        print(i, 'True: %s' % true_sent)\n",
    "        print(i, 'Pred: %s' % pred_sent)\n",
    "\n",
    "def print_topic_sample():\n",
    "    pred_topics_freq_bow_indices, pred_topic_token_idxs = sess.run([topics_freq_bow_indices, topic_beam_output_token_idxs], \n",
    "                                                                                                           feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "    pred_topic_sents = idxs_to_sents(pred_topic_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Topic Samples-----------')\n",
    "    for i, (topic_freq_bow_idxs, pred_topic_sent) in enumerate(zip(topics_freq_bow_idxs, pred_topic_sents)):\n",
    "        print(i, ' bow:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' sent:', pred_topic_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "logs = []\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "saver = tf.train.Saver(max_to_keep=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "010[s], Ep: 00, Ct: 00000|TR LOSS: 376, PPL: 2661|TM NLL: 365, KL: 0.73, REG:0.00 | LM NLL: 10.34, KL: 1.44|DE LOSS: 347, PPL: 2658, TM: 337, LM: 10.34|BETA: 0.000069\n",
      "087[s], Ep: 00, Ct: 00500|TR LOSS: 337, PPL: 2013|TM NLL: 329, KL: 0.54, REG:0.09 | LM NLL: 7.44, KL: 2.20|DE LOSS: 325, PPL: 1720, TM: 318, LM: 6.97|BETA: 0.034743\n",
      "088[s], Ep: 00, Ct: 01000|TR LOSS: 332, PPL: 1837|TM NLL: 324, KL: 0.32, REG:0.06 | LM NLL: 7.16, KL: 1.52|DE LOSS: 323, PPL: 1649, TM: 316, LM: 6.82|BETA: 0.069417\n",
      "088[s], Ep: 00, Ct: 01500|TR LOSS: 330, PPL: 1774|TM NLL: 323, KL: 0.21, REG:0.04 | LM NLL: 7.05, KL: 1.08|DE LOSS: 322, PPL: 1636, TM: 316, LM: 6.77|BETA: 0.104092\n",
      "088[s], Ep: 00, Ct: 02000|TR LOSS: 329, PPL: 1739|TM NLL: 322, KL: 0.16, REG:0.04 | LM NLL: 6.99, KL: 0.84|DE LOSS: 322, PPL: 1631, TM: 316, LM: 6.76|BETA: 0.138766\n",
      "088[s], Ep: 00, Ct: 02500|TR LOSS: 329, PPL: 1719|TM NLL: 321, KL: 0.13, REG:0.03 | LM NLL: 6.95, KL: 0.68|DE LOSS: 322, PPL: 1630, TM: 315, LM: 6.75|BETA: 0.173440\n",
      "070[s], Ep: 01, Ct: 00000|TR LOSS: 328, PPL: 1709|TM NLL: 321, KL: 0.11, REG:0.03 | LM NLL: 6.93, KL: 0.60|DE LOSS: 322, PPL: 1629, TM: 315, LM: 6.75|BETA: 0.200069\n",
      "089[s], Ep: 01, Ct: 00500|TR LOSS: 328, PPL: 1698|TM NLL: 321, KL: 0.10, REG:0.03 | LM NLL: 6.91, KL: 0.52|DE LOSS: 322, PPL: 1629, TM: 315, LM: 6.72|BETA: 0.234743\n",
      "088[s], Ep: 01, Ct: 01000|TR LOSS: 328, PPL: 1690|TM NLL: 321, KL: 0.08, REG:0.02 | LM NLL: 6.89, KL: 0.46|DE LOSS: 322, PPL: 1629, TM: 315, LM: 6.67|BETA: 0.269417\n",
      "088[s], Ep: 01, Ct: 01500|TR LOSS: 328, PPL: 1684|TM NLL: 321, KL: 0.07, REG:0.02 | LM NLL: 6.87, KL: 0.41|DE LOSS: 322, PPL: 1629, TM: 315, LM: 6.60|BETA: 0.304092\n",
      "089[s], Ep: 01, Ct: 02000|TR LOSS: 328, PPL: 1679|TM NLL: 321, KL: 0.07, REG:0.02 | LM NLL: 6.84, KL: 0.37|DE LOSS: 322, PPL: 1628, TM: 315, LM: 6.55|BETA: 0.338766\n",
      "089[s], Ep: 01, Ct: 02500|TR LOSS: 327, PPL: 1675|TM NLL: 321, KL: 0.06, REG:0.02 | LM NLL: 6.82, KL: 0.34|DE LOSS: 322, PPL: 1628, TM: 315, LM: 6.47|BETA: 0.373440\n",
      "070[s], Ep: 02, Ct: 00000|TR LOSS: 327, PPL: 1672|TM NLL: 320, KL: 0.06, REG:0.02 | LM NLL: 6.79, KL: 0.31|DE LOSS: 322, PPL: 1628, TM: 315, LM: 6.44|BETA: 0.400069\n",
      "088[s], Ep: 02, Ct: 00500|TR LOSS: 327, PPL: 1669|TM NLL: 320, KL: 0.05, REG:0.02 | LM NLL: 6.77, KL: 0.29|DE LOSS: 322, PPL: 1628, TM: 315, LM: 6.39|BETA: 0.434743\n",
      "089[s], Ep: 02, Ct: 01000|TR LOSS: 327, PPL: 1666|TM NLL: 320, KL: 0.05, REG:0.02 | LM NLL: 6.74, KL: 0.27|DE LOSS: 322, PPL: 1628, TM: 315, LM: 6.35|BETA: 0.469417\n",
      "089[s], Ep: 02, Ct: 01500|TR LOSS: 327, PPL: 1664|TM NLL: 320, KL: 0.04, REG:0.02 | LM NLL: 6.72, KL: 0.25|DE LOSS: 322, PPL: 1628, TM: 315, LM: 6.32|BETA: 0.504092\n",
      "089[s], Ep: 02, Ct: 02000|TR LOSS: 327, PPL: 1662|TM NLL: 320, KL: 0.04, REG:0.02 | LM NLL: 6.69, KL: 0.23|DE LOSS: 322, PPL: 1628, TM: 315, LM: 6.26|BETA: 0.538766\n",
      "088[s], Ep: 02, Ct: 02500|TR LOSS: 327, PPL: 1661|TM NLL: 320, KL: 0.04, REG:0.02 | LM NLL: 6.67, KL: 0.22|DE LOSS: 322, PPL: 1628, TM: 315, LM: 6.22|BETA: 0.573440\n",
      "070[s], Ep: 03, Ct: 00000|TR LOSS: 327, PPL: 1659|TM NLL: 320, KL: 0.04, REG:0.01 | LM NLL: 6.65, KL: 0.21|DE LOSS: 322, PPL: 1628, TM: 315, LM: 6.20|BETA: 0.600069\n",
      "089[s], Ep: 03, Ct: 00500|TR LOSS: 327, PPL: 1658|TM NLL: 320, KL: 0.04, REG:0.01 | LM NLL: 6.63, KL: 0.20|DE LOSS: 322, PPL: 1628, TM: 315, LM: 6.16|BETA: 0.634743\n",
      "088[s], Ep: 03, Ct: 01000|TR LOSS: 327, PPL: 1657|TM NLL: 320, KL: 0.03, REG:0.01 | LM NLL: 6.61, KL: 0.19|DE LOSS: 322, PPL: 1628, TM: 315, LM: 6.12|BETA: 0.669418\n",
      "088[s], Ep: 03, Ct: 01500|TR LOSS: 327, PPL: 1656|TM NLL: 320, KL: 0.03, REG:0.01 | LM NLL: 6.59, KL: 0.18|DE LOSS: 322, PPL: 1628, TM: 315, LM: 6.09|BETA: 0.704092\n",
      "087[s], Ep: 03, Ct: 02000|TR LOSS: 327, PPL: 1655|TM NLL: 320, KL: 0.03, REG:0.01 | LM NLL: 6.57, KL: 0.17|DE LOSS: 321, PPL: 1628, TM: 315, LM: 6.06|BETA: 0.738766\n",
      "088[s], Ep: 03, Ct: 02500|TR LOSS: 327, PPL: 1654|TM NLL: 320, KL: 0.03, REG:0.01 | LM NLL: 6.55, KL: 0.16|DE LOSS: 321, PPL: 1627, TM: 315, LM: 6.04|BETA: 0.773440\n",
      "069[s], Ep: 04, Ct: 00000|TR LOSS: 326, PPL: 1653|TM NLL: 320, KL: 0.03, REG:0.01 | LM NLL: 6.53, KL: 0.16|DE LOSS: 321, PPL: 1628, TM: 315, LM: 6.02|BETA: 0.800069\n",
      "088[s], Ep: 04, Ct: 00500|TR LOSS: 326, PPL: 1652|TM NLL: 320, KL: 0.03, REG:0.01 | LM NLL: 6.51, KL: 0.15|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.99|BETA: 0.834743\n",
      "087[s], Ep: 04, Ct: 01000|TR LOSS: 326, PPL: 1652|TM NLL: 320, KL: 0.03, REG:0.01 | LM NLL: 6.49, KL: 0.15|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.97|BETA: 0.869417\n",
      "088[s], Ep: 04, Ct: 01500|TR LOSS: 326, PPL: 1651|TM NLL: 320, KL: 0.03, REG:0.01 | LM NLL: 6.48, KL: 0.14|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.94|BETA: 0.904092\n",
      "087[s], Ep: 04, Ct: 02000|TR LOSS: 326, PPL: 1651|TM NLL: 320, KL: 0.02, REG:0.01 | LM NLL: 6.46, KL: 0.14|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.92|BETA: 0.938766\n",
      "087[s], Ep: 04, Ct: 02500|TR LOSS: 326, PPL: 1650|TM NLL: 320, KL: 0.02, REG:0.01 | LM NLL: 6.44, KL: 0.13|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.89|BETA: 0.973440\n",
      "069[s], Ep: 05, Ct: 00000|TR LOSS: 326, PPL: 1649|TM NLL: 320, KL: 0.02, REG:0.01 | LM NLL: 6.43, KL: 0.13|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.88|BETA: 1.000000\n",
      "087[s], Ep: 05, Ct: 00500|TR LOSS: 326, PPL: 1649|TM NLL: 320, KL: 0.02, REG:0.01 | LM NLL: 6.41, KL: 0.12|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.86|BETA: 1.000000\n",
      "087[s], Ep: 05, Ct: 01000|TR LOSS: 326, PPL: 1648|TM NLL: 320, KL: 0.02, REG:0.01 | LM NLL: 6.40, KL: 0.12|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.86|BETA: 1.000000\n",
      "087[s], Ep: 05, Ct: 01500|TR LOSS: 326, PPL: 1647|TM NLL: 320, KL: 0.02, REG:0.01 | LM NLL: 6.38, KL: 0.12|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.83|BETA: 1.000000\n",
      "087[s], Ep: 05, Ct: 02000|TR LOSS: 326, PPL: 1647|TM NLL: 320, KL: 0.02, REG:0.01 | LM NLL: 6.37, KL: 0.11|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.81|BETA: 1.000000\n",
      "088[s], Ep: 05, Ct: 02500|TR LOSS: 326, PPL: 1647|TM NLL: 320, KL: 0.02, REG:0.01 | LM NLL: 6.35, KL: 0.11|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.81|BETA: 1.000000\n",
      "069[s], Ep: 06, Ct: 00000|TR LOSS: 326, PPL: 1647|TM NLL: 320, KL: 0.02, REG:0.01 | LM NLL: 6.34, KL: 0.11|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.78|BETA: 1.000000\n",
      "087[s], Ep: 06, Ct: 00500|TR LOSS: 326, PPL: 1646|TM NLL: 320, KL: 0.02, REG:0.01 | LM NLL: 6.33, KL: 0.10|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.77|BETA: 1.000000\n",
      "087[s], Ep: 06, Ct: 01000|TR LOSS: 326, PPL: 1646|TM NLL: 320, KL: 0.02, REG:0.01 | LM NLL: 6.31, KL: 0.10|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.75|BETA: 1.000000\n",
      "088[s], Ep: 06, Ct: 01500|TR LOSS: 326, PPL: 1645|TM NLL: 320, KL: 0.02, REG:0.01 | LM NLL: 6.30, KL: 0.10|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.74|BETA: 1.000000\n",
      "087[s], Ep: 06, Ct: 02000|TR LOSS: 326, PPL: 1645|TM NLL: 320, KL: 0.02, REG:0.01 | LM NLL: 6.29, KL: 0.10|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.73|BETA: 1.000000\n",
      "088[s], Ep: 06, Ct: 02500|TR LOSS: 326, PPL: 1645|TM NLL: 320, KL: 0.02, REG:0.01 | LM NLL: 6.27, KL: 0.09|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.72|BETA: 1.000000\n",
      "069[s], Ep: 07, Ct: 00000|TR LOSS: 326, PPL: 1645|TM NLL: 320, KL: 0.02, REG:0.01 | LM NLL: 6.26, KL: 0.09|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.70|BETA: 1.000000\n",
      "088[s], Ep: 07, Ct: 00500|TR LOSS: 326, PPL: 1645|TM NLL: 320, KL: 0.02, REG:0.01 | LM NLL: 6.25, KL: 0.09|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.69|BETA: 1.000000\n",
      "088[s], Ep: 07, Ct: 01000|TR LOSS: 326, PPL: 1645|TM NLL: 320, KL: 0.02, REG:0.01 | LM NLL: 6.24, KL: 0.09|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.68|BETA: 1.000000\n",
      "087[s], Ep: 07, Ct: 01500|TR LOSS: 326, PPL: 1644|TM NLL: 320, KL: 0.02, REG:0.01 | LM NLL: 6.23, KL: 0.09|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.67|BETA: 1.000000\n",
      "087[s], Ep: 07, Ct: 02000|TR LOSS: 326, PPL: 1644|TM NLL: 320, KL: 0.01, REG:0.01 | LM NLL: 6.22, KL: 0.08|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.66|BETA: 1.000000\n",
      "087[s], Ep: 07, Ct: 02500|TR LOSS: 326, PPL: 1643|TM NLL: 320, KL: 0.01, REG:0.01 | LM NLL: 6.21, KL: 0.08|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.64|BETA: 1.000000\n",
      "069[s], Ep: 08, Ct: 00000|TR LOSS: 326, PPL: 1643|TM NLL: 320, KL: 0.01, REG:0.01 | LM NLL: 6.20, KL: 0.08|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.64|BETA: 1.000000\n",
      "088[s], Ep: 08, Ct: 00500|TR LOSS: 326, PPL: 1643|TM NLL: 320, KL: 0.01, REG:0.01 | LM NLL: 6.19, KL: 0.08|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.62|BETA: 1.000000\n",
      "087[s], Ep: 08, Ct: 01000|TR LOSS: 326, PPL: 1643|TM NLL: 320, KL: 0.01, REG:0.01 | LM NLL: 6.18, KL: 0.08|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.61|BETA: 1.000000\n",
      "088[s], Ep: 08, Ct: 01500|TR LOSS: 326, PPL: 1643|TM NLL: 320, KL: 0.01, REG:0.01 | LM NLL: 6.17, KL: 0.08|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.61|BETA: 1.000000\n",
      "087[s], Ep: 08, Ct: 02000|TR LOSS: 326, PPL: 1643|TM NLL: 320, KL: 0.01, REG:0.01 | LM NLL: 6.16, KL: 0.07|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.60|BETA: 1.000000\n",
      "088[s], Ep: 08, Ct: 02500|TR LOSS: 326, PPL: 1642|TM NLL: 320, KL: 0.01, REG:0.01 | LM NLL: 6.15, KL: 0.07|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.59|BETA: 1.000000\n",
      "070[s], Ep: 09, Ct: 00000|TR LOSS: 326, PPL: 1642|TM NLL: 320, KL: 0.01, REG:0.01 | LM NLL: 6.14, KL: 0.07|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.58|BETA: 1.000000\n",
      "087[s], Ep: 09, Ct: 00500|TR LOSS: 326, PPL: 1642|TM NLL: 320, KL: 0.01, REG:0.01 | LM NLL: 6.13, KL: 0.07|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.57|BETA: 1.000000\n",
      "088[s], Ep: 09, Ct: 01000|TR LOSS: 326, PPL: 1642|TM NLL: 320, KL: 0.01, REG:0.01 | LM NLL: 6.12, KL: 0.07|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.56|BETA: 1.000000\n",
      "088[s], Ep: 09, Ct: 01500|TR LOSS: 326, PPL: 1642|TM NLL: 320, KL: 0.01, REG:0.01 | LM NLL: 6.11, KL: 0.07|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.56|BETA: 1.000000\n",
      "088[s], Ep: 09, Ct: 02000|TR LOSS: 326, PPL: 1642|TM NLL: 320, KL: 0.01, REG:0.01 | LM NLL: 6.10, KL: 0.07|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.54|BETA: 1.000000\n",
      "088[s], Ep: 09, Ct: 02500|TR LOSS: 326, PPL: 1641|TM NLL: 320, KL: 0.01, REG:0.01 | LM NLL: 6.09, KL: 0.06|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.53|BETA: 1.000000\n",
      "069[s], Ep: 10, Ct: 00000|TR LOSS: 326, PPL: 1641|TM NLL: 320, KL: 0.01, REG:0.01 | LM NLL: 6.09, KL: 0.06|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.53|BETA: 0.000069\n",
      "087[s], Ep: 10, Ct: 00500|TR LOSS: 326, PPL: 1641|TM NLL: 320, KL: 0.01, REG:0.01 | LM NLL: 6.08, KL: 0.06|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.52|BETA: 0.034743\n",
      "088[s], Ep: 10, Ct: 01000|TR LOSS: 326, PPL: 1641|TM NLL: 320, KL: 0.01, REG:0.00 | LM NLL: 6.07, KL: 0.06|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.51|BETA: 0.069417\n",
      "088[s], Ep: 10, Ct: 01500|TR LOSS: 326, PPL: 1641|TM NLL: 320, KL: 0.01, REG:0.00 | LM NLL: 6.06, KL: 0.06|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.51|BETA: 0.104092\n",
      "088[s], Ep: 10, Ct: 02000|TR LOSS: 326, PPL: 1641|TM NLL: 320, KL: 0.01, REG:0.00 | LM NLL: 6.05, KL: 0.06|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.50|BETA: 0.138766\n",
      "090[s], Ep: 10, Ct: 02500|TR LOSS: 326, PPL: 1641|TM NLL: 320, KL: 0.01, REG:0.00 | LM NLL: 6.05, KL: 0.06|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.49|BETA: 0.173440\n",
      "070[s], Ep: 11, Ct: 00000|TR LOSS: 326, PPL: 1641|TM NLL: 320, KL: 0.01, REG:0.00 | LM NLL: 6.04, KL: 0.06|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.49|BETA: 0.200069\n",
      "088[s], Ep: 11, Ct: 00500|TR LOSS: 326, PPL: 1640|TM NLL: 320, KL: 0.01, REG:0.00 | LM NLL: 6.03, KL: 0.06|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.47|BETA: 0.234743\n",
      "088[s], Ep: 11, Ct: 01000|TR LOSS: 326, PPL: 1640|TM NLL: 320, KL: 0.01, REG:0.00 | LM NLL: 6.03, KL: 0.06|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.47|BETA: 0.269417\n",
      "088[s], Ep: 11, Ct: 01500|TR LOSS: 326, PPL: 1640|TM NLL: 320, KL: 0.01, REG:0.00 | LM NLL: 6.02, KL: 0.06|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.46|BETA: 0.304092\n",
      "088[s], Ep: 11, Ct: 02000|TR LOSS: 326, PPL: 1640|TM NLL: 320, KL: 0.01, REG:0.00 | LM NLL: 6.01, KL: 0.05|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.46|BETA: 0.338766\n",
      "088[s], Ep: 11, Ct: 02500|TR LOSS: 326, PPL: 1640|TM NLL: 320, KL: 0.01, REG:0.00 | LM NLL: 6.00, KL: 0.05|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.45|BETA: 0.373440\n",
      "070[s], Ep: 12, Ct: 00000|TR LOSS: 326, PPL: 1640|TM NLL: 320, KL: 0.01, REG:0.00 | LM NLL: 6.00, KL: 0.05|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.45|BETA: 0.400069\n",
      "088[s], Ep: 12, Ct: 00500|TR LOSS: 326, PPL: 1640|TM NLL: 320, KL: 0.01, REG:0.00 | LM NLL: 5.99, KL: 0.05|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.44|BETA: 0.434743\n",
      "087[s], Ep: 12, Ct: 01000|TR LOSS: 326, PPL: 1640|TM NLL: 320, KL: 0.01, REG:0.00 | LM NLL: 5.98, KL: 0.05|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.43|BETA: 0.469417\n",
      "089[s], Ep: 12, Ct: 01500|TR LOSS: 326, PPL: 1640|TM NLL: 320, KL: 0.01, REG:0.00 | LM NLL: 5.98, KL: 0.05|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.42|BETA: 0.504092\n",
      "088[s], Ep: 12, Ct: 02000|TR LOSS: 325, PPL: 1640|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.97, KL: 0.05|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.42|BETA: 0.538766\n",
      "089[s], Ep: 12, Ct: 02500|TR LOSS: 325, PPL: 1640|TM NLL: 320, KL: 0.01, REG:0.00 | LM NLL: 5.96, KL: 0.05|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.41|BETA: 0.573440\n",
      "070[s], Ep: 13, Ct: 00000|TR LOSS: 325, PPL: 1639|TM NLL: 320, KL: 0.01, REG:0.00 | LM NLL: 5.96, KL: 0.05|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.40|BETA: 0.600069\n",
      "088[s], Ep: 13, Ct: 00500|TR LOSS: 325, PPL: 1639|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.95, KL: 0.05|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.41|BETA: 0.634743\n",
      "088[s], Ep: 13, Ct: 01000|TR LOSS: 325, PPL: 1639|TM NLL: 320, KL: 0.01, REG:0.00 | LM NLL: 5.94, KL: 0.05|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.39|BETA: 0.669418\n",
      "088[s], Ep: 13, Ct: 01500|TR LOSS: 325, PPL: 1639|TM NLL: 320, KL: 0.01, REG:0.00 | LM NLL: 5.94, KL: 0.05|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.38|BETA: 0.704092\n",
      "089[s], Ep: 13, Ct: 02000|TR LOSS: 325, PPL: 1639|TM NLL: 320, KL: 0.01, REG:0.00 | LM NLL: 5.93, KL: 0.05|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.38|BETA: 0.738766\n",
      "088[s], Ep: 13, Ct: 02500|TR LOSS: 325, PPL: 1639|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.92, KL: 0.05|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.37|BETA: 0.773440\n",
      "070[s], Ep: 14, Ct: 00000|TR LOSS: 325, PPL: 1639|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.92, KL: 0.05|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.37|BETA: 0.800069\n",
      "089[s], Ep: 14, Ct: 00500|TR LOSS: 325, PPL: 1639|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.91, KL: 0.05|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.36|BETA: 0.834743\n",
      "088[s], Ep: 14, Ct: 01000|TR LOSS: 325, PPL: 1639|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.91, KL: 0.05|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.36|BETA: 0.869417\n",
      "088[s], Ep: 14, Ct: 01500|TR LOSS: 325, PPL: 1639|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.90, KL: 0.04|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.35|BETA: 0.904092\n",
      "088[s], Ep: 14, Ct: 02000|TR LOSS: 325, PPL: 1639|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.90, KL: 0.04|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.34|BETA: 0.938766\n",
      "088[s], Ep: 14, Ct: 02500|TR LOSS: 325, PPL: 1639|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.89, KL: 0.04|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.34|BETA: 0.973440\n",
      "070[s], Ep: 15, Ct: 00000|TR LOSS: 325, PPL: 1639|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.89, KL: 0.04|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.33|BETA: 1.000000\n",
      "088[s], Ep: 15, Ct: 00500|TR LOSS: 325, PPL: 1639|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.88, KL: 0.04|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.33|BETA: 1.000000\n",
      "088[s], Ep: 15, Ct: 01000|TR LOSS: 325, PPL: 1639|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.87, KL: 0.04|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.33|BETA: 1.000000\n",
      "088[s], Ep: 15, Ct: 01500|TR LOSS: 325, PPL: 1638|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.87, KL: 0.04|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.33|BETA: 1.000000\n",
      "087[s], Ep: 15, Ct: 02000|TR LOSS: 325, PPL: 1638|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.86, KL: 0.04|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.31|BETA: 1.000000\n",
      "087[s], Ep: 15, Ct: 02500|TR LOSS: 325, PPL: 1638|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.86, KL: 0.04|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.31|BETA: 1.000000\n",
      "069[s], Ep: 16, Ct: 00000|TR LOSS: 325, PPL: 1638|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.85, KL: 0.04|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.31|BETA: 1.000000\n",
      "088[s], Ep: 16, Ct: 00500|TR LOSS: 325, PPL: 1638|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.85, KL: 0.04|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.30|BETA: 1.000000\n",
      "087[s], Ep: 16, Ct: 01000|TR LOSS: 325, PPL: 1638|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.84, KL: 0.04|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.30|BETA: 1.000000\n",
      "087[s], Ep: 16, Ct: 01500|TR LOSS: 325, PPL: 1638|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.84, KL: 0.04|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.30|BETA: 1.000000\n",
      "088[s], Ep: 16, Ct: 02000|TR LOSS: 325, PPL: 1638|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.83, KL: 0.04|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.29|BETA: 1.000000\n",
      "087[s], Ep: 16, Ct: 02500|TR LOSS: 325, PPL: 1638|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.83, KL: 0.04|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.28|BETA: 1.000000\n",
      "069[s], Ep: 17, Ct: 00000|TR LOSS: 325, PPL: 1638|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.82, KL: 0.04|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.28|BETA: 1.000000\n",
      "087[s], Ep: 17, Ct: 00500|TR LOSS: 325, PPL: 1638|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.82, KL: 0.04|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.28|BETA: 1.000000\n",
      "088[s], Ep: 17, Ct: 01000|TR LOSS: 325, PPL: 1638|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.81, KL: 0.04|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.27|BETA: 1.000000\n",
      "087[s], Ep: 17, Ct: 01500|TR LOSS: 325, PPL: 1638|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.81, KL: 0.04|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.26|BETA: 1.000000\n",
      "087[s], Ep: 17, Ct: 02000|TR LOSS: 325, PPL: 1638|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.80, KL: 0.04|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.26|BETA: 1.000000\n",
      "088[s], Ep: 17, Ct: 02500|TR LOSS: 325, PPL: 1638|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.80, KL: 0.04|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.25|BETA: 1.000000\n",
      "069[s], Ep: 18, Ct: 00000|TR LOSS: 325, PPL: 1638|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.79, KL: 0.04|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.25|BETA: 1.000000\n",
      "087[s], Ep: 18, Ct: 00500|TR LOSS: 325, PPL: 1638|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.79, KL: 0.04|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.25|BETA: 1.000000\n",
      "087[s], Ep: 18, Ct: 01000|TR LOSS: 325, PPL: 1638|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.78, KL: 0.04|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.24|BETA: 1.000000\n",
      "088[s], Ep: 18, Ct: 01500|TR LOSS: 325, PPL: 1638|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.78, KL: 0.04|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.24|BETA: 1.000000\n",
      "088[s], Ep: 18, Ct: 02000|TR LOSS: 325, PPL: 1638|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.77, KL: 0.03|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.24|BETA: 1.000000\n",
      "087[s], Ep: 18, Ct: 02500|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.77, KL: 0.03|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.23|BETA: 1.000000\n",
      "069[s], Ep: 19, Ct: 00000|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.77, KL: 0.03|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.23|BETA: 1.000000\n",
      "087[s], Ep: 19, Ct: 00500|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.76, KL: 0.03|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.22|BETA: 1.000000\n",
      "088[s], Ep: 19, Ct: 01000|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.76, KL: 0.03|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.22|BETA: 1.000000\n",
      "087[s], Ep: 19, Ct: 01500|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.75, KL: 0.03|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.22|BETA: 1.000000\n",
      "087[s], Ep: 19, Ct: 02000|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.75, KL: 0.03|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.21|BETA: 1.000000\n",
      "088[s], Ep: 19, Ct: 02500|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.74, KL: 0.03|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.20|BETA: 1.000000\n",
      "069[s], Ep: 20, Ct: 00000|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.74, KL: 0.03|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.21|BETA: 0.000069\n",
      "087[s], Ep: 20, Ct: 00500|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.74, KL: 0.03|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.20|BETA: 0.034743\n",
      "087[s], Ep: 20, Ct: 01000|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.73, KL: 0.03|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.21|BETA: 0.069417\n",
      "087[s], Ep: 20, Ct: 01500|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.73, KL: 0.03|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.19|BETA: 0.104092\n",
      "088[s], Ep: 20, Ct: 02000|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.72, KL: 0.03|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.20|BETA: 0.138766\n",
      "087[s], Ep: 20, Ct: 02500|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.72, KL: 0.03|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.19|BETA: 0.173440\n",
      "069[s], Ep: 21, Ct: 00000|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.72, KL: 0.03|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.18|BETA: 0.200069\n",
      "088[s], Ep: 21, Ct: 00500|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.71, KL: 0.03|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.18|BETA: 0.234743\n",
      "087[s], Ep: 21, Ct: 01000|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.71, KL: 0.03|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.18|BETA: 0.269417\n",
      "087[s], Ep: 21, Ct: 01500|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.70, KL: 0.03|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.17|BETA: 0.304092\n",
      "087[s], Ep: 21, Ct: 02000|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.70, KL: 0.03|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.17|BETA: 0.338766\n",
      "088[s], Ep: 21, Ct: 02500|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.70, KL: 0.03|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.17|BETA: 0.373440\n",
      "070[s], Ep: 22, Ct: 00000|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.69, KL: 0.03|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.16|BETA: 0.400069\n",
      "088[s], Ep: 22, Ct: 00500|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.69, KL: 0.03|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.15|BETA: 0.434743\n",
      "087[s], Ep: 22, Ct: 01000|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.68, KL: 0.03|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.16|BETA: 0.469417\n",
      "087[s], Ep: 22, Ct: 01500|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.68, KL: 0.03|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.16|BETA: 0.504092\n",
      "087[s], Ep: 22, Ct: 02000|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.01, REG:0.00 | LM NLL: 5.68, KL: 0.03|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.15|BETA: 0.538766\n",
      "087[s], Ep: 22, Ct: 02500|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.67, KL: 0.03|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.15|BETA: 0.573440\n",
      "070[s], Ep: 23, Ct: 00000|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.67, KL: 0.03|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.15|BETA: 0.600069\n",
      "087[s], Ep: 23, Ct: 00500|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.67, KL: 0.03|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.15|BETA: 0.634743\n",
      "087[s], Ep: 23, Ct: 01000|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.66, KL: 0.03|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.14|BETA: 0.669418\n",
      "087[s], Ep: 23, Ct: 01500|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.66, KL: 0.03|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.14|BETA: 0.704092\n",
      "087[s], Ep: 23, Ct: 02000|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.66, KL: 0.03|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.13|BETA: 0.738766\n",
      "088[s], Ep: 23, Ct: 02500|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.65, KL: 0.03|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.13|BETA: 0.773440\n",
      "069[s], Ep: 24, Ct: 00000|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.65, KL: 0.03|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.13|BETA: 0.800069\n",
      "087[s], Ep: 24, Ct: 00500|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.65, KL: 0.03|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.13|BETA: 0.834743\n",
      "087[s], Ep: 24, Ct: 01000|TR LOSS: 325, PPL: 1637|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.64, KL: 0.03|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.12|BETA: 0.869417\n",
      "088[s], Ep: 24, Ct: 01500|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.64, KL: 0.03|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.12|BETA: 0.904092\n",
      "088[s], Ep: 24, Ct: 02000|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.63, KL: 0.03|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.12|BETA: 0.938766\n",
      "088[s], Ep: 24, Ct: 02500|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.63, KL: 0.03|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.12|BETA: 0.973440\n",
      "069[s], Ep: 25, Ct: 00000|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.63, KL: 0.03|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.12|BETA: 1.000000\n",
      "087[s], Ep: 25, Ct: 00500|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.62, KL: 0.03|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.11|BETA: 1.000000\n",
      "087[s], Ep: 25, Ct: 01000|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.62, KL: 0.03|DE LOSS: 321, PPL: 1628, TM: 315, LM: 5.10|BETA: 1.000000\n",
      "087[s], Ep: 25, Ct: 01500|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.62, KL: 0.03|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.10|BETA: 1.000000\n",
      "087[s], Ep: 25, Ct: 02000|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.61, KL: 0.03|DE LOSS: 320, PPL: 1627, TM: 315, LM: 5.10|BETA: 1.000000\n",
      "088[s], Ep: 25, Ct: 02500|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.61, KL: 0.03|DE LOSS: 320, PPL: 1627, TM: 315, LM: 5.10|BETA: 1.000000\n",
      "070[s], Ep: 26, Ct: 00000|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.61, KL: 0.03|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.10|BETA: 1.000000\n",
      "088[s], Ep: 26, Ct: 00500|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.61, KL: 0.03|DE LOSS: 320, PPL: 1628, TM: 315, LM: 5.09|BETA: 1.000000\n",
      "087[s], Ep: 26, Ct: 01000|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.60, KL: 0.02|DE LOSS: 321, PPL: 1627, TM: 315, LM: 5.10|BETA: 1.000000\n",
      "087[s], Ep: 26, Ct: 01500|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.60, KL: 0.02|DE LOSS: 320, PPL: 1628, TM: 315, LM: 5.09|BETA: 1.000000\n",
      "087[s], Ep: 26, Ct: 02000|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.60, KL: 0.02|DE LOSS: 320, PPL: 1628, TM: 315, LM: 5.09|BETA: 1.000000\n",
      "088[s], Ep: 26, Ct: 02500|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.59, KL: 0.02|DE LOSS: 320, PPL: 1628, TM: 315, LM: 5.08|BETA: 1.000000\n",
      "069[s], Ep: 27, Ct: 00000|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.59, KL: 0.02|DE LOSS: 320, PPL: 1627, TM: 315, LM: 5.08|BETA: 1.000000\n",
      "087[s], Ep: 27, Ct: 00500|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.59, KL: 0.02|DE LOSS: 320, PPL: 1627, TM: 315, LM: 5.08|BETA: 1.000000\n",
      "087[s], Ep: 27, Ct: 01000|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.58, KL: 0.02|DE LOSS: 320, PPL: 1628, TM: 315, LM: 5.08|BETA: 1.000000\n",
      "087[s], Ep: 27, Ct: 01500|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.58, KL: 0.02|DE LOSS: 320, PPL: 1628, TM: 315, LM: 5.07|BETA: 1.000000\n",
      "087[s], Ep: 27, Ct: 02000|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.58, KL: 0.02|DE LOSS: 320, PPL: 1627, TM: 315, LM: 5.07|BETA: 1.000000\n",
      "087[s], Ep: 27, Ct: 02500|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.57, KL: 0.02|DE LOSS: 320, PPL: 1628, TM: 315, LM: 5.07|BETA: 1.000000\n",
      "069[s], Ep: 28, Ct: 00000|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.57, KL: 0.02|DE LOSS: 320, PPL: 1627, TM: 315, LM: 5.07|BETA: 1.000000\n",
      "088[s], Ep: 28, Ct: 00500|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.57, KL: 0.02|DE LOSS: 320, PPL: 1628, TM: 315, LM: 5.07|BETA: 1.000000\n",
      "087[s], Ep: 28, Ct: 01000|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.57, KL: 0.02|DE LOSS: 320, PPL: 1628, TM: 315, LM: 5.06|BETA: 1.000000\n",
      "087[s], Ep: 28, Ct: 01500|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.56, KL: 0.02|DE LOSS: 320, PPL: 1628, TM: 315, LM: 5.06|BETA: 1.000000\n",
      "087[s], Ep: 28, Ct: 02000|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.56, KL: 0.02|DE LOSS: 320, PPL: 1628, TM: 315, LM: 5.06|BETA: 1.000000\n",
      "087[s], Ep: 28, Ct: 02500|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.56, KL: 0.02|DE LOSS: 320, PPL: 1628, TM: 315, LM: 5.06|BETA: 1.000000\n",
      "069[s], Ep: 29, Ct: 00000|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.55, KL: 0.02|DE LOSS: 320, PPL: 1627, TM: 315, LM: 5.05|BETA: 1.000000\n",
      "087[s], Ep: 29, Ct: 00500|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.55, KL: 0.02|DE LOSS: 320, PPL: 1628, TM: 315, LM: 5.06|BETA: 1.000000\n",
      "088[s], Ep: 29, Ct: 01000|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.55, KL: 0.02|DE LOSS: 320, PPL: 1627, TM: 315, LM: 5.04|BETA: 1.000000\n",
      "087[s], Ep: 29, Ct: 01500|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.55, KL: 0.02|DE LOSS: 320, PPL: 1628, TM: 315, LM: 5.05|BETA: 1.000000\n",
      "088[s], Ep: 29, Ct: 02000|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.54, KL: 0.02|DE LOSS: 320, PPL: 1628, TM: 315, LM: 5.04|BETA: 1.000000\n",
      "088[s], Ep: 29, Ct: 02500|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.54, KL: 0.02|DE LOSS: 320, PPL: 1628, TM: 315, LM: 5.05|BETA: 1.000000\n",
      "069[s], Ep: 30, Ct: 00000|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.54, KL: 0.02|DE LOSS: 320, PPL: 1627, TM: 315, LM: 5.04|BETA: 0.000069\n",
      "088[s], Ep: 30, Ct: 00500|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.53, KL: 0.02|DE LOSS: 320, PPL: 1628, TM: 315, LM: 5.04|BETA: 0.034743\n",
      "087[s], Ep: 30, Ct: 01000|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.53, KL: 0.02|DE LOSS: 320, PPL: 1627, TM: 315, LM: 5.04|BETA: 0.069417\n",
      "088[s], Ep: 30, Ct: 01500|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.53, KL: 0.02|DE LOSS: 320, PPL: 1627, TM: 315, LM: 5.03|BETA: 0.104092\n",
      "088[s], Ep: 30, Ct: 02000|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.53, KL: 0.02|DE LOSS: 320, PPL: 1627, TM: 315, LM: 5.03|BETA: 0.138766\n",
      "088[s], Ep: 30, Ct: 02500|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.52, KL: 0.02|DE LOSS: 320, PPL: 1627, TM: 315, LM: 5.03|BETA: 0.173440\n",
      "070[s], Ep: 31, Ct: 00000|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.52, KL: 0.02|DE LOSS: 320, PPL: 1627, TM: 315, LM: 5.03|BETA: 0.200069\n",
      "089[s], Ep: 31, Ct: 00500|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.52, KL: 0.02|DE LOSS: 320, PPL: 1627, TM: 315, LM: 5.03|BETA: 0.234743\n",
      "088[s], Ep: 31, Ct: 01000|TR LOSS: 325, PPL: 1636|TM NLL: 319, KL: 0.00, REG:0.00 | LM NLL: 5.52, KL: 0.02|DE LOSS: 320, PPL: 1628, TM: 315, LM: 5.03|BETA: 0.269417\n",
      "0 True: a # year old man from georgia has pleaded guilty to possessing stolen u.s. mail\n",
      "0 Pred: the # of has man has the has been charged to death degree murder district in in in\n",
      "1 True: u.s. attorney stephanie a. finley said tuesday that terry ulysses morris , of lithonia , entered the plea before u.s. district judge richard t. haik\n",
      "1 Pred: the attorney general <unk> <unk> says wednesday that the <unk> was was sentenced the , was sentenced , in the district judge in <unk> <unk> on in\n",
      "2 True: evidence shows authorities pulled morris ' vehicle over during a jan. # , # , traffic stop on interstate # in calcasieu parish\n",
      "2 Pred: the says the have the the <unk> , the the traffic\n",
      "3 True: a strong odor of marijuana prompted them to ask if they could search the car\n",
      "3 Pred: the # year in the and has in the\n",
      "4 True: after receiving permission , authorities found two <unk> containing # u.s. treasury checks that had not been cashed\n",
      "4 Pred: the says the to the say the men were the year and and and were the be found\n",
      "5 True: the checks were from social security administration accounts with georgia addresses\n",
      "5 Pred: the # was been to to #\n",
      "6 True: the checks ' face value totaled $ #\n",
      "6 Pred: the state were names up of $ # ,\n",
      "7 True: morris faces up to five years in prison , a $ # fine and three years of supervised release\n",
      "7 Pred: the says a to # years in prison\n",
      "8 True: a sentencing date has not been set\n",
      "8 Pred: the says is is been been set\n",
      "9 True: a panel of state budget experts predicts a slight increase in tax revenue during the current fiscal year\n",
      "9 Pred: the federal has the police is are the # increase in the credits for the # fiscal year\n",
      "10 True: the revenue estimating conference met thursday to project state revenue numbers\n",
      "10 Pred: the state of $ in with to the in 's of\n",
      "11 True: the three member panel includes the governor 's top budget adviser , the financial adviser to the legislature and an independent member , mason city economist david underwood\n",
      "11 Pred: the state times of is the # 's and of for\n",
      "12 True: the group predicts net state revenue\n",
      "12 Pred: the # of the income of for in to and , and and , ,\n",
      "13 True: made up of taxes and other sources\n",
      "13 Pred: the the to # the , details ,\n",
      "14 True: will come in at about $ # billion in the current fiscal year , which ends june #\n",
      "14 Pred: the be to the the # # million\n",
      "15 True: that represents a slight increase over the previous year , though a direct comparison is difficult because state lawmakers have changed some accounting practices\n",
      "15 Pred: the says the # increase in the state 's year which the the the , the to of 's are been the of\n",
      "16 True: the panel will meet again in december to set the revenue projection that gov . terry branstad must use to craft his next budget\n",
      "16 Pred: the city of be the to the\n",
      "17 True: washington state attorney general bob ferguson says his office has reached a $ # million agreement with nine lcd manufacturers accused of fixing prices for products like tvs , laptops and cellphones\n",
      "17 Pred: the 's say general <unk> <unk> says the client has not a lawsuit # million for with the years and and of a and in the\n",
      "18 True: the settlement still needs a judge 's approval\n",
      "18 Pred: the say , the to new to # to to and\n",
      "19 True: but if approved , ferguson says it will be one of the largest <unk> by his office 's antitrust division in state history\n",
      "19 Pred: the the says the the 's the was be able to the # state of the #\n",
      "20 True: the state had sued a list of companies that make products that use a liquid crystal display , or lcd\n",
      "20 Pred: the say 's a # $ of # to would the to can of new to <unk>\n",
      "21 True: the lawsuit claimed these companies conspired to fix prices between # until #\n",
      "21 Pred: the # was the evidence were to the the and\n",
      "22 True: ferguson says the scheme may have increased the prices that customers paid by as much as # percent\n",
      "22 Pred: the says the man was the the and #\n",
      "23 True: he says the bulk of the agreement will be returned to consumers who purchased the products\n",
      "23 Pred: the says the state of the # is be the to the\n",
      "24 True: the national zoo is celebrating the # th birthday of its male panda , tian tian\n",
      "24 Pred: the state weather is a the # th anniversary of the # <unk> <unk> which , , ,\n",
      "25 True: the zoo says he 'll get a <unk> cake tuesday that will include sweet potatoes , carrots , pears , apples and some juice\n",
      "25 Pred: the state of the will be the the to to\n",
      "26 True: tian tian and his mate mei xiang are the second pair of pandas to live at the zoo and arrived in #\n",
      "26 Pred: the says , the wife , <unk> , in only largest of the\n",
      "27 True: also on tuesday the zoo said that it has not yet been able to examine its days old panda cub for a second time\n",
      "27 Pred: the says the , # 's the the will been been been determined to determine the the\n",
      "28 True: a team of zookeepers attempted to get a second close look at the cub tuesday morning but was unable to\n",
      "28 Pred: the says of the and to , a call degree to for the <unk>\n",
      "29 True: the zoo says the cub 's mother mei xiang positioned her body and the cub so keepers could n't reach it\n",
      "29 Pred: the city is the # 's <unk> , , , , to\n",
      "30 True: the zoo says attempts to distract her also failed\n",
      "30 Pred: the state of the to be the the the to\n",
      "31 True: an argument in a central oahu bar escalated into a shooting at a nearby park that injured two men\n",
      "31 Pred: the attorney man the new pennsylvania city has to the man that a man man in has # people and\n",
      "32 True: neighbors near <unk> park in <unk> say the early monday gunshots sounded like fourth of july fireworks\n",
      "32 Pred: the say the , in the are the # morning morning and the # the the #\n",
      "33 True: police say an argument started at the shack restaurant and spilled over to the park a few blocks away where a # year old suspect pulled out a gun and shot two men\n",
      "33 Pred: the say the investigation with the the scene 's in the in the the head\n",
      "34 True: khon tv reports the argument stemmed from the suspect seeing one of the men with his ex girlfriend\n",
      "34 Pred: the tv the ( man was from the to 's a <unk> the man\n",
      "35 True: the victims were shot multiple times and hospitalized\n",
      "35 Pred: the # says taken to the the was\n",
      "36 True: the # year old suspect later turned himself in to police\n",
      "36 Pred: the # year old man was died the in the a\n",
      "37 True: the honolulu star advertiser reports he has five previous convictions for misdemeanors or violations , and one felony conviction for assault in #\n",
      "37 Pred: the state star advertiser ( that was been a year in the year of of but the was of of the\n",
      "38 True: a man will spend the rest of his life behind bars for the # shooting of a philadelphia teen who was slain on his front porch\n",
      "38 Pred: the new year be $ next of the # sentence the\n",
      "39 True: the philadelphia inquirer ( http : //bit.ly/ # <unk> ) said the mandatory life without parole sentence for # year old julian frisby was handed down friday after a jury convicted him of first degree murder\n",
      "39 Pred: the state daily reports the : <unk> # <unk> ) reports the judge 's the the\n",
      "40 True: his first trial last year ended in a hung jury\n",
      "40 Pred: the name was was month was a # court county\n",
      "41 True: frisby denied killing # year son <unk> <unk> , who was shot to death in front of his father on easter sunday in #\n",
      "41 Pred: the says the the year old and <unk> <unk>\n",
      "42 True: prosecutors said <unk> was apparently the mistaken target of a dispute between two rival neighborhood gangs ; he was not involved with either group\n",
      "42 Pred: the say the was not shot to and of the #\n",
      "43 True: frisby said in court that he felt sorry for the victim 's family but insisted he was innocent\n",
      "43 Pred: the says the the has he was the and the state\n",
      "44 True: a # magnitude earthquake has shaken residents of a small community near oklahoma city\n",
      "44 Pred: the 's the # , been a of the # wildfire of the city\n",
      "45 True: the u.s. geological survey says the earthquake was recorded at about # a.m. wednesday in jones , about # miles northeast of oklahoma city in central oklahoma\n",
      "45 Pred: the # attorney service says the fire is closed in the # a.m. monday\n",
      "46 True: the temblor occurred at a depth of about three miles\n",
      "46 Pred: the say was the the gas of # # miles\n",
      "47 True: no injuries or damage was reported\n",
      "47 Pred: the one have been\n",
      "48 True: on tuesday , a # magnitude earthquake was <unk> about # miles south southeast of medford in northern oklahoma , one of several quakes recorded in the state during the day\n",
      "48 Pred: the the , , , year # , a to # miles\n",
      "49 True: geologists say earthquakes of magnitude # to # are generally the smallest that are felt by humans and damage is not likely in quakes below magnitude #\n",
      "49 Pred: the are the to the # and # to expected to state\n",
      "50 True: dutch bank rabobank says its men 's cycling team will be renamed blanco pro cycling team next year\n",
      "50 Pred: the says is is the # and the and , be able to and <unk> base in the\n",
      "51 True: the team is still looking for a sponsor\n",
      "51 Pred: the state says a to for the new\n",
      "52 True: rabobank announced this year it was ending its sponsorship of the men 's team because of lost trust in the sport after the u.s. anti doping agency report on lance armstrong\n",
      "52 Pred: the says the week that would a in # in the # who body\n",
      "53 True: on thursday , rabobank said in a statement if a new sponsor does not come forward , the team will fold at the end of #\n",
      "53 Pred: the monday , the was the a statement that the federal york will n't have to to but state will be the the city of the\n",
      "54 True: the team includes dutch rider robert <unk> and <unk> luis leon sanchez , who each won a stage in the # and # tours de france\n",
      "54 Pred: the department of a # and <unk> , the <unk>\n",
      "55 True: the charleston police department is using computer software to better identify crime hot spots and prevent crimes before they happen\n",
      "55 Pred: the says the say said investigating to to to help identify the\n",
      "56 True: the department said monday it is using <unk> analytics software from ibm in a pilot project to identify places where there is crime and to better allocate police resources\n",
      "56 Pred: the # says the that was the a to to to the to the new to to help the to the is no\n",
      "57 True: the computer system quickly <unk> past and present crime records and evaluates incident and arrest patterns throughout the city\n",
      "57 Pred: the state is is the the # the the the\n",
      "58 True: by <unk> the information , the department has a better idea of where crime is happening\n",
      "58 Pred: the the , , , the state of said a than to the the\n",
      "59 True: that allows the department to allocate officers to prevent crimes in those areas\n",
      "59 Pred: the 's the state to the the to the the\n",
      "60 True: police in atlanta are searching for a suspect known as the `` buckhead bandit `` believed to be responsible for a spree of armed robberies in the city 's buckhead neighborhood\n",
      "60 Pred: the say the , investigating a a man who as a man <unk> `` . and `` be a for a\n",
      "61 True: atlanta police say most of other robberies have taken place at drugstores and a sandwich shop on peachtree road in buckhead , a large neighborhood on atlanta 's north side\n",
      "61 Pred: the police say a # the men in been to the\n",
      "62 True: the atlanta journal constitution reports ( http : //bit.ly/ # <unk> ) that the suspect is also believed to have hit two businesses in the nearby brookhaven area\n",
      "62 Pred: the # journal constitution reports the http : //bit.ly/ # <unk> ) that the state 's expected held to be a to years\n",
      "-----------Topic Samples-----------\n",
      "0  bow: wednesday percent http school fire monday members federal attorney company\n",
      "0  sent: the # year old man has been charged with first degree murder in the death of a # year old boy\n",
      "1  bow: million york people reports federal thursday years http trade senate\n",
      "1  sent: the # year old man has been sentenced to # years in prison\n",
      "2  bow: million percent monday found based plant court years company tuesday\n",
      "2  sent: the # year old man has been charged with first degree murder in the death of a # year old boy\n",
      "3  bow: million home federal york kill http money dayton positive students\n",
      "3  sent: the # year old man has been sentenced to # years in prison\n",
      "4  bow: board ' bill house department home ? thursday http found\n",
      "4  sent: the # year old man has been sentenced to # years in prison\n",
      "5  bow: school thursday university mayor incumbent students north term million approved\n",
      "5  sent: the # year old man has been sentenced to # years in prison\n",
      "6  bow: http million tuesday reports court percent wednesday monday thursday years\n",
      "6  sent: the # year old man has been charged with first degree murder in the death of a # year old boy\n",
      "7  bow: percent thursday million home office u.s. reports neighborhood arrested helped\n",
      "7  sent: the # year old man has been charged with first degree murder in the death of a # year old boy\n",
      "8  bow: monday wednesday tuesday years found http reports week thursday office\n",
      "8  sent: the # year old man has been sentenced to # years in prison\n",
      "9  bow: http million tuesday officer years court billings u.s. health people\n",
      "9  sent: the # year old man has been sentenced to # years in prison\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-50e0e0534bec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_recon_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_kl_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_reg_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_recon_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_kl_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mppl_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mppls_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mloss_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppl_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarmup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbeta_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-7cabbf2ac912>\u001b[0m in \u001b[0;36mget_loss\u001b[0;34m(sess, batches)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mloss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppls_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_ppls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mppl_list\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mppls_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if len(logs) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "while epoch < config.epochs:\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = get_feed_dict(batch)\n",
    "\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, sent_loss_kl_categ_batch, sent_loss_kl_gmm_batch, ppls_batch = \\\n",
    "        sess.run([opt, loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, sent_loss_kl_categ, sent_loss_kl_gmm, topic_ppls], feed_dict = feed_dict)\n",
    "   \n",
    "        if sent_loss_kl_batch == np.inf:\n",
    "            print('Nan occured')\n",
    "            ckpt = tf.train.get_checkpoint_state(config.modeldir)\n",
    "            model_checkpoint_path = ckpt.all_model_checkpoint_paths[-1]\n",
    "            saver.restore(sess, model_checkpoint_path)            \n",
    "            break\n",
    "            \n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if ct%config.log_period==0:\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train, sent_loss_recon_train, sent_loss_kl_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_dev, sent_loss_dev, ppl_dev = get_loss(sess, dev_batches)\n",
    "\n",
    "            if config.warmup: beta_eval = beta.eval(session=sess)\n",
    "            global_step_log = sess.run(tf.train.get_global_step())            \n",
    "            \n",
    "#             if loss_dev < loss_min:\n",
    "#                 loss_min = loss_dev\n",
    "#                 saver.save(sess, config.modelpath, global_step=global_step_log)\n",
    "\n",
    "            clear_output()\n",
    "\n",
    "            time_finish = time.time()\n",
    "            time_log = int(time_finish - time_start)\n",
    "            logs += [(time_log, epoch, ct, loss_train, ppl_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train, sent_loss_recon_train, sent_loss_kl_train, loss_dev, ppl_dev, topic_loss_dev, sent_loss_dev, beta_eval)]\n",
    "            for log in logs:\n",
    "                print('%03d[s], Ep: %02d, Ct: %05d|TR LOSS: %.0f, PPL: %.0f|TM NLL: %.0f, KL: %.2f, REG:%.2f | LM NLL: %.2f, KL: %.2f|DE LOSS: %.0f, PPL: %.0f, TM: %.0f, LM: %.2f|BETA: %.6f' %  log)\n",
    "\n",
    "            print_sample(batch)\n",
    "\n",
    "            time_start = time.time()\n",
    "            \n",
    "            print_topic_sample()\n",
    "                \n",
    "    epoch += 1\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_prob_topic, _prob_topic_sents, _prob_topic_infer, _means_topic_infer = debug_value([prob_topic, prob_topic_sents, prob_topic_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_i = 4\n",
    "_prob_topic_sents[batch_i], _prob_topic_infer[batch_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_means_topic_infer[0][:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_means_topic, b_means_topic = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"topic/dec/mean_topic\")\n",
    "\n",
    "pred_topic_embeddings, pred_topic_bow, pred_means_topic, pred_logvars_topic, pred_token_idxs, _w_means_topic, _b_means_topic, _w_mean_topic_infer = \\\n",
    "                                sess.run([topic_embeddings, topic_bow, means_topic, logvars_topic, topic_beam_output_token_idxs, w_means_topic, b_means_topic, w_mean_topic_infer], \n",
    "                                         feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "\n",
    "pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "\n",
    "pred_topics_freq_bow_indices = np.argsort(pred_topic_bow, 1)[:, ::-1][:, :10]\n",
    "pred_topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idxs in pred_topics_freq_bow_idxs:\n",
    "    print([idx_to_word[idx] for idx in idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_topic_embeddings[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_topic_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_w_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_w_mean_topic_infer[:, :10, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_enc_state_infer, _means_topic_infer = debug_value([enc_state_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_enc_state_infer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_means_topic_infer[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
