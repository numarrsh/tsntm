{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from data_structure import get_batches, get_test_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "from topic_model import TopicModel\n",
    "\n",
    "from topic_beam_search_decoder import BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '3', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/bags/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/tglm_vae_tmp2', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'bags', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 50, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 500, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.1, 'lr')\n",
    "flags.DEFINE_float('reg', 1., 'regularization term')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_bool('warmup', True, 'flg of warming up')\n",
    "flags.DEFINE_integer('epochs_cycle', 5, 'number of epochs within a cycle')\n",
    "flags.DEFINE_float('r_cycle', 0.5, 'proportion used to increase beta within a cycle')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 50, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_bow', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_hidden_topic', 512, 'dim_hidden_topic')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "flags.DEFINE_bool('bidirectional', True, 'flg of bidirectional encoding')\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_test_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('PAD_IDX', word_to_idx[PAD], 'PAD_IDX')\n",
    "flags.DEFINE_integer('UNK_IDX', word_to_idx[UNK], 'UNK_IDX')\n",
    "flags.DEFINE_integer('BOS_IDX', word_to_idx[BOS], 'BOS_IDX')\n",
    "flags.DEFINE_integer('EOS_IDX', word_to_idx[EOS], 'EOS_IDX')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "maximum_iterations = max([max([instance.max_sent_l for instance in batch]) for ct, batch in dev_batches])\n",
    "flags.DEFINE_integer('maximum_iterations', maximum_iterations, 'maximum_iterations')\n",
    "\n",
    "flags.DEFINE_integer('cycle_steps', len(train_batches)*config.epochs_cycle, 'number of steps for each cycle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build language model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['bow'] = tf.placeholder(tf.float32, [None, config.dim_bow], name='bow')\n",
    "t_variables['input_token_idxs'] = tf.placeholder(tf.int32, [None, None], name='input_token_idxs')\n",
    "t_variables['dec_input_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_input_idxs')\n",
    "t_variables['dec_target_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_target_idxs')\n",
    "t_variables['batch_l'] = tf.placeholder(tf.int32, name='batch_l')\n",
    "t_variables['doc_l'] = tf.placeholder(tf.int32, [None], name='doc_l')\n",
    "t_variables['sent_l'] = tf.placeholder(tf.int32, [None], name='sent_l')\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train', assertion=False):\n",
    "    def token_dropout(sent_idxs):\n",
    "        sent_idxs_dropout = np.asarray(sent_idxs)\n",
    "        sent_idxs_dropout[np.random.rand(len(sent_idxs)) > config.word_keep_prob] = config.UNK_IDX\n",
    "        return list(sent_idxs_dropout)\n",
    "\n",
    "    bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "    \n",
    "    doc_l = np.array([len(instance.token_idxs) for instance in batch])\n",
    "    \n",
    "    feed_input_token_idxs_list = [sent_idxs for instance in batch for sent_idxs in instance.token_idxs]\n",
    "    feed_dec_input_idxs_list = [[config.BOS_IDX] + token_dropout(sent_idxs) for sent_idxs in feed_input_token_idxs_list]\n",
    "    feed_dec_target_idxs_list = [sent_idxs + [config.EOS_IDX]  for sent_idxs in feed_input_token_idxs_list]\n",
    "        \n",
    "    sent_l = np.array([len(sent_idxs) for sent_idxs in feed_input_token_idxs_list], np.int32)\n",
    "    batch_l = len(sent_l)\n",
    "    \n",
    "    feed_input_token_idxs = pad_sequences(feed_input_token_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_input_idxs = pad_sequences(feed_dec_input_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_target_idxs = pad_sequences(feed_dec_target_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    \n",
    "    if assertion:\n",
    "        index = 0\n",
    "        for instance in batch:\n",
    "            for line_idxs in instance.token_idxs:\n",
    "                assert feed_input_token_idxs_list[index] == line_idxs\n",
    "                index += 1\n",
    "        assert feed_input_token_idxs.shape[1] == np.max(sent_l)\n",
    "        assert feed_dec_input_idxs.shape[1] == np.max(sent_l) + 1\n",
    "        assert feed_dec_target_idxs.shape[1] == np.max(sent_l) + 1\n",
    "    \n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "\n",
    "    feed_dict = {\n",
    "                t_variables['bow']: bow, \n",
    "                t_variables['batch_l']: batch_l, t_variables['doc_l']: doc_l, t_variables['sent_l']: sent_l, \n",
    "                t_variables['input_token_idxs']: feed_input_token_idxs, t_variables['dec_input_idxs']: feed_dec_input_idxs, t_variables['dec_target_idxs']: feed_dec_target_idxs, \n",
    "                t_variables['keep_prob']: keep_prob\n",
    "    }\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     10,
     24,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables):\n",
    "    sample_batch = dev_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "def check_shape(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()\n",
    "    \n",
    "def check_value(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()    \n",
    "    \n",
    "# sent_loss_kl_categ_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, tf_log(prob_topic_infer/prob_topic_sents)), 1))\n",
    "# debug_value([sent_loss_kl_categ, sent_loss_kl_categ_tmp])\n",
    "# sent_loss_kl_gauss_tmp = 0.5 * tf.reduce_sum(tf.exp(logvars_topic_infer-logvars_topic) + tf.square(means_topic - means_topic_infer) / tf.exp(logvars_topic) - 1 + (logvars_topic - logvars_topic_infer), -1)\n",
    "# sent_loss_kl_gmm_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss_tmp), -1))\n",
    "# debug_value([sent_loss_kl_gmm_tmp, sent_loss_kl_gmm])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encode bow\n",
    "with tf.variable_scope('topic/enc', reuse=False):\n",
    "    hidden_bow_ = tf.layers.Dense(units=config.dim_hidden_bow, activation=tf.nn.relu, name='hidden_bow')(t_variables['bow'])\n",
    "    hidden_bow = tf.layers.Dropout(t_variables['keep_prob'])(hidden_bow_)\n",
    "    means_bow = tf.layers.Dense(units=config.dim_latent_bow, name='mean_bow')(hidden_bow)\n",
    "    logvars_bow = tf.layers.Dense(units=config.dim_latent_bow, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_bow)\n",
    "    latents_bow = sample_latents(means_bow, logvars_bow) # sample latent vectors\n",
    "\n",
    "    prob_topic = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic')(latents_bow) # inference of topic probabilities\n",
    "\n",
    "# decode bow\n",
    "with tf.variable_scope('shared', reuse=False):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "bow_embeddings = tf.nn.embedding_lookup(embeddings, bow_idxs) # embeddings of each bow features\n",
    "\n",
    "with tf.variable_scope('topic/dec', reuse=False):\n",
    "    topic_embeddings = tf.get_variable('topic_emb', [config.n_topic, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of topics\n",
    "\n",
    "    topic_bow = tf.nn.softmax(tf.matmul(topic_embeddings, bow_embeddings, transpose_b=True), 1) # bow vectors for each topic\n",
    "    logits_bow = tf_log(tf.matmul(prob_topic, topic_bow)) # predicted bow distribution\n",
    "\n",
    "    # prior of each gaussian distribution (computed for each topic)\n",
    "    hidden_topic = tf.layers.Dense(units=config.dim_hidden_topic, activation=tf.nn.relu, name='hidden_topic')(topic_bow)\n",
    "    means_topic = tf.layers.Dense(units=config.dim_latent, name='mean_topic')(hidden_topic)\n",
    "    logvars_topic = tf.layers.Dense(units=config.dim_latent, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_topic)\n",
    "    sigma_topic = tf.exp(0.5 * logvars_topic)\n",
    "    gauss_topic = tfd.Normal(loc=means_topic, scale=sigma_topic)    \n",
    "    \n",
    "# define losses\n",
    "topic_losses_recon = -tf.reduce_sum(tf.multiply(t_variables['bow'], logits_bow), 1)\n",
    "topic_loss_recon = tf.reduce_mean(topic_losses_recon) # negative log likelihood of each words\n",
    "\n",
    "topic_loss_kl = compute_kl_loss(means_bow, logvars_bow) # KL divergence b/w latent dist & gaussian std\n",
    "\n",
    "topic_bow_norm = topic_bow / tf.norm(topic_bow, axis=1, keepdims=True)\n",
    "topic_dots = tf.clip_by_value(tf.matmul(topic_bow_norm, tf.transpose(topic_bow_norm)), -1., 1.)\n",
    "topic_loss_reg = tf.reduce_mean(tf.square(topic_dots - tf.eye(config.n_topic)))\n",
    "# topic_angles = tf.acos(topic_dots)\n",
    "# topic_angles_mean = tf.reduce_mean(topic_angles)\n",
    "# topic_angles_vars = tf.reduce_mean(tf.square(topic_angles - topic_angles_mean))\n",
    "# topic_loss_reg = tf.exp(topic_angles_vars - topic_angles_mean)\n",
    "\n",
    "# monitor\n",
    "n_bow = tf.reduce_sum(t_variables['bow'], 1)\n",
    "topic_ppls = tf.divide(topic_losses_recon, tf.maximum(1e-5, n_bow))\n",
    "topics_freq_bow_indices = tf.nn.top_k(topic_bow, 10, name='topic_freq_bow').indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "input_token_idxs = t_variables['input_token_idxs']\n",
    "batch_l = t_variables['batch_l']\n",
    "sent_l = t_variables['sent_l']\n",
    "max_sent_l = tf.reduce_max(sent_l)\n",
    "\n",
    "with tf.variable_scope('sent/enc', reuse=False):\n",
    "    # get word embedding\n",
    "    enc_input = tf.nn.embedding_lookup(embeddings, input_token_idxs)\n",
    "\n",
    "    # get sentence embedding\n",
    "    _, enc_state = dynamic_bi_rnn(enc_input, sent_l, config.dim_hidden, t_variables['keep_prob'])\n",
    "\n",
    "    # TODO House Holder flow\n",
    "    hidden_topic_infer =  tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='hidden_topic_infer')(enc_state)\n",
    "    prob_topic_infer = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic_infer')(hidden_topic_infer)\n",
    "\n",
    "    w_mean_topic_infer = tf.get_variable('mean_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32)\n",
    "    b_mean_topic_infer = tf.get_variable('mean_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32)\n",
    "    means_topic_infer = tf.tensordot(enc_state, w_mean_topic_infer, axes=[[1], [1]]) + b_mean_topic_infer\n",
    "    \n",
    "    w_logvar_topic_infer = tf.get_variable('logvar_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    b_logvar_topic_infer = tf.get_variable('logvar_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    logvars_topic_infer = tf.tensordot(enc_state, w_logvar_topic_infer, axes=[[1], [1]]) + b_logvar_topic_infer\n",
    "    sigma_topic_infer = tf.exp(0.5 * logvars_topic_infer)\n",
    "    gauss_topic_infer = tfd.Normal(loc=means_topic_infer, scale=sigma_topic_infer)\n",
    "    \n",
    "    # latent vectors from each gaussian dist.\n",
    "    latents_topic_infer = sample_latents(means_topic_infer, logvars_topic_infer) \n",
    "    # latent vector from gaussian mixture\n",
    "    latents_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), latents_topic_infer, transpose_a=True)\n",
    "    \n",
    "    # for beam search\n",
    "    means_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), means_topic_infer, transpose_a=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for decoding\n",
    "dec_sent_l = tf.add(sent_l, 1)\n",
    "dec_input_idxs = t_variables['dec_input_idxs']\n",
    "dec_input = tf.nn.embedding_lookup(embeddings, dec_input_idxs)\n",
    "\n",
    "dec_latents_input = tf.tile(latents_input, [1, tf.shape(dec_input)[1], 1])\n",
    "dec_concat_input = tf.concat([dec_input, dec_latents_input], -1)\n",
    "\n",
    "# decode for training\n",
    "with tf.variable_scope('sent/dec/rnn', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32, reuse=False):\n",
    "    dec_cell = tf.contrib.rnn.GRUCell(config.dim_hidden)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob = t_variables['keep_prob'])\n",
    "\n",
    "    dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(latents_input, 1))\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_concat_input, sequence_length=dec_sent_l)\n",
    "\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=dec_cell,\n",
    "        helper=helper,\n",
    "        initial_state=dec_initial_state)\n",
    "\n",
    "    dec_outputs, _, output_sent_l = tf.contrib.seq2seq.dynamic_decode(train_decoder)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(config.n_vocab, use_bias=False, name='out')\n",
    "    output_logits = output_layer(dec_outputs.rnn_output)\n",
    "    \n",
    "    output_token_idxs = tf.argmax(output_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tokens = tf.fill([batch_l], config.BOS_IDX)\n",
    "end_token = config.EOS_IDX\n",
    "\n",
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    infer_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(means_input, 1))\n",
    "    beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(infer_dec_initial_state, multiplier=config.beam_width)\n",
    "    beam_latents_input = tf.contrib.seq2seq.tile_batch(tf.squeeze(means_input, 1), multiplier=config.beam_width) # added\n",
    "    \n",
    "    beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=beam_latents_input)\n",
    "\n",
    "    beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    beam_output_token_idxs = beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    inter_means_input = tf.placeholder(tf.float32, [None, config.dim_latent])\n",
    "    \n",
    "    inter_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(inter_means_input)\n",
    "    inter_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(inter_dec_initial_state, multiplier=config.beam_width)\n",
    "    inter_beam_latents_input = tf.contrib.seq2seq.tile_batch(inter_means_input, multiplier=config.beam_width) # added\n",
    "    \n",
    "    inter_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=inter_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=inter_beam_latents_input)\n",
    "\n",
    "    inter_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inter_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    inter_beam_output_token_idxs = inter_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    topic_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic)\n",
    "    topic_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(topic_dec_initial_state, multiplier=config.beam_width)\n",
    "    topic_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic, multiplier=config.beam_width) # added\n",
    "    \n",
    "    topic_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=topic_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=topic_beam_latents_input)\n",
    "\n",
    "    topic_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        topic_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    topic_beam_output_token_idxs = topic_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    means_topic_summary = tf.reduce_mean(means_topic_infer, 0)\n",
    "    \n",
    "    summary_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic_summary)\n",
    "    summary_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(summary_dec_initial_state, multiplier=config.beam_width)\n",
    "    summary_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic_summary, multiplier=config.beam_width) # added\n",
    "    \n",
    "    summary_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=summary_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width,\n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=summary_beam_latents_input)\n",
    "\n",
    "    summary_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        summary_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    summary_beam_output_token_idxs = summary_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language modeling cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target and mask\n",
    "dec_target_idxs = t_variables['dec_target_idxs']\n",
    "dec_mask_tokens = tf.sequence_mask(dec_sent_l, maxlen=max_sent_l+1, dtype=tf.float32)\n",
    "\n",
    "# nll for each token (averaged over batch & sentence)\n",
    "sent_loss_recon = tf.contrib.seq2seq.sequence_loss(output_logits, dec_target_idxs, dec_mask_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_l = t_variables['doc_l']\n",
    "mask_sents = tf.sequence_mask(doc_l)\n",
    "mask_sents_flatten = tf.reshape(mask_sents, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1]])\n",
    "\n",
    "prob_topic_tiled = tf.tile(tf.expand_dims(prob_topic, 1), [1, tf.shape(mask_sents)[1], 1])\n",
    "prob_topic_flatten = tf.reshape(prob_topic_tiled, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1], config.n_topic])\n",
    "prob_topic_sents = tf.boolean_mask(prob_topic_flatten, mask_sents_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferred mixture probabilities (computed for each sentence)\n",
    "categ_topic_infer = tfd.Categorical(probs=prob_topic_infer)\n",
    "\n",
    "# prior of mixture probabilities (computed for each document, tiled for each sentence)\n",
    "categ_topic = tfd.Categorical(probs=prob_topic_sents)\n",
    "\n",
    "sent_loss_kl_categ = tf.reduce_mean(tfd.kl_divergence(categ_topic_infer, categ_topic))\n",
    "\n",
    "# inference of each gaussian gaussribution (computed for each sentence)\n",
    "\n",
    "sent_loss_kl_gauss = tf.reduce_sum(tfd.kl_divergence(gauss_topic_infer, gauss_topic), -1)\n",
    "sent_loss_kl_gmm = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss), -1))\n",
    "\n",
    "sent_loss_kl = sent_loss_kl_categ + sent_loss_kl_gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "tau = tf.cast(tf.divide(tf.mod(global_step, tf.constant(config.cycle_steps)), tf.constant(config.cycle_steps)), dtype=tf.float32)\n",
    "beta = tf.minimum(1., tau/config.r_cycle)\n",
    "\n",
    "sent_loss = sent_loss_recon + beta * sent_loss_kl\n",
    "\n",
    "topic_loss = topic_loss_recon + topic_loss_kl + config.reg * topic_loss_reg\n",
    "loss = topic_loss + sent_loss\n",
    "\n",
    "# define optimizer\n",
    "if config.opt == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif config.opt == 'Adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "    \n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_sents(token_idxs, config, idx_to_word):\n",
    "    sents = []\n",
    "    for sent_idxs in token_idxs:\n",
    "        tokens = []\n",
    "        for idx in sent_idxs:\n",
    "            if idx == config.EOS_IDX: break\n",
    "            tokens.append(idx_to_word[idx])\n",
    "        sent = ' '.join(tokens)\n",
    "        sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, ppls_batch \\\n",
    "            = sess.run([loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, topic_ppls], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "    loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, sent_loss_recon_mean, sent_loss_kl_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    return loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, sent_loss_recon_mean, sent_loss_kl_mean, ppl_mean\n",
    "\n",
    "def get_all_losses(sess, batches):\n",
    "    losses = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch = \\\n",
    "        sess.run([loss, topic_loss_recon, topic_loss_kl, sent_loss_recon, sent_loss_kl], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "    print('LOSS %.2f | TM NLL: %.2f, KL: %.4f | LM NLL: %.2f, KL: %.4f' %  np.mean(losses, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_batch):\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    pred_token_idxs = sess.run(output_token_idxs, feed_dict = feed_dict)\n",
    "    true_token_idxs = [sent_idxs for instance in sample_batch for sent_idxs in instance.token_idxs]\n",
    "    \n",
    "    assert len(pred_token_idxs) == len(true_token_idxs)\n",
    "    \n",
    "    pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "    true_sents = idxs_to_sents(true_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    for i, (true_sent, pred_sent) in enumerate(zip(true_sents, pred_sents)):        \n",
    "        print(i, 'TRUE: %s' % true_sent)\n",
    "        print(i, 'PRED: %s' % pred_sent)\n",
    "\n",
    "def print_topic_sample():\n",
    "    pred_topics_freq_bow_indices, pred_topic_token_idxs = sess.run([topics_freq_bow_indices, topic_beam_output_token_idxs], \n",
    "                                                                                                           feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "    pred_topic_sents = idxs_to_sents(pred_topic_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Topic Samples-----------')\n",
    "    for i, (topic_freq_bow_idxs, pred_topic_sent) in enumerate(zip(topics_freq_bow_idxs, pred_topic_sents)):\n",
    "        print(i, ' BOW:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' SENTENCE:', pred_topic_sent)\n",
    "        \n",
    "def print_summary(test_batch):\n",
    "    feed_dict = get_feed_dict(test_batch)\n",
    "    feed_dict[t_variables['batch_l']] = config.n_topic\n",
    "    feed_dict[t_variables['keep_prob']] = 1.\n",
    "    pred_topics_freq_bow_indices, pred_summary_token_idxs = sess.run([topics_freq_bow_indices, summary_beam_output_token_idxs], feed_dict=feed_dict)\n",
    "    pred_summary_sents = idxs_to_sents(pred_summary_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Output sentences for each topic-----------')\n",
    "    print('Item idx:', test_batch[0].item_idx)\n",
    "    for i, (topic_freq_bow_idxs, pred_summary_sent) in enumerate(zip(topics_freq_bow_idxs, pred_summary_sents)):\n",
    "        print(i, ' BOW:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' SENTENCE:', pred_summary_sent)\n",
    "        \n",
    "    print('-----------Summaries-----------')\n",
    "    for i, summary in enumerate(test_batch[0].summaries):\n",
    "        print('SUMMARY %i :'%i, '\\n', summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "saver = tf.train.Saver(max_to_keep=10)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','TM','','','','LM','','VALID:','TM','','','','LM','', ''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL','REG','NLL','KL','LOSS','PPL','NLL','KL','REG','NLL','KL', 'Beta']]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>LM</th>\n",
       "      <th></th>\n",
       "      <th>VALID:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>LM</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>133.01</td>\n",
       "      <td>1035</td>\n",
       "      <td>122.48</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.98</td>\n",
       "      <td>9.13</td>\n",
       "      <td>0.80</td>\n",
       "      <td>126.55</td>\n",
       "      <td>1035</td>\n",
       "      <td>116.16</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.98</td>\n",
       "      <td>9.12</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>121.21</td>\n",
       "      <td>614</td>\n",
       "      <td>114.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.71</td>\n",
       "      <td>6.34</td>\n",
       "      <td>0.98</td>\n",
       "      <td>111.72</td>\n",
       "      <td>536</td>\n",
       "      <td>105.34</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.49</td>\n",
       "      <td>5.76</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>120.42</td>\n",
       "      <td>584</td>\n",
       "      <td>113.63</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.57</td>\n",
       "      <td>6.07</td>\n",
       "      <td>0.78</td>\n",
       "      <td>111.25</td>\n",
       "      <td>521</td>\n",
       "      <td>104.89</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.41</td>\n",
       "      <td>5.69</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>1500</td>\n",
       "      <td>119.95</td>\n",
       "      <td>570</td>\n",
       "      <td>113.25</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.51</td>\n",
       "      <td>5.96</td>\n",
       "      <td>0.66</td>\n",
       "      <td>111.21</td>\n",
       "      <td>521</td>\n",
       "      <td>104.78</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.33</td>\n",
       "      <td>5.62</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>119.55</td>\n",
       "      <td>560</td>\n",
       "      <td>112.89</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.46</td>\n",
       "      <td>5.88</td>\n",
       "      <td>0.59</td>\n",
       "      <td>110.76</td>\n",
       "      <td>513</td>\n",
       "      <td>104.53</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.28</td>\n",
       "      <td>5.48</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119.40</td>\n",
       "      <td>554</td>\n",
       "      <td>112.75</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.43</td>\n",
       "      <td>5.84</td>\n",
       "      <td>0.56</td>\n",
       "      <td>110.46</td>\n",
       "      <td>506</td>\n",
       "      <td>104.26</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.23</td>\n",
       "      <td>5.39</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2776</th>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>119.02</td>\n",
       "      <td>547</td>\n",
       "      <td>112.39</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.39</td>\n",
       "      <td>5.76</td>\n",
       "      <td>0.51</td>\n",
       "      <td>110.20</td>\n",
       "      <td>495</td>\n",
       "      <td>103.92</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.19</td>\n",
       "      <td>5.26</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3276</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>118.66</td>\n",
       "      <td>539</td>\n",
       "      <td>112.05</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.36</td>\n",
       "      <td>5.69</td>\n",
       "      <td>0.48</td>\n",
       "      <td>109.85</td>\n",
       "      <td>488</td>\n",
       "      <td>103.66</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.15</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3776</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>118.51</td>\n",
       "      <td>534</td>\n",
       "      <td>111.92</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.33</td>\n",
       "      <td>5.63</td>\n",
       "      <td>0.45</td>\n",
       "      <td>109.62</td>\n",
       "      <td>481</td>\n",
       "      <td>103.42</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.14</td>\n",
       "      <td>5.09</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4276</th>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>118.46</td>\n",
       "      <td>528</td>\n",
       "      <td>111.88</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.31</td>\n",
       "      <td>5.57</td>\n",
       "      <td>0.43</td>\n",
       "      <td>109.48</td>\n",
       "      <td>475</td>\n",
       "      <td>103.16</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.13</td>\n",
       "      <td>5.01</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4551</th>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>118.33</td>\n",
       "      <td>525</td>\n",
       "      <td>111.76</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.30</td>\n",
       "      <td>5.54</td>\n",
       "      <td>0.42</td>\n",
       "      <td>109.41</td>\n",
       "      <td>475</td>\n",
       "      <td>103.19</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.12</td>\n",
       "      <td>4.97</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5051</th>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>118.18</td>\n",
       "      <td>521</td>\n",
       "      <td>111.61</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.28</td>\n",
       "      <td>5.49</td>\n",
       "      <td>0.40</td>\n",
       "      <td>109.21</td>\n",
       "      <td>468</td>\n",
       "      <td>102.91</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.11</td>\n",
       "      <td>4.92</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5551</th>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>118.00</td>\n",
       "      <td>516</td>\n",
       "      <td>111.43</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.27</td>\n",
       "      <td>5.45</td>\n",
       "      <td>0.39</td>\n",
       "      <td>109.08</td>\n",
       "      <td>461</td>\n",
       "      <td>102.72</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.10</td>\n",
       "      <td>4.87</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6051</th>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>1500</td>\n",
       "      <td>117.82</td>\n",
       "      <td>512</td>\n",
       "      <td>111.24</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.25</td>\n",
       "      <td>5.41</td>\n",
       "      <td>0.38</td>\n",
       "      <td>109.19</td>\n",
       "      <td>462</td>\n",
       "      <td>102.81</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.09</td>\n",
       "      <td>4.82</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6551</th>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>117.75</td>\n",
       "      <td>508</td>\n",
       "      <td>111.16</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.24</td>\n",
       "      <td>5.37</td>\n",
       "      <td>0.38</td>\n",
       "      <td>108.88</td>\n",
       "      <td>457</td>\n",
       "      <td>102.56</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.79</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6826</th>\n",
       "      <td>44</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>117.67</td>\n",
       "      <td>506</td>\n",
       "      <td>111.08</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.23</td>\n",
       "      <td>5.35</td>\n",
       "      <td>0.37</td>\n",
       "      <td>108.87</td>\n",
       "      <td>459</td>\n",
       "      <td>102.60</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4.77</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7326</th>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>117.55</td>\n",
       "      <td>503</td>\n",
       "      <td>110.96</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.22</td>\n",
       "      <td>5.32</td>\n",
       "      <td>0.37</td>\n",
       "      <td>108.85</td>\n",
       "      <td>457</td>\n",
       "      <td>102.55</td>\n",
       "      <td>1.22</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.73</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7826</th>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>117.40</td>\n",
       "      <td>500</td>\n",
       "      <td>110.81</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.21</td>\n",
       "      <td>5.29</td>\n",
       "      <td>0.36</td>\n",
       "      <td>108.60</td>\n",
       "      <td>448</td>\n",
       "      <td>102.21</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4.70</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8326</th>\n",
       "      <td>78</td>\n",
       "      <td>3</td>\n",
       "      <td>1500</td>\n",
       "      <td>117.32</td>\n",
       "      <td>497</td>\n",
       "      <td>110.73</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.20</td>\n",
       "      <td>5.26</td>\n",
       "      <td>0.36</td>\n",
       "      <td>108.39</td>\n",
       "      <td>446</td>\n",
       "      <td>102.10</td>\n",
       "      <td>1.27</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4.68</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8826</th>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>117.24</td>\n",
       "      <td>494</td>\n",
       "      <td>110.65</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.19</td>\n",
       "      <td>5.23</td>\n",
       "      <td>0.35</td>\n",
       "      <td>108.60</td>\n",
       "      <td>449</td>\n",
       "      <td>102.31</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4.64</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9101</th>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>117.20</td>\n",
       "      <td>493</td>\n",
       "      <td>110.61</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.19</td>\n",
       "      <td>5.22</td>\n",
       "      <td>0.35</td>\n",
       "      <td>108.54</td>\n",
       "      <td>445</td>\n",
       "      <td>102.17</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4.62</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9601</th>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "      <td>117.14</td>\n",
       "      <td>490</td>\n",
       "      <td>110.55</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.18</td>\n",
       "      <td>5.19</td>\n",
       "      <td>0.35</td>\n",
       "      <td>108.46</td>\n",
       "      <td>443</td>\n",
       "      <td>102.10</td>\n",
       "      <td>1.42</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4.61</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10101</th>\n",
       "      <td>66</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>117.06</td>\n",
       "      <td>488</td>\n",
       "      <td>110.47</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.17</td>\n",
       "      <td>0.35</td>\n",
       "      <td>108.30</td>\n",
       "      <td>440</td>\n",
       "      <td>101.96</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10601</th>\n",
       "      <td>58</td>\n",
       "      <td>4</td>\n",
       "      <td>1500</td>\n",
       "      <td>117.00</td>\n",
       "      <td>486</td>\n",
       "      <td>110.40</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.14</td>\n",
       "      <td>0.34</td>\n",
       "      <td>108.36</td>\n",
       "      <td>442</td>\n",
       "      <td>102.03</td>\n",
       "      <td>1.42</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.55</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11101</th>\n",
       "      <td>81</td>\n",
       "      <td>4</td>\n",
       "      <td>2000</td>\n",
       "      <td>116.90</td>\n",
       "      <td>484</td>\n",
       "      <td>110.30</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.16</td>\n",
       "      <td>5.12</td>\n",
       "      <td>0.34</td>\n",
       "      <td>108.22</td>\n",
       "      <td>436</td>\n",
       "      <td>101.90</td>\n",
       "      <td>1.43</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.53</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11376</th>\n",
       "      <td>41</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>116.86</td>\n",
       "      <td>483</td>\n",
       "      <td>110.26</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.16</td>\n",
       "      <td>5.11</td>\n",
       "      <td>0.34</td>\n",
       "      <td>107.76</td>\n",
       "      <td>435</td>\n",
       "      <td>101.75</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.51</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11876</th>\n",
       "      <td>53</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>116.76</td>\n",
       "      <td>481</td>\n",
       "      <td>110.16</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.15</td>\n",
       "      <td>5.09</td>\n",
       "      <td>0.35</td>\n",
       "      <td>107.83</td>\n",
       "      <td>432</td>\n",
       "      <td>101.71</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.46</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12376</th>\n",
       "      <td>65</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>116.67</td>\n",
       "      <td>479</td>\n",
       "      <td>110.08</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.15</td>\n",
       "      <td>5.07</td>\n",
       "      <td>0.36</td>\n",
       "      <td>107.76</td>\n",
       "      <td>431</td>\n",
       "      <td>101.57</td>\n",
       "      <td>1.64</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.41</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12876</th>\n",
       "      <td>65</td>\n",
       "      <td>5</td>\n",
       "      <td>1500</td>\n",
       "      <td>116.61</td>\n",
       "      <td>477</td>\n",
       "      <td>110.03</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.14</td>\n",
       "      <td>5.05</td>\n",
       "      <td>0.37</td>\n",
       "      <td>107.68</td>\n",
       "      <td>423</td>\n",
       "      <td>101.44</td>\n",
       "      <td>1.66</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.40</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13376</th>\n",
       "      <td>53</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>116.55</td>\n",
       "      <td>475</td>\n",
       "      <td>109.97</td>\n",
       "      <td>1.21</td>\n",
       "      <td>0.14</td>\n",
       "      <td>5.03</td>\n",
       "      <td>0.38</td>\n",
       "      <td>107.89</td>\n",
       "      <td>431</td>\n",
       "      <td>101.67</td>\n",
       "      <td>1.63</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.36</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22476</th>\n",
       "      <td>56</td>\n",
       "      <td>9</td>\n",
       "      <td>2000</td>\n",
       "      <td>115.73</td>\n",
       "      <td>449</td>\n",
       "      <td>109.01</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.09</td>\n",
       "      <td>4.77</td>\n",
       "      <td>0.41</td>\n",
       "      <td>107.28</td>\n",
       "      <td>403</td>\n",
       "      <td>100.59</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.14</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22751</th>\n",
       "      <td>61</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>115.72</td>\n",
       "      <td>449</td>\n",
       "      <td>108.99</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.09</td>\n",
       "      <td>4.76</td>\n",
       "      <td>0.41</td>\n",
       "      <td>106.77</td>\n",
       "      <td>403</td>\n",
       "      <td>100.52</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.13</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23251</th>\n",
       "      <td>83</td>\n",
       "      <td>10</td>\n",
       "      <td>500</td>\n",
       "      <td>115.68</td>\n",
       "      <td>448</td>\n",
       "      <td>108.95</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.09</td>\n",
       "      <td>4.75</td>\n",
       "      <td>0.42</td>\n",
       "      <td>106.62</td>\n",
       "      <td>398</td>\n",
       "      <td>100.31</td>\n",
       "      <td>2.22</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23751</th>\n",
       "      <td>53</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>115.62</td>\n",
       "      <td>447</td>\n",
       "      <td>108.90</td>\n",
       "      <td>1.63</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.74</td>\n",
       "      <td>0.43</td>\n",
       "      <td>106.68</td>\n",
       "      <td>396</td>\n",
       "      <td>100.29</td>\n",
       "      <td>2.26</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.98</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24251</th>\n",
       "      <td>53</td>\n",
       "      <td>10</td>\n",
       "      <td>1500</td>\n",
       "      <td>115.57</td>\n",
       "      <td>446</td>\n",
       "      <td>108.85</td>\n",
       "      <td>1.64</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.72</td>\n",
       "      <td>0.44</td>\n",
       "      <td>106.69</td>\n",
       "      <td>393</td>\n",
       "      <td>100.24</td>\n",
       "      <td>2.26</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.98</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24751</th>\n",
       "      <td>53</td>\n",
       "      <td>10</td>\n",
       "      <td>2000</td>\n",
       "      <td>115.55</td>\n",
       "      <td>445</td>\n",
       "      <td>108.82</td>\n",
       "      <td>1.66</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.71</td>\n",
       "      <td>0.44</td>\n",
       "      <td>106.91</td>\n",
       "      <td>401</td>\n",
       "      <td>100.41</td>\n",
       "      <td>2.26</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25026</th>\n",
       "      <td>29</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>115.53</td>\n",
       "      <td>444</td>\n",
       "      <td>108.80</td>\n",
       "      <td>1.67</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.71</td>\n",
       "      <td>0.45</td>\n",
       "      <td>106.84</td>\n",
       "      <td>396</td>\n",
       "      <td>100.33</td>\n",
       "      <td>2.26</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.97</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25526</th>\n",
       "      <td>57</td>\n",
       "      <td>11</td>\n",
       "      <td>500</td>\n",
       "      <td>115.49</td>\n",
       "      <td>443</td>\n",
       "      <td>108.76</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.69</td>\n",
       "      <td>0.45</td>\n",
       "      <td>106.70</td>\n",
       "      <td>393</td>\n",
       "      <td>100.16</td>\n",
       "      <td>2.23</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26026</th>\n",
       "      <td>57</td>\n",
       "      <td>11</td>\n",
       "      <td>1000</td>\n",
       "      <td>115.44</td>\n",
       "      <td>442</td>\n",
       "      <td>108.71</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.68</td>\n",
       "      <td>0.45</td>\n",
       "      <td>107.05</td>\n",
       "      <td>400</td>\n",
       "      <td>100.42</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26526</th>\n",
       "      <td>58</td>\n",
       "      <td>11</td>\n",
       "      <td>1500</td>\n",
       "      <td>115.42</td>\n",
       "      <td>441</td>\n",
       "      <td>108.68</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.67</td>\n",
       "      <td>0.46</td>\n",
       "      <td>107.00</td>\n",
       "      <td>396</td>\n",
       "      <td>100.26</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27026</th>\n",
       "      <td>57</td>\n",
       "      <td>11</td>\n",
       "      <td>2000</td>\n",
       "      <td>115.41</td>\n",
       "      <td>440</td>\n",
       "      <td>108.66</td>\n",
       "      <td>1.73</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.66</td>\n",
       "      <td>0.46</td>\n",
       "      <td>107.08</td>\n",
       "      <td>398</td>\n",
       "      <td>100.37</td>\n",
       "      <td>2.28</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27301</th>\n",
       "      <td>32</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>115.39</td>\n",
       "      <td>440</td>\n",
       "      <td>108.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.66</td>\n",
       "      <td>0.46</td>\n",
       "      <td>107.07</td>\n",
       "      <td>398</td>\n",
       "      <td>100.41</td>\n",
       "      <td>2.19</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27801</th>\n",
       "      <td>58</td>\n",
       "      <td>12</td>\n",
       "      <td>500</td>\n",
       "      <td>115.37</td>\n",
       "      <td>439</td>\n",
       "      <td>108.61</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4.65</td>\n",
       "      <td>0.46</td>\n",
       "      <td>106.87</td>\n",
       "      <td>388</td>\n",
       "      <td>99.99</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.98</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28301</th>\n",
       "      <td>57</td>\n",
       "      <td>12</td>\n",
       "      <td>1000</td>\n",
       "      <td>115.33</td>\n",
       "      <td>438</td>\n",
       "      <td>108.57</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4.64</td>\n",
       "      <td>0.46</td>\n",
       "      <td>107.04</td>\n",
       "      <td>394</td>\n",
       "      <td>100.17</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.98</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28801</th>\n",
       "      <td>58</td>\n",
       "      <td>12</td>\n",
       "      <td>1500</td>\n",
       "      <td>115.30</td>\n",
       "      <td>437</td>\n",
       "      <td>108.53</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4.63</td>\n",
       "      <td>0.47</td>\n",
       "      <td>107.10</td>\n",
       "      <td>395</td>\n",
       "      <td>100.29</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.97</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29301</th>\n",
       "      <td>58</td>\n",
       "      <td>12</td>\n",
       "      <td>2000</td>\n",
       "      <td>115.28</td>\n",
       "      <td>436</td>\n",
       "      <td>108.51</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4.62</td>\n",
       "      <td>0.47</td>\n",
       "      <td>106.87</td>\n",
       "      <td>389</td>\n",
       "      <td>100.06</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.97</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29576</th>\n",
       "      <td>33</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>115.27</td>\n",
       "      <td>436</td>\n",
       "      <td>108.49</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4.61</td>\n",
       "      <td>0.47</td>\n",
       "      <td>106.99</td>\n",
       "      <td>393</td>\n",
       "      <td>100.19</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.96</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30076</th>\n",
       "      <td>55</td>\n",
       "      <td>13</td>\n",
       "      <td>500</td>\n",
       "      <td>115.25</td>\n",
       "      <td>435</td>\n",
       "      <td>108.46</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4.61</td>\n",
       "      <td>0.47</td>\n",
       "      <td>106.91</td>\n",
       "      <td>391</td>\n",
       "      <td>100.16</td>\n",
       "      <td>2.28</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30576</th>\n",
       "      <td>54</td>\n",
       "      <td>13</td>\n",
       "      <td>1000</td>\n",
       "      <td>115.22</td>\n",
       "      <td>434</td>\n",
       "      <td>108.43</td>\n",
       "      <td>1.82</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0.47</td>\n",
       "      <td>106.80</td>\n",
       "      <td>390</td>\n",
       "      <td>100.04</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31076</th>\n",
       "      <td>54</td>\n",
       "      <td>13</td>\n",
       "      <td>1500</td>\n",
       "      <td>115.19</td>\n",
       "      <td>434</td>\n",
       "      <td>108.40</td>\n",
       "      <td>1.83</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4.59</td>\n",
       "      <td>0.47</td>\n",
       "      <td>107.07</td>\n",
       "      <td>396</td>\n",
       "      <td>100.24</td>\n",
       "      <td>2.36</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31576</th>\n",
       "      <td>54</td>\n",
       "      <td>13</td>\n",
       "      <td>2000</td>\n",
       "      <td>115.18</td>\n",
       "      <td>433</td>\n",
       "      <td>108.38</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.47</td>\n",
       "      <td>106.93</td>\n",
       "      <td>394</td>\n",
       "      <td>100.18</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.91</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31851</th>\n",
       "      <td>30</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>115.17</td>\n",
       "      <td>433</td>\n",
       "      <td>108.36</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.47</td>\n",
       "      <td>106.70</td>\n",
       "      <td>386</td>\n",
       "      <td>99.95</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.91</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32351</th>\n",
       "      <td>54</td>\n",
       "      <td>14</td>\n",
       "      <td>500</td>\n",
       "      <td>115.14</td>\n",
       "      <td>432</td>\n",
       "      <td>108.34</td>\n",
       "      <td>1.86</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4.57</td>\n",
       "      <td>0.47</td>\n",
       "      <td>106.87</td>\n",
       "      <td>397</td>\n",
       "      <td>100.19</td>\n",
       "      <td>2.28</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.90</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32851</th>\n",
       "      <td>53</td>\n",
       "      <td>14</td>\n",
       "      <td>1000</td>\n",
       "      <td>115.12</td>\n",
       "      <td>431</td>\n",
       "      <td>108.31</td>\n",
       "      <td>1.87</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.56</td>\n",
       "      <td>0.47</td>\n",
       "      <td>106.88</td>\n",
       "      <td>393</td>\n",
       "      <td>100.16</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.89</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33351</th>\n",
       "      <td>53</td>\n",
       "      <td>14</td>\n",
       "      <td>1500</td>\n",
       "      <td>115.10</td>\n",
       "      <td>431</td>\n",
       "      <td>108.28</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.55</td>\n",
       "      <td>0.47</td>\n",
       "      <td>106.86</td>\n",
       "      <td>390</td>\n",
       "      <td>100.07</td>\n",
       "      <td>2.39</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.89</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33851</th>\n",
       "      <td>53</td>\n",
       "      <td>14</td>\n",
       "      <td>2000</td>\n",
       "      <td>115.08</td>\n",
       "      <td>430</td>\n",
       "      <td>108.26</td>\n",
       "      <td>1.89</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.54</td>\n",
       "      <td>0.47</td>\n",
       "      <td>106.73</td>\n",
       "      <td>387</td>\n",
       "      <td>99.96</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.89</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34126</th>\n",
       "      <td>41</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>115.07</td>\n",
       "      <td>430</td>\n",
       "      <td>108.25</td>\n",
       "      <td>1.89</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.54</td>\n",
       "      <td>0.47</td>\n",
       "      <td>106.27</td>\n",
       "      <td>389</td>\n",
       "      <td>100.05</td>\n",
       "      <td>2.35</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.86</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34626</th>\n",
       "      <td>54</td>\n",
       "      <td>15</td>\n",
       "      <td>500</td>\n",
       "      <td>115.04</td>\n",
       "      <td>429</td>\n",
       "      <td>108.22</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.53</td>\n",
       "      <td>0.48</td>\n",
       "      <td>106.29</td>\n",
       "      <td>391</td>\n",
       "      <td>100.12</td>\n",
       "      <td>2.36</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.71</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35126</th>\n",
       "      <td>53</td>\n",
       "      <td>15</td>\n",
       "      <td>1000</td>\n",
       "      <td>115.02</td>\n",
       "      <td>429</td>\n",
       "      <td>108.20</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.52</td>\n",
       "      <td>0.49</td>\n",
       "      <td>106.39</td>\n",
       "      <td>390</td>\n",
       "      <td>100.14</td>\n",
       "      <td>2.35</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35626</th>\n",
       "      <td>68</td>\n",
       "      <td>15</td>\n",
       "      <td>1500</td>\n",
       "      <td>114.98</td>\n",
       "      <td>428</td>\n",
       "      <td>108.17</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>106.23</td>\n",
       "      <td>386</td>\n",
       "      <td>99.90</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.71</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      TRAIN:    TM                        LM        VALID:  \\\n",
       "      Time  Ep    Ct    LOSS   PPL     NLL    KL   REG   NLL    KL    LOSS   \n",
       "1       17   0     0  133.01  1035  122.48  0.42  0.98  9.13  0.80  126.55   \n",
       "501     72   0   500  121.21   614  114.07  0.05  0.71  6.34  0.98  111.72   \n",
       "1001    71   0  1000  120.42   584  113.63  0.09  0.57  6.07  0.78  111.25   \n",
       "1501    71   0  1500  119.95   570  113.25  0.16  0.51  5.96  0.66  111.21   \n",
       "2001    81   0  2000  119.55   560  112.89  0.25  0.46  5.88  0.59  110.76   \n",
       "2276    44   1     0  119.40   554  112.75  0.29  0.43  5.84  0.56  110.46   \n",
       "2776    72   1   500  119.02   547  112.39  0.37  0.39  5.76  0.51  110.20   \n",
       "3276    70   1  1000  118.66   539  112.05  0.45  0.36  5.69  0.48  109.85   \n",
       "3776    70   1  1500  118.51   534  111.92  0.52  0.33  5.63  0.45  109.62   \n",
       "4276    72   1  2000  118.46   528  111.88  0.57  0.31  5.57  0.43  109.48   \n",
       "4551    45   2     0  118.33   525  111.76  0.61  0.30  5.54  0.42  109.41   \n",
       "5051    70   2   500  118.18   521  111.61  0.66  0.28  5.49  0.40  109.21   \n",
       "5551    71   2  1000  118.00   516  111.43  0.71  0.27  5.45  0.39  109.08   \n",
       "6051    59   2  1500  117.82   512  111.24  0.76  0.25  5.41  0.38  109.19   \n",
       "6551    72   2  2000  117.75   508  111.16  0.81  0.24  5.37  0.38  108.88   \n",
       "6826    44   3     0  117.67   506  111.08  0.83  0.23  5.35  0.37  108.87   \n",
       "7326    70   3   500  117.55   503  110.96  0.87  0.22  5.32  0.37  108.85   \n",
       "7826    71   3  1000  117.40   500  110.81  0.91  0.21  5.29  0.36  108.60   \n",
       "8326    78   3  1500  117.32   497  110.73  0.94  0.20  5.26  0.36  108.39   \n",
       "8826    55   3  2000  117.24   494  110.65  0.97  0.19  5.23  0.35  108.60   \n",
       "9101    30   4     0  117.20   493  110.61  0.99  0.19  5.22  0.35  108.54   \n",
       "9601    53   4   500  117.14   490  110.55  1.02  0.18  5.19  0.35  108.46   \n",
       "10101   66   4  1000  117.06   488  110.47  1.05  0.17  5.17  0.35  108.30   \n",
       "10601   58   4  1500  117.00   486  110.40  1.07  0.17  5.14  0.34  108.36   \n",
       "11101   81   4  2000  116.90   484  110.30  1.10  0.16  5.12  0.34  108.22   \n",
       "11376   41   5     0  116.86   483  110.26  1.11  0.16  5.11  0.34  107.76   \n",
       "11876   53   5   500  116.76   481  110.16  1.14  0.15  5.09  0.35  107.83   \n",
       "12376   65   5  1000  116.67   479  110.08  1.16  0.15  5.07  0.36  107.76   \n",
       "12876   65   5  1500  116.61   477  110.03  1.19  0.14  5.05  0.37  107.68   \n",
       "13376   53   5  2000  116.55   475  109.97  1.21  0.14  5.03  0.38  107.89   \n",
       "...    ...  ..   ...     ...   ...     ...   ...   ...   ...   ...     ...   \n",
       "22476   56   9  2000  115.73   449  109.01  1.59  0.09  4.77  0.41  107.28   \n",
       "22751   61  10     0  115.72   449  108.99  1.59  0.09  4.76  0.41  106.77   \n",
       "23251   83  10   500  115.68   448  108.95  1.61  0.09  4.75  0.42  106.62   \n",
       "23751   53  10  1000  115.62   447  108.90  1.63  0.08  4.74  0.43  106.68   \n",
       "24251   53  10  1500  115.57   446  108.85  1.64  0.08  4.72  0.44  106.69   \n",
       "24751   53  10  2000  115.55   445  108.82  1.66  0.08  4.71  0.44  106.91   \n",
       "25026   29  11     0  115.53   444  108.80  1.67  0.08  4.71  0.45  106.84   \n",
       "25526   57  11   500  115.49   443  108.76  1.68  0.08  4.69  0.45  106.70   \n",
       "26026   57  11  1000  115.44   442  108.71  1.70  0.08  4.68  0.45  107.05   \n",
       "26526   58  11  1500  115.42   441  108.68  1.71  0.08  4.67  0.46  107.00   \n",
       "27026   57  11  2000  115.41   440  108.66  1.73  0.08  4.66  0.46  107.08   \n",
       "27301   32  12     0  115.39   440  108.64  1.74  0.08  4.66  0.46  107.07   \n",
       "27801   58  12   500  115.37   439  108.61  1.75  0.07  4.65  0.46  106.87   \n",
       "28301   57  12  1000  115.33   438  108.57  1.76  0.07  4.64  0.46  107.04   \n",
       "28801   58  12  1500  115.30   437  108.53  1.78  0.07  4.63  0.47  107.10   \n",
       "29301   58  12  2000  115.28   436  108.51  1.79  0.07  4.62  0.47  106.87   \n",
       "29576   33  13     0  115.27   436  108.49  1.80  0.07  4.61  0.47  106.99   \n",
       "30076   55  13   500  115.25   435  108.46  1.81  0.07  4.61  0.47  106.91   \n",
       "30576   54  13  1000  115.22   434  108.43  1.82  0.07  4.60  0.47  106.80   \n",
       "31076   54  13  1500  115.19   434  108.40  1.83  0.07  4.59  0.47  107.07   \n",
       "31576   54  13  2000  115.18   433  108.38  1.84  0.07  4.58  0.47  106.93   \n",
       "31851   30  14     0  115.17   433  108.36  1.85  0.07  4.58  0.47  106.70   \n",
       "32351   54  14   500  115.14   432  108.34  1.86  0.07  4.57  0.47  106.87   \n",
       "32851   53  14  1000  115.12   431  108.31  1.87  0.06  4.56  0.47  106.88   \n",
       "33351   53  14  1500  115.10   431  108.28  1.88  0.06  4.55  0.47  106.86   \n",
       "33851   53  14  2000  115.08   430  108.26  1.89  0.06  4.54  0.47  106.73   \n",
       "34126   41  15     0  115.07   430  108.25  1.89  0.06  4.54  0.47  106.27   \n",
       "34626   54  15   500  115.04   429  108.22  1.90  0.06  4.53  0.48  106.29   \n",
       "35126   53  15  1000  115.02   429  108.20  1.91  0.06  4.52  0.49  106.39   \n",
       "35626   68  15  1500  114.98   428  108.17  1.92  0.06  4.51  0.49  106.23   \n",
       "\n",
       "         TM                        LM               \n",
       "        PPL     NLL    KL   REG   NLL    KL   Beta  \n",
       "1      1035  116.16  0.29  0.98  9.12  0.76  0.000  \n",
       "501     536  105.34  0.06  0.49  5.76  0.77  0.088  \n",
       "1001    521  104.89  0.18  0.41  5.69  0.47  0.176  \n",
       "1501    521  104.78  0.39  0.33  5.62  0.36  0.264  \n",
       "2001    513  104.53  0.38  0.28  5.48  0.25  0.352  \n",
       "2276    506  104.26  0.45  0.23  5.39  0.29  0.400  \n",
       "2776    495  103.92  0.68  0.19  5.26  0.28  0.488  \n",
       "3276    488  103.66  0.72  0.17  5.15  0.27  0.576  \n",
       "3776    481  103.42  0.78  0.14  5.09  0.27  0.664  \n",
       "4276    475  103.16  0.97  0.13  5.01  0.28  0.752  \n",
       "4551    475  103.19  0.92  0.12  4.97  0.26  0.800  \n",
       "5051    468  102.91  1.03  0.11  4.92  0.27  0.888  \n",
       "5551    461  102.72  1.14  0.10  4.87  0.27  0.976  \n",
       "6051    462  102.81  1.16  0.09  4.82  0.30  1.000  \n",
       "6551    457  102.56  1.17  0.08  4.79  0.28  1.000  \n",
       "6826    459  102.60  1.17  0.07  4.77  0.26  1.000  \n",
       "7326    457  102.55  1.22  0.06  4.73  0.29  1.000  \n",
       "7826    448  102.21  1.35  0.05  4.70  0.29  1.000  \n",
       "8326    446  102.10  1.27  0.05  4.68  0.29  1.000  \n",
       "8826    449  102.31  1.30  0.05  4.64  0.31  1.000  \n",
       "9101    445  102.17  1.41  0.05  4.62  0.30  1.000  \n",
       "9601    443  102.10  1.42  0.04  4.61  0.30  1.000  \n",
       "10101   440  101.96  1.40  0.04  4.58  0.31  1.000  \n",
       "10601   442  102.03  1.42  0.03  4.55  0.32  1.000  \n",
       "11101   436  101.90  1.43  0.03  4.53  0.33  1.000  \n",
       "11376   435  101.75  1.46  0.03  4.51  0.31  0.000  \n",
       "11876   432  101.71  1.58  0.03  4.46  0.61  0.088  \n",
       "12376   431  101.57  1.64  0.03  4.41  0.56  0.176  \n",
       "12876   423  101.44  1.66  0.03  4.40  0.58  0.264  \n",
       "13376   431  101.67  1.63  0.03  4.36  0.57  0.352  \n",
       "...     ...     ...   ...   ...   ...   ...    ...  \n",
       "22476   403  100.59  2.08  0.01  4.14  0.46  1.000  \n",
       "22751   403  100.52  2.10  0.01  4.13  0.45  0.000  \n",
       "23251   398  100.31  2.22  0.01  4.00  0.89  0.088  \n",
       "23751   396  100.29  2.26  0.01  3.98  0.78  0.176  \n",
       "24251   393  100.24  2.26  0.01  3.98  0.73  0.264  \n",
       "24751   401  100.41  2.26  0.01  3.99  0.68  0.352  \n",
       "25026   396  100.33  2.26  0.01  3.97  0.67  0.400  \n",
       "25526   393  100.16  2.23  0.01  3.99  0.65  0.488  \n",
       "26026   400  100.42  2.25  0.01  3.99  0.64  0.576  \n",
       "26526   396  100.26  2.33  0.01  3.99  0.60  0.664  \n",
       "27026   398  100.37  2.28  0.01  3.99  0.56  0.752  \n",
       "27301   398  100.41  2.19  0.01  3.99  0.59  0.800  \n",
       "27801   388   99.99  2.37  0.01  3.98  0.58  0.888  \n",
       "28301   394  100.17  2.34  0.01  3.98  0.56  0.976  \n",
       "28801   395  100.29  2.29  0.01  3.97  0.54  1.000  \n",
       "29301   389  100.06  2.33  0.01  3.97  0.51  1.000  \n",
       "29576   393  100.19  2.34  0.01  3.96  0.49  1.000  \n",
       "30076   391  100.16  2.28  0.01  3.95  0.50  1.000  \n",
       "30576   390  100.04  2.29  0.01  3.94  0.51  1.000  \n",
       "31076   396  100.24  2.36  0.01  3.95  0.51  1.000  \n",
       "31576   394  100.18  2.33  0.01  3.91  0.50  1.000  \n",
       "31851   386   99.95  2.33  0.01  3.91  0.49  1.000  \n",
       "32351   397  100.19  2.28  0.01  3.90  0.49  1.000  \n",
       "32851   393  100.16  2.32  0.01  3.89  0.50  1.000  \n",
       "33351   390  100.07  2.39  0.01  3.89  0.50  1.000  \n",
       "33851   387   99.96  2.40  0.01  3.89  0.48  1.000  \n",
       "34126   389  100.05  2.35  0.01  3.86  0.51  0.000  \n",
       "34626   391  100.12  2.36  0.01  3.71  1.00  0.088  \n",
       "35126   390  100.14  2.35  0.01  3.73  0.91  0.176  \n",
       "35626   386   99.90  2.40  0.01  3.71  0.78  0.264  \n",
       "\n",
       "[79 rows x 18 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Output sentences for each topic-----------\n",
      "Item idx: B000VB7EFW\n",
      "0  BOW: nice bit pretty put 'm works made feels feel big\n",
      "0  SENTENCE: the only problem is that it does n't fit the macbook pro , but it does n't fit\n",
      "1  BOW: protection sleeve perfectly bought protect soft snug inch chromebook perfect\n",
      "1  SENTENCE: the sleeve fits my macbook pro # & # # ; macbook pro perfectly and fits perfectly in the sleeve\n",
      "2  BOW: print saved surprise realized wont nicer personally website user ugly\n",
      "2  SENTENCE: the only thing i have is that it is a little bit more than the picture\n",
      "3  BOW: cover hard nice shell apple easily scratches logo easy clean\n",
      "3  SENTENCE: the keyboard cover does n't snap on the bottom of the keyboard cover , but the keyboard cover does n't stay on\n",
      "4  BOW: back work heavy carry pack bags school lot books straps\n",
      "4  SENTENCE: lots of compartments and compartments to be comfortable to wear\n",
      "5  BOW: plain neat nicer user personally fair hit surprise wont listed\n",
      "5  SENTENCE: the only thing i have is that it is a little bit more than a <unk>\n",
      "6  BOW: compartment main front bottle zippered pocket padded access area pouches\n",
      "6  SENTENCE: the main compartment is large enough to hold the power supply , and a couple of other items in the main compartment\n",
      "7  BOW: local searched user nicer surprise lose trust realized caught saved\n",
      "7  SENTENCE: however , it 's a little bit of the bag , but i would have given it\n",
      "8  BOW: ! cute super beautiful buy awesome colors perfectly recommend absolutely\n",
      "8  SENTENCE: love love ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "9  BOW: bought 'm buy made purchased recommend time ... find put\n",
      "9  SENTENCE: when i received the package i was disappointed\n",
      "10  BOW: cheaply wont realized caught hit fair nicer user neat personally\n",
      "10  SENTENCE: it 's not a problem , but i 'm not sure it will last\n",
      "11  BOW: ! loves absolutely amazing awesome daughter compliments fast beautiful shipping\n",
      "11  SENTENCE: love love ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "12  BOW: color mac perfectly easy recommend put cover feel pro protects\n",
      "12  SENTENCE: the keyboard cover fits perfectly , but the keyboard cover is a little darker than the keyboard cover\n",
      "13  BOW: sleeve protection neoprene inside sleeves foam snug tight padding thick\n",
      "13  SENTENCE: the sleeve fits my # . # & # # ; macbook pro # . # & # # ; macbook pro fits snugly with the sleeve inside the sleeve fits the macbook pro # . # & # #\n",
      "14  BOW: cover keyboard protector love keys screen typing type key ipearl\n",
      "14  SENTENCE: the keyboard cover is easy to snap on the keyboard cover , but the keyboard cover does n't snap on the keyboard cover\n",
      "15  BOW: canvas average waterproof nylon targus test stitching floor zippers single\n",
      "15  SENTENCE: the only thing i have is that it is a little bit of the bag , but it is not a problem\n",
      "16  BOW: zipper flap side close velcro open closed end zip pull\n",
      "16  SENTENCE: the only thing is that the top of the case is a little difficult to open the top of the bag , but the zipper does n't seem to be a little loose\n",
      "17  BOW: received item amazon return shipping company seller order ordered service\n",
      "17  SENTENCE: when i received it , i was disappointed to the company , but i did n't return it\n",
      "18  BOW: padding zipper inside handles zippers notebook padded logic velcro dell\n",
      "18  SENTENCE: the outside pocket is a little bit to hold the power cord , mouse , mouse , etc .\n",
      "19  BOW: perfect recommend made needed price excellent durable size stylish sturdy\n",
      "19  SENTENCE: i bought this bag to replace a gift for my laptop\n",
      "20  BOW: bottom top plastic part rubber corners feet speck back piece\n",
      "20  SENTENCE: however , the bottom piece of the case is not a little difficult to get the top of the top of the case , but i do n't have to remove it off\n",
      "21  BOW: camera plenty carry compartments room lots comfortable storage pockets books\n",
      "21  SENTENCE: plenty of room for my camera , camera , camera , camera , camera , and a few lenses , pens , pens , pens , etc .\n",
      "22  BOW: color picture pink blue purple green bright red ordered colors\n",
      "22  SENTENCE: the keyboard cover was a little darker than i expected , but the keyboard cover was not too\n",
      "23  BOW: gift ... purchased buy amazon school purchase christmas husband college\n",
      "23  SENTENCE: when i received it , i was disappointed to return it\n",
      "24  BOW: air protect protection smell scratches bit pro mbp job scratch\n",
      "24  SENTENCE: the case does not fit the macbook pro , but the bottom of the case is not a little bit of the top of the case\n",
      "25  BOW: material design - made inside zipper bit pretty fabric 're\n",
      "25  SENTENCE: the only thing i have is that it is a little difficult to get the top of the case\n",
      "26  BOW: ! absolutely awesome highly cute compliments love daughter beautiful amazing\n",
      "26  SENTENCE: love love ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "27  BOW: months broke weeks started month year years ago week bought\n",
      "27  SENTENCE: after two months of use , i started to return it , i had to return it back and the zipper broke off\n",
      "28  BOW: pro mac perfectly book air retina protects display hard soft\n",
      "28  SENTENCE: the keyboard cover fits my macbook pro # . # & # # ; retina display display display display display\n",
      "29  BOW: cheap return pay returning $ guess disappointed opened flimsy item\n",
      "29  SENTENCE: however , the bottom part of the case broke off after a month\n",
      "30  BOW: ; & laptops big pro hp description model tablet ordered\n",
      "30  SENTENCE: this sleeve fits my # . # & # # ; macbook pro # & # # ; # & # # ; macbook pro # & # # ; # & # # ; # & # # ;\n",
      "31  BOW: size work big sturdy made lot carrying small durable put\n",
      "31  SENTENCE: there are plenty of room for all the other things i need to carry\n",
      "32  BOW: cheaply wont realized saved ugly nicer personally surprise packaging fair\n",
      "32  SENTENCE: the only thing i have is that it is a little bit more than the picture\n",
      "33  BOW: strap handle shoulder straps zippers top : - back side\n",
      "33  SENTENCE: the only thing i have is that the shoulder strap is a little heavy , but i can not have a shoulder strap on the top side of the bag\n",
      "34  BOW: seat clothes gear airplane loaded shoulders plane bottle overhead roll\n",
      "34  SENTENCE: the only thing i have is that the shoulder strap is a little heavy\n",
      "35  BOW: hard feel pretty : cases problem plastic cheap - bit\n",
      "35  SENTENCE: the bottom part of the case does n't seem to snap on the bottom of the case\n",
      "36  BOW: 'm ... time 've buy $ money back thing ?\n",
      "36  SENTENCE: when i first received the first one i had to return it , i had to return it\n",
      "37  BOW: love recommend perfect highly ! absolutely awesome glad stylish compliments\n",
      "37  SENTENCE: love ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "38  BOW: power pocket cord mouse usb adapter drive dvd external supply\n",
      "38  SENTENCE: the front pocket has a pocket for the power cord , mouse , power cord , mouse , mouse , etc .\n",
      "39  BOW: sewn caught test closure user trust hit nicer pulls fair\n",
      "39  SENTENCE: however , it 's a little bit of the bag , but i do n't think it will last a long time\n",
      "40  BOW: room carry charger ipad accessories mouse perfect cords extra pocket\n",
      "40  SENTENCE: there has plenty of room for my laptop , plenty of room for my laptop , power cords , chargers , chargers , pens , pens , pens , and a few pens , and a small pocket for pens\n",
      "41  BOW: quality ... time made leather 'm high price buy find\n",
      "41  SENTENCE: when i received it back i received it back and i received it back\n",
      "42  BOW: pocket inside small side hard works put tablet front nicely\n",
      "42  SENTENCE: the front pocket is a little bit to hold the power cord , mouse , power cord , etc .\n",
      "43  BOW: nice inside handles made design bit material padding big zipper\n",
      "43  SENTENCE: the only downside is that it is a little bit of the side of the laptop , but it is not a tight fit\n",
      "44  BOW: price item reviews buy expected $ cases happy read job\n",
      "44  SENTENCE: when i received the package i was excited to return it\n",
      "45  BOW: happy buy recommend purchase ordered arrived received worth thought wanted\n",
      "45  SENTENCE: when i received the package i was excited to return it\n",
      "46  BOW: inch netbook ipad hp toshiba acer inches memory amazon purchased\n",
      "46  SENTENCE: the case fits my # . # inch laptop and has a pocket for the power cord and mouse\n",
      "47  BOW: pockets small large hold carry space pocket easily storage strap\n",
      "47  SENTENCE: plenty of room for my camera , pens , pens , pens , pens , pens , pens , pens , etc .\n",
      "48  BOW: apple white package packaging early scratched dirty wont scratches texture\n",
      "48  SENTENCE: the only thing i did n't like is that the keyboard cover is a little difficult to get off\n",
      "49  BOW: dimensions trust test stated listed odor realize function charge returned\n",
      "49  SENTENCE: the only thing i have is that it is a little bit of the material , but it is not a problem\n",
      "-----------Summaries-----------\n",
      "SUMMARY 0 : \n",
      " This is a very well made bag, nice construction, lots of pockets.\n",
      "the straps are very comfortable.\n",
      "and protects everything inside.\n",
      "It says\n",
      "it fits a 17inch notebook,\n",
      "however it did not.\n",
      "after using the pack for less than a month,\n",
      "it is ripping out already.\n",
      "SUMMARY 1 : \n",
      " This is a very well made bag, nice construction, lots of pockets.\n",
      "The quality is excellent\n",
      "and it is very durable.\n",
      "the straps are very comfortable.\n",
      "and protects everything inside.\n",
      "The color is a true red\n",
      "and it fits nicely.\n",
      "it is ripping out already.\n",
      "and it doesnt fit.\n",
      "It's just not the lightest backpack\n",
      "because some zipper teeth were not aligned.\n",
      "SUMMARY 2 : \n",
      " The quality is excellent\n",
      "and I love all the pockets and compartments.\n",
      "and it is very durable.\n",
      "and can't beleive the price\n",
      "and protects everything inside.\n",
      "The laptop\n",
      "doesn't fit in it.\n",
      "it is ripping out already.\n",
      "It's just not the lightest backpack\n",
      "0 TRUE: having used many different kinds of labels over the years i know that most labels do n't stick to fabrics\n",
      "0 PRED: after a a other bags of the , the first of the that i was to n't have the the\n",
      "1 TRUE: most stick on name tags do n't do well unless they 're <unk> to a simple cotton t-shirt or something similar\n",
      "1 PRED: but of , the , , you , , , , are to\n",
      "2 TRUE: i tried these on many different fabrics - denim , cable knit sweaters of varying <unk> , polyester , <unk> , cotton , even suede , leather and water resistant jackets\n",
      "2 PRED: i have to to the bag bags , the , and , , , , <unk> , , , etc , etc , etc <unk> <unk> etc , <unk> <unk>\n",
      "3 TRUE: i tried bending the fabrics so the corners would start to come off\n",
      "3 PRED: i have to the the the the bottom on be off be off\n",
      "4 TRUE: although the corners may lift a bit , the rest of the tag holds tight and remained that way for over # hours when i eventually removed them\n",
      "4 PRED: the only case of be the little of the the the the bag , the and the the the to the the month\n",
      "5 TRUE: removal is fine - no damage to the fabric even on suede and leather -lrb- the manufacturer <unk> against using the tags on these fabrics but i did n't run into any problems -rrb-\n",
      "5 PRED: the , the , the problem , the zippers , the the , the , not <unk> of -rrb- the the bag to the -rrb- -rrb- the 've n't\n",
      "6 TRUE: i did n't run into any issues printing on the labels\n",
      "6 PRED: i do n't have the the of or the the corners\n",
      "7 TRUE: i was able to print edge to edge on the labels using the <unk> on the avery # template\n",
      "7 PRED: i have worried to be the of the the the bottom\n",
      "8 TRUE: sometimes when using ` compatible ' templates i have to work a little inside the <unk> of the template to get all the text to fit , but this one works fine as is\n",
      "8 PRED: one , i it , , , , , a take the laptop , , case , the case , the it the way side\n",
      "9 TRUE: i highly recommend these name badges\n",
      "9 PRED: i bought this this\n",
      "10 TRUE: this is a great bag if you are just trying to carry your gear around\n",
      "10 PRED: this is a great bag and i i very comfortable to carry\n",
      "11 TRUE: nothing fancy and no easy access type zippers to whip out your camera quickly , but it is roomy\n",
      "11 PRED: the only is the strap to to the and keep\n",
      "12 TRUE: i can pack a lot in it\n",
      "12 PRED: i have carry everything a\n",
      "13 TRUE: i usually use it when i am traveling\n",
      "13 PRED: i i buy it for i travel traveling\n",
      "14 TRUE: it works out well\n",
      "14 PRED: it is well\n",
      "15 TRUE: the reason why i bought it is because i needed a nice upgrade from the regular canon shoulder bag that came with my camera\n",
      "15 PRED: the bag i i bought this is a i carry a bag bag to work airport bag bag\n",
      "16 TRUE: i wanted something that i could fit all of my equipment in ... and more\n",
      "16 PRED: i bought a to and i be my of my books and my and i of for\n",
      "17 TRUE: well-padded , simple protection for a # `` laptop , like my apple macbook pro\n",
      "17 PRED: fits , fits protection for my macbook . macbook\n",
      "18 TRUE: this case does n't have pockets ... just a protected environment for your laptop\n",
      "18 PRED: this is is n't fit a\n",
      "19 TRUE: i slip my mbp into this sleeve before putting it into my backpack\n",
      "19 PRED: i have my laptop in this case and and\n",
      "20 TRUE: also convenient for toting my laptop to the <unk> coffee shop if i do n't need my charger or anything else\n",
      "20 PRED: the , , the your laptop in be bag , , , the , n't want to laptop\n",
      "21 TRUE: simple , functional , protective , and sleek\n",
      "21 PRED: great , , , and , and\n",
      "22 TRUE: very nice product\n",
      "22 PRED: it nice and\n",
      "23 TRUE: for a very simplistic case , this is what you get\n",
      "23 PRED: great , good , , , but is is\n",
      "24 TRUE: very cheap and gets the job done carrying card\n",
      "24 PRED: but good , not a job\n",
      "25 TRUE: the static free slots seem a bit flimsy and might rip during times of stressful shooting and needing to swap out cards quickly\n",
      "25 PRED: the only thing i are the little of , i not to a long time and and the to be\n",
      "26 TRUE: not water proof\n",
      "26 PRED: very , ,\n",
      "27 TRUE: the string <unk> on my were a little bit undone upon receiving in the mail . . and i would have contacted the seller , but i doubt they would do something because its such a cheap item\n",
      "27 PRED: the only was i the and a , bit of than than the the picture , but i i i not been the same for , i was it are a n't better i a a good product\n",
      "28 TRUE: the seller on the other hand was fast with shipping . # stars for seller , # star for product\n",
      "28 PRED: the seller was the same # was a and the # # #\n",
      "29 TRUE: it 's a great sleeve that will protect the newest mbp late # model from falls\n",
      "29 PRED: it is a a case for the the the computer # from #\n",
      "30 TRUE: i know this cause like a <unk> i dropped mine on the floor in my accounting class at my university , that was about three months ago , and there is nothing wrong with my mbp\n",
      "30 PRED: i 've a one the a one reviews have my my , time of the # and and the <unk>\n",
      "31 TRUE: also it 's reasonable price , and it comes in cool colors\n",
      "31 PRED: it , is a , and and it love with color\n",
      "32 TRUE: but it is just a tad bit too big for the current gen mbp , not sure about the <unk> gen\n",
      "32 PRED: but if is a to little of , much to the top side ,\n",
      "33 TRUE: defiantly worth your money if your looking for a great laptop sleeve that can take a beating\n",
      "33 PRED: if , it money , it is for a case case case for will fit your laptop\n",
      "34 TRUE: bought this case to use when i travel\n",
      "34 PRED: i this for my for for and needed\n",
      "35 TRUE: it is perfect at the airport while i am waiting for my flight\n",
      "35 PRED: it is very and all and i i travel traveling\n",
      "36 TRUE: very <unk> and stylish\n",
      "36 PRED: very a for this\n",
      "37 TRUE: ca n't beat the price too !\n",
      "37 PRED: overall n't beat this price\n",
      "38 TRUE: see this item at department stores for almost triple the price\n",
      "38 PRED: bought the one for the for # a # # price\n",
      "39 TRUE: i have a # . # laptop and wanted a case for it\n",
      "39 PRED: i bought a # . # inch and it it case case my\n",
      "40 TRUE: this case does the job , it is a little bigger then the laptop but it works out to not be much of a problem\n",
      "40 PRED: it case is n't job of but is not little bit than i laptop is\n",
      "41 TRUE: this case is very basic and really only good for preventing scratches on the outside casing of your laptop\n",
      "41 PRED: the case is a and and the like a for\n",
      "42 TRUE: in saying that however , it is thick enough to prevent any scratching that may occur from having your computer in a backpack which is what i wanted it for\n",
      "42 PRED: but fact , i , it is a , to protect my laptop my i be\n",
      "43 TRUE: the metal latch on the strap that connects to the bag broke after a month\n",
      "43 PRED: the only thing is the back broke is to the bag is off a month\n",
      "44 TRUE: there is also a piece of sharp metal that sticks out of the latch that has cut my arm several times\n",
      "44 PRED: i do the the little of the side on the on to the case\n",
      "45 TRUE: when i bump the bag against something and my laptop is in it there is not protection for it in the bag\n",
      "45 PRED: so i put the bag , my , would laptop is protected the\n",
      "46 TRUE: would not recommend\n",
      "46 PRED: i recommend recommend\n",
      "47 TRUE: this bag is nice\n",
      "47 PRED: this bag is perfect\n",
      "48 TRUE: i can fit my # & # # ; laptop and power cord in it , no problem\n",
      "48 PRED: i have fit my # & # # # # and and cord and there\n",
      "49 TRUE: i like all the compartments to keep any paperwork and extras organized\n",
      "49 PRED: i can carry to laptop and carry my of and other\n",
      "50 TRUE: definitely happy with this purchase\n",
      "50 PRED: it recommend with the\n",
      "51 TRUE: i ended up returning this backpack because it is n't near the quality i was anticipating\n",
      "51 PRED: i 've up returning it bag for i was not a the same of\n",
      "52 TRUE: it has a lot of storage space and compartments , but looks like it was assembled at the same factory that does all the walmart backpacks\n",
      "52 PRED: it has a lot of pockets compartments and it to be i very it will i\n",
      "53 TRUE: the straps are hard , and do n't feel like they would be comfortable for any long period of time\n",
      "53 PRED: the only is very and and i n't have like i are to a to me\n",
      "54 TRUE: i spent $ # more on a ogio backpack , and i am very happy\n",
      "54 PRED: i bought a bag . # my year trip and and i am very happy with\n",
      "55 TRUE: make sure if you order this you do not have a retna mb\n",
      "55 PRED: but sure you you can to one will n't have it\n",
      "56 TRUE: this fits like a glove and the is no room for error\n",
      "56 PRED: this fits my a glove # a # perfect protection for the\n",
      "57 TRUE: only con i have is that you have to guess on the top which way it fit\n",
      "57 PRED: but thing it have that that the can a remove the the case of the to\n",
      "58 TRUE: poorly built , flimsy , cheap , bulky , low grade plastics and <unk> , not dust or water proof at all\n",
      "58 PRED: however , , but , and , but , and , ,\n",
      "59 TRUE: thoroughly disappointed in a company i thought <unk> themselves on quality\n",
      "59 PRED: after <unk> , the replacement and was a the\n",
      "60 TRUE: would n't recommend to anyone\n",
      "60 PRED: i definitely recommend this anyone\n",
      "61 TRUE: snaps on snug ; hard plastic wo n't cushion from drops , but will prevent nicks and scratches if transported around\n",
      "61 PRED: the on easily , not to , and fit the scratches , and it not your\n",
      "62 TRUE: bought for my high school son 's chromebook that he takes to school with him every day\n",
      "62 PRED: bought this my husband school and and very and i loves a school and this\n",
      "63 TRUE: so far it 's lasted over two months without anything breaking on it\n",
      "63 PRED: but i , i n't to the i i i i\n",
      "64 TRUE: i found the roocase neoprene sleeve case to be quite adequate but left room for more needs of additional space\n",
      "64 PRED: i like the case pocket for for for my a for for for for for my than\n",
      "65 TRUE: this is particularly <unk> if one has a couple or more accessories to load\n",
      "65 PRED: it is a second bag you had a for pockets shoulder compartments\n"
     ]
    }
   ],
   "source": [
    "if len(log_df) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "while epoch < config.epochs:\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = get_feed_dict(batch)\n",
    "\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, sent_loss_kl_categ_batch, sent_loss_kl_gmm_batch, ppls_batch = \\\n",
    "        sess.run([opt, loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, sent_loss_kl_categ, sent_loss_kl_gmm, topic_ppls], feed_dict = feed_dict)\n",
    "   \n",
    "        if sent_loss_kl_batch == np.inf:\n",
    "            print('Nan occured')\n",
    "            ckpt = tf.train.get_checkpoint_state(config.modeldir)\n",
    "            model_checkpoint_path = ckpt.all_model_checkpoint_paths[-1]\n",
    "            saver.restore(sess, model_checkpoint_path)            \n",
    "            break\n",
    "            \n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if ct%config.log_period==0:\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train, sent_loss_recon_train, sent_loss_kl_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, sent_loss_recon_dev, sent_loss_kl_dev, ppl_dev = get_loss(sess, dev_batches)\n",
    "            global_step_log, beta_eval = sess.run([tf.train.get_global_step(), beta])\n",
    "            \n",
    "            if loss_dev < loss_min:\n",
    "                loss_min = loss_dev\n",
    "                saver.save(sess, config.modelpath, global_step=global_step_log)\n",
    "\n",
    "            clear_output()\n",
    "    \n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_reg_train, '%.2f'%sent_loss_recon_train, '%.2f'%sent_loss_kl_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_reg_dev, '%.2f'%sent_loss_recon_dev, '%.2f'%sent_loss_kl_dev,  '%.3f'%beta_eval],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "\n",
    "            print_summary(test_batches[1][1])\n",
    "            print_sample(batch)\n",
    "            \n",
    "            time_start = time.time()\n",
    "            \n",
    "    epoch += 1\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_prob_topic, _prob_topic_sents, _prob_topic_infer, _means_topic_infer = debug_value([prob_topic, prob_topic_sents, prob_topic_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_i = 4\n",
    "_prob_topic_sents[batch_i], _prob_topic_infer[batch_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_means_topic_infer[0][:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_means_topic, b_means_topic = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"topic/dec/mean_topic\")\n",
    "\n",
    "pred_topic_embeddings, pred_topic_bow, pred_means_topic, pred_logvars_topic, pred_token_idxs, _w_means_topic, _b_means_topic, _w_mean_topic_infer = \\\n",
    "                                sess.run([topic_embeddings, topic_bow, means_topic, logvars_topic, topic_beam_output_token_idxs, w_means_topic, b_means_topic, w_mean_topic_infer], \n",
    "                                         feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "\n",
    "pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "\n",
    "pred_topics_freq_bow_indices = np.argsort(pred_topic_bow, 1)[:, ::-1][:, :10]\n",
    "pred_topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idxs in pred_topics_freq_bow_idxs:\n",
    "    print([idx_to_word[idx] for idx in idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_topic_embeddings[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_topic_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_w_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_w_mean_topic_infer[:, :10, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_enc_state_infer, _means_topic_infer = debug_value([enc_state_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_enc_state_infer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_means_topic_infer[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
