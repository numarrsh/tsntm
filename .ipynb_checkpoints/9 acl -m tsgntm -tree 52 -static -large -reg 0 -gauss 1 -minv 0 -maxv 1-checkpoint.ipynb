{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from data_structure import get_batches\n",
    "from hntm import HierarchicalNeuralTopicModel\n",
    "from nhdp import nestedHierarchicalNeuralTopicModel\n",
    "from tsgntm import TreeStructuredGaussianNeuralTopicModel\n",
    "from tree import get_descendant_idxs\n",
    "from evaluation import get_hierarchical_affinity, get_topic_specialization, print_topic_sample\n",
    "from configure import get_config\n",
    "\n",
    "pd.set_option('display.max_columns', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(nb_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu\n",
    "np.random.seed(config.seed)\n",
    "random.seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.path_data,'rb'))\n",
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)\n",
    "config.dim_bow = len(bow_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     6
    ]
   },
   "outputs": [],
   "source": [
    "def debug(variable, sample_batch=None):\n",
    "    if sample_batch is None: sample_batch = test_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch, mode='test')\n",
    "    _variable = sess.run(variable, feed_dict=feed_dict)\n",
    "    return _variable\n",
    "\n",
    "def check(variable):\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sample_batch = test_batches[0]\n",
    "    feed_dict = model.get_feed_dict(sample_batch, mode='test', assertion=True)\n",
    "    _variable = sess.run(variable, feed_dict=feed_dict)\n",
    "    return _variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "TopicModels = {'hntm': HierarchicalNeuralTopicModel, 'nhdp': nestedHierarchicalNeuralTopicModel, 'tsgntm': TreeStructuredGaussianNeuralTopicModel}\n",
    "TopicModel = TopicModels[config.model]\n",
    "model = TopicModel(config)\n",
    "sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(max_to_keep=config.max_to_keep)\n",
    "update_tree_flg = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = []\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "ppl_min = np.inf\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','','','','','','VALID:','','','','','','TEST:','', 'SPEC:', '', '', 'HIER:', ''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL', 'GAUSS', 'REG','LOSS','PPL','NLL','KL', 'GAUSS','REG','LOSS','PPL', '1', '2', '3', 'CHILD', 'OTHER']]))))\n",
    "\n",
    "cmd_rm = 'rm -r %s' % config.dir_model\n",
    "res = subprocess.call(cmd_rm.split())\n",
    "cmd_mk = 'mkdir %s' % config.dir_model\n",
    "res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "def update_checkpoint(config, checkpoint, global_step):\n",
    "    checkpoint.append(config.path_model + '-%i' % global_step)\n",
    "    if len(checkpoint) > config.max_to_keep:\n",
    "        path_model = checkpoint.pop(0) + '.*'\n",
    "        for p in glob.glob(path_model):\n",
    "            os.remove(p)\n",
    "    cPickle.dump(checkpoint, open(config.path_checkpoint, 'wb'))\n",
    "    \n",
    "def validate(sess, batches, model):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    prob_topic_list = []\n",
    "    n_bow_list = []\n",
    "    n_topics_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = model.get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch, ppls_batch, prob_topic_batch, n_bow_batch, n_topics_batch \\\n",
    "            = sess.run([model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_gauss, model.topic_loss_reg, model.topic_ppls, model.prob_topic, model.n_bow, model.n_topics], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "        prob_topic_list.append(prob_topic_batch)\n",
    "        n_bow_list.append(n_bow_batch)\n",
    "        n_topics_list.append(n_topics_batch)\n",
    "    loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_gauss_mean, topic_loss_reg_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    \n",
    "    probs_topic = np.concatenate(prob_topic_list, 0)\n",
    "    \n",
    "    n_bow = np.concatenate(n_bow_list, 0)\n",
    "    n_topics = np.concatenate(n_topics_list, 0)\n",
    "    probs_topic_mean = np.sum(n_topics, 0) / np.sum(n_bow)\n",
    "    \n",
    "    return loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_gauss_mean, topic_loss_reg_mean, ppl_mean, probs_topic_mean    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train & validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th colspan=\"5\" halign=\"left\"></th>\n",
       "      <th>VALID:</th>\n",
       "      <th colspan=\"5\" halign=\"left\"></th>\n",
       "      <th>TEST:</th>\n",
       "      <th></th>\n",
       "      <th>SPEC:</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "      <th>HIER:</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>GAUSS</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>GAUSS</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>CHILD</th>\n",
       "      <th>OTHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>77</td>\n",
       "      <td>8</td>\n",
       "      <td>463</td>\n",
       "      <td>9266.38</td>\n",
       "      <td>1971</td>\n",
       "      <td>9219.21</td>\n",
       "      <td>16.53</td>\n",
       "      <td>30.64</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9240.28</td>\n",
       "      <td>1861</td>\n",
       "      <td>9182.74</td>\n",
       "      <td>20.37</td>\n",
       "      <td>37.17</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9238.98</td>\n",
       "      <td>1858</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>72</td>\n",
       "      <td>17</td>\n",
       "      <td>360</td>\n",
       "      <td>9226.75</td>\n",
       "      <td>1894</td>\n",
       "      <td>9169.75</td>\n",
       "      <td>18.57</td>\n",
       "      <td>38.43</td>\n",
       "      <td>0.13</td>\n",
       "      <td>9236.92</td>\n",
       "      <td>1830</td>\n",
       "      <td>9162.81</td>\n",
       "      <td>20.43</td>\n",
       "      <td>53.67</td>\n",
       "      <td>0.13</td>\n",
       "      <td>9238.99</td>\n",
       "      <td>1834</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>72</td>\n",
       "      <td>26</td>\n",
       "      <td>257</td>\n",
       "      <td>9214.31</td>\n",
       "      <td>1864</td>\n",
       "      <td>9149.26</td>\n",
       "      <td>19.29</td>\n",
       "      <td>45.73</td>\n",
       "      <td>0.13</td>\n",
       "      <td>9242.35</td>\n",
       "      <td>1823</td>\n",
       "      <td>9156.83</td>\n",
       "      <td>20.77</td>\n",
       "      <td>64.76</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9242.00</td>\n",
       "      <td>1823</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>74</td>\n",
       "      <td>35</td>\n",
       "      <td>154</td>\n",
       "      <td>9207.12</td>\n",
       "      <td>1845</td>\n",
       "      <td>9136.51</td>\n",
       "      <td>19.76</td>\n",
       "      <td>50.80</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9233.41</td>\n",
       "      <td>1808</td>\n",
       "      <td>9145.03</td>\n",
       "      <td>21.14</td>\n",
       "      <td>67.24</td>\n",
       "      <td>0.15</td>\n",
       "      <td>9233.44</td>\n",
       "      <td>1807</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>73</td>\n",
       "      <td>44</td>\n",
       "      <td>51</td>\n",
       "      <td>9202.10</td>\n",
       "      <td>1832</td>\n",
       "      <td>9127.21</td>\n",
       "      <td>20.09</td>\n",
       "      <td>54.80</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9236.87</td>\n",
       "      <td>1803</td>\n",
       "      <td>9142.52</td>\n",
       "      <td>21.35</td>\n",
       "      <td>73.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9239.36</td>\n",
       "      <td>1806</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>72</td>\n",
       "      <td>52</td>\n",
       "      <td>515</td>\n",
       "      <td>9198.77</td>\n",
       "      <td>1822</td>\n",
       "      <td>9120.34</td>\n",
       "      <td>20.32</td>\n",
       "      <td>58.10</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9236.95</td>\n",
       "      <td>1802</td>\n",
       "      <td>9141.24</td>\n",
       "      <td>21.29</td>\n",
       "      <td>74.42</td>\n",
       "      <td>0.15</td>\n",
       "      <td>9238.17</td>\n",
       "      <td>1803</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35000</th>\n",
       "      <td>74</td>\n",
       "      <td>61</td>\n",
       "      <td>412</td>\n",
       "      <td>9196.06</td>\n",
       "      <td>1814</td>\n",
       "      <td>9114.93</td>\n",
       "      <td>20.50</td>\n",
       "      <td>60.61</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9234.92</td>\n",
       "      <td>1795</td>\n",
       "      <td>9137.19</td>\n",
       "      <td>21.39</td>\n",
       "      <td>76.33</td>\n",
       "      <td>0.16</td>\n",
       "      <td>9236.31</td>\n",
       "      <td>1797</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40000</th>\n",
       "      <td>71</td>\n",
       "      <td>70</td>\n",
       "      <td>309</td>\n",
       "      <td>9194.00</td>\n",
       "      <td>1807</td>\n",
       "      <td>9110.57</td>\n",
       "      <td>20.64</td>\n",
       "      <td>62.78</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9234.03</td>\n",
       "      <td>1791</td>\n",
       "      <td>9133.34</td>\n",
       "      <td>21.49</td>\n",
       "      <td>79.20</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9236.62</td>\n",
       "      <td>1794</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45000</th>\n",
       "      <td>72</td>\n",
       "      <td>79</td>\n",
       "      <td>206</td>\n",
       "      <td>9192.04</td>\n",
       "      <td>1802</td>\n",
       "      <td>9106.73</td>\n",
       "      <td>20.75</td>\n",
       "      <td>64.54</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9235.15</td>\n",
       "      <td>1795</td>\n",
       "      <td>9136.35</td>\n",
       "      <td>21.33</td>\n",
       "      <td>77.47</td>\n",
       "      <td>0.15</td>\n",
       "      <td>9236.62</td>\n",
       "      <td>1794</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000</th>\n",
       "      <td>72</td>\n",
       "      <td>88</td>\n",
       "      <td>103</td>\n",
       "      <td>9190.22</td>\n",
       "      <td>1797</td>\n",
       "      <td>9103.36</td>\n",
       "      <td>20.83</td>\n",
       "      <td>66.01</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9234.32</td>\n",
       "      <td>1788</td>\n",
       "      <td>9131.98</td>\n",
       "      <td>21.54</td>\n",
       "      <td>80.80</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9233.64</td>\n",
       "      <td>1787</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55000</th>\n",
       "      <td>72</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>9188.66</td>\n",
       "      <td>1793</td>\n",
       "      <td>9100.31</td>\n",
       "      <td>20.90</td>\n",
       "      <td>67.43</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9232.81</td>\n",
       "      <td>1783</td>\n",
       "      <td>9128.26</td>\n",
       "      <td>21.36</td>\n",
       "      <td>83.19</td>\n",
       "      <td>0.15</td>\n",
       "      <td>9235.14</td>\n",
       "      <td>1787</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60000</th>\n",
       "      <td>71</td>\n",
       "      <td>105</td>\n",
       "      <td>464</td>\n",
       "      <td>9187.59</td>\n",
       "      <td>1789</td>\n",
       "      <td>9097.77</td>\n",
       "      <td>20.96</td>\n",
       "      <td>68.87</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9237.49</td>\n",
       "      <td>1785</td>\n",
       "      <td>9129.78</td>\n",
       "      <td>21.40</td>\n",
       "      <td>86.31</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9235.14</td>\n",
       "      <td>1787</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65000</th>\n",
       "      <td>71</td>\n",
       "      <td>114</td>\n",
       "      <td>361</td>\n",
       "      <td>9186.87</td>\n",
       "      <td>1786</td>\n",
       "      <td>9095.61</td>\n",
       "      <td>21.00</td>\n",
       "      <td>70.29</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9237.46</td>\n",
       "      <td>1783</td>\n",
       "      <td>9128.02</td>\n",
       "      <td>21.54</td>\n",
       "      <td>87.90</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9235.14</td>\n",
       "      <td>1787</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70000</th>\n",
       "      <td>70</td>\n",
       "      <td>123</td>\n",
       "      <td>258</td>\n",
       "      <td>9186.28</td>\n",
       "      <td>1783</td>\n",
       "      <td>9093.65</td>\n",
       "      <td>21.05</td>\n",
       "      <td>71.61</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9240.59</td>\n",
       "      <td>1785</td>\n",
       "      <td>9129.82</td>\n",
       "      <td>21.51</td>\n",
       "      <td>89.26</td>\n",
       "      <td>0.13</td>\n",
       "      <td>9235.14</td>\n",
       "      <td>1787</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75000</th>\n",
       "      <td>72</td>\n",
       "      <td>132</td>\n",
       "      <td>155</td>\n",
       "      <td>9185.75</td>\n",
       "      <td>1780</td>\n",
       "      <td>9091.85</td>\n",
       "      <td>21.08</td>\n",
       "      <td>72.85</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9240.32</td>\n",
       "      <td>1782</td>\n",
       "      <td>9126.62</td>\n",
       "      <td>21.41</td>\n",
       "      <td>92.28</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9241.64</td>\n",
       "      <td>1783</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80000</th>\n",
       "      <td>70</td>\n",
       "      <td>141</td>\n",
       "      <td>52</td>\n",
       "      <td>9185.33</td>\n",
       "      <td>1778</td>\n",
       "      <td>9090.21</td>\n",
       "      <td>21.11</td>\n",
       "      <td>74.05</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9242.29</td>\n",
       "      <td>1782</td>\n",
       "      <td>9127.63</td>\n",
       "      <td>21.43</td>\n",
       "      <td>93.23</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9241.64</td>\n",
       "      <td>1783</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85000</th>\n",
       "      <td>70</td>\n",
       "      <td>149</td>\n",
       "      <td>516</td>\n",
       "      <td>9185.04</td>\n",
       "      <td>1776</td>\n",
       "      <td>9088.78</td>\n",
       "      <td>21.14</td>\n",
       "      <td>75.17</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9246.55</td>\n",
       "      <td>1787</td>\n",
       "      <td>9131.15</td>\n",
       "      <td>21.49</td>\n",
       "      <td>93.91</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9241.64</td>\n",
       "      <td>1783</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90000</th>\n",
       "      <td>71</td>\n",
       "      <td>158</td>\n",
       "      <td>413</td>\n",
       "      <td>9184.86</td>\n",
       "      <td>1774</td>\n",
       "      <td>9087.54</td>\n",
       "      <td>21.16</td>\n",
       "      <td>76.23</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9243.08</td>\n",
       "      <td>1782</td>\n",
       "      <td>9126.87</td>\n",
       "      <td>21.46</td>\n",
       "      <td>94.75</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9241.64</td>\n",
       "      <td>1783</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95000</th>\n",
       "      <td>71</td>\n",
       "      <td>167</td>\n",
       "      <td>310</td>\n",
       "      <td>9184.75</td>\n",
       "      <td>1773</td>\n",
       "      <td>9086.42</td>\n",
       "      <td>21.18</td>\n",
       "      <td>77.22</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9245.49</td>\n",
       "      <td>1784</td>\n",
       "      <td>9128.67</td>\n",
       "      <td>21.37</td>\n",
       "      <td>95.46</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9241.64</td>\n",
       "      <td>1783</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       TRAIN:                                      VALID:  \\\n",
       "      Time   Ep   Ct     LOSS   PPL      NLL     KL  GAUSS   REG     LOSS   \n",
       "5000    77    8  463  9266.38  1971  9219.21  16.53  30.64  0.14  9240.28   \n",
       "10000   72   17  360  9226.75  1894  9169.75  18.57  38.43  0.13  9236.92   \n",
       "15000   72   26  257  9214.31  1864  9149.26  19.29  45.73  0.13  9242.35   \n",
       "20000   74   35  154  9207.12  1845  9136.51  19.76  50.80  0.14  9233.41   \n",
       "25000   73   44   51  9202.10  1832  9127.21  20.09  54.80  0.14  9236.87   \n",
       "30000   72   52  515  9198.77  1822  9120.34  20.32  58.10  0.14  9236.95   \n",
       "35000   74   61  412  9196.06  1814  9114.93  20.50  60.61  0.14  9234.92   \n",
       "40000   71   70  309  9194.00  1807  9110.57  20.64  62.78  0.14  9234.03   \n",
       "45000   72   79  206  9192.04  1802  9106.73  20.75  64.54  0.14  9235.15   \n",
       "50000   72   88  103  9190.22  1797  9103.36  20.83  66.01  0.14  9234.32   \n",
       "55000   72   97    0  9188.66  1793  9100.31  20.90  67.43  0.14  9232.81   \n",
       "60000   71  105  464  9187.59  1789  9097.77  20.96  68.87  0.14  9237.49   \n",
       "65000   71  114  361  9186.87  1786  9095.61  21.00  70.29  0.14  9237.46   \n",
       "70000   70  123  258  9186.28  1783  9093.65  21.05  71.61  0.14  9240.59   \n",
       "75000   72  132  155  9185.75  1780  9091.85  21.08  72.85  0.14  9240.32   \n",
       "80000   70  141   52  9185.33  1778  9090.21  21.11  74.05  0.14  9242.29   \n",
       "85000   70  149  516  9185.04  1776  9088.78  21.14  75.17  0.14  9246.55   \n",
       "90000   71  158  413  9184.86  1774  9087.54  21.16  76.23  0.14  9243.08   \n",
       "95000   71  167  310  9184.75  1773  9086.42  21.18  77.22  0.14  9245.49   \n",
       "\n",
       "                                            TEST:       SPEC:              \\\n",
       "        PPL      NLL     KL  GAUSS   REG     LOSS   PPL     1     2     3   \n",
       "5000   1861  9182.74  20.37  37.17  0.12  9238.98  1858  0.16  0.49  0.34   \n",
       "10000  1830  9162.81  20.43  53.67  0.13  9238.99  1834  0.31  0.49  0.34   \n",
       "15000  1823  9156.83  20.77  64.76  0.14  9242.00  1823  0.13  0.60  0.35   \n",
       "20000  1808  9145.03  21.14  67.24  0.15  9233.44  1807  0.17  0.63  0.37   \n",
       "25000  1803  9142.52  21.35  73.00  0.14  9239.36  1806  0.19  0.61  0.39   \n",
       "30000  1802  9141.24  21.29  74.42  0.15  9238.17  1803  0.17  0.57  0.37   \n",
       "35000  1795  9137.19  21.39  76.33  0.16  9236.31  1797  0.61  0.67  0.36   \n",
       "40000  1791  9133.34  21.49  79.20  0.14  9236.62  1794  0.48  0.55  0.40   \n",
       "45000  1795  9136.35  21.33  77.47  0.15  9236.62  1794  0.46  0.62  0.37   \n",
       "50000  1788  9131.98  21.54  80.80  0.14  9233.64  1787  0.20  0.65  0.39   \n",
       "55000  1783  9128.26  21.36  83.19  0.15  9235.14  1787  0.28  0.67  0.39   \n",
       "60000  1785  9129.78  21.40  86.31  0.14  9235.14  1787  0.34  0.66  0.39   \n",
       "65000  1783  9128.02  21.54  87.90  0.14  9235.14  1787  0.28  0.60  0.39   \n",
       "70000  1785  9129.82  21.51  89.26  0.13  9235.14  1787  0.21  0.61  0.41   \n",
       "75000  1782  9126.62  21.41  92.28  0.14  9241.64  1783  0.57  0.56  0.41   \n",
       "80000  1782  9127.63  21.43  93.23  0.14  9241.64  1783  0.40  0.64  0.37   \n",
       "85000  1787  9131.15  21.49  93.91  0.14  9241.64  1783  0.32  0.67  0.39   \n",
       "90000  1782  9126.87  21.46  94.75  0.14  9241.64  1783  0.17  0.61  0.39   \n",
       "95000  1784  9128.67  21.37  95.46  0.14  9241.64  1783  0.66  0.68  0.38   \n",
       "\n",
       "      HIER:        \n",
       "      CHILD OTHER  \n",
       "5000   0.50  0.31  \n",
       "10000  0.34  0.21  \n",
       "15000  0.35  0.22  \n",
       "20000  0.30  0.19  \n",
       "25000  0.32  0.19  \n",
       "30000  0.33  0.21  \n",
       "35000  0.27  0.17  \n",
       "40000  0.20  0.13  \n",
       "45000  0.21  0.14  \n",
       "50000  0.35  0.23  \n",
       "55000  0.28  0.19  \n",
       "60000  0.24  0.15  \n",
       "65000  0.24  0.16  \n",
       "70000  0.35  0.25  \n",
       "75000  0.22  0.14  \n",
       "80000  0.21  0.15  \n",
       "85000  0.30  0.20  \n",
       "90000  0.25  0.19  \n",
       "95000  0.36  0.25  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 R: 1.000 P: 0.001 type bridging hpsg category modal tense coreference graph arg connectives\n",
      "   1 R: 0.266 P: 0.056 translation english source target phrase alignment systems sentences pairs translations\n",
      "     11 R: 0.114 P: 0.114 models training embeddings neural network vector learning vectors performance context\n",
      "     12 R: 0.096 P: 0.096 features sentences topic sentiment analysis scores score level human evaluation\n",
      "   2 R: 0.225 P: 0.036 document query documents question answer questions terms retrieval search queries\n",
      "     21 R: 0.101 P: 0.101 features training performance feature classification learning test domain classifier systems\n",
      "     22 R: 0.088 P: 0.088 similarity sense semantic wordnet relations senses relation method pairs knowledge\n",
      "   3 R: 0.179 P: 0.011 annotation annotated agreement annotations annotators annotator scheme inter annotate guidelines\n",
      "     31 R: 0.082 P: 0.082 morphological languages english character dictionary errors forms form chinese characters\n",
      "     32 R: 0.086 P: 0.086 verb semantic noun verbs syntactic lexical nouns analysis subject phrase\n",
      "   4 R: 0.176 P: 0.031 dialogue user human utterance utterances systems response dialogues interaction users\n",
      "     41 R: 0.084 P: 0.084 research linguistic resources processing systems tools user process development texts\n",
      "     42 R: 0.061 P: 0.061 relations discourse event relation events semantic knowledge representation structure context\n",
      "   5 R: 0.153 P: 0.025 speech speaker speakers recognition acoustic phonetic syllable phoneme transcription pronunciation\n",
      "     51 R: 0.061 P: 0.061 dependency parsing tree parser parse treebank trees syntactic sentences pos\n",
      "     52 R: 0.067 P: 0.067 grammar rules tree rule input algorithm grammars node structure state\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b188cebb9b55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_recon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_kl_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_gauss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_reg_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppls_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_log\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_loss_recon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_loss_kl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_loss_gauss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_loss_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_ppls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mlosses_train\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_recon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_kl_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_gauss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_reg_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "while epoch < config.n_epochs:\n",
    "    # train\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = model.get_feed_dict(batch)\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch, ppls_batch, global_step_log = \\\n",
    "        sess.run([model.opt, model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_gauss, model.topic_loss_reg, model.topic_ppls, tf.train.get_global_step()], feed_dict = feed_dict)\n",
    "\n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if global_step_log % config.log_period == 0:\n",
    "            # validate\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_gauss_train, topic_loss_reg_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_gauss_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "\n",
    "            # test\n",
    "            if ppl_dev < ppl_min:\n",
    "                ppl_min = ppl_dev\n",
    "                loss_test, _, _, _, _, ppl_test, _ = validate(sess, test_batches, model)\n",
    "                saver.save(sess, config.path_model, global_step=global_step_log)\n",
    "                cPickle.dump(config, open(config.path_config % global_step_log, 'wb'))\n",
    "                update_checkpoint(config, checkpoint, global_step_log)\n",
    "            \n",
    "            # visualize topic\n",
    "            topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "            topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "            topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "            topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "            descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "            recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "            \n",
    "            depth_specs = get_topic_specialization(sess, model, instances_test)\n",
    "            hierarchical_affinities = get_hierarchical_affinity(sess, model)\n",
    "            \n",
    "            # log\n",
    "            clear_output()\n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_gauss_train, '%.2f'%topic_loss_reg_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_gauss_dev, '%.2f'%topic_loss_reg_dev, \\\n",
    "                    '%.2f'%loss_test, '%.0f'%ppl_test, \\\n",
    "                    '%.2f'%depth_specs[1], '%.2f'%depth_specs[2], '%.2f'%depth_specs[3], \\\n",
    "                    '%.2f'%hierarchical_affinities[0], '%.2f'%hierarchical_affinities[1]],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "            cPickle.dump(log_df, open(os.path.join(config.path_log), 'wb'))\n",
    "            print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)\n",
    "\n",
    "            # update tree\n",
    "            if not config.static:\n",
    "                config.tree_idxs, update_tree_flg = model.update_tree(topic_prob_topic, recur_prob_topic)\n",
    "                if update_tree_flg:\n",
    "                    print(config.tree_idxs)\n",
    "                    name_variables = {tensor.name: variable for tensor, variable in zip(tf.global_variables(), sess.run(tf.global_variables()))} # store paremeters\n",
    "                    if 'sess' in globals(): sess.close()\n",
    "                    model = HierarchicalNeuralTopicModel(config)\n",
    "                    sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "                    name_tensors = {tensor.name: tensor for tensor in tf.global_variables()}\n",
    "                    sess.run([name_tensors[name].assign(variable) for name, variable in name_variables.items()]) # restore parameters\n",
    "                    saver = tf.train.Saver(max_to_keep=1)\n",
    "                \n",
    "            time_start = time.time()\n",
    "\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "    epoch += 1\n",
    "\n",
    "loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "display(log_df)\n",
    "print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([108.88487 ,  98.85533 , 101.73428 , 138.29723 ,  90.70738 ,\n",
       "        97.048004,  37.88733 ,  38.16417 ,  41.808304,  45.578796,\n",
       "        53.009644,  54.871647,  50.3262  ,  45.501328,  54.289352,\n",
       "        48.97226 ], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(debug(model.topic_logvars), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        ],\n",
       "       [1.        , 0.89822125, 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.27806905,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.44281808, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.89801407, 1.        , 0.8412844 , 0.        , 0.        ,\n",
       "        0.        , 0.08885161],\n",
       "       [0.        , 1.        , 1.        , 1.        , 0.85427874,\n",
       "        1.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.11601029, 1.        , 0.7707927 , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.6883917 , 1.        ,\n",
       "        0.45872793, 0.        ],\n",
       "       [0.52471024, 1.        , 1.        , 1.        , 0.6504079 ,\n",
       "        1.        , 0.6567889 , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 1.        ,\n",
       "        0.52700347, 1.        ],\n",
       "       [0.40891832, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.22201705, 1.        , 1.        , 0.4112398 ,\n",
       "        1.        , 0.6689772 , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.40885088, 0.        ,\n",
       "        1.        , 0.26235488],\n",
       "       [1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.4257449 , 1.        , 1.        , 0.20237619,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.8571189 , 0.57496834, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 0.9077977 ],\n",
       "       [1.        , 0.40043268, 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 0.33619043, 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.29791537, 1.        , 0.3914893 , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.42816406, 0.55807114, 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 0.45249104, 1.        , 1.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.517357  , 1.        , 0.5776879 , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 1.        , 1.        , 1.        , 0.15381739,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.26557758,\n",
       "        0.6081972 , 0.6988956 , 1.        , 1.        , 1.        ,\n",
       "        0.09716322, 0.        , 0.9993727 , 0.24593279, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.63761675, 1.        , 1.        , 0.14468601, 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 1.        , 1.        , 1.        , 0.29560885,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.13323885,\n",
       "        0.81577575, 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.34154072, 0.        , 0.5820842 , 0.3465056 , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.5693822 , 1.        , 1.        , 0.04571179, 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 1.        , 1.        , 1.        , 0.225647  ,\n",
       "        1.        , 0.15274996, 1.        , 1.        , 0.90654063,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.9495543 , 0.        , 0.97972274, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.04506119, 0.89740974],\n",
       "       [0.        , 1.        , 1.        , 1.        , 0.28910694,\n",
       "        1.        , 0.01450629, 1.        , 1.        , 0.71948063,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.6396949 , 0.21736023, 0.9748905 , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.06055434, 0.8333564 ],\n",
       "       [0.02942948, 1.        , 1.        , 1.        , 0.71574134,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 0.20990103, 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.66853875, 0.        ],\n",
       "       [0.        , 1.        , 1.        , 1.        , 0.6861878 ,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 0.1650204 , 1.        , 1.        , 0.9752864 ,\n",
       "        1.        , 0.        , 0.6942106 , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.        , 0.        ,\n",
       "        0.6913219 , 0.        ],\n",
       "       [0.7997632 , 0.97149944, 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 0.77256256, 1.        , 1.        , 1.        ,\n",
       "        0.36341658, 0.        , 0.6827079 , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 0.31228662],\n",
       "       [0.5387886 , 0.9335183 , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.        , 1.        , 1.        , 0.        ,\n",
       "        0.7539307 , 0.6723412 , 1.        , 1.        , 1.        ,\n",
       "        0.22833528, 0.        , 0.5440874 , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.        ,\n",
       "        1.        , 0.13488993]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
