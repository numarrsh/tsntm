{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from data_structure import get_batches, get_test_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "from topic_model import TopicModel\n",
    "\n",
    "from topic_beam_search_decoder import BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '2', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/bags/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/tglm_vae_tmp3', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'bags', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 50, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 500, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.5, 'lr')\n",
    "flags.DEFINE_float('reg', 1., 'regularization term')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_bool('warmup', True, 'flg of warming up')\n",
    "flags.DEFINE_integer('epochs_cycle', 5, 'number of epochs within a cycle')\n",
    "flags.DEFINE_float('r_cycle', 0.5, 'proportion used to increase beta within a cycle')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 10, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_bow', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_hidden_topic', 512, 'dim_hidden_topic')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "flags.DEFINE_bool('bidirectional', True, 'flg of bidirectional encoding')\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_test_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('PAD_IDX', word_to_idx[PAD], 'PAD_IDX')\n",
    "flags.DEFINE_integer('UNK_IDX', word_to_idx[UNK], 'UNK_IDX')\n",
    "flags.DEFINE_integer('BOS_IDX', word_to_idx[BOS], 'BOS_IDX')\n",
    "flags.DEFINE_integer('EOS_IDX', word_to_idx[EOS], 'EOS_IDX')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "maximum_iterations = max([max([instance.max_sent_l for instance in batch]) for ct, batch in dev_batches])\n",
    "flags.DEFINE_integer('maximum_iterations', maximum_iterations, 'maximum_iterations')\n",
    "\n",
    "flags.DEFINE_integer('cycle_steps', len(train_batches)*config.epochs_cycle, 'number of steps for each cycle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build language model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['bow'] = tf.placeholder(tf.float32, [None, config.dim_bow], name='bow')\n",
    "t_variables['input_token_idxs'] = tf.placeholder(tf.int32, [None, None], name='input_token_idxs')\n",
    "t_variables['dec_input_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_input_idxs')\n",
    "t_variables['dec_target_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_target_idxs')\n",
    "t_variables['batch_l'] = tf.placeholder(tf.int32, name='batch_l')\n",
    "t_variables['doc_l'] = tf.placeholder(tf.int32, [None], name='doc_l')\n",
    "t_variables['sent_l'] = tf.placeholder(tf.int32, [None], name='sent_l')\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train', assertion=False):\n",
    "    def token_dropout(sent_idxs):\n",
    "        sent_idxs_dropout = np.asarray(sent_idxs)\n",
    "        sent_idxs_dropout[np.random.rand(len(sent_idxs)) > config.word_keep_prob] = config.UNK_IDX\n",
    "        return list(sent_idxs_dropout)\n",
    "\n",
    "    bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "    \n",
    "    doc_l = np.array([len(instance.token_idxs) for instance in batch])\n",
    "    \n",
    "    feed_input_token_idxs_list = [sent_idxs for instance in batch for sent_idxs in instance.token_idxs]\n",
    "    feed_dec_input_idxs_list = [[config.BOS_IDX] + token_dropout(sent_idxs) for sent_idxs in feed_input_token_idxs_list]\n",
    "    feed_dec_target_idxs_list = [sent_idxs + [config.EOS_IDX]  for sent_idxs in feed_input_token_idxs_list]\n",
    "        \n",
    "    sent_l = np.array([len(sent_idxs) for sent_idxs in feed_input_token_idxs_list], np.int32)\n",
    "    batch_l = len(sent_l)\n",
    "    \n",
    "    feed_input_token_idxs = pad_sequences(feed_input_token_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_input_idxs = pad_sequences(feed_dec_input_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_target_idxs = pad_sequences(feed_dec_target_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    \n",
    "    if assertion:\n",
    "        index = 0\n",
    "        for instance in batch:\n",
    "            for line_idxs in instance.token_idxs:\n",
    "                assert feed_input_token_idxs_list[index] == line_idxs\n",
    "                index += 1\n",
    "        assert feed_input_token_idxs.shape[1] == np.max(sent_l)\n",
    "        assert feed_dec_input_idxs.shape[1] == np.max(sent_l) + 1\n",
    "        assert feed_dec_target_idxs.shape[1] == np.max(sent_l) + 1\n",
    "    \n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "\n",
    "    feed_dict = {\n",
    "                t_variables['bow']: bow, \n",
    "                t_variables['batch_l']: batch_l, t_variables['doc_l']: doc_l, t_variables['sent_l']: sent_l, \n",
    "                t_variables['input_token_idxs']: feed_input_token_idxs, t_variables['dec_input_idxs']: feed_dec_input_idxs, t_variables['dec_target_idxs']: feed_dec_target_idxs, \n",
    "                t_variables['keep_prob']: keep_prob\n",
    "    }\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     10,
     24,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables):\n",
    "    sample_batch = dev_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "def check_shape(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()\n",
    "    \n",
    "def check_value(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()    \n",
    "    \n",
    "# sent_loss_kl_categ_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, tf_log(prob_topic_infer/prob_topic_sents)), 1))\n",
    "# debug_value([sent_loss_kl_categ, sent_loss_kl_categ_tmp])\n",
    "# sent_loss_kl_gauss_tmp = 0.5 * tf.reduce_sum(tf.exp(logvars_topic_infer-logvars_topic) + tf.square(means_topic - means_topic_infer) / tf.exp(logvars_topic) - 1 + (logvars_topic - logvars_topic_infer), -1)\n",
    "# sent_loss_kl_gmm_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss_tmp), -1))\n",
    "# debug_value([sent_loss_kl_gmm_tmp, sent_loss_kl_gmm])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encode bow\n",
    "with tf.variable_scope('topic/enc', reuse=False):\n",
    "    hidden_bow_ = tf.layers.Dense(units=config.dim_hidden_bow, activation=tf.nn.relu, name='hidden_bow')(t_variables['bow'])\n",
    "    hidden_bow = tf.layers.Dropout(t_variables['keep_prob'])(hidden_bow_)\n",
    "    means_bow = tf.layers.Dense(units=config.dim_latent_bow, name='mean_bow')(hidden_bow)\n",
    "    logvars_bow = tf.layers.Dense(units=config.dim_latent_bow, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_bow)\n",
    "    latents_bow = sample_latents(means_bow, logvars_bow) # sample latent vectors\n",
    "\n",
    "    prob_topic = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic')(latents_bow) # inference of topic probabilities\n",
    "\n",
    "# decode bow\n",
    "with tf.variable_scope('shared', reuse=False):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "bow_embeddings = tf.nn.embedding_lookup(embeddings, bow_idxs) # embeddings of each bow features\n",
    "\n",
    "with tf.variable_scope('topic/dec', reuse=False):\n",
    "    topic_embeddings = tf.get_variable('topic_emb', [config.n_topic, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of topics\n",
    "\n",
    "    topic_bow = tf.nn.softmax(tf.matmul(topic_embeddings, bow_embeddings, transpose_b=True), 1) # bow vectors for each topic\n",
    "    logits_bow = tf_log(tf.matmul(prob_topic, topic_bow)) # predicted bow distribution\n",
    "\n",
    "    # prior of each gaussian distribution (computed for each topic)\n",
    "    hidden_topic = tf.layers.Dense(units=config.dim_hidden_topic, activation=tf.nn.relu, name='hidden_topic')(topic_bow)\n",
    "    means_topic = tf.layers.Dense(units=config.dim_latent, name='mean_topic')(hidden_topic)\n",
    "    logvars_topic = tf.layers.Dense(units=config.dim_latent, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_topic)\n",
    "    sigma_topic = tf.exp(0.5 * logvars_topic)\n",
    "    gauss_topic = tfd.Normal(loc=means_topic, scale=sigma_topic)    \n",
    "    \n",
    "# define losses\n",
    "topic_losses_recon = -tf.reduce_sum(tf.multiply(t_variables['bow'], logits_bow), 1)\n",
    "topic_loss_recon = tf.reduce_mean(topic_losses_recon) # negative log likelihood of each words\n",
    "\n",
    "topic_loss_kl = compute_kl_loss(means_bow, logvars_bow) # KL divergence b/w latent dist & gaussian std\n",
    "\n",
    "topic_bow_norm = topic_bow / tf.norm(topic_bow, axis=1, keepdims=True)\n",
    "topic_dots = tf.clip_by_value(tf.matmul(topic_bow_norm, tf.transpose(topic_bow_norm)), -1., 1.)\n",
    "topic_loss_reg = tf.reduce_mean(tf.square(topic_dots - tf.eye(config.n_topic)))\n",
    "# topic_angles = tf.acos(topic_dots)\n",
    "# topic_angles_mean = tf.reduce_mean(topic_angles)\n",
    "# topic_angles_vars = tf.reduce_mean(tf.square(topic_angles - topic_angles_mean))\n",
    "# topic_loss_reg = tf.exp(topic_angles_vars - topic_angles_mean)\n",
    "\n",
    "# monitor\n",
    "n_bow = tf.reduce_sum(t_variables['bow'], 1)\n",
    "topic_ppls = tf.divide(topic_losses_recon, tf.maximum(1e-5, n_bow))\n",
    "topics_freq_bow_indices = tf.nn.top_k(topic_bow, 10, name='topic_freq_bow').indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "input_token_idxs = t_variables['input_token_idxs']\n",
    "batch_l = t_variables['batch_l']\n",
    "sent_l = t_variables['sent_l']\n",
    "max_sent_l = tf.reduce_max(sent_l)\n",
    "\n",
    "with tf.variable_scope('sent/enc', reuse=False):\n",
    "    # get word embedding\n",
    "    enc_input = tf.nn.embedding_lookup(embeddings, input_token_idxs)\n",
    "\n",
    "    # get sentence embedding\n",
    "    _, enc_state = dynamic_bi_rnn(enc_input, sent_l, config.dim_hidden, t_variables['keep_prob'])\n",
    "\n",
    "    # TODO House Holder flow\n",
    "    hidden_topic_infer =  tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='hidden_topic_infer')(enc_state)\n",
    "    prob_topic_infer = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic_infer')(hidden_topic_infer)\n",
    "\n",
    "    w_mean_topic_infer = tf.get_variable('mean_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32)\n",
    "    b_mean_topic_infer = tf.get_variable('mean_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32)\n",
    "    means_topic_infer = tf.tensordot(enc_state, w_mean_topic_infer, axes=[[1], [1]]) + b_mean_topic_infer\n",
    "    \n",
    "    w_logvar_topic_infer = tf.get_variable('logvar_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    b_logvar_topic_infer = tf.get_variable('logvar_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    logvars_topic_infer = tf.tensordot(enc_state, w_logvar_topic_infer, axes=[[1], [1]]) + b_logvar_topic_infer\n",
    "    sigma_topic_infer = tf.exp(0.5 * logvars_topic_infer)\n",
    "    gauss_topic_infer = tfd.Normal(loc=means_topic_infer, scale=sigma_topic_infer)\n",
    "    \n",
    "    # latent vectors from each gaussian dist.\n",
    "    latents_topic_infer = sample_latents(means_topic_infer, logvars_topic_infer) \n",
    "    # latent vector from gaussian mixture\n",
    "    latents_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), latents_topic_infer, transpose_a=True)\n",
    "    \n",
    "    # for beam search\n",
    "    means_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), means_topic_infer, transpose_a=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for decoding\n",
    "dec_sent_l = tf.add(sent_l, 1)\n",
    "dec_input_idxs = t_variables['dec_input_idxs']\n",
    "dec_input = tf.nn.embedding_lookup(embeddings, dec_input_idxs)\n",
    "\n",
    "dec_latents_input = tf.tile(latents_input, [1, tf.shape(dec_input)[1], 1])\n",
    "dec_concat_input = tf.concat([dec_input, dec_latents_input], -1)\n",
    "\n",
    "# decode for training\n",
    "with tf.variable_scope('sent/dec/rnn', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32, reuse=False):\n",
    "    dec_cell = tf.contrib.rnn.GRUCell(config.dim_hidden)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob = t_variables['keep_prob'])\n",
    "\n",
    "    dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(latents_input, 1))\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_concat_input, sequence_length=dec_sent_l)\n",
    "\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=dec_cell,\n",
    "        helper=helper,\n",
    "        initial_state=dec_initial_state)\n",
    "\n",
    "    dec_outputs, _, output_sent_l = tf.contrib.seq2seq.dynamic_decode(train_decoder)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(config.n_vocab, use_bias=False, name='out')\n",
    "    output_logits = output_layer(dec_outputs.rnn_output)\n",
    "    \n",
    "    output_token_idxs = tf.argmax(output_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tokens = tf.fill([batch_l], config.BOS_IDX)\n",
    "end_token = config.EOS_IDX\n",
    "\n",
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    infer_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(means_input, 1))\n",
    "    beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(infer_dec_initial_state, multiplier=config.beam_width)\n",
    "    beam_latents_input = tf.contrib.seq2seq.tile_batch(tf.squeeze(means_input, 1), multiplier=config.beam_width) # added\n",
    "    \n",
    "    beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=beam_latents_input)\n",
    "\n",
    "    beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    beam_output_token_idxs = beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    inter_means_input = tf.placeholder(tf.float32, [None, config.dim_latent])\n",
    "    \n",
    "    inter_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(inter_means_input)\n",
    "    inter_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(inter_dec_initial_state, multiplier=config.beam_width)\n",
    "    inter_beam_latents_input = tf.contrib.seq2seq.tile_batch(inter_means_input, multiplier=config.beam_width) # added\n",
    "    \n",
    "    inter_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=inter_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=inter_beam_latents_input)\n",
    "\n",
    "    inter_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inter_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    inter_beam_output_token_idxs = inter_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    topic_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic)\n",
    "    topic_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(topic_dec_initial_state, multiplier=config.beam_width)\n",
    "    topic_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic, multiplier=config.beam_width) # added\n",
    "    \n",
    "    topic_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=topic_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=topic_beam_latents_input)\n",
    "\n",
    "    topic_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        topic_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    topic_beam_output_token_idxs = topic_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    means_topic_summary = tf.reduce_mean(means_topic_infer, 0)\n",
    "    \n",
    "    summary_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic_summary)\n",
    "    summary_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(summary_dec_initial_state, multiplier=config.beam_width)\n",
    "    summary_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic_summary, multiplier=config.beam_width) # added\n",
    "    \n",
    "    summary_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=summary_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width,\n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=summary_beam_latents_input)\n",
    "\n",
    "    summary_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        summary_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    summary_beam_output_token_idxs = summary_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language modeling cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target and mask\n",
    "dec_target_idxs = t_variables['dec_target_idxs']\n",
    "dec_mask_tokens = tf.sequence_mask(dec_sent_l, maxlen=max_sent_l+1, dtype=tf.float32)\n",
    "\n",
    "# nll for each token (averaged over batch & sentence)\n",
    "sent_loss_recon = tf.contrib.seq2seq.sequence_loss(output_logits, dec_target_idxs, dec_mask_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_l = t_variables['doc_l']\n",
    "mask_sents = tf.sequence_mask(doc_l)\n",
    "mask_sents_flatten = tf.reshape(mask_sents, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1]])\n",
    "\n",
    "prob_topic_tiled = tf.tile(tf.expand_dims(prob_topic, 1), [1, tf.shape(mask_sents)[1], 1])\n",
    "prob_topic_flatten = tf.reshape(prob_topic_tiled, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1], config.n_topic])\n",
    "prob_topic_sents = tf.boolean_mask(prob_topic_flatten, mask_sents_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferred mixture probabilities (computed for each sentence)\n",
    "categ_topic_infer = tfd.Categorical(probs=prob_topic_infer)\n",
    "\n",
    "# prior of mixture probabilities (computed for each document, tiled for each sentence)\n",
    "categ_topic = tfd.Categorical(probs=prob_topic_sents)\n",
    "\n",
    "sent_loss_kl_categ = tf.reduce_mean(tfd.kl_divergence(categ_topic_infer, categ_topic))\n",
    "\n",
    "# inference of each gaussian gaussribution (computed for each sentence)\n",
    "\n",
    "sent_loss_kl_gauss = tf.reduce_sum(tfd.kl_divergence(gauss_topic_infer, gauss_topic), -1)\n",
    "sent_loss_kl_gmm = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss), -1))\n",
    "\n",
    "sent_loss_kl = sent_loss_kl_categ + sent_loss_kl_gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "tau = tf.cast(tf.divide(tf.mod(global_step, tf.constant(config.cycle_steps)), tf.constant(config.cycle_steps)), dtype=tf.float32)\n",
    "beta = tf.minimum(1., tau/config.r_cycle)\n",
    "\n",
    "sent_loss = sent_loss_recon + beta * sent_loss_kl\n",
    "\n",
    "topic_loss = topic_loss_recon + topic_loss_kl + config.reg * topic_loss_reg\n",
    "loss = topic_loss + sent_loss\n",
    "\n",
    "# define optimizer\n",
    "if config.opt == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif config.opt == 'Adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "    \n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_sents(token_idxs, config, idx_to_word):\n",
    "    sents = []\n",
    "    for sent_idxs in token_idxs:\n",
    "        tokens = []\n",
    "        for idx in sent_idxs:\n",
    "            if idx == config.EOS_IDX: break\n",
    "            tokens.append(idx_to_word[idx])\n",
    "        sent = ' '.join(tokens)\n",
    "        sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, ppls_batch \\\n",
    "            = sess.run([loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, topic_ppls], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "    loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, sent_loss_recon_mean, sent_loss_kl_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    return loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, sent_loss_recon_mean, sent_loss_kl_mean, ppl_mean\n",
    "\n",
    "def get_all_losses(sess, batches):\n",
    "    losses = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch = \\\n",
    "        sess.run([loss, topic_loss_recon, topic_loss_kl, sent_loss_recon, sent_loss_kl], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "    print('LOSS %.2f | TM NLL: %.2f, KL: %.4f | LM NLL: %.2f, KL: %.4f' %  np.mean(losses, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_batch):\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    pred_token_idxs = sess.run(output_token_idxs, feed_dict = feed_dict)\n",
    "    true_token_idxs = [sent_idxs for instance in sample_batch for sent_idxs in instance.token_idxs]\n",
    "    \n",
    "    assert len(pred_token_idxs) == len(true_token_idxs)\n",
    "    \n",
    "    pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "    true_sents = idxs_to_sents(true_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    for i, (true_sent, pred_sent) in enumerate(zip(true_sents, pred_sents)):        \n",
    "        print(i, 'TRUE: %s' % true_sent)\n",
    "        print(i, 'PRED: %s' % pred_sent)\n",
    "\n",
    "def print_topic_sample():\n",
    "    pred_topics_freq_bow_indices, pred_topic_token_idxs = sess.run([topics_freq_bow_indices, topic_beam_output_token_idxs], \n",
    "                                                                                                           feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "    pred_topic_sents = idxs_to_sents(pred_topic_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Topic Samples-----------')\n",
    "    for i, (topic_freq_bow_idxs, pred_topic_sent) in enumerate(zip(topics_freq_bow_idxs, pred_topic_sents)):\n",
    "        print(i, ' BOW:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' SENTENCE:', pred_topic_sent)\n",
    "        \n",
    "def print_summary(test_batch):\n",
    "    feed_dict = get_feed_dict(test_batch)\n",
    "    feed_dict[t_variables['batch_l']] = config.n_topic\n",
    "    feed_dict[t_variables['keep_prob']] = 1.\n",
    "    pred_topics_freq_bow_indices, pred_summary_token_idxs = sess.run([topics_freq_bow_indices, summary_beam_output_token_idxs], feed_dict=feed_dict)\n",
    "    pred_summary_sents = idxs_to_sents(pred_summary_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Output sentences for each topic-----------')\n",
    "    print('Item idx:', test_batch[0].item_idx)\n",
    "    for i, (topic_freq_bow_idxs, pred_summary_sent) in enumerate(zip(topics_freq_bow_idxs, pred_summary_sents)):\n",
    "        print(i, ' BOW:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' SENTENCE:', pred_summary_sent)\n",
    "        \n",
    "    print('-----------Summaries-----------')\n",
    "    for i, summary in enumerate(test_batch[0].summaries):\n",
    "        print('SUMMARY %i :'%i, '\\n', summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "saver = tf.train.Saver(max_to_keep=10)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','TM','','','','LM','','VALID:','TM','','','','LM','', ''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL','REG','NLL','KL','LOSS','PPL','NLL','KL','REG','NLL','KL', 'Beta']]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>LM</th>\n",
       "      <th></th>\n",
       "      <th>VALID:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>LM</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>128.53</td>\n",
       "      <td>1036</td>\n",
       "      <td>118.02</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.90</td>\n",
       "      <td>9.12</td>\n",
       "      <td>1.49</td>\n",
       "      <td>128.62</td>\n",
       "      <td>1017</td>\n",
       "      <td>115.86</td>\n",
       "      <td>2.74</td>\n",
       "      <td>0.90</td>\n",
       "      <td>9.12</td>\n",
       "      <td>1.64</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>121.50</td>\n",
       "      <td>602</td>\n",
       "      <td>114.68</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.36</td>\n",
       "      <td>6.08</td>\n",
       "      <td>3.51</td>\n",
       "      <td>111.73</td>\n",
       "      <td>550</td>\n",
       "      <td>105.85</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.22</td>\n",
       "      <td>5.47</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>120.08</td>\n",
       "      <td>586</td>\n",
       "      <td>113.88</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.24</td>\n",
       "      <td>5.73</td>\n",
       "      <td>2.03</td>\n",
       "      <td>110.66</td>\n",
       "      <td>536</td>\n",
       "      <td>105.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>5.11</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>1500</td>\n",
       "      <td>119.25</td>\n",
       "      <td>578</td>\n",
       "      <td>113.40</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.53</td>\n",
       "      <td>1.36</td>\n",
       "      <td>110.23</td>\n",
       "      <td>532</td>\n",
       "      <td>105.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.91</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>118.88</td>\n",
       "      <td>573</td>\n",
       "      <td>113.25</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.13</td>\n",
       "      <td>5.39</td>\n",
       "      <td>1.03</td>\n",
       "      <td>110.08</td>\n",
       "      <td>531</td>\n",
       "      <td>105.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.79</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>118.93</td>\n",
       "      <td>572</td>\n",
       "      <td>113.38</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "      <td>5.33</td>\n",
       "      <td>0.90</td>\n",
       "      <td>110.12</td>\n",
       "      <td>537</td>\n",
       "      <td>105.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.73</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2776</th>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>118.63</td>\n",
       "      <td>569</td>\n",
       "      <td>113.22</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.09</td>\n",
       "      <td>5.24</td>\n",
       "      <td>0.74</td>\n",
       "      <td>109.90</td>\n",
       "      <td>531</td>\n",
       "      <td>105.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.65</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3276</th>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>118.54</td>\n",
       "      <td>567</td>\n",
       "      <td>113.22</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.16</td>\n",
       "      <td>0.63</td>\n",
       "      <td>109.81</td>\n",
       "      <td>530</td>\n",
       "      <td>105.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.59</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3776</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>118.35</td>\n",
       "      <td>567</td>\n",
       "      <td>113.11</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.07</td>\n",
       "      <td>5.10</td>\n",
       "      <td>0.55</td>\n",
       "      <td>109.83</td>\n",
       "      <td>532</td>\n",
       "      <td>105.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.53</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4276</th>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>118.30</td>\n",
       "      <td>566</td>\n",
       "      <td>113.12</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.06</td>\n",
       "      <td>5.05</td>\n",
       "      <td>0.48</td>\n",
       "      <td>109.70</td>\n",
       "      <td>531</td>\n",
       "      <td>105.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.47</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4551</th>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>118.30</td>\n",
       "      <td>565</td>\n",
       "      <td>113.15</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.06</td>\n",
       "      <td>5.02</td>\n",
       "      <td>0.46</td>\n",
       "      <td>109.67</td>\n",
       "      <td>530</td>\n",
       "      <td>105.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5051</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>118.30</td>\n",
       "      <td>564</td>\n",
       "      <td>113.20</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.41</td>\n",
       "      <td>109.64</td>\n",
       "      <td>530</td>\n",
       "      <td>105.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5551</th>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>118.23</td>\n",
       "      <td>564</td>\n",
       "      <td>113.19</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4.94</td>\n",
       "      <td>0.37</td>\n",
       "      <td>109.57</td>\n",
       "      <td>528</td>\n",
       "      <td>105.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6051</th>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>1500</td>\n",
       "      <td>118.12</td>\n",
       "      <td>563</td>\n",
       "      <td>113.12</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4.90</td>\n",
       "      <td>0.34</td>\n",
       "      <td>109.64</td>\n",
       "      <td>533</td>\n",
       "      <td>105.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6551</th>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>118.04</td>\n",
       "      <td>562</td>\n",
       "      <td>113.08</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4.87</td>\n",
       "      <td>0.32</td>\n",
       "      <td>109.52</td>\n",
       "      <td>529</td>\n",
       "      <td>105.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6826</th>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>118.00</td>\n",
       "      <td>562</td>\n",
       "      <td>113.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4.86</td>\n",
       "      <td>0.31</td>\n",
       "      <td>109.53</td>\n",
       "      <td>528</td>\n",
       "      <td>105.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7326</th>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>117.96</td>\n",
       "      <td>562</td>\n",
       "      <td>113.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4.83</td>\n",
       "      <td>0.28</td>\n",
       "      <td>109.56</td>\n",
       "      <td>533</td>\n",
       "      <td>105.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7826</th>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>117.92</td>\n",
       "      <td>561</td>\n",
       "      <td>113.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4.80</td>\n",
       "      <td>0.27</td>\n",
       "      <td>109.49</td>\n",
       "      <td>532</td>\n",
       "      <td>105.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8326</th>\n",
       "      <td>65</td>\n",
       "      <td>3</td>\n",
       "      <td>1500</td>\n",
       "      <td>117.87</td>\n",
       "      <td>561</td>\n",
       "      <td>113.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4.77</td>\n",
       "      <td>0.25</td>\n",
       "      <td>109.36</td>\n",
       "      <td>527</td>\n",
       "      <td>105.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8826</th>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>117.85</td>\n",
       "      <td>561</td>\n",
       "      <td>113.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.75</td>\n",
       "      <td>0.24</td>\n",
       "      <td>109.42</td>\n",
       "      <td>529</td>\n",
       "      <td>105.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9101</th>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>117.82</td>\n",
       "      <td>561</td>\n",
       "      <td>113.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.74</td>\n",
       "      <td>0.23</td>\n",
       "      <td>109.46</td>\n",
       "      <td>531</td>\n",
       "      <td>105.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9601</th>\n",
       "      <td>54</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "      <td>117.76</td>\n",
       "      <td>560</td>\n",
       "      <td>112.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.72</td>\n",
       "      <td>0.22</td>\n",
       "      <td>109.38</td>\n",
       "      <td>528</td>\n",
       "      <td>105.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10101</th>\n",
       "      <td>65</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>117.73</td>\n",
       "      <td>560</td>\n",
       "      <td>112.97</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.70</td>\n",
       "      <td>0.21</td>\n",
       "      <td>109.33</td>\n",
       "      <td>528</td>\n",
       "      <td>105.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10601</th>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>1500</td>\n",
       "      <td>117.69</td>\n",
       "      <td>560</td>\n",
       "      <td>112.95</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.68</td>\n",
       "      <td>0.20</td>\n",
       "      <td>109.26</td>\n",
       "      <td>527</td>\n",
       "      <td>105.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11101</th>\n",
       "      <td>55</td>\n",
       "      <td>4</td>\n",
       "      <td>2000</td>\n",
       "      <td>117.69</td>\n",
       "      <td>559</td>\n",
       "      <td>112.97</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.66</td>\n",
       "      <td>0.19</td>\n",
       "      <td>109.39</td>\n",
       "      <td>532</td>\n",
       "      <td>105.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11376</th>\n",
       "      <td>31</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>117.69</td>\n",
       "      <td>559</td>\n",
       "      <td>112.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.65</td>\n",
       "      <td>0.18</td>\n",
       "      <td>109.34</td>\n",
       "      <td>530</td>\n",
       "      <td>105.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11876</th>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>117.69</td>\n",
       "      <td>559</td>\n",
       "      <td>113.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.64</td>\n",
       "      <td>0.18</td>\n",
       "      <td>109.31</td>\n",
       "      <td>529</td>\n",
       "      <td>105.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12376</th>\n",
       "      <td>56</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>117.66</td>\n",
       "      <td>559</td>\n",
       "      <td>112.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.62</td>\n",
       "      <td>0.17</td>\n",
       "      <td>109.30</td>\n",
       "      <td>529</td>\n",
       "      <td>105.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12876</th>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>1500</td>\n",
       "      <td>117.62</td>\n",
       "      <td>559</td>\n",
       "      <td>112.96</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0.16</td>\n",
       "      <td>109.34</td>\n",
       "      <td>532</td>\n",
       "      <td>105.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13376</th>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>117.59</td>\n",
       "      <td>559</td>\n",
       "      <td>112.95</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.59</td>\n",
       "      <td>0.16</td>\n",
       "      <td>109.31</td>\n",
       "      <td>531</td>\n",
       "      <td>105.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13651</th>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>117.58</td>\n",
       "      <td>559</td>\n",
       "      <td>112.95</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.15</td>\n",
       "      <td>109.28</td>\n",
       "      <td>529</td>\n",
       "      <td>105.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14151</th>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>500</td>\n",
       "      <td>117.57</td>\n",
       "      <td>558</td>\n",
       "      <td>112.96</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.57</td>\n",
       "      <td>0.15</td>\n",
       "      <td>109.28</td>\n",
       "      <td>531</td>\n",
       "      <td>105.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14651</th>\n",
       "      <td>54</td>\n",
       "      <td>6</td>\n",
       "      <td>1000</td>\n",
       "      <td>117.55</td>\n",
       "      <td>558</td>\n",
       "      <td>112.95</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.56</td>\n",
       "      <td>0.14</td>\n",
       "      <td>109.35</td>\n",
       "      <td>533</td>\n",
       "      <td>105.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15151</th>\n",
       "      <td>65</td>\n",
       "      <td>6</td>\n",
       "      <td>1500</td>\n",
       "      <td>117.56</td>\n",
       "      <td>558</td>\n",
       "      <td>112.97</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.54</td>\n",
       "      <td>0.14</td>\n",
       "      <td>109.19</td>\n",
       "      <td>527</td>\n",
       "      <td>105.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15651</th>\n",
       "      <td>54</td>\n",
       "      <td>6</td>\n",
       "      <td>2000</td>\n",
       "      <td>nan</td>\n",
       "      <td>750</td>\n",
       "      <td>118.21</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15926</th>\n",
       "      <td>31</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>nan</td>\n",
       "      <td>991</td>\n",
       "      <td>123.12</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16426</th>\n",
       "      <td>54</td>\n",
       "      <td>7</td>\n",
       "      <td>500</td>\n",
       "      <td>nan</td>\n",
       "      <td>1609</td>\n",
       "      <td>131.72</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.08</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16926</th>\n",
       "      <td>55</td>\n",
       "      <td>7</td>\n",
       "      <td>1000</td>\n",
       "      <td>nan</td>\n",
       "      <td>2537</td>\n",
       "      <td>139.82</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.10</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17426</th>\n",
       "      <td>55</td>\n",
       "      <td>7</td>\n",
       "      <td>1500</td>\n",
       "      <td>nan</td>\n",
       "      <td>3897</td>\n",
       "      <td>147.50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.13</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17926</th>\n",
       "      <td>54</td>\n",
       "      <td>7</td>\n",
       "      <td>2000</td>\n",
       "      <td>nan</td>\n",
       "      <td>5847</td>\n",
       "      <td>154.69</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.15</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18201</th>\n",
       "      <td>31</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>nan</td>\n",
       "      <td>7240</td>\n",
       "      <td>158.48</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.16</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>250525106176.00</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.03</td>\n",
       "      <td>250525106176.00</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18701</th>\n",
       "      <td>54</td>\n",
       "      <td>8</td>\n",
       "      <td>500</td>\n",
       "      <td>nan</td>\n",
       "      <td>10507</td>\n",
       "      <td>165.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.18</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>1462924672.00</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.01</td>\n",
       "      <td>1462923776.00</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     TRAIN:     TM                        LM        \\\n",
       "      Time Ep    Ct    LOSS    PPL     NLL    KL   REG   NLL    KL   \n",
       "1       14  0     0  128.53   1036  118.02  0.48  0.90  9.12  1.49   \n",
       "501     79  0   500  121.50    602  114.68  0.24  0.36  6.08  3.51   \n",
       "1001    63  0  1000  120.08    586  113.88  0.13  0.24  5.73  2.03   \n",
       "1501    65  0  1500  119.25    578  113.40  0.09  0.17  5.53  1.36   \n",
       "2001    62  0  2000  118.88    573  113.25  0.07  0.13  5.39  1.03   \n",
       "2276    29  1     0  118.93    572  113.38  0.06  0.11  5.33  0.90   \n",
       "2776    72  1   500  118.63    569  113.22  0.05  0.09  5.24  0.74   \n",
       "3276    71  1  1000  118.54    567  113.22  0.04  0.08  5.16  0.63   \n",
       "3776    55  1  1500  118.35    567  113.11  0.04  0.07  5.10  0.55   \n",
       "4276    65  1  2000  118.30    566  113.12  0.04  0.06  5.05  0.48   \n",
       "4551    41  2     0  118.30    565  113.15  0.04  0.06  5.02  0.46   \n",
       "5051    64  2   500  118.30    564  113.20  0.04  0.06  4.98  0.41   \n",
       "5551    65  2  1000  118.23    564  113.19  0.03  0.05  4.94  0.37   \n",
       "6051    55  2  1500  118.12    563  113.12  0.03  0.05  4.90  0.34   \n",
       "6551    75  2  2000  118.04    562  113.08  0.03  0.04  4.87  0.32   \n",
       "6826    30  3     0  118.00    562  113.06  0.03  0.04  4.86  0.31   \n",
       "7326    55  3   500  117.96    562  113.05  0.03  0.04  4.83  0.28   \n",
       "7826    64  3  1000  117.92    561  113.04  0.03  0.04  4.80  0.27   \n",
       "8326    65  3  1500  117.87    561  113.02  0.02  0.04  4.77  0.25   \n",
       "8826    55  3  2000  117.85    561  113.02  0.02  0.03  4.75  0.24   \n",
       "9101    31  4     0  117.82    561  113.01  0.02  0.03  4.74  0.23   \n",
       "9601    54  4   500  117.76    560  112.98  0.02  0.03  4.72  0.22   \n",
       "10101   65  4  1000  117.73    560  112.97  0.02  0.03  4.70  0.21   \n",
       "10601   64  4  1500  117.69    560  112.95  0.02  0.03  4.68  0.20   \n",
       "11101   55  4  2000  117.69    559  112.97  0.02  0.03  4.66  0.19   \n",
       "11376   31  5     0  117.69    559  112.98  0.02  0.03  4.65  0.18   \n",
       "11876   55  5   500  117.69    559  113.00  0.02  0.03  4.64  0.18   \n",
       "12376   56  5  1000  117.66    559  112.98  0.02  0.03  4.62  0.17   \n",
       "12876   55  5  1500  117.62    559  112.96  0.02  0.03  4.60  0.16   \n",
       "13376   55  5  2000  117.59    559  112.95  0.01  0.02  4.59  0.16   \n",
       "13651   30  6     0  117.58    559  112.95  0.01  0.02  4.58  0.15   \n",
       "14151   55  6   500  117.57    558  112.96  0.01  0.02  4.57  0.15   \n",
       "14651   54  6  1000  117.55    558  112.95  0.01  0.02  4.56  0.14   \n",
       "15151   65  6  1500  117.56    558  112.97  0.01  0.02  4.54  0.14   \n",
       "15651   54  6  2000     nan    750  118.21  0.01  0.04   nan   nan   \n",
       "15926   31  7     0     nan    991  123.12  0.01  0.05   nan   nan   \n",
       "16426   54  7   500     nan   1609  131.72  0.01  0.08   nan   nan   \n",
       "16926   55  7  1000     nan   2537  139.82  0.01  0.10   nan   nan   \n",
       "17426   55  7  1500     nan   3897  147.50  0.01  0.13   nan   nan   \n",
       "17926   54  7  2000     nan   5847  154.69  0.01  0.15   nan   nan   \n",
       "18201   31  8     0     nan   7240  158.48  0.01  0.16   nan   nan   \n",
       "18701   54  8   500     nan  10507  165.11  0.01  0.18   nan   nan   \n",
       "\n",
       "                VALID:          TM                        LM                   \\\n",
       "                  LOSS         PPL     NLL    KL   REG   NLL               KL   \n",
       "1               128.62        1017  115.86  2.74  0.90  9.12             1.64   \n",
       "501             111.73         550  105.85  0.01  0.22  5.47             2.06   \n",
       "1001            110.66         536  105.49  0.00  0.04  5.11             0.07   \n",
       "1501            110.23         532  105.31  0.00  0.01  4.91             0.02   \n",
       "2001            110.08         531  105.28  0.00  0.01  4.79             0.01   \n",
       "2276            110.12         537  105.38  0.00  0.01  4.73             0.01   \n",
       "2776            109.90         531  105.24  0.00  0.01  4.65             0.01   \n",
       "3276            109.81         530  105.21  0.00  0.01  4.59             0.01   \n",
       "3776            109.83         532  105.29  0.00  0.01  4.53             0.01   \n",
       "4276            109.70         531  105.21  0.00  0.01  4.47             0.01   \n",
       "4551            109.67         530  105.20  0.00  0.01  4.46             0.00   \n",
       "5051            109.64         530  105.21  0.00  0.01  4.42             0.00   \n",
       "5551            109.57         528  105.19  0.00  0.01  4.38             0.00   \n",
       "6051            109.64         533  105.28  0.00  0.01  4.35             0.00   \n",
       "6551            109.52         529  105.19  0.00  0.01  4.32             0.00   \n",
       "6826            109.53         528  105.21  0.00  0.01  4.31             0.00   \n",
       "7326            109.56         533  105.29  0.00  0.01  4.26             0.00   \n",
       "7826            109.49         532  105.24  0.00  0.01  4.24             0.00   \n",
       "8326            109.36         527  105.12  0.00  0.01  4.22             0.00   \n",
       "8826            109.42         529  105.20  0.00  0.01  4.22             0.00   \n",
       "9101            109.46         531  105.26  0.00  0.01  4.19             0.00   \n",
       "9601            109.38         528  105.19  0.00  0.01  4.18             0.00   \n",
       "10101           109.33         528  105.17  0.00  0.01  4.15             0.00   \n",
       "10601           109.26         527  105.12  0.00  0.01  4.14             0.00   \n",
       "11101           109.39         532  105.25  0.00  0.01  4.13             0.00   \n",
       "11376           109.34         530  105.23  0.00  0.01  4.10             0.00   \n",
       "11876           109.31         529  105.21  0.00  0.01  4.10             0.01   \n",
       "12376           109.30         529  105.20  0.00  0.01  4.10             0.00   \n",
       "12876           109.34         532  105.26  0.00  0.01  4.07             0.00   \n",
       "13376           109.31         531  105.23  0.00  0.01  4.07             0.00   \n",
       "13651           109.28         529  105.19  0.00  0.01  4.08             0.01   \n",
       "14151           109.28         531  105.21  0.00  0.01  4.06             0.00   \n",
       "14651           109.35         533  105.30  0.00  0.01  4.05             0.00   \n",
       "15151           109.19         527  105.15  0.00  0.01  4.03             0.00   \n",
       "15651              nan  8555976704  382.77  0.00  0.90   nan              nan   \n",
       "15926              nan  8555976704  382.77  0.00  0.90   nan              nan   \n",
       "16426              nan  8555976704  382.77  0.00  0.90   nan              nan   \n",
       "16926              nan  8555976704  382.77  0.00  0.90   nan              nan   \n",
       "17426              nan  8555976704  382.77  0.00  0.90   nan              nan   \n",
       "17926              nan  8555976704  382.77  0.00  0.90   nan              nan   \n",
       "18201  250525106176.00  8555976704  382.77  0.00  0.90  6.03  250525106176.00   \n",
       "18701    1462924672.00  8555976704  382.77  0.00  0.90  6.01    1462923776.00   \n",
       "\n",
       "              \n",
       "        Beta  \n",
       "1      0.000  \n",
       "501    0.088  \n",
       "1001   0.176  \n",
       "1501   0.264  \n",
       "2001   0.352  \n",
       "2276   0.400  \n",
       "2776   0.488  \n",
       "3276   0.576  \n",
       "3776   0.664  \n",
       "4276   0.752  \n",
       "4551   0.800  \n",
       "5051   0.888  \n",
       "5551   0.976  \n",
       "6051   1.000  \n",
       "6551   1.000  \n",
       "6826   1.000  \n",
       "7326   1.000  \n",
       "7826   1.000  \n",
       "8326   1.000  \n",
       "8826   1.000  \n",
       "9101   1.000  \n",
       "9601   1.000  \n",
       "10101  1.000  \n",
       "10601  1.000  \n",
       "11101  1.000  \n",
       "11376  0.000  \n",
       "11876  0.088  \n",
       "12376  0.176  \n",
       "12876  0.264  \n",
       "13376  0.352  \n",
       "13651  0.400  \n",
       "14151  0.488  \n",
       "14651  0.576  \n",
       "15151  0.664  \n",
       "15651  0.752  \n",
       "15926  0.800  \n",
       "16426  0.888  \n",
       "16926  0.976  \n",
       "17426  1.000  \n",
       "17926  1.000  \n",
       "18201  1.000  \n",
       "18701  1.000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Output sentences for each topic-----------\n",
      "Item idx: B000VB7EFW\n",
      "0  BOW: cover ! $ % & ' 'd 'll 'm 're\n",
      "0  SENTENCE: \n",
      "1  BOW: cover ! $ % & ' 'd 'll 'm 're\n",
      "1  SENTENCE: \n",
      "2  BOW: cover ; - ! $ % & ' 'd 'll\n",
      "2  SENTENCE: \n",
      "3  BOW: cover ! $ % & ' 'd 'll 'm 're\n",
      "3  SENTENCE: \n",
      "4  BOW: cover ! $ % & ' 'd 'll 'm 're\n",
      "4  SENTENCE: \n",
      "5  BOW: cover ! $ % & ' 'd 'll 'm 're\n",
      "5  SENTENCE: \n",
      "6  BOW: cover ! $ % & ' 'd 'll 'm 're\n",
      "6  SENTENCE: \n",
      "7  BOW: cover ! $ % & ' 'd 'll 'm 're\n",
      "7  SENTENCE: \n",
      "8  BOW: cover ! $ % & ' 'd 'll 'm 're\n",
      "8  SENTENCE: \n",
      "9  BOW: cover ! $ % & ' 'd 'll 'm 're\n",
      "9  SENTENCE: \n",
      "-----------Summaries-----------\n",
      "SUMMARY 0 : \n",
      " This is a very well made bag, nice construction, lots of pockets.\n",
      "the straps are very comfortable.\n",
      "and protects everything inside.\n",
      "It says\n",
      "it fits a 17inch notebook,\n",
      "however it did not.\n",
      "after using the pack for less than a month,\n",
      "it is ripping out already.\n",
      "SUMMARY 1 : \n",
      " This is a very well made bag, nice construction, lots of pockets.\n",
      "The quality is excellent\n",
      "and it is very durable.\n",
      "the straps are very comfortable.\n",
      "and protects everything inside.\n",
      "The color is a true red\n",
      "and it fits nicely.\n",
      "it is ripping out already.\n",
      "and it doesnt fit.\n",
      "It's just not the lightest backpack\n",
      "because some zipper teeth were not aligned.\n",
      "SUMMARY 2 : \n",
      " The quality is excellent\n",
      "and I love all the pockets and compartments.\n",
      "and it is very durable.\n",
      "and can't beleive the price\n",
      "and protects everything inside.\n",
      "The laptop\n",
      "doesn't fit in it.\n",
      "it is ripping out already.\n",
      "It's just not the lightest backpack\n",
      "0 TRUE: i really like the design of this case , as do my friends\n",
      "0 PRED: is i it a\n",
      "1 TRUE: i use my laptop to create electronic music a lot , so i think it fits well\n",
      "1 PRED: \n",
      "2 TRUE: the inside and outside is a very soft neoprene\n",
      "2 PRED: to the\n",
      "3 TRUE: the inside zipper track has a tiny sleeve so that the zippers do n't scratch your cover up\n",
      "3 PRED: the\n",
      "4 TRUE: my laptop is # . # `` and it fits well with a little bit of extra space for the power chord or mouse\n",
      "4 PRED: to\n",
      "5 TRUE: i 'd prefer that it be on an outside pocket so that i dont scratch my case cover\n",
      "5 PRED: \n",
      "6 TRUE: it also has two <unk> hooks on each side , however it doesnt come with the strap\n",
      "6 PRED: ,\n",
      "7 TRUE: i carry my computer in my backpack , so there is plenty of room for accessories\n",
      "7 PRED: it\n",
      "8 TRUE: if you dont carry a backpack , i suggest you purchase something more <unk>\n",
      "8 PRED: \n",
      "9 TRUE: i purchased this sleeve to protect my # & # # ; laptop as i would be travelling with it frequently\n",
      "9 PRED: n't\n",
      "10 TRUE: this sleeve feels like it was made from quality materials and my laptop fits perfectly\n",
      "10 PRED: \n",
      "11 TRUE: it feels soft , almost like memory foam , which adds protection in case it is dropped or otherwise potentially damaged\n",
      "11 PRED: it\n",
      "12 TRUE: it has a nice extra pocket on the outside which can include the charging adapter and cable plus it has small pockets inside for a laptop mouse and usb stick\n",
      "12 PRED: the\n",
      "13 TRUE: i mentioned the outside feels like memory foam which unfortunately also means that there is a big chance that over time there will be permanent marks and tears on it\n",
      "13 PRED: the\n",
      "14 TRUE: overall i would say this is a very nice sleeve at a good <unk>\n",
      "14 PRED: \n",
      "15 TRUE: - # & # # ; fits <unk> <unk> outside to protect against <unk> extra pocket for <unk>\n",
      "15 PRED: n't , the\n",
      "16 TRUE: - the outside will get marks and tears over\n",
      "16 PRED: i the\n",
      "17 TRUE: i bought this about a month or so ago and already the suitcase is showing a defect\n",
      "17 PRED: \n",
      "18 TRUE: the back pocket is a large compartment and the second back compartment is the laptop compartment\n",
      "18 PRED: the the i # is i\n",
      "19 TRUE: i usually try to fill up the back compartment but sometimes there is a bit of space\n",
      "19 PRED: the\n",
      "20 TRUE: if there is any space not filled in the back compartment and you place your laptop in the second to back compartment , the laptop compartment falls underneath the back compartment and now looks <unk> in like an accordion\n",
      "20 PRED: a that\n",
      "21 TRUE: this has to be product defect in the product i bought or if all the suitcases do this , then samsonite made a bad design\n",
      "21 PRED: this\n",
      "22 TRUE: i will be calling samsonite to see if i can get a replacement because my item is defective ; will hopefully get a replacement because i have had this item for a short period of time and have this issue\n",
      "22 PRED: \n",
      "23 TRUE: i ordered this for my new air\n",
      "23 PRED: it i , it is a\n",
      "24 TRUE: it came # days early which was a great surprise\n",
      "24 PRED: it to\n",
      "25 TRUE: it fits well , i have no complaints as of now\n",
      "25 PRED: \n",
      "26 TRUE: it is very plain , nothing too fancy\n",
      "26 PRED: is the\n",
      "27 TRUE: i kind of wish i bought a case that gave more personality to my laptop , but the protection this seems to provide is good for now\n",
      "27 PRED: the\n",
      "28 TRUE: feel free to get one , it wo n't hurt\n",
      "28 PRED: this and\n",
      "29 TRUE: hold # & # # ; laptops and accessories with ease\n",
      "29 PRED: my\n",
      "30 TRUE: also able to hold many others despite it 's size\n",
      "30 PRED: i the\n",
      "31 TRUE: works well over the shoulder if on a motorcycle as well\n",
      "31 PRED: \n",
      "32 TRUE: blue is different than the black i am always seeing everywhere\n",
      "32 PRED: the\n",
      "33 TRUE: very happy to have found this\n",
      "33 PRED: \n",
      "34 TRUE: this is the prettiest laptop sleeve there is\n",
      "34 PRED: \n",
      "35 TRUE: it is a little small , so i only recommend it if you have a skinny laptop\n",
      "35 PRED: it the i\n",
      "36 TRUE: i have a samsung series # and the sleeve fits beautifully\n",
      "36 PRED: i i , the fit the the the the\n",
      "37 TRUE: i love the material , which seems to be protecting my laptop really well , the zippers work perfectly , and the design is exactly as the picture\n",
      "37 PRED: my i\n",
      "38 TRUE: i highly recommend it\n",
      "38 PRED: a the it the\n",
      "39 TRUE: it was tight for my # . # macbook air\n",
      "39 PRED: \n",
      "40 TRUE: i just have a <unk> shield cover on it , however when i removed it and tried it still looked very tough and didnt want to force my new air into it\n",
      "40 PRED: i ,\n",
      "41 TRUE: the material looks good so i would say good for < # . # inch laptops\n",
      "41 PRED: and\n",
      "42 TRUE: but the extra star for the seller\n",
      "42 PRED: it\n",
      "43 TRUE: indeed an a + seller\n",
      "43 PRED: a\n",
      "44 TRUE: i did n't think this little thing could be so powerful and fast\n",
      "44 PRED: the\n",
      "45 TRUE: although this is true i recommend buying # gb ram to get the fullest <unk>\n",
      "45 PRED: the case it\n",
      "46 TRUE: it is sleek , light , fast , nicely fit , and pure netbook\n",
      "46 PRED: the\n",
      "47 TRUE: i have tried out many computer through out my life time and this one is among one of the best ones\n",
      "47 PRED: \n",
      "48 TRUE: the laptop case provides a nice pop of color to any macbook\n",
      "48 PRED: it it\n",
      "49 TRUE: wanted to write a little review about baseline since they provided great customer service\n",
      "49 PRED: \n",
      "50 TRUE: i had emailed about exchanging a product and baseline was extremely accommodating and not only responded quickly to my <unk> but was also extremely understanding and worked with me to help resolve the issue\n",
      "50 PRED: the fits the is , , and i\n",
      "51 TRUE: so many times people tend to write to complain about poor customer services , but really wanted to <unk> baseline , especially <unk> who i had emailed with several times\n",
      "51 PRED: \n",
      "52 TRUE: it is not as big as i thought it would be and it needs more <unk> compartments\n",
      "52 PRED: the\n",
      "53 TRUE: other than that the item is fine\n",
      "53 PRED: \n",
      "54 TRUE: i 've used it for around # week\n",
      "54 PRED: it the\n",
      "55 TRUE: this cover comes in great covers and protects your mac book air well\n",
      "55 PRED: and the\n",
      "56 TRUE: i have had issue that after some use , the case slips off\n",
      "56 PRED: it , the # the\n",
      "57 TRUE: not to worry though , ipearl provides great customer service and sent me a free replacement quickly\n",
      "57 PRED: i the the #\n",
      "58 TRUE: this was my first caselogic purchase , but it will not be the last\n",
      "58 PRED: is , it\n",
      "59 TRUE: i am surprised by the simplicity and <unk> of this case\n",
      "59 PRED: would\n",
      "60 TRUE: i just wish that this product had rubber wheels -lrb- it makes a very annoying rolling noise -rrb- i know this sounds <unk> , but that was the only short coming this product had\n",
      "60 PRED: with the the this it\n",
      "61 TRUE: you ca n't navigate buttons on side\n",
      "61 PRED: \n",
      "62 TRUE: the case extends too far\n",
      "62 PRED: the\n",
      "63 TRUE: also the black on inside cover is running into the opposite side so it 's staining it black\n",
      "63 PRED: the\n",
      "64 TRUE: i purchased two of these sleeves , one for my ipad -lrb- i have the first generation ipad -rrb- and one for my mothers ipad -lrb- she has the second generation ipad -rrb- and they fit like a glove\n",
      "64 PRED: \n"
     ]
    }
   ],
   "source": [
    "if len(log_df) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "while epoch < config.epochs:\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = get_feed_dict(batch)\n",
    "\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, sent_loss_kl_categ_batch, sent_loss_kl_gmm_batch, ppls_batch = \\\n",
    "        sess.run([opt, loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, sent_loss_kl_categ, sent_loss_kl_gmm, topic_ppls], feed_dict = feed_dict)\n",
    "   \n",
    "        if sent_loss_kl_batch == np.inf:\n",
    "            print('Nan occured')\n",
    "            ckpt = tf.train.get_checkpoint_state(config.modeldir)\n",
    "            model_checkpoint_path = ckpt.all_model_checkpoint_paths[-1]\n",
    "            saver.restore(sess, model_checkpoint_path)            \n",
    "            break\n",
    "            \n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if ct%config.log_period==0:\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train, sent_loss_recon_train, sent_loss_kl_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, sent_loss_recon_dev, sent_loss_kl_dev, ppl_dev = get_loss(sess, dev_batches)\n",
    "            global_step_log, beta_eval = sess.run([tf.train.get_global_step(), beta])\n",
    "            \n",
    "            if loss_dev < loss_min:\n",
    "                loss_min = loss_dev\n",
    "                saver.save(sess, config.modelpath, global_step=global_step_log)\n",
    "\n",
    "            clear_output()\n",
    "    \n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_reg_train, '%.2f'%sent_loss_recon_train, '%.2f'%sent_loss_kl_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_reg_dev, '%.2f'%sent_loss_recon_dev, '%.2f'%sent_loss_kl_dev,  '%.3f'%beta_eval],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "\n",
    "            print_summary(test_batches[1][1])\n",
    "            print_sample(batch)\n",
    "            \n",
    "            time_start = time.time()\n",
    "            \n",
    "    epoch += 1\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_prob_topic, _prob_topic_sents, _prob_topic_infer, _means_topic_infer = debug_value([prob_topic, prob_topic_sents, prob_topic_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_i = 4\n",
    "_prob_topic_sents[batch_i], _prob_topic_infer[batch_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_means_topic_infer[0][:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_means_topic, b_means_topic = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"topic/dec/mean_topic\")\n",
    "\n",
    "pred_topic_embeddings, pred_topic_bow, pred_means_topic, pred_logvars_topic, pred_token_idxs, _w_means_topic, _b_means_topic, _w_mean_topic_infer = \\\n",
    "                                sess.run([topic_embeddings, topic_bow, means_topic, logvars_topic, topic_beam_output_token_idxs, w_means_topic, b_means_topic, w_mean_topic_infer], \n",
    "                                         feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "\n",
    "pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "\n",
    "pred_topics_freq_bow_indices = np.argsort(pred_topic_bow, 1)[:, ::-1][:, :10]\n",
    "pred_topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idxs in pred_topics_freq_bow_idxs:\n",
    "    print([idx_to_word[idx] for idx in idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_topic_embeddings[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_topic_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_w_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_w_mean_topic_infer[:, :10, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_enc_state_infer, _means_topic_infer = debug_value([enc_state_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_enc_state_infer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_means_topic_infer[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
