{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "# %matplotlib nbagg\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from data_structure import get_batches, get_test_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "\n",
    "from topic_beam_search_decoder import BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '1', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/bags/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/topic_vae', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'bags', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 1000, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 1000, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "# flags.DEFINE_string('opt', 'Adam', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.01, 'lr')\n",
    "flags.DEFINE_float('reg', 1., 'regularization term')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_bool('warmup', True, 'flg of warming up')\n",
    "flags.DEFINE_integer('epochs_cycle', 5, 'number of epochs within a cycle')\n",
    "flags.DEFINE_float('r_cycle', 0.5, 'proportion used to increase beta within a cycle')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 20, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_bow', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_hidden_topic', 512, 'dim_hidden_topic')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "flags.DEFINE_bool('bidirectional', True, 'flg of bidirectional encoding')\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(instances, batch_size, iterator=False):\n",
    "    iter_instances = iter(instances)\n",
    "    n_batch = len(instances)//batch_size\n",
    "    \n",
    "    batches = [(i_batch, [next(iter_instances) for i_doc in range(batch_size)]) for i_batch in range(n_batch)]\n",
    "    \n",
    "    if iterator: batches = iter(batches)\n",
    "    return batches\n",
    "\n",
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "flags.DEFINE_integer('cycle_steps', len(train_batches)*config.epochs_cycle, 'number of steps for each cycle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0,
     10,
     18,
     24
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables):\n",
    "    sample_batch = dev_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "def check_shape(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()\n",
    "    \n",
    "def check_value(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train'):\n",
    "    bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "    feed_dict = {\n",
    "                t_variables['bow']: bow, \n",
    "                t_variables['keep_prob']: keep_prob\n",
    "    }\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def get_depth(parent_idx=0, tree_depth=None, depth=1):\n",
    "    if tree_depth is None: tree_depth={0: depth}\n",
    "\n",
    "    child_idxs = tree_idxs[parent_idx]\n",
    "    depth +=1\n",
    "    for child_idx in child_idxs:\n",
    "        tree_depth[child_idx] = depth\n",
    "        if child_idx in tree_idxs: get_depth(child_idx, tree_depth, depth)\n",
    "    return tree_depth\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['bow'] = tf.placeholder(tf.float32, [None, config.dim_bow])\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32)\n",
    "\n",
    "# tree_idxs = {0:[1, 2, 3], \n",
    "#                       1:[10, 11], 2:[20, 21], 3:[30, 31]}\n",
    "\n",
    "tree_idxs = {0:[1, 2, 3], \n",
    "              1:[10, 11, 12], 2:[20, 21, 22], 3:[30, 31, 32]}\n",
    "\n",
    "# tree_idxs = {0:[1, 2], \n",
    "#              1:[11, 12], 2:[21]}\n",
    "\n",
    "# tree_idxs = {0:[1, 2, 3], \n",
    "#                       1:[10, 11], 2:[20, 21], 3:[30, 31],\n",
    "#                       10: [100, 101], 11: [110, 111], 20: [200, 201], 21: [210, 211], 30:[300, 301], 31:[310, 311]}\n",
    "\n",
    "topic_idxs = [0] + [idx for child_idxs in tree_idxs.values() for idx in child_idxs]\n",
    "\n",
    "child_to_parent_idxs = {child_idx: parent_idx for parent_idx, child_idxs in tree_idxs.items() for child_idx in child_idxs}\n",
    "\n",
    "tree_depth = get_depth()\n",
    "max_depth = max(tree_depth.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doubly rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoublyRNNCell:\n",
    "    def __init__(self, dim_hidden, output_layer=None):\n",
    "        self.dim_hidden = dim_hidden\n",
    "        \n",
    "        self.ancestral_layer=tf.layers.Dense(units=dim_hidden, activation=tf.nn.tanh, name='ancestral')\n",
    "        self.fraternal_layer=tf.layers.Dense(units=dim_hidden, activation=tf.nn.tanh, name='fraternal')\n",
    "#         self.hidden_layer = tf.layers.Dense(units=dim_hidden, activation=tf.nn.tanh, name='hidden')\n",
    "        self.hidden_layer = tf.layers.Dense(units=dim_hidden, name='hidden')\n",
    "        \n",
    "        self.output_layer=output_layer\n",
    "        \n",
    "    def __call__(self, state_ancestral, state_fraternal, reuse=True):\n",
    "        with tf.variable_scope('input', reuse=reuse):\n",
    "            state_ancestral = self.ancestral_layer(state_ancestral)\n",
    "            state_fraternal = self.fraternal_layer(state_fraternal)\n",
    "\n",
    "        with tf.variable_scope('output', reuse=reuse):\n",
    "            state_hidden = self.hidden_layer(state_ancestral + state_fraternal)\n",
    "            if self.output_layer is not None: \n",
    "                output = self.output_layer(state_hidden)\n",
    "            else:\n",
    "                output = state_hidden\n",
    "            \n",
    "        return output, state_hidden\n",
    "    \n",
    "    def get_initial_state(self, name):\n",
    "        initial_state = tf.get_variable(name, [1, self.dim_hidden], dtype=tf.float32)\n",
    "        return initial_state\n",
    "    \n",
    "    def get_zero_state(self, name):\n",
    "        zero_state = tf.zeros([1, self.dim_hidden], dtype=tf.float32, name=name)\n",
    "        return zero_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubly_rnn(dim_hidden, tree_idxs, initial_state_parent=None, initial_state_sibling=None, output_layer=None, name=''):\n",
    "    outputs, states_parent = {}, {}\n",
    "    \n",
    "    with tf.variable_scope(name, reuse=False):\n",
    "        doubly_rnn_cell = DoublyRNNCell(dim_hidden, output_layer)\n",
    "\n",
    "        if initial_state_parent is None: \n",
    "            initial_state_parent = doubly_rnn_cell.get_initial_state('init_state_parent')\n",
    "#             initial_state_parent = doubly_rnn_cell.get_zero_state('init_state_parent')\n",
    "        if initial_state_sibling is None: \n",
    "#             initial_state_sibling = doubly_rnn_cell.get_initial_state('init_state_sibling')\n",
    "            initial_state_sibling = doubly_rnn_cell.get_zero_state('init_state_sibling')\n",
    "        output, state_sibling = doubly_rnn_cell(initial_state_parent, initial_state_sibling, reuse=False)\n",
    "        outputs[0], states_parent[0] = output, state_sibling\n",
    "\n",
    "        for parent_idx, child_idxs in tree_idxs.items():\n",
    "            state_parent = states_parent[parent_idx]\n",
    "            state_sibling = initial_state_sibling\n",
    "            for child_idx in child_idxs:\n",
    "                output, state_sibling = doubly_rnn_cell(state_parent, state_sibling)\n",
    "                outputs[child_idx], states_parent[child_idx] = output, state_sibling\n",
    "\n",
    "    return outputs, states_parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stick break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nCRP(tree_sticks_topic):\n",
    "    tree_prob_topic = {}\n",
    "    tree_prob_leaf = {}\n",
    "    # calculate topic probability and save\n",
    "    tree_prob_topic[0] = 1.\n",
    "    \n",
    "    for parent_idx, child_idxs in tree_idxs.items():\n",
    "        rest_prob_topic = tree_prob_topic[parent_idx]\n",
    "        for child_idx in child_idxs:\n",
    "            stick_topic = tree_sticks_topic[child_idx]\n",
    "            if child_idx == child_idxs[-1]:\n",
    "                prob_topic = rest_prob_topic * 1.\n",
    "            else:\n",
    "                prob_topic = rest_prob_topic * stick_topic\n",
    "            \n",
    "            if not child_idx in tree_idxs: # leaf childs\n",
    "                tree_prob_leaf[child_idx] = prob_topic\n",
    "            else:\n",
    "                tree_prob_topic[child_idx] = prob_topic\n",
    "                \n",
    "            rest_prob_topic -= prob_topic\n",
    "            \n",
    "    return tree_prob_leaf\n",
    "\n",
    "def get_ancestor_idxs(leaf_idx, ancestor_idxs = None):\n",
    "    if ancestor_idxs is None: ancestor_idxs = [leaf_idx]\n",
    "    \n",
    "    parent_idx = child_to_parent_idxs[leaf_idx]\n",
    "    ancestor_idxs += [parent_idx]\n",
    "    if parent_idx in child_to_parent_idxs: get_ancestor_idxs(parent_idx, ancestor_idxs)\n",
    "    return ancestor_idxs[::-1]\n",
    "\n",
    "def get_prob_topic(tree_prob_leaf, prob_depth):\n",
    "    tree_prob_topic = defaultdict(float)\n",
    "    \n",
    "    leaf_ancestor_idxs = {leaf_idx: get_ancestor_idxs(leaf_idx) for leaf_idx in tree_prob_leaf}\n",
    "    for leaf_idx, ancestor_idxs in leaf_ancestor_idxs.items():\n",
    "        prob_leaf = tree_prob_leaf[leaf_idx]\n",
    "        for i, ancestor_idx in enumerate(ancestor_idxs):\n",
    "            prob_ancestor = prob_leaf * tf.expand_dims(prob_depth[:, i], -1)\n",
    "            tree_prob_topic[ancestor_idx] += prob_ancestor\n",
    "    prob_topic = tf.concat([tree_prob_topic[topic_idx] for topic_idx in topic_idxs], -1)\n",
    "    return prob_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree_topic_bow(tree_topic_embeddings):\n",
    "    def softmax_with_temperature(logits, axis=None, name=None, temperature=1.):\n",
    "        if axis is None:\n",
    "            axis = -1\n",
    "        return tf.exp(logits / temperature) / tf.reduce_sum(tf.exp(logits / temperature), axis=axis)\n",
    "\n",
    "    tree_topic_bow = {}\n",
    "    for topic_idx, depth in tree_depth.items():\n",
    "        topic_embedding = tree_topic_embeddings[topic_idx]\n",
    "        temperature = tf.constant(10 ** (1./depth), dtype=tf.float32)\n",
    "        logits = tf.matmul(topic_embedding, bow_embeddings, transpose_b=True)\n",
    "        tree_topic_bow[topic_idx] = softmax_with_temperature(logits, axis=-1, temperature=temperature)\n",
    "    \n",
    "    return tree_topic_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode bow\n",
    "with tf.variable_scope('topic/enc', reuse=False):\n",
    "    hidden_bow_ = tf.layers.Dense(units=config.dim_hidden_bow, activation=tf.nn.tanh, name='hidden_bow')(t_variables['bow'])\n",
    "    hidden_bow = tf.layers.Dropout(t_variables['keep_prob'])(hidden_bow_)\n",
    "    means_bow = tf.layers.Dense(units=config.dim_latent_bow, name='mean_bow')(hidden_bow)\n",
    "    logvars_bow = tf.layers.Dense(units=config.dim_latent_bow, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_bow')(hidden_bow)\n",
    "    latents_bow = sample_latents(means_bow, logvars_bow) # sample latent vectors\n",
    "    prob_layer = lambda h: tf.nn.sigmoid(tf.matmul(latents_bow, h, transpose_b=True))\n",
    "    \n",
    "    tree_sticks_topic, tree_states_sticks_topic = doubly_rnn(config.dim_latent_bow, tree_idxs, output_layer=prob_layer, name='sticks_topic')\n",
    "    tree_prob_leaf = nCRP(tree_sticks_topic)\n",
    "    prob_depth = tf.layers.Dense(units=max_depth, activation=tf.nn.softmax, name='prob_topic')(latents_bow) # inference of topic probabilities\n",
    "    \n",
    "    prob_topic = get_prob_topic(tree_prob_leaf, prob_depth)\n",
    "\n",
    "# decode bow\n",
    "with tf.variable_scope('shared', reuse=False):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "bow_embeddings = tf.nn.embedding_lookup(embeddings, bow_idxs) # embeddings of each bow features\n",
    "\n",
    "with tf.variable_scope('topic/dec', reuse=False):\n",
    "#     tree_topic_embeddings, tree_states_topic_embeddings = doubly_rnn(config.dim_emb, tree_idxs, name='emb_topic')\n",
    "    emb_layer = lambda h: tf.layers.Dense(units=config.dim_emb, name='output')(tf.nn.tanh(h))\n",
    "    tree_topic_embeddings, tree_states_topic_embeddings = doubly_rnn(config.dim_emb, tree_idxs, output_layer=emb_layer, name='emb_topic')\n",
    "#     topic_embeddings = tf.get_variable('topic_emb', [len(topic_idxs), config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of topics\n",
    "#     tree_topic_embeddings = {topic_idx: tf.expand_dims(topic_embeddings[topic_idxs.index(topic_idx)], 0) for topic_idx in topic_idxs}\n",
    "\n",
    "    tree_topic_bow = get_tree_topic_bow(tree_topic_embeddings) # bow vectors for each topic\n",
    "    \n",
    "    topic_bow = tf.concat([tree_topic_bow[topic_idx] for topic_idx in topic_idxs], 0)\n",
    "    logits_bow = tf_log(tf.matmul(prob_topic, topic_bow)) # predicted bow distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree_mask_reg(tree_idxs):\n",
    "    tree_mask_reg = np.ones([len(topic_idxs), len(topic_idxs)], dtype=np.float32)\n",
    "    parent_to_descendant_idxs = {parent_idx: get_descendant_idxs(parent_idx) for parent_idx in tree_idxs}\n",
    "    \n",
    "    for parent_idx, descendant_idxs in parent_to_descendant_idxs.items():\n",
    "        for descendant_idx in descendant_idxs:\n",
    "            tree_mask_reg[topic_idxs.index(parent_idx), topic_idxs.index(descendant_idx)] = tree_mask_reg[topic_idxs.index(descendant_idx), topic_idxs.index(parent_idx)] = 0.\n",
    "            \n",
    "    return tree_mask_reg\n",
    "\n",
    "def get_depth_mask_reg(tree_idxs):\n",
    "    depth_mask_reg = np.zeros([len(topic_idxs), len(topic_idxs)], dtype=np.float32)\n",
    "    \n",
    "    depth_mask_reg[0, 0] = 1.\n",
    "    for parent_idx, child_idxs in tree_idxs.items():\n",
    "        for child_idx1 in child_idxs:\n",
    "            for child_idx2 in child_idxs:\n",
    "                depth_mask_reg[topic_idxs.index(child_idx1), topic_idxs.index(child_idx2)] = 1.\n",
    "                \n",
    "    return depth_mask_reg\n",
    "\n",
    "def get_descendant_idxs(parent_idx, descendant_idxs = None):\n",
    "    if descendant_idxs is None: descendant_idxs = []\n",
    "    \n",
    "    child_idxs = tree_idxs[parent_idx]\n",
    "    descendant_idxs += child_idxs\n",
    "    for child_idx in child_idxs:\n",
    "        if child_idx in tree_idxs: get_descendant_idxs(child_idx, descendant_idxs)\n",
    "    return descendant_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define losses\n",
    "topic_losses_recon = -tf.reduce_sum(tf.multiply(t_variables['bow'], logits_bow), 1)\n",
    "topic_loss_recon = tf.reduce_mean(topic_losses_recon) # negative log likelihood of each words\n",
    "\n",
    "topic_loss_kl = compute_kl_loss(means_bow, logvars_bow) # KL divergence b/w latent dist & gaussian std\n",
    "\n",
    "topic_bow_norm = topic_bow / tf.norm(topic_bow, axis=1, keepdims=True)\n",
    "topic_dots = tf.clip_by_value(tf.matmul(topic_bow_norm, tf.transpose(topic_bow_norm)), -1., 1.)\n",
    "# topic_loss_reg = tf.reduce_mean(tf.square(topic_dots - tf.eye(len(topic_idxs))))\n",
    "tree_mask_reg = get_tree_mask_reg(tree_idxs)\n",
    "# tree_mask_reg = get_depth_mask_reg(tree_idxs)\n",
    "topic_losses_reg = tf.square(topic_dots - tf.eye(len(topic_idxs))) * tree_mask_reg\n",
    "topic_loss_reg = tf.reduce_sum(topic_losses_reg) / tf.reduce_sum(tree_mask_reg)\n",
    "\n",
    "# topic_embeddings = tf.concat([tree_topic_embeddings[topic_idx] for topic_idx in topic_idxs], 0)\n",
    "# topic_embeddings_norm = topic_embeddings / tf.norm(topic_embeddings, axis=1, keepdims=True)\n",
    "# topic_dots = tf.clip_by_value(tf.matmul(topic_embeddings_norm, tf.transpose(topic_embeddings_norm)), -1., 1.)\n",
    "# tree_mask_reg = get_tree_mask_reg(tree_idxs)\n",
    "# topic_loss_reg = tf.reduce_sum(tf.square(topic_dots - tf.eye(len(topic_idxs))) * tree_mask_reg) / tf.reduce_sum(tree_mask_reg)\n",
    "\n",
    "# topic_embeddings = tf.concat([tree_topic_embeddings[topic_idx] for topic_idx in topic_idxs], 0)\n",
    "# topic_embeddings_norm = topic_embeddings / tf.norm(topic_embeddings, axis=1, keepdims=True)\n",
    "# topic_dots = tf.clip_by_value(tf.matmul(topic_embeddings_norm, tf.transpose(topic_embeddings_norm)), -1., 1.)\n",
    "# topic_angles = tf.acos(topic_dots)\n",
    "# tree_mask_reg = get_tree_mask_reg(tree_idxs)\n",
    "\n",
    "# mean_angles = tf.reduce_sum(topic_angles*tree_mask_reg) / tf.reduce_sum(tree_mask_reg)\n",
    "# var_angles = tf.reduce_sum(tf.square(topic_angles-mean_angles)*tree_mask_reg) / tf.reduce_sum(tree_mask_reg)\n",
    "# mean_angles = tf.reduce_mean(topic_angles)\n",
    "# var_angles = tf.reduce_mean(tf.square(topic_angles-mean_angles))\n",
    "# mean_angles = tf.asin(tf.sqrt(tf.linalg.det(topic_dots * tree_mask_reg)))\n",
    "# var_angles = tf.square(tf.constant(np.pi/2., dtype=tf.float32)-mean_angles)\n",
    "\n",
    "# topic_loss_reg = var_angles - mean_angles\n",
    "\n",
    "global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "\n",
    "loss = topic_loss_recon + topic_loss_kl + config.reg * topic_loss_reg\n",
    "\n",
    "# define optimizer\n",
    "if config.opt == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif config.opt == 'Adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "\n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars, global_step=global_step)\n",
    "\n",
    "# monitor\n",
    "n_bow = tf.reduce_sum(t_variables['bow'], 1)\n",
    "topic_ppls = tf.divide(topic_losses_recon, tf.maximum(1e-5, n_bow))\n",
    "topics_freq_bow_indices = tf.nn.top_k(topic_bow, 10, name='topic_freq_bow').indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, ppls_batch \\\n",
    "            = sess.run([loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, topic_ppls], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "    loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    return loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, ppl_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topic_sample(parent_idx=0, topics_freq_bow_idxs=None, depth = 0):\n",
    "    if topics_freq_bow_idxs is None:\n",
    "        topics_freq_bow_idxs = bow_idxs[sess.run(topics_freq_bow_indices)]\n",
    "        topic_freq_bow_idxs = topics_freq_bow_idxs[topic_idxs.index(parent_idx)]\n",
    "        print(parent_idx, ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "    \n",
    "    child_idxs = tree_idxs[parent_idx]\n",
    "    depth += 1\n",
    "    for child_idx in child_idxs:\n",
    "        topic_freq_bow_idxs = topics_freq_bow_idxs[topic_idxs.index(child_idx)]\n",
    "        print('  '*depth, child_idx, ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        \n",
    "        if child_idx in tree_idxs: print_topic_sample(child_idx, topics_freq_bow_idxs, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "saver = tf.train.Saver(max_to_keep=10)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','TM','','','','VALID:','TM','','',''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL','REG','LOSS','PPL','NLL','KL','REG']]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>VALID:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>113.78</td>\n",
       "      <td>556</td>\n",
       "      <td>112.81</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.50</td>\n",
       "      <td>105.57</td>\n",
       "      <td>518</td>\n",
       "      <td>104.80</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>112.94</td>\n",
       "      <td>532</td>\n",
       "      <td>111.93</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.38</td>\n",
       "      <td>104.69</td>\n",
       "      <td>492</td>\n",
       "      <td>103.69</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>112.50</td>\n",
       "      <td>517</td>\n",
       "      <td>111.40</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.32</td>\n",
       "      <td>104.54</td>\n",
       "      <td>481</td>\n",
       "      <td>103.37</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>112.19</td>\n",
       "      <td>507</td>\n",
       "      <td>111.02</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.28</td>\n",
       "      <td>104.28</td>\n",
       "      <td>471</td>\n",
       "      <td>103.06</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>111.95</td>\n",
       "      <td>499</td>\n",
       "      <td>110.71</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.24</td>\n",
       "      <td>103.97</td>\n",
       "      <td>460</td>\n",
       "      <td>102.64</td>\n",
       "      <td>1.22</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6000</th>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>111.74</td>\n",
       "      <td>492</td>\n",
       "      <td>110.45</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.22</td>\n",
       "      <td>103.79</td>\n",
       "      <td>455</td>\n",
       "      <td>102.38</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7000</th>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>111.57</td>\n",
       "      <td>486</td>\n",
       "      <td>110.23</td>\n",
       "      <td>1.15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>103.64</td>\n",
       "      <td>447</td>\n",
       "      <td>102.20</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8000</th>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>111.42</td>\n",
       "      <td>480</td>\n",
       "      <td>110.03</td>\n",
       "      <td>1.21</td>\n",
       "      <td>0.18</td>\n",
       "      <td>103.52</td>\n",
       "      <td>439</td>\n",
       "      <td>102.03</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9000</th>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>111.29</td>\n",
       "      <td>476</td>\n",
       "      <td>109.86</td>\n",
       "      <td>1.27</td>\n",
       "      <td>0.17</td>\n",
       "      <td>103.65</td>\n",
       "      <td>442</td>\n",
       "      <td>102.06</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>111.19</td>\n",
       "      <td>472</td>\n",
       "      <td>109.72</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.15</td>\n",
       "      <td>103.55</td>\n",
       "      <td>438</td>\n",
       "      <td>101.94</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11000</th>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>111.09</td>\n",
       "      <td>468</td>\n",
       "      <td>109.59</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.14</td>\n",
       "      <td>103.45</td>\n",
       "      <td>437</td>\n",
       "      <td>101.85</td>\n",
       "      <td>1.56</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12000</th>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>111.01</td>\n",
       "      <td>465</td>\n",
       "      <td>109.47</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.13</td>\n",
       "      <td>103.46</td>\n",
       "      <td>432</td>\n",
       "      <td>101.81</td>\n",
       "      <td>1.62</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13000</th>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>25</td>\n",
       "      <td>110.93</td>\n",
       "      <td>462</td>\n",
       "      <td>109.37</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.13</td>\n",
       "      <td>103.51</td>\n",
       "      <td>432</td>\n",
       "      <td>101.83</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14000</th>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>110.86</td>\n",
       "      <td>459</td>\n",
       "      <td>109.27</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.12</td>\n",
       "      <td>103.37</td>\n",
       "      <td>431</td>\n",
       "      <td>101.68</td>\n",
       "      <td>1.66</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>110.80</td>\n",
       "      <td>457</td>\n",
       "      <td>109.19</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.11</td>\n",
       "      <td>103.43</td>\n",
       "      <td>429</td>\n",
       "      <td>101.69</td>\n",
       "      <td>1.72</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16000</th>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>31</td>\n",
       "      <td>110.74</td>\n",
       "      <td>455</td>\n",
       "      <td>109.11</td>\n",
       "      <td>1.53</td>\n",
       "      <td>0.11</td>\n",
       "      <td>103.39</td>\n",
       "      <td>429</td>\n",
       "      <td>101.65</td>\n",
       "      <td>1.72</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17000</th>\n",
       "      <td>11</td>\n",
       "      <td>34</td>\n",
       "      <td>33</td>\n",
       "      <td>110.69</td>\n",
       "      <td>453</td>\n",
       "      <td>109.03</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.10</td>\n",
       "      <td>103.39</td>\n",
       "      <td>430</td>\n",
       "      <td>101.62</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18000</th>\n",
       "      <td>11</td>\n",
       "      <td>36</td>\n",
       "      <td>35</td>\n",
       "      <td>110.64</td>\n",
       "      <td>451</td>\n",
       "      <td>108.97</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.10</td>\n",
       "      <td>103.31</td>\n",
       "      <td>425</td>\n",
       "      <td>101.51</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19000</th>\n",
       "      <td>11</td>\n",
       "      <td>38</td>\n",
       "      <td>37</td>\n",
       "      <td>110.60</td>\n",
       "      <td>449</td>\n",
       "      <td>108.91</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.09</td>\n",
       "      <td>103.27</td>\n",
       "      <td>425</td>\n",
       "      <td>101.46</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>12</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>110.56</td>\n",
       "      <td>448</td>\n",
       "      <td>108.85</td>\n",
       "      <td>1.62</td>\n",
       "      <td>0.09</td>\n",
       "      <td>103.22</td>\n",
       "      <td>423</td>\n",
       "      <td>101.39</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21000</th>\n",
       "      <td>13</td>\n",
       "      <td>42</td>\n",
       "      <td>41</td>\n",
       "      <td>110.52</td>\n",
       "      <td>446</td>\n",
       "      <td>108.79</td>\n",
       "      <td>1.64</td>\n",
       "      <td>0.09</td>\n",
       "      <td>103.29</td>\n",
       "      <td>422</td>\n",
       "      <td>101.42</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22000</th>\n",
       "      <td>11</td>\n",
       "      <td>44</td>\n",
       "      <td>43</td>\n",
       "      <td>110.49</td>\n",
       "      <td>445</td>\n",
       "      <td>108.74</td>\n",
       "      <td>1.66</td>\n",
       "      <td>0.08</td>\n",
       "      <td>103.18</td>\n",
       "      <td>417</td>\n",
       "      <td>101.31</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23000</th>\n",
       "      <td>11</td>\n",
       "      <td>46</td>\n",
       "      <td>45</td>\n",
       "      <td>110.45</td>\n",
       "      <td>443</td>\n",
       "      <td>108.69</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.08</td>\n",
       "      <td>103.28</td>\n",
       "      <td>421</td>\n",
       "      <td>101.37</td>\n",
       "      <td>1.89</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24000</th>\n",
       "      <td>14</td>\n",
       "      <td>48</td>\n",
       "      <td>47</td>\n",
       "      <td>110.42</td>\n",
       "      <td>442</td>\n",
       "      <td>108.64</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.08</td>\n",
       "      <td>103.08</td>\n",
       "      <td>417</td>\n",
       "      <td>101.16</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>11</td>\n",
       "      <td>50</td>\n",
       "      <td>49</td>\n",
       "      <td>110.39</td>\n",
       "      <td>441</td>\n",
       "      <td>108.60</td>\n",
       "      <td>1.72</td>\n",
       "      <td>0.08</td>\n",
       "      <td>103.10</td>\n",
       "      <td>417</td>\n",
       "      <td>101.16</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26000</th>\n",
       "      <td>11</td>\n",
       "      <td>52</td>\n",
       "      <td>51</td>\n",
       "      <td>110.36</td>\n",
       "      <td>440</td>\n",
       "      <td>108.56</td>\n",
       "      <td>1.73</td>\n",
       "      <td>0.07</td>\n",
       "      <td>103.23</td>\n",
       "      <td>420</td>\n",
       "      <td>101.28</td>\n",
       "      <td>1.94</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27000</th>\n",
       "      <td>12</td>\n",
       "      <td>54</td>\n",
       "      <td>53</td>\n",
       "      <td>110.34</td>\n",
       "      <td>439</td>\n",
       "      <td>108.51</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.07</td>\n",
       "      <td>103.05</td>\n",
       "      <td>413</td>\n",
       "      <td>101.07</td>\n",
       "      <td>1.96</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28000</th>\n",
       "      <td>11</td>\n",
       "      <td>56</td>\n",
       "      <td>55</td>\n",
       "      <td>110.31</td>\n",
       "      <td>438</td>\n",
       "      <td>108.48</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.07</td>\n",
       "      <td>103.16</td>\n",
       "      <td>417</td>\n",
       "      <td>101.16</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29000</th>\n",
       "      <td>11</td>\n",
       "      <td>58</td>\n",
       "      <td>57</td>\n",
       "      <td>110.29</td>\n",
       "      <td>437</td>\n",
       "      <td>108.44</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.07</td>\n",
       "      <td>103.15</td>\n",
       "      <td>415</td>\n",
       "      <td>101.15</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>14</td>\n",
       "      <td>60</td>\n",
       "      <td>59</td>\n",
       "      <td>110.26</td>\n",
       "      <td>436</td>\n",
       "      <td>108.40</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.07</td>\n",
       "      <td>103.07</td>\n",
       "      <td>414</td>\n",
       "      <td>101.06</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225000</th>\n",
       "      <td>12</td>\n",
       "      <td>450</td>\n",
       "      <td>449</td>\n",
       "      <td>109.33</td>\n",
       "      <td>397</td>\n",
       "      <td>106.87</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0.03</td>\n",
       "      <td>102.49</td>\n",
       "      <td>381</td>\n",
       "      <td>99.91</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226000</th>\n",
       "      <td>12</td>\n",
       "      <td>452</td>\n",
       "      <td>451</td>\n",
       "      <td>109.33</td>\n",
       "      <td>397</td>\n",
       "      <td>106.87</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0.03</td>\n",
       "      <td>102.46</td>\n",
       "      <td>384</td>\n",
       "      <td>99.88</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227000</th>\n",
       "      <td>12</td>\n",
       "      <td>454</td>\n",
       "      <td>453</td>\n",
       "      <td>109.33</td>\n",
       "      <td>397</td>\n",
       "      <td>106.87</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0.03</td>\n",
       "      <td>102.37</td>\n",
       "      <td>379</td>\n",
       "      <td>99.81</td>\n",
       "      <td>2.54</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228000</th>\n",
       "      <td>12</td>\n",
       "      <td>456</td>\n",
       "      <td>455</td>\n",
       "      <td>109.33</td>\n",
       "      <td>397</td>\n",
       "      <td>106.86</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0.03</td>\n",
       "      <td>102.47</td>\n",
       "      <td>382</td>\n",
       "      <td>99.89</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229000</th>\n",
       "      <td>12</td>\n",
       "      <td>458</td>\n",
       "      <td>457</td>\n",
       "      <td>109.32</td>\n",
       "      <td>397</td>\n",
       "      <td>106.86</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0.03</td>\n",
       "      <td>102.48</td>\n",
       "      <td>385</td>\n",
       "      <td>99.91</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230000</th>\n",
       "      <td>12</td>\n",
       "      <td>460</td>\n",
       "      <td>459</td>\n",
       "      <td>109.32</td>\n",
       "      <td>396</td>\n",
       "      <td>106.86</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0.03</td>\n",
       "      <td>102.38</td>\n",
       "      <td>381</td>\n",
       "      <td>99.82</td>\n",
       "      <td>2.54</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231000</th>\n",
       "      <td>12</td>\n",
       "      <td>462</td>\n",
       "      <td>461</td>\n",
       "      <td>109.32</td>\n",
       "      <td>396</td>\n",
       "      <td>106.86</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0.03</td>\n",
       "      <td>102.59</td>\n",
       "      <td>388</td>\n",
       "      <td>100.01</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232000</th>\n",
       "      <td>12</td>\n",
       "      <td>464</td>\n",
       "      <td>463</td>\n",
       "      <td>109.32</td>\n",
       "      <td>396</td>\n",
       "      <td>106.85</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0.03</td>\n",
       "      <td>102.43</td>\n",
       "      <td>383</td>\n",
       "      <td>99.85</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233000</th>\n",
       "      <td>12</td>\n",
       "      <td>466</td>\n",
       "      <td>465</td>\n",
       "      <td>109.32</td>\n",
       "      <td>396</td>\n",
       "      <td>106.85</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0.03</td>\n",
       "      <td>102.44</td>\n",
       "      <td>382</td>\n",
       "      <td>99.87</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234000</th>\n",
       "      <td>12</td>\n",
       "      <td>468</td>\n",
       "      <td>467</td>\n",
       "      <td>109.32</td>\n",
       "      <td>396</td>\n",
       "      <td>106.85</td>\n",
       "      <td>2.45</td>\n",
       "      <td>0.03</td>\n",
       "      <td>102.41</td>\n",
       "      <td>380</td>\n",
       "      <td>99.85</td>\n",
       "      <td>2.54</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235000</th>\n",
       "      <td>12</td>\n",
       "      <td>470</td>\n",
       "      <td>469</td>\n",
       "      <td>109.32</td>\n",
       "      <td>396</td>\n",
       "      <td>106.85</td>\n",
       "      <td>2.45</td>\n",
       "      <td>0.03</td>\n",
       "      <td>102.48</td>\n",
       "      <td>382</td>\n",
       "      <td>99.90</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236000</th>\n",
       "      <td>12</td>\n",
       "      <td>472</td>\n",
       "      <td>471</td>\n",
       "      <td>109.31</td>\n",
       "      <td>396</td>\n",
       "      <td>106.85</td>\n",
       "      <td>2.45</td>\n",
       "      <td>0.03</td>\n",
       "      <td>102.42</td>\n",
       "      <td>382</td>\n",
       "      <td>99.85</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237000</th>\n",
       "      <td>12</td>\n",
       "      <td>474</td>\n",
       "      <td>473</td>\n",
       "      <td>109.31</td>\n",
       "      <td>396</td>\n",
       "      <td>106.84</td>\n",
       "      <td>2.45</td>\n",
       "      <td>0.03</td>\n",
       "      <td>102.48</td>\n",
       "      <td>383</td>\n",
       "      <td>99.91</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238000</th>\n",
       "      <td>12</td>\n",
       "      <td>476</td>\n",
       "      <td>475</td>\n",
       "      <td>109.31</td>\n",
       "      <td>396</td>\n",
       "      <td>106.84</td>\n",
       "      <td>2.45</td>\n",
       "      <td>0.03</td>\n",
       "      <td>102.48</td>\n",
       "      <td>382</td>\n",
       "      <td>99.90</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239000</th>\n",
       "      <td>12</td>\n",
       "      <td>478</td>\n",
       "      <td>477</td>\n",
       "      <td>109.31</td>\n",
       "      <td>396</td>\n",
       "      <td>106.84</td>\n",
       "      <td>2.45</td>\n",
       "      <td>0.03</td>\n",
       "      <td>102.52</td>\n",
       "      <td>384</td>\n",
       "      <td>99.94</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240000</th>\n",
       "      <td>12</td>\n",
       "      <td>480</td>\n",
       "      <td>479</td>\n",
       "      <td>109.31</td>\n",
       "      <td>396</td>\n",
       "      <td>106.84</td>\n",
       "      <td>2.45</td>\n",
       "      <td>0.03</td>\n",
       "      <td>102.51</td>\n",
       "      <td>383</td>\n",
       "      <td>99.92</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241000</th>\n",
       "      <td>12</td>\n",
       "      <td>482</td>\n",
       "      <td>481</td>\n",
       "      <td>109.31</td>\n",
       "      <td>396</td>\n",
       "      <td>106.84</td>\n",
       "      <td>2.45</td>\n",
       "      <td>0.03</td>\n",
       "      <td>102.47</td>\n",
       "      <td>380</td>\n",
       "      <td>99.87</td>\n",
       "      <td>2.57</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242000</th>\n",
       "      <td>12</td>\n",
       "      <td>484</td>\n",
       "      <td>483</td>\n",
       "      <td>109.31</td>\n",
       "      <td>396</td>\n",
       "      <td>106.83</td>\n",
       "      <td>2.45</td>\n",
       "      <td>0.03</td>\n",
       "      <td>102.54</td>\n",
       "      <td>382</td>\n",
       "      <td>99.95</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243000</th>\n",
       "      <td>12</td>\n",
       "      <td>486</td>\n",
       "      <td>485</td>\n",
       "      <td>109.31</td>\n",
       "      <td>396</td>\n",
       "      <td>106.83</td>\n",
       "      <td>2.45</td>\n",
       "      <td>0.03</td>\n",
       "      <td>102.39</td>\n",
       "      <td>379</td>\n",
       "      <td>99.82</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244000</th>\n",
       "      <td>12</td>\n",
       "      <td>488</td>\n",
       "      <td>487</td>\n",
       "      <td>109.30</td>\n",
       "      <td>396</td>\n",
       "      <td>106.83</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.03</td>\n",
       "      <td>102.32</td>\n",
       "      <td>377</td>\n",
       "      <td>99.75</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245000</th>\n",
       "      <td>13</td>\n",
       "      <td>490</td>\n",
       "      <td>489</td>\n",
       "      <td>109.30</td>\n",
       "      <td>396</td>\n",
       "      <td>106.83</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.03</td>\n",
       "      <td>102.54</td>\n",
       "      <td>385</td>\n",
       "      <td>99.95</td>\n",
       "      <td>2.57</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246000</th>\n",
       "      <td>12</td>\n",
       "      <td>492</td>\n",
       "      <td>491</td>\n",
       "      <td>109.30</td>\n",
       "      <td>396</td>\n",
       "      <td>106.82</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.03</td>\n",
       "      <td>102.40</td>\n",
       "      <td>379</td>\n",
       "      <td>99.81</td>\n",
       "      <td>2.57</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247000</th>\n",
       "      <td>12</td>\n",
       "      <td>494</td>\n",
       "      <td>493</td>\n",
       "      <td>109.30</td>\n",
       "      <td>396</td>\n",
       "      <td>106.82</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.03</td>\n",
       "      <td>102.36</td>\n",
       "      <td>383</td>\n",
       "      <td>99.77</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248000</th>\n",
       "      <td>13</td>\n",
       "      <td>496</td>\n",
       "      <td>495</td>\n",
       "      <td>109.30</td>\n",
       "      <td>396</td>\n",
       "      <td>106.82</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.03</td>\n",
       "      <td>102.61</td>\n",
       "      <td>386</td>\n",
       "      <td>100.04</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249000</th>\n",
       "      <td>12</td>\n",
       "      <td>498</td>\n",
       "      <td>497</td>\n",
       "      <td>109.30</td>\n",
       "      <td>395</td>\n",
       "      <td>106.82</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.03</td>\n",
       "      <td>102.29</td>\n",
       "      <td>376</td>\n",
       "      <td>99.72</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250000</th>\n",
       "      <td>12</td>\n",
       "      <td>501</td>\n",
       "      <td>0</td>\n",
       "      <td>109.30</td>\n",
       "      <td>395</td>\n",
       "      <td>106.82</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.03</td>\n",
       "      <td>102.35</td>\n",
       "      <td>378</td>\n",
       "      <td>99.77</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251000</th>\n",
       "      <td>13</td>\n",
       "      <td>503</td>\n",
       "      <td>2</td>\n",
       "      <td>109.30</td>\n",
       "      <td>395</td>\n",
       "      <td>106.81</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.03</td>\n",
       "      <td>102.57</td>\n",
       "      <td>384</td>\n",
       "      <td>99.99</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252000</th>\n",
       "      <td>12</td>\n",
       "      <td>505</td>\n",
       "      <td>4</td>\n",
       "      <td>109.29</td>\n",
       "      <td>395</td>\n",
       "      <td>106.81</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.03</td>\n",
       "      <td>102.39</td>\n",
       "      <td>381</td>\n",
       "      <td>99.81</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253000</th>\n",
       "      <td>13</td>\n",
       "      <td>507</td>\n",
       "      <td>6</td>\n",
       "      <td>109.29</td>\n",
       "      <td>395</td>\n",
       "      <td>106.81</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.03</td>\n",
       "      <td>102.48</td>\n",
       "      <td>384</td>\n",
       "      <td>99.90</td>\n",
       "      <td>2.57</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254000</th>\n",
       "      <td>12</td>\n",
       "      <td>509</td>\n",
       "      <td>8</td>\n",
       "      <td>109.29</td>\n",
       "      <td>395</td>\n",
       "      <td>106.81</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.03</td>\n",
       "      <td>102.49</td>\n",
       "      <td>381</td>\n",
       "      <td>99.90</td>\n",
       "      <td>2.57</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>254 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       TRAIN:   TM                      VALID:   TM          \\\n",
       "       Time   Ep   Ct    LOSS  PPL     NLL    KL   REG    LOSS  PPL     NLL   \n",
       "1000     13    2    1  113.78  556  112.81  0.46  0.50  105.57  518  104.80   \n",
       "2000     11    4    3  112.94  532  111.93  0.63  0.38  104.69  492  103.69   \n",
       "3000     10    6    5  112.50  517  111.40  0.78  0.32  104.54  481  103.37   \n",
       "4000     11    8    7  112.19  507  111.02  0.90  0.28  104.28  471  103.06   \n",
       "5000     11   10    9  111.95  499  110.71  0.99  0.24  103.97  460  102.64   \n",
       "6000     11   12   11  111.74  492  110.45  1.07  0.22  103.79  455  102.38   \n",
       "7000     12   14   13  111.57  486  110.23  1.15  0.20  103.64  447  102.20   \n",
       "8000     12   16   15  111.42  480  110.03  1.21  0.18  103.52  439  102.03   \n",
       "9000     11   18   17  111.29  476  109.86  1.27  0.17  103.65  442  102.06   \n",
       "10000    11   20   19  111.19  472  109.72  1.32  0.15  103.55  438  101.94   \n",
       "11000    11   22   21  111.09  468  109.59  1.36  0.14  103.45  437  101.85   \n",
       "12000    11   24   23  111.01  465  109.47  1.40  0.13  103.46  432  101.81   \n",
       "13000    11   26   25  110.93  462  109.37  1.44  0.13  103.51  432  101.83   \n",
       "14000    11   28   27  110.86  459  109.27  1.47  0.12  103.37  431  101.68   \n",
       "15000    11   30   29  110.80  457  109.19  1.50  0.11  103.43  429  101.69   \n",
       "16000    11   32   31  110.74  455  109.11  1.53  0.11  103.39  429  101.65   \n",
       "17000    11   34   33  110.69  453  109.03  1.55  0.10  103.39  430  101.62   \n",
       "18000    11   36   35  110.64  451  108.97  1.58  0.10  103.31  425  101.51   \n",
       "19000    11   38   37  110.60  449  108.91  1.60  0.09  103.27  425  101.46   \n",
       "20000    12   40   39  110.56  448  108.85  1.62  0.09  103.22  423  101.39   \n",
       "21000    13   42   41  110.52  446  108.79  1.64  0.09  103.29  422  101.42   \n",
       "22000    11   44   43  110.49  445  108.74  1.66  0.08  103.18  417  101.31   \n",
       "23000    11   46   45  110.45  443  108.69  1.68  0.08  103.28  421  101.37   \n",
       "24000    14   48   47  110.42  442  108.64  1.70  0.08  103.08  417  101.16   \n",
       "25000    11   50   49  110.39  441  108.60  1.72  0.08  103.10  417  101.16   \n",
       "26000    11   52   51  110.36  440  108.56  1.73  0.07  103.23  420  101.28   \n",
       "27000    12   54   53  110.34  439  108.51  1.75  0.07  103.05  413  101.07   \n",
       "28000    11   56   55  110.31  438  108.48  1.77  0.07  103.16  417  101.16   \n",
       "29000    11   58   57  110.29  437  108.44  1.78  0.07  103.15  415  101.15   \n",
       "30000    14   60   59  110.26  436  108.40  1.79  0.07  103.07  414  101.06   \n",
       "...     ...  ...  ...     ...  ...     ...   ...   ...     ...  ...     ...   \n",
       "225000   12  450  449  109.33  397  106.87  2.44  0.03  102.49  381   99.91   \n",
       "226000   12  452  451  109.33  397  106.87  2.44  0.03  102.46  384   99.88   \n",
       "227000   12  454  453  109.33  397  106.87  2.44  0.03  102.37  379   99.81   \n",
       "228000   12  456  455  109.33  397  106.86  2.44  0.03  102.47  382   99.89   \n",
       "229000   12  458  457  109.32  397  106.86  2.44  0.03  102.48  385   99.91   \n",
       "230000   12  460  459  109.32  396  106.86  2.44  0.03  102.38  381   99.82   \n",
       "231000   12  462  461  109.32  396  106.86  2.44  0.03  102.59  388  100.01   \n",
       "232000   12  464  463  109.32  396  106.85  2.44  0.03  102.43  383   99.85   \n",
       "233000   12  466  465  109.32  396  106.85  2.44  0.03  102.44  382   99.87   \n",
       "234000   12  468  467  109.32  396  106.85  2.45  0.03  102.41  380   99.85   \n",
       "235000   12  470  469  109.32  396  106.85  2.45  0.03  102.48  382   99.90   \n",
       "236000   12  472  471  109.31  396  106.85  2.45  0.03  102.42  382   99.85   \n",
       "237000   12  474  473  109.31  396  106.84  2.45  0.03  102.48  383   99.91   \n",
       "238000   12  476  475  109.31  396  106.84  2.45  0.03  102.48  382   99.90   \n",
       "239000   12  478  477  109.31  396  106.84  2.45  0.03  102.52  384   99.94   \n",
       "240000   12  480  479  109.31  396  106.84  2.45  0.03  102.51  383   99.92   \n",
       "241000   12  482  481  109.31  396  106.84  2.45  0.03  102.47  380   99.87   \n",
       "242000   12  484  483  109.31  396  106.83  2.45  0.03  102.54  382   99.95   \n",
       "243000   12  486  485  109.31  396  106.83  2.45  0.03  102.39  379   99.82   \n",
       "244000   12  488  487  109.30  396  106.83  2.46  0.03  102.32  377   99.75   \n",
       "245000   13  490  489  109.30  396  106.83  2.46  0.03  102.54  385   99.95   \n",
       "246000   12  492  491  109.30  396  106.82  2.46  0.03  102.40  379   99.81   \n",
       "247000   12  494  493  109.30  396  106.82  2.46  0.03  102.36  383   99.77   \n",
       "248000   13  496  495  109.30  396  106.82  2.46  0.03  102.61  386  100.04   \n",
       "249000   12  498  497  109.30  395  106.82  2.46  0.03  102.29  376   99.72   \n",
       "250000   12  501    0  109.30  395  106.82  2.46  0.03  102.35  378   99.77   \n",
       "251000   13  503    2  109.30  395  106.81  2.46  0.03  102.57  384   99.99   \n",
       "252000   12  505    4  109.29  395  106.81  2.46  0.03  102.39  381   99.81   \n",
       "253000   13  507    6  109.29  395  106.81  2.46  0.03  102.48  384   99.90   \n",
       "254000   12  509    8  109.29  395  106.81  2.46  0.03  102.49  381   99.90   \n",
       "\n",
       "                    \n",
       "          KL   REG  \n",
       "1000    0.45  0.32  \n",
       "2000    0.80  0.20  \n",
       "3000    0.99  0.17  \n",
       "4000    1.10  0.12  \n",
       "5000    1.22  0.10  \n",
       "6000    1.33  0.08  \n",
       "7000    1.38  0.06  \n",
       "8000    1.44  0.06  \n",
       "9000    1.54  0.05  \n",
       "10000   1.57  0.04  \n",
       "11000   1.56  0.03  \n",
       "12000   1.62  0.03  \n",
       "13000   1.65  0.03  \n",
       "14000   1.66  0.02  \n",
       "15000   1.72  0.02  \n",
       "16000   1.72  0.02  \n",
       "17000   1.75  0.02  \n",
       "18000   1.78  0.02  \n",
       "19000   1.79  0.02  \n",
       "20000   1.81  0.02  \n",
       "21000   1.85  0.02  \n",
       "22000   1.85  0.02  \n",
       "23000   1.89  0.02  \n",
       "24000   1.90  0.02  \n",
       "25000   1.93  0.02  \n",
       "26000   1.94  0.02  \n",
       "27000   1.96  0.02  \n",
       "28000   1.98  0.02  \n",
       "29000   1.98  0.02  \n",
       "30000   1.99  0.02  \n",
       "...      ...   ...  \n",
       "225000  2.56  0.02  \n",
       "226000  2.56  0.02  \n",
       "227000  2.54  0.02  \n",
       "228000  2.56  0.02  \n",
       "229000  2.55  0.02  \n",
       "230000  2.54  0.02  \n",
       "231000  2.55  0.02  \n",
       "232000  2.56  0.02  \n",
       "233000  2.55  0.02  \n",
       "234000  2.54  0.02  \n",
       "235000  2.56  0.02  \n",
       "236000  2.55  0.02  \n",
       "237000  2.55  0.02  \n",
       "238000  2.56  0.02  \n",
       "239000  2.56  0.02  \n",
       "240000  2.56  0.02  \n",
       "241000  2.57  0.02  \n",
       "242000  2.56  0.02  \n",
       "243000  2.55  0.02  \n",
       "244000  2.55  0.02  \n",
       "245000  2.57  0.02  \n",
       "246000  2.57  0.02  \n",
       "247000  2.56  0.02  \n",
       "248000  2.56  0.02  \n",
       "249000  2.55  0.02  \n",
       "250000  2.56  0.02  \n",
       "251000  2.56  0.02  \n",
       "252000  2.56  0.02  \n",
       "253000  2.57  0.02  \n",
       "254000  2.57  0.02  \n",
       "\n",
       "[254 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 quality ... price - 'm 've $ made nice buy\n",
      "   1 pocket room small ipad mouse power charger carry perfect cord\n",
      "     10 sleeve protection inside inch air zipper padding snug nice protect\n",
      "     11 mac perfectly pro recommend love air price perfect protect book\n",
      "     12 cover keyboard hard screen easy apple nice bottom easily air\n",
      "   2 ! carry love perfect recommend room pockets books work school\n",
      "     20 item received ordered amazon arrived shipping smell days time bought\n",
      "     21 ! love ... perfect buy absolutely awesome cute recommend compliments\n",
      "     22 color picture love blue pink purple bright colors green black\n",
      "   3 pockets back strap compartment carry shoulder straps pack camera handle\n",
      "     30 ; & size pro big small work hp made dimensions\n",
      "     31 zipper strap handle months broke years year zippers started bought\n",
      "     32 bottom top back part corners plastic piece months cracked corner\n"
     ]
    }
   ],
   "source": [
    "if len(log_df) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "while epoch < config.epochs:\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = get_feed_dict(batch)\n",
    "\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, ppls_batch, global_step_log = \\\n",
    "        sess.run([opt, loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, topic_ppls, tf.train.get_global_step()], feed_dict = feed_dict)\n",
    "\n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if global_step_log%config.log_period==0:\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev = get_loss(sess, dev_batches)\n",
    "            \n",
    "#             if loss_dev < loss_min:\n",
    "#                 loss_min = loss_dev\n",
    "#                 saver.save(sess, config.modelpath, global_step=global_step_log)\n",
    "\n",
    "            clear_output()\n",
    "    \n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_reg_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_reg_dev],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "            \n",
    "            # visualize topic\n",
    "            print_topic_sample()\n",
    "\n",
    "            time_start = time.time()\n",
    "            \n",
    "    epoch += 1\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "    \n",
    "display(log_df)\n",
    "print_topic_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_topic_embeddings = tf.concat([tree_states_topic_embeddings[topic_idx] for topic_idx in topic_idxs], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strided_slice:0 : [[-3.493163   3.0747242 -2.4175787 -1.0355564 -2.62035   -3.5552971]\n",
      " [ 1.9786702  3.101428  -2.0053372 -2.6443536 -3.337943  -4.692594 ]\n",
      " [-2.2452912  6.99325   -3.5794563  2.213888  -2.6374078 -6.0528803]\n",
      " [ 3.2919352  3.965441  -4.178067  -1.3712263 -6.6184907 -5.151393 ]\n",
      " [ 2.2294612  2.6336317  2.7059941 -2.5288596 -2.3058517 -2.9022043]\n",
      " [-2.0506585  5.9458885  2.57303   -1.7188107  2.2250035 -3.2060401]\n",
      " [ 2.4546201  4.95021    6.123608  -3.2342207 -3.2869685 -3.0453057]\n",
      " [-1.8599154  4.1635885  1.9583844  2.0897892 -3.0990636 -2.7427351]\n",
      " [-4.18529    6.4663844  2.6379447  3.2429883 -1.5800574 -2.8184955]\n",
      " [-1.8693986  5.990916   3.9811754  3.894637  -5.7321553 -3.2843661]\n",
      " [-2.3744626  3.480035  -1.7458328  1.5136576 -3.524344  -2.6967387]\n",
      " [-3.893432   5.517426  -3.9145036  5.5148644 -2.5373993 -2.2495008]\n",
      " [-3.033714   3.8584168 -2.5547328  3.5370529 -5.4481564 -3.041221 ]]\n"
     ]
    }
   ],
   "source": [
    "debug_value([states_topic_embeddings[:, :6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_embeddings = tf.concat([tree_topic_embeddings[topic_idx] for topic_idx in topic_idxs], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strided_slice_1:0 : [[-10.839177     8.95292      2.357443    -6.6049685   -6.970813\n",
      "   -3.624138  ]\n",
      " [ -1.1442002   -1.6624496   -4.721718    -1.454425    -0.6388903\n",
      "    1.8796544 ]\n",
      " [ -6.0102506    1.7565602   -3.088316    -2.9084616   -0.99523634\n",
      "   -0.9548088 ]\n",
      " [ -6.1030507    0.6625058   -2.3521712   -2.7005482   -5.2328405\n",
      "    0.33589053]\n",
      " [ -1.9050764    0.33263054  -4.1853633   -0.5844636   -0.02884544\n",
      "    0.3071067 ]\n",
      " [ -2.893249    -0.8110833    0.74537545  -0.33640772   2.8377495\n",
      "   -3.3805513 ]\n",
      " [ -1.5380007   -0.7769438    4.0518785    0.3708176    0.05717204\n",
      "    0.41925895]\n",
      " [ -5.3317895    0.05119149   1.4404511   -3.8187716   -0.5571164\n",
      "   -0.68577427]\n",
      " [ -6.2956944    3.396152     5.340511    -1.6320771    1.1005126\n",
      "   -4.227513  ]\n",
      " [ -4.5166945    2.1967628    4.640637     1.2401124   -1.8405858\n",
      "   -4.9873934 ]\n",
      " [ -4.1608796   -2.8953247   -2.101975    -4.588814    -1.7345304\n",
      "    1.3300496 ]\n",
      " [ -4.5042114    0.79118836   0.38203987  -4.1397347   -2.5799642\n",
      "    1.3090005 ]\n",
      " [ -4.157875    -0.85255265   2.4345424   -2.1628652   -3.8708222\n",
      "   -0.06296726]]\n"
     ]
    }
   ],
   "source": [
    "debug_value([topic_embeddings[:, :6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5717416"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_topics_bow, = debug_value([topic_bow], return_value=True)\n",
    "np.max(_topics_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x1440 with 13 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_topics_bow, = debug_value([topic_bow], return_value=True)\n",
    "\n",
    "plt.figure(figsize=(12, 20))\n",
    "    \n",
    "_topic_bow = _topics_bow[0]\n",
    "plt.subplot(5,3,2)\n",
    "plt.ylim([0, 0.1])\n",
    "plt.bar(bow_idxs, _topic_bow)\n",
    "\n",
    "for i in range(1, len(topic_idxs)):\n",
    "    _topic_bow = _topics_bow[i]\n",
    "    plt.subplot(5,3,i+3)\n",
    "    plt.ylim([0, 0.1])\n",
    "#     plt.axis('off')\n",
    "    plt.bar(bow_idxs, _topic_bow)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15302156 0.14417332 0.11149017 0.08513445 0.10930682 0.06587461\n",
      " 0.08745968 0.06420095 0.03602182 0.04447792 0.0287645  0.02760366\n",
      " 0.04247059]\n"
     ]
    }
   ],
   "source": [
    "_prob_topics = []\n",
    "for ct, batch in dev_batches:\n",
    "    feed_dict = get_feed_dict(batch)\n",
    "    _prob_topic, = sess.run([prob_topic], feed_dict = feed_dict)\n",
    "    _prob_topics.append(_prob_topic)\n",
    "    \n",
    "_prob_topics = np.concatenate(_prob_topics, 0)\n",
    "_prob_topic_mean = np.mean(_prob_topics, 0)\n",
    "\n",
    "print(_prob_topic_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip_by_value:0 : [[1.00000000e+00 1.92476332e-01 2.53611594e-01 2.78632998e-01\n",
      "  1.96983725e-01 2.07820192e-01 1.99194491e-01 2.94495434e-01\n",
      "  8.01934823e-02 6.89710006e-02 3.74173820e-02 1.90637663e-01\n",
      "  2.27430031e-01]\n",
      " [1.92476332e-01 9.99999881e-01 3.44869494e-01 4.40126091e-01\n",
      "  2.50804842e-01 2.48355061e-01 1.73624530e-01 1.08319670e-01\n",
      "  1.57099571e-02 3.91810499e-02 2.95992754e-02 1.03385106e-01\n",
      "  8.82391930e-02]\n",
      " [2.53611594e-01 3.44869494e-01 9.99999702e-01 4.69184726e-01\n",
      "  1.07341796e-01 2.91121244e-01 9.69141424e-02 1.74878448e-01\n",
      "  5.94107866e-01 8.28888640e-02 1.76144112e-02 1.15359843e-01\n",
      "  4.90007550e-02]\n",
      " [2.78632998e-01 4.40126091e-01 4.69184726e-01 1.00000000e+00\n",
      "  1.54174998e-01 9.72025022e-02 1.45996124e-01 8.63274783e-02\n",
      "  1.62842497e-02 3.40978168e-02 2.71125622e-02 3.38478565e-01\n",
      "  2.19821543e-01]\n",
      " [1.96983725e-01 2.50804842e-01 1.07341796e-01 1.54174998e-01\n",
      "  9.99999881e-01 2.73282319e-01 1.49612308e-01 1.01003408e-01\n",
      "  5.90104423e-03 2.67267805e-02 2.22556535e-02 9.94952396e-02\n",
      "  6.43259510e-02]\n",
      " [2.07820192e-01 2.48355061e-01 2.91121244e-01 9.72025022e-02\n",
      "  2.73282319e-01 9.99999881e-01 2.95273930e-01 1.92839235e-01\n",
      "  1.29479244e-01 1.10494375e-01 1.91930607e-02 3.52499969e-02\n",
      "  3.55085805e-02]\n",
      " [1.99194491e-01 1.73624530e-01 9.69141424e-02 1.45996124e-01\n",
      "  1.49612308e-01 2.95273930e-01 9.99999821e-01 7.03338981e-02\n",
      "  3.87225226e-02 8.86352211e-02 1.29326507e-02 4.05265987e-02\n",
      "  2.08742768e-01]\n",
      " [2.94495434e-01 1.08319670e-01 1.74878448e-01 8.63274783e-02\n",
      "  1.01003408e-01 1.92839235e-01 7.03338981e-02 9.99999881e-01\n",
      "  2.66987793e-02 8.31765458e-02 3.20685804e-02 1.69042662e-01\n",
      "  1.59109250e-01]\n",
      " [8.01934823e-02 1.57099571e-02 5.94107866e-01 1.62842497e-02\n",
      "  5.90104423e-03 1.29479244e-01 3.87225226e-02 2.66987793e-02\n",
      "  9.99999940e-01 3.77519876e-02 9.11777548e-04 7.33162370e-03\n",
      "  6.17904123e-03]\n",
      " [6.89710006e-02 3.91810499e-02 8.28888640e-02 3.40978168e-02\n",
      "  2.67267805e-02 1.10494375e-01 8.86352211e-02 8.31765458e-02\n",
      "  3.77519876e-02 9.99999642e-01 3.73813114e-03 8.99754371e-03\n",
      "  2.40850002e-02]\n",
      " [3.74173820e-02 2.95992754e-02 1.76144112e-02 2.71125622e-02\n",
      "  2.22556535e-02 1.91930607e-02 1.29326507e-02 3.20685804e-02\n",
      "  9.11777548e-04 3.73813114e-03 9.99999881e-01 2.99275853e-02\n",
      "  2.88920589e-02]\n",
      " [1.90637663e-01 1.03385106e-01 1.15359843e-01 3.38478565e-01\n",
      "  9.94952396e-02 3.52499969e-02 4.05265987e-02 1.69042662e-01\n",
      "  7.33162370e-03 8.99754371e-03 2.99275853e-02 9.99999881e-01\n",
      "  3.10155302e-01]\n",
      " [2.27430031e-01 8.82391930e-02 4.90007550e-02 2.19821543e-01\n",
      "  6.43259510e-02 3.55085805e-02 2.08742768e-01 1.59109250e-01\n",
      "  6.17904123e-03 2.40850002e-02 2.88920589e-02 3.10155302e-01\n",
      "  9.99999881e-01]]\n"
     ]
    }
   ],
   "source": [
    "debug_value([topic_dots])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mul_2:0 : [[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 1.4210855e-14 1.1893497e-01 1.9371098e-01 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 1.1733151e-02 2.4680275e-04 1.5351546e-03\n",
      "  8.7611709e-04 1.0688480e-02 7.7861552e-03]\n",
      " [0.0000000e+00 1.1893497e-01 8.8817842e-14 2.2013430e-01 1.1522261e-02\n",
      "  8.4751576e-02 9.3923509e-03 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  3.1026747e-04 1.3307894e-02 2.4010739e-03]\n",
      " [0.0000000e+00 1.9371098e-01 2.2013430e-01 0.0000000e+00 2.3769930e-02\n",
      "  9.4483262e-03 2.1314869e-02 7.4524335e-03 2.6517679e-04 1.1626611e-03\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 1.1522261e-02 2.3769930e-02 1.4210855e-14\n",
      "  7.4683227e-02 2.2383843e-02 1.0201689e-02 3.4822322e-05 7.1432081e-04\n",
      "  4.9531413e-04 9.8993024e-03 4.1378280e-03]\n",
      " [0.0000000e+00 0.0000000e+00 8.4751576e-02 9.4483262e-03 7.4683227e-02\n",
      "  1.4210855e-14 8.7186694e-02 3.7186969e-02 1.6764876e-02 1.2209007e-02\n",
      "  3.6837358e-04 1.2425623e-03 1.2608593e-03]\n",
      " [0.0000000e+00 0.0000000e+00 9.3923509e-03 2.1314869e-02 2.2383843e-02\n",
      "  8.7186694e-02 3.1974423e-14 4.9468572e-03 1.4994338e-03 7.8562023e-03\n",
      "  1.6725346e-04 1.6424052e-03 4.3573543e-02]\n",
      " [0.0000000e+00 1.1733151e-02 0.0000000e+00 7.4524335e-03 1.0201689e-02\n",
      "  3.7186969e-02 4.9468572e-03 1.4210855e-14 7.1282481e-04 6.9183377e-03\n",
      "  1.0283939e-03 2.8575422e-02 2.5315754e-02]\n",
      " [0.0000000e+00 2.4680275e-04 0.0000000e+00 2.6517679e-04 3.4822322e-05\n",
      "  1.6764876e-02 1.4994338e-03 7.1282481e-04 3.5527137e-15 1.4252126e-03\n",
      "  8.3133830e-07 5.3752705e-05 3.8180551e-05]\n",
      " [0.0000000e+00 1.5351546e-03 0.0000000e+00 1.1626611e-03 7.1432081e-04\n",
      "  1.2209007e-02 7.8562023e-03 6.9183377e-03 1.4252126e-03 1.2789769e-13\n",
      "  1.3973625e-05 8.0955790e-05 5.8008725e-04]\n",
      " [0.0000000e+00 8.7611709e-04 3.1026747e-04 0.0000000e+00 4.9531413e-04\n",
      "  3.6837358e-04 1.6725346e-04 1.0283939e-03 8.3133830e-07 1.3973625e-05\n",
      "  1.4210855e-14 8.9566037e-04 8.3475106e-04]\n",
      " [0.0000000e+00 1.0688480e-02 1.3307894e-02 0.0000000e+00 9.8993024e-03\n",
      "  1.2425623e-03 1.6424052e-03 2.8575422e-02 5.3752705e-05 8.0955790e-05\n",
      "  8.9566037e-04 1.4210855e-14 9.6196309e-02]\n",
      " [0.0000000e+00 7.7861552e-03 2.4010739e-03 0.0000000e+00 4.1378280e-03\n",
      "  1.2608593e-03 4.3573543e-02 2.5315754e-02 3.8180551e-05 5.8008725e-04\n",
      "  8.3475106e-04 9.6196309e-02 1.4210855e-14]]\n"
     ]
    }
   ],
   "source": [
    "debug_value([topic_losses_reg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_mask_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_topic_bow, = debug_value([topic_bow], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bow_idxs, _topic_bow[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bow_idxs, _topic_bow[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bow_idxs, _topic_bow[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bow_idxs, _topic_bow[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(_topic_bow, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bow_idxs, _topic_bow[-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bow_idxs, _topic_bow[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bow_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_value([prob_topic[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_value([tf.exp(-tf.divide(topic_losses_recon, n_bow))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_shape([bow, hidden_bow, latents_bow, prob_topic, bow_embeddings, topic_embeddings, topic_bow, prob_bow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_shape([topic_losses_recon, topic_loss_recon, n_bow, ppls, topic_embeddings_norm, tf.expand_dims(topic_angles_mean, -1), topic_angles_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_value([tf.reduce_sum(tf.square(topic_embeddings_norm), 1)], return_value=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_value([tf.reduce_sum(prob_topic, -1), tf.reduce_sum(topic_bow, -1), tf.reduce_sum(tf.exp(prob_bow), 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_bow = tf.exp(0.5 * logvars_bow)\n",
    "dist_bow = tfd.Normal(means_bow, sigma_bow)\n",
    "dist_std = tfd.Normal(0., 1.)\n",
    "topic_loss_kl_tmp = tf.reduce_mean(tf.reduce_sum(tfd.kl_divergence(dist_bow, dist_std), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_value([topic_loss_recon, topic_loss_kl, topic_loss_kl_tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_logvars, _means, _kl_losses, _latents, _output_logits = sess.run([logvars, means, kl_losses, latents, output_logits], feed_dict=feed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_logvars.shape, _means.shape, _kl_losses.shape, _latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_output_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_output_logits, _dec_target_idxs_do, _dec_mask_tokens_do, _recon_loss, _kl_losses, _ = sess.run([output_logits, dec_target_idxs_do, dec_mask_tokens_do, recon_loss, kl_losses, opt], feed_dict=feed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_max(output_logits, 2).eval(session=sess, feed_dict=feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_output_logits.shape, _dec_target_idxs_do.shape, _dec_mask_tokens_do.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_logits = np.exp(_output_logits) / np.sum(np.exp(_output_logits), 2)[:, :, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_idxs = _dec_target_idxs_do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_losses = np.array([[-np.log(_logits[i, j, _idxs[i, j]]) for j in range(_idxs.shape[1])] for i in range(_idxs.shape[0])]) * _dec_mask_tokens_do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(_losses)/np.sum(_dec_mask_tokens_do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_recon_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_kl_losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
