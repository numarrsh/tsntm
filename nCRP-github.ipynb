{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pdb\n",
    "\n",
    "from math import log\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "\n",
    "import tensorflow as tf\n",
    "import _pickle as cPickle\n",
    "\n",
    "from hlda import HierarchicalLDA\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/bags/instances.pkl', 'path of data')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "config = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus_raw = [[[bow_idxs[bow_index]]*int(instance.bow[bow_index]) for bow_index in np.where(instance.bow > 0)[0]] for instance in instances_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = [[idx for idxs in doc for idx in idxs] for doc in train_corpus_raw]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCRPNode(object):\n",
    "\n",
    "    # class variable to keep track of total nodes created so far\n",
    "    total_nodes = 0\n",
    "    last_node_id = 0\n",
    "\n",
    "    def __init__(self, num_levels, vocab, parent=None, level=0,\n",
    "                 random_state=None):\n",
    "\n",
    "        self.node_id = NCRPNode.last_node_id\n",
    "        NCRPNode.last_node_id += 1\n",
    "\n",
    "        self.customers = 0\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.level = level\n",
    "        self.total_words = 0\n",
    "        self.num_levels = num_levels\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.word_counts = np.zeros(len(vocab))\n",
    "\n",
    "        if random_state is None:\n",
    "            self.random_state = RandomState()\n",
    "        else:\n",
    "            self.random_state = random_state\n",
    "\n",
    "    def __repr__(self):\n",
    "        parent_id = None\n",
    "        if self.parent is not None:\n",
    "            parent_id = self.parent.node_id\n",
    "        return 'Node=%d level=%d customers=%d total_words=%d parent=%s' % (self.node_id,\n",
    "            self.level, self.customers, self.total_words, parent_id)\n",
    "\n",
    "    def add_child(self):\n",
    "        ''' Adds a child to the next level of this node '''\n",
    "        node = NCRPNode(self.num_levels, self.vocab, parent=self, level=self.level+1)\n",
    "        self.children.append(node)\n",
    "        NCRPNode.total_nodes += 1\n",
    "        return node\n",
    "\n",
    "    def is_leaf(self):\n",
    "        ''' Check if this node is a leaf node '''\n",
    "        return self.level == self.num_levels-1\n",
    "\n",
    "    def get_new_leaf(self):\n",
    "        ''' Keeps adding nodes along the path until a leaf node is generated'''\n",
    "        node = self\n",
    "        for l in range(self.level, self.num_levels-1):\n",
    "            node = node.add_child()\n",
    "        return node\n",
    "\n",
    "    def drop_path(self):\n",
    "        ''' Removes a document from a path starting from this node '''\n",
    "        node = self\n",
    "        node.customers -= 1\n",
    "        if node.customers == 0:\n",
    "            node.parent.remove(node)\n",
    "        for level in range(1, self.num_levels): # skip the root\n",
    "            node = node.parent\n",
    "            node.customers -= 1\n",
    "            if node.customers == 0:\n",
    "                node.parent.remove(node)\n",
    "\n",
    "    def remove(self, node):\n",
    "        ''' Removes a child node '''\n",
    "        self.children.remove(node)\n",
    "        NCRPNode.total_nodes -= 1\n",
    "\n",
    "    def add_path(self):\n",
    "        ''' Adds a document to a path starting from this node '''\n",
    "        node = self\n",
    "        node.customers += 1\n",
    "        for level in range(1, self.num_levels):\n",
    "            node = node.parent\n",
    "            node.customers += 1\n",
    "\n",
    "    def select(self, gamma):\n",
    "        ''' Selects an existing child or create a new one according to the CRP '''\n",
    "\n",
    "        weights = np.zeros(len(self.children)+1)\n",
    "        weights[0] = float(gamma) / (gamma+self.customers)\n",
    "        i = 1\n",
    "        for child in self.children:\n",
    "            weights[i] = float(child.customers) / (gamma + self.customers)\n",
    "            i += 1\n",
    "\n",
    "        choice = self.random_state.multinomial(1, weights).argmax()\n",
    "        if choice == 0:\n",
    "            return self.add_child()\n",
    "        else:\n",
    "            return self.children[choice-1]\n",
    "\n",
    "    def get_top_words(self, n_words, with_weight):\n",
    "        ''' Get the top n words in this node '''\n",
    "        pos = np.argsort(self.word_counts)[::-1]\n",
    "\n",
    "        sorted_vocab = np.array(list(self.vocab.values()))[pos]\n",
    "        sorted_vocab = sorted_vocab[:n_words]\n",
    "        sorted_weights = self.word_counts[pos]\n",
    "        sorted_weights = sorted_weights[:n_words]\n",
    "\n",
    "        output = ''\n",
    "        for word, weight in zip(sorted_vocab, sorted_weights):\n",
    "            if with_weight:\n",
    "                output += '%s (%d), ' % (word, weight)\n",
    "            else:\n",
    "                output += '%s, ' % word\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalLDA(object):\n",
    "\n",
    "    def __init__(self, corpus, vocab,\n",
    "                 alpha=10.0, gamma=1.0, eta=0.1,\n",
    "                 seed=0, verbose=True, num_levels=3):\n",
    "\n",
    "        NCRPNode.total_nodes = 0\n",
    "        NCRPNode.last_node_id = 0\n",
    "\n",
    "        self.corpus = corpus\n",
    "        self.vocab = vocab\n",
    "        self.alpha = alpha  # smoothing on doc-topic distributions\n",
    "        self.gamma = gamma  # \"imaginary\" customers at the next, as yet unused table\n",
    "        self.eta = eta      # smoothing on topic-word distributions\n",
    "\n",
    "        self.seed = seed\n",
    "        self.random_state = RandomState(seed)\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.num_levels = num_levels\n",
    "        self.num_documents = len(corpus)\n",
    "        self.num_types = len(vocab)\n",
    "        self.eta_sum = eta * self.num_types\n",
    "\n",
    "        # if self.verbose:\n",
    "        #     for d in range(len(self.corpus)):\n",
    "        #         doc = self.corpus[d]\n",
    "        #         words = ' '.join([self.vocab[n] for n in doc])\n",
    "        #         print 'doc_%d = %s' % (d, words)\n",
    "\n",
    "        # initialise a single path\n",
    "        path = np.zeros(self.num_levels, dtype=np.object)\n",
    "\n",
    "        # initialize and fill the topic pointer arrays for\n",
    "        # every document. Set everything to the single path that\n",
    "        # we added earlier.\n",
    "        self.root_node = NCRPNode(self.num_levels, self.vocab)\n",
    "        self.document_leaves = {}                                   # currently selected path (ie leaf node) through the NCRP tree\n",
    "        self.levels = np.zeros(self.num_documents, dtype=np.object) # indexed < doc, token >\n",
    "        for d in range(len(self.corpus)):\n",
    "\n",
    "            # populate nodes into the path of this document\n",
    "            doc = self.corpus[d]\n",
    "            doc_len = len(doc)\n",
    "            path[0] = self.root_node\n",
    "            self.root_node.customers += 1 # always add to the root node first\n",
    "            for level in range(1, self.num_levels):\n",
    "                # at each level, a node is selected by its parent node based on the CRP prior\n",
    "                parent_node = path[level-1]\n",
    "                level_node = parent_node.select(self.gamma)\n",
    "                level_node.customers += 1\n",
    "                path[level] = level_node\n",
    "\n",
    "            # set the leaf node for this document\n",
    "            leaf_node = path[self.num_levels-1]\n",
    "            self.document_leaves[d] = leaf_node\n",
    "\n",
    "            # randomly assign each word in the document to a level (node) along the path\n",
    "            self.levels[d] = np.zeros(doc_len, dtype=np.int)\n",
    "            for n in range(doc_len):\n",
    "                w = doc[n]\n",
    "                random_level = self.random_state.randint(self.num_levels)\n",
    "                random_node = path[random_level]\n",
    "                random_node.word_counts[w] += 1\n",
    "                random_node.total_words += 1\n",
    "                self.levels[d][n] = random_level\n",
    "\n",
    "    def estimate(self, num_samples, display_topics=50, n_words=5, with_weights=True):\n",
    "\n",
    "        print('HierarchicalLDA sampling\\n')\n",
    "        for s in range(num_samples):\n",
    "\n",
    "            sys.stdout.write('.')\n",
    "\n",
    "            for d in range(len(self.corpus)):\n",
    "                self.sample_path(d)\n",
    "\n",
    "            for d in range(len(self.corpus)):\n",
    "                self.sample_topics(d)\n",
    "\n",
    "            if (s > 0) and ((s+1) % display_topics == 0):\n",
    "                print(\" %d\" % (s+1))\n",
    "                self.print_nodes(n_words, with_weights)\n",
    "                print('')\n",
    "\n",
    "    def sample_path(self, d):\n",
    "\n",
    "        # define a path starting from the leaf node of this doc\n",
    "        path = np.zeros(self.num_levels, dtype=np.object)\n",
    "        node = self.document_leaves[d]\n",
    "        for level in range(self.num_levels-1, -1, -1): # e.g. [3, 2, 1, 0] for num_levels = 4\n",
    "            path[level] = node\n",
    "            node = node.parent\n",
    "\n",
    "        # remove this document from the path, deleting empty nodes if necessary\n",
    "        self.document_leaves[d].drop_path()\n",
    "\n",
    "        ############################################################\n",
    "        # calculates the prior p(c_d | c_{-d}) in eq. (4)\n",
    "        ############################################################\n",
    "\n",
    "        node_weights = {}\n",
    "        self.calculate_ncrp_prior(node_weights, self.root_node, 0.0)\n",
    "        pdb.set_trace()\n",
    "\n",
    "        ############################################################\n",
    "        # calculates the likelihood p(w_d | c, w_{-d}, z) in eq. (4)\n",
    "        ############################################################\n",
    "\n",
    "        level_word_counts = {level: defaultdict(int) for level in range(self.num_levels)}\n",
    "        doc_levels = self.levels[d]\n",
    "        doc_words = self.corpus[d]\n",
    "\n",
    "        # remove words in doc from path\n",
    "        for n, word in enumerate(doc_words): # for each word in the doc\n",
    "            # count the word at each level\n",
    "            level = doc_levels[n]\n",
    "            level_word_counts[level][word] += 1\n",
    "\n",
    "            # remove word count from the node at that level\n",
    "            level_node = path[level]\n",
    "            level_node.word_counts[word] -= 1\n",
    "            level_node.total_words -= 1\n",
    "            assert level_node.word_counts[word] >= 0\n",
    "            assert level_node.total_words >= 0\n",
    "\n",
    "        self.calculate_doc_likelihood(node_weights, level_word_counts)\n",
    "\n",
    "        ############################################################\n",
    "        # pick a new path\n",
    "        ############################################################\n",
    "\n",
    "        nodes = np.array(list(node_weights.keys()))\n",
    "        weights = np.array([node_weights[node] for node in nodes])\n",
    "        weights = np.exp(weights - np.max(weights)) # normalise so the largest weight is 1\n",
    "        weights = weights / np.sum(weights)\n",
    "\n",
    "        choice = self.random_state.multinomial(1, weights).argmax()\n",
    "        node = nodes[choice]\n",
    "\n",
    "        # if we picked an internal node, we need to add a new path to the leaf\n",
    "        if not node.is_leaf():\n",
    "            node = node.get_new_leaf()\n",
    "\n",
    "        # add the doc back to the path\n",
    "        node.add_path()                     # add a customer to the path\n",
    "        self.document_leaves[d] = node      # store the leaf node for this doc\n",
    "\n",
    "        # add the words\n",
    "        for level in range(self.num_levels-1, -1, -1): # e.g. [3, 2, 1, 0] for num_levels = 4\n",
    "            word_counts = level_word_counts[level]\n",
    "            for w in word_counts:\n",
    "                node.word_counts[w] += word_counts[w]\n",
    "                node.total_words += word_counts[w]\n",
    "            node = node.parent\n",
    "\n",
    "    def calculate_ncrp_prior(self, node_weights, node, weight):\n",
    "        ''' Calculates the prior on the path according to the nested CRP '''\n",
    "\n",
    "        for child in node.children:\n",
    "            child_weight = log( float(child.customers) / (node.customers + self.gamma) )\n",
    "            self.calculate_ncrp_prior(node_weights, child, weight + child_weight)\n",
    "\n",
    "        node_weights[node] = weight + log( self.gamma / (node.customers + self.gamma)) ## Why?\n",
    "\n",
    "    def calculate_doc_likelihood(self, node_weights, level_word_counts):\n",
    "\n",
    "        # calculate the weight for a new path at a given level\n",
    "        new_topic_weights = np.zeros(self.num_levels)\n",
    "        for level in range(1, self.num_levels):  # skip the root\n",
    "\n",
    "            word_counts = level_word_counts[level]\n",
    "            total_tokens = 0\n",
    "\n",
    "            for w in word_counts:\n",
    "                count = word_counts[w]\n",
    "                for i in range(count):  # why ?????????\n",
    "                    new_topic_weights[level] += log((self.eta + i) / (self.eta_sum + total_tokens))\n",
    "                    total_tokens += 1\n",
    "\n",
    "        self.calculate_word_likelihood(node_weights, self.root_node, 0.0, level_word_counts, new_topic_weights, 0)\n",
    "\n",
    "    def calculate_word_likelihood(self, node_weights, node, weight, level_word_counts, new_topic_weights, level):\n",
    "\n",
    "        # first calculate the likelihood of the words at this level, given this topic\n",
    "        node_weight = 0.0\n",
    "        word_counts = level_word_counts[level]\n",
    "        total_words = 0\n",
    "\n",
    "        for w in word_counts:\n",
    "            count = word_counts[w]\n",
    "            for i in range(count): # why ?????????\n",
    "                node_weight += log( (self.eta + node.word_counts[w] + i) /\n",
    "                                    (self.eta_sum + node.total_words + total_words) )\n",
    "                total_words += 1\n",
    "\n",
    "        # propagate that weight to the child nodes\n",
    "        for child in node.children:\n",
    "            self.calculate_word_likelihood(node_weights, child, weight + node_weight,\n",
    "                                           level_word_counts, new_topic_weights, level+1)\n",
    "\n",
    "        # finally if this is an internal node, add the weight of a new path\n",
    "        level += 1\n",
    "        while level < self.num_levels:\n",
    "            node_weight += new_topic_weights[level]\n",
    "            level += 1\n",
    "\n",
    "        node_weights[node] += node_weight\n",
    "\n",
    "    def sample_topics(self, d):\n",
    "\n",
    "        doc = self.corpus[d]\n",
    "\n",
    "        # initialise level counts\n",
    "        doc_levels = self.levels[d]\n",
    "        level_counts = np.zeros(self.num_levels, dtype=np.int)\n",
    "        for c in doc_levels:\n",
    "            level_counts[c] += 1\n",
    "\n",
    "        # get the leaf node and populate the path\n",
    "        path = np.zeros(self.num_levels, dtype=np.object)\n",
    "        node = self.document_leaves[d]\n",
    "        for level in range(self.num_levels-1, -1, -1): # e.g. [3, 2, 1, 0] for num_levels = 4\n",
    "            path[level] = node\n",
    "            node = node.parent\n",
    "\n",
    "        # sample a new level for each word\n",
    "        level_weights = np.zeros(self.num_levels)\n",
    "        for n in range(len(doc)):\n",
    "\n",
    "            w = doc[n]\n",
    "            word_level = doc_levels[n]\n",
    "\n",
    "            # remove from model\n",
    "            level_counts[word_level] -= 1\n",
    "            node = path[word_level]\n",
    "            node.word_counts[w] -= 1\n",
    "            node.total_words -= 1\n",
    "\n",
    "            # pick new level\n",
    "            for level in range(self.num_levels):\n",
    "                level_weights[level] = (self.alpha + level_counts[level]) *                     \\\n",
    "                    (self.eta + path[level].word_counts[w]) /                                   \\\n",
    "                    (self.eta_sum + path[level].total_words)\n",
    "            level_weights = level_weights / np.sum(level_weights)\n",
    "            level = self.random_state.multinomial(1, level_weights).argmax()\n",
    "\n",
    "            # put the word back into the model\n",
    "            doc_levels[n] = level\n",
    "            level_counts[level] += 1\n",
    "            node = path[level]\n",
    "            node.word_counts[w] += 1\n",
    "            node.total_words += 1\n",
    "\n",
    "    def print_nodes(self, n_words, with_weights):\n",
    "        self.print_node(self.root_node, 0, n_words, with_weights)\n",
    "\n",
    "    def print_node(self, node, indent, n_words, with_weights):\n",
    "        out = '    ' * indent\n",
    "        out += 'topic=%d level=%d (documents=%d): ' % (node.node_id, node.level, node.customers)\n",
    "        out += node.get_top_words(n_words, with_weights)\n",
    "        print(out)\n",
    "        for child in node.children:\n",
    "            self.print_node(child, indent+1, n_words, with_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hlda = HierarchicalLDA(train_corpus, vocab=idx_to_word, alpha=1, gamma=1.0, eta=1.0, num_levels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HierarchicalLDA sampling\n",
      "\n",
      ".> <ipython-input-13-752f82b696ff>(110)sample_path()\n",
      "-> level_word_counts = {level: defaultdict(int) for level in range(self.num_levels)}\n",
      "(Pdb) l\n",
      "105  \t\n",
      "106  \t        ############################################################\n",
      "107  \t        # calculates the likelihood p(w_d | c, w_{-d}, z) in eq. (4)\n",
      "108  \t        ############################################################\n",
      "109  \t\n",
      "110  ->\t        level_word_counts = {level: defaultdict(int) for level in range(self.num_levels)}\n",
      "111  \t        doc_levels = self.levels[d]\n",
      "112  \t        doc_words = self.corpus[d]\n",
      "113  \t\n",
      "114  \t        # remove words in doc from path\n",
      "115  \t        for n, word in enumerate(doc_words): # for each word in the doc\n",
      "(Pdb) node_weights\n",
      "{Node=2 level=2 customers=1732 total_words=10233 parent=1: -10.37237728297403, Node=6 level=2 customers=7462 total_words=44555 parent=1: -10.37193408573109, Node=9 level=2 customers=926 total_words=5632 parent=1: -10.372879413300481, Node=11 level=2 customers=713 total_words=4104 parent=1: -10.373201624306093, Node=21 level=2 customers=8 total_words=51 parent=1: -10.489583118037189, Node=27 level=2 customers=17 total_words=84 parent=1: -10.428958496220755, Node=34 level=2 customers=2 total_words=6 parent=1: -10.777265190488968, Node=42 level=2 customers=20 total_words=112 parent=1: -10.420590246550237, Node=43 level=2 customers=7 total_words=52 parent=1: -10.505331475005327, Node=55 level=2 customers=4 total_words=12 parent=1: -10.594943633695014, Node=57 level=2 customers=9 total_words=51 parent=1: -10.47716059803863, Node=1 level=1 customers=10900 total_words=64700 parent=0: -10.371800082380805, Node=4 level=2 customers=3059 total_words=18039 parent=3: -10.372163473400779, Node=5 level=2 customers=3873 total_words=22925 parent=3: -10.372094787057446, Node=16 level=2 customers=173 total_words=1037 parent=3: -10.377600327322007, Node=19 level=2 customers=390 total_words=2218 parent=3: -10.374397443466929, Node=20 level=2 customers=85 total_words=436 parent=3: -10.383532662368447, Node=26 level=2 customers=146 total_words=952 parent=3: -10.378662587675656, Node=28 level=2 customers=5 total_words=18 parent=3: -10.55415817939921, Node=29 level=2 customers=9 total_words=62 parent=3: -10.477197138263081, Node=30 level=2 customers=40 total_words=204 parent=3: -10.396529235195626, Node=54 level=2 customers=15 total_words=95 parent=3: -10.436375143742827, Node=3 level=1 customers=7795 total_words=46063 parent=0: -10.371836622605256, Node=8 level=2 customers=6112 total_words=35818 parent=7: -10.371949803048398, Node=10 level=2 customers=2247 total_words=13347 parent=7: -10.372231142694282, Node=12 level=2 customers=3012 total_words=18096 parent=7: -10.372118154076547, Node=15 level=2 customers=1310 total_words=7579 parent=7: -10.372549271434533, Node=22 level=2 customers=44 total_words=234 parent=7: -10.394259059718088, Node=37 level=2 customers=13 total_words=72 parent=7: -10.445894176019753, Node=38 level=2 customers=44 total_words=254 parent=7: -10.394259059718088, Node=46 level=2 customers=6 total_words=35 parent=7: -10.525936883693289, Node=51 level=2 customers=26 total_words=153 parent=7: -10.409526531848877, Node=53 level=2 customers=9 total_words=44 parent=7: -10.477146719523857, Node=58 level=2 customers=4 total_words=22 parent=7: -10.59492975518024, Node=60 level=2 customers=12 total_words=71 parent=7: -10.451828911539566, Node=71 level=2 customers=3 total_words=12 parent=7: -10.659468276317812, Node=77 level=2 customers=1 total_words=7 parent=7: -11.064933384425975, Node=7 level=1 customers=12843 total_words=75946 parent=0: -10.37178620386603, Node=14 level=2 customers=118 total_words=737 parent=13: -10.385185006145502, Node=25 level=2 customers=74 total_words=434 parent=13: -10.390169157831778, Node=65 level=2 customers=1 total_words=10 parent=13: -11.069893318059583, Node=69 level=2 customers=5 total_words=16 parent=13: -10.559067694293592, Node=13 level=1 customers=198 total_words=1212 parent=0: -10.376746137499637, Node=18 level=2 customers=62 total_words=380 parent=17: -10.395805895048742, Node=31 level=2 customers=37 total_words=227 parent=17: -10.406473800784461, Node=41 level=2 customers=23 total_words=130 parent=17: -10.422365168121097, Node=68 level=2 customers=1 total_words=3 parent=17: -11.072952734262246, Node=17 level=1 customers=123 total_words=757 parent=0: -10.379805553702301, Node=24 level=2 customers=13 total_words=95 parent=23: -10.519924287777126, Node=23 level=1 customers=13 total_words=81 parent=0: -10.445816315623404, Node=33 level=2 customers=2 total_words=12 parent=32: -11.064855524029626, Node=52 level=2 customers=1 total_words=3 parent=32: -11.352537596481408, Node=32 level=1 customers=3 total_words=19 parent=0: -10.659390415921463, Node=36 level=2 customers=5 total_words=29 parent=35: -10.60809712153391, Node=50 level=2 customers=6 total_words=28 parent=35: -10.579926244567215, Node=64 level=2 customers=3 total_words=13 parent=35: -10.713457637191738, Node=66 level=2 customers=3 total_words=12 parent=35: -10.713457637191738, Node=76 level=2 customers=1 total_words=4 parent=35: -11.118922745299903, Node=35 level=1 customers=18 total_words=91 parent=0: -10.425775564739958, Node=40 level=2 customers=4 total_words=14 parent=39: -10.817995446098099, Node=39 level=1 customers=4 total_words=9 parent=0: -10.59485189478389, Node=45 level=2 customers=2 total_words=14 parent=44: -10.894956487234229, Node=56 level=2 customers=2 total_words=2 parent=44: -10.894956487234229, Node=74 level=2 customers=4 total_words=12 parent=44: -10.712634930440274, Node=44 level=1 customers=8 total_words=50 parent=0: -10.489491379126065, Node=48 level=2 customers=1 total_words=6 parent=47: -11.125480145846062, Node=49 level=2 customers=6 total_words=16 parent=47: -10.586483645113375, Node=59 level=2 customers=4 total_words=14 parent=47: -10.655476516600325, Node=70 level=2 customers=5 total_words=20 parent=47: -10.61465452208007, Node=47 level=1 customers=16 total_words=72 parent=0: -10.432332965286117, Node=62 level=2 customers=7 total_words=47 parent=61: -10.556533030481754, Node=63 level=2 customers=4 total_words=26 parent=61: -10.64614518917144, Node=67 level=2 customers=7 total_words=32 parent=61: -10.556533030481754, Node=75 level=2 customers=1 total_words=4 parent=61: -11.116148818417177, Node=61 level=1 customers=19 total_words=123 parent=0: -10.423001637857231, Node=73 level=2 customers=2 total_words=14 parent=72: -11.182638559686009, Node=72 level=1 customers=2 total_words=8 parent=0: -10.777173451577845, Node=0 level=0 customers=31942 total_words=190294 parent=None: -10.371708343469681}\n",
      "(Pdb) node_weights.values()\n",
      "dict_values([-10.37237728297403, -10.37193408573109, -10.372879413300481, -10.373201624306093, -10.489583118037189, -10.428958496220755, -10.777265190488968, -10.420590246550237, -10.505331475005327, -10.594943633695014, -10.47716059803863, -10.371800082380805, -10.372163473400779, -10.372094787057446, -10.377600327322007, -10.374397443466929, -10.383532662368447, -10.378662587675656, -10.55415817939921, -10.477197138263081, -10.396529235195626, -10.436375143742827, -10.371836622605256, -10.371949803048398, -10.372231142694282, -10.372118154076547, -10.372549271434533, -10.394259059718088, -10.445894176019753, -10.394259059718088, -10.525936883693289, -10.409526531848877, -10.477146719523857, -10.59492975518024, -10.451828911539566, -10.659468276317812, -11.064933384425975, -10.37178620386603, -10.385185006145502, -10.390169157831778, -11.069893318059583, -10.559067694293592, -10.376746137499637, -10.395805895048742, -10.406473800784461, -10.422365168121097, -11.072952734262246, -10.379805553702301, -10.519924287777126, -10.445816315623404, -11.064855524029626, -11.352537596481408, -10.659390415921463, -10.60809712153391, -10.579926244567215, -10.713457637191738, -10.713457637191738, -11.118922745299903, -10.425775564739958, -10.817995446098099, -10.59485189478389, -10.894956487234229, -10.894956487234229, -10.712634930440274, -10.489491379126065, -11.125480145846062, -10.586483645113375, -10.655476516600325, -10.61465452208007, -10.432332965286117, -10.556533030481754, -10.64614518917144, -10.556533030481754, -11.116148818417177, -10.423001637857231, -11.182638559686009, -10.777173451577845, -10.371708343469681])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Pdb) np.exp(node_weights.values())\n",
      "*** AttributeError: 'dict_values' object has no attribute 'exp'\n",
      "(Pdb) np.exp(list(node_weights.values()))\n",
      "array([3.12848287e-05, 3.12986972e-05, 3.12691236e-05, 3.12590500e-05,\n",
      "       2.78247925e-05, 2.95638420e-05, 2.08685944e-05, 2.98122777e-05,\n",
      "       2.73900301e-05, 2.50423133e-05, 2.81726024e-05, 3.13028916e-05,\n",
      "       3.12915184e-05, 3.12936678e-05, 3.11218527e-05, 3.12216922e-05,\n",
      "       3.09377740e-05, 3.10888107e-05, 2.60847898e-05, 2.81715730e-05,\n",
      "       3.05382905e-05, 2.93453885e-05, 3.13017478e-05, 3.12982052e-05,\n",
      "       3.12894010e-05, 3.12929366e-05, 3.12794486e-05, 3.06076965e-05,\n",
      "       2.90673741e-05, 3.06076965e-05, 2.68314223e-05, 3.01439436e-05,\n",
      "       2.81729934e-05, 2.50426608e-05, 2.88953779e-05, 2.34774945e-05,\n",
      "       1.56516630e-05, 3.13033260e-05, 3.08866963e-05, 3.07331354e-05,\n",
      "       1.55742240e-05, 2.59570400e-05, 3.11484480e-05, 3.05603881e-05,\n",
      "       3.02361055e-05, 2.97594102e-05, 1.55266488e-05, 3.10532976e-05,\n",
      "       2.69932348e-05, 2.90696374e-05, 1.56528817e-05, 1.17396613e-05,\n",
      "       2.34793225e-05, 2.47150764e-05, 2.54212214e-05, 2.22435687e-05,\n",
      "       2.22435687e-05, 1.48290458e-05, 2.96580916e-05, 2.00356886e-05,\n",
      "       2.50446107e-05, 1.85515635e-05, 1.85515635e-05, 2.22618762e-05,\n",
      "       2.78273452e-05, 1.47321239e-05, 2.52550696e-05, 2.35713983e-05,\n",
      "       2.45535399e-05, 2.94642479e-05, 2.60229158e-05, 2.37923802e-05,\n",
      "       2.60229158e-05, 1.48702376e-05, 2.97404752e-05, 1.39136726e-05,\n",
      "       2.08705089e-05, 3.13057634e-05])\n",
      "(Pdb) sum(np.exp(list(node_weights.values())))\n",
      "0.0020563163900014997\n",
      "(Pdb) q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2664f00a57fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-752f82b696ff>\u001b[0m in \u001b[0;36mestimate\u001b[0;34m(self, num_samples, display_topics, n_words, with_weights)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-752f82b696ff>\u001b[0m in \u001b[0;36msample_path\u001b[0;34m(self, d)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m############################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mlevel_word_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_levels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mdoc_levels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mdoc_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-752f82b696ff>\u001b[0m in \u001b[0;36msample_path\u001b[0;34m(self, d)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m############################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mlevel_word_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_levels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mdoc_levels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mdoc_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hlda.estimate(num_samples=100, display_topics=10, n_words=5, with_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
