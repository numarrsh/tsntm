{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from data_structure import get_batches, get_test_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "from topic_model import TopicModel\n",
    "\n",
    "from topic_beam_search_decoder import BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '2', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/bags/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/tglm_vae_tmp3', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'bags', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 50, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 500, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.5, 'lr')\n",
    "flags.DEFINE_float('reg', 1., 'regularization term')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_bool('warmup', True, 'flg of warming up')\n",
    "flags.DEFINE_integer('epochs_cycle', 5, 'number of epochs within a cycle')\n",
    "flags.DEFINE_float('r_cycle', 0.5, 'proportion used to increase beta within a cycle')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 10, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_bow', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_hidden_topic', 512, 'dim_hidden_topic')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "flags.DEFINE_bool('bidirectional', True, 'flg of bidirectional encoding')\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_test_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('PAD_IDX', word_to_idx[PAD], 'PAD_IDX')\n",
    "flags.DEFINE_integer('UNK_IDX', word_to_idx[UNK], 'UNK_IDX')\n",
    "flags.DEFINE_integer('BOS_IDX', word_to_idx[BOS], 'BOS_IDX')\n",
    "flags.DEFINE_integer('EOS_IDX', word_to_idx[EOS], 'EOS_IDX')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "maximum_iterations = max([max([instance.max_sent_l for instance in batch]) for ct, batch in dev_batches])\n",
    "flags.DEFINE_integer('maximum_iterations', maximum_iterations, 'maximum_iterations')\n",
    "\n",
    "flags.DEFINE_integer('cycle_steps', len(train_batches)*config.epochs_cycle, 'number of steps for each cycle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build language model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['bow'] = tf.placeholder(tf.float32, [None, config.dim_bow], name='bow')\n",
    "t_variables['input_token_idxs'] = tf.placeholder(tf.int32, [None, None], name='input_token_idxs')\n",
    "t_variables['dec_input_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_input_idxs')\n",
    "t_variables['dec_target_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_target_idxs')\n",
    "t_variables['batch_l'] = tf.placeholder(tf.int32, name='batch_l')\n",
    "t_variables['doc_l'] = tf.placeholder(tf.int32, [None], name='doc_l')\n",
    "t_variables['sent_l'] = tf.placeholder(tf.int32, [None], name='sent_l')\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train', assertion=False):\n",
    "    def token_dropout(sent_idxs):\n",
    "        sent_idxs_dropout = np.asarray(sent_idxs)\n",
    "        sent_idxs_dropout[np.random.rand(len(sent_idxs)) > config.word_keep_prob] = config.UNK_IDX\n",
    "        return list(sent_idxs_dropout)\n",
    "\n",
    "    bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "    \n",
    "    doc_l = np.array([len(instance.token_idxs) for instance in batch])\n",
    "    \n",
    "    feed_input_token_idxs_list = [sent_idxs for instance in batch for sent_idxs in instance.token_idxs]\n",
    "    feed_dec_input_idxs_list = [[config.BOS_IDX] + token_dropout(sent_idxs) for sent_idxs in feed_input_token_idxs_list]\n",
    "    feed_dec_target_idxs_list = [sent_idxs + [config.EOS_IDX]  for sent_idxs in feed_input_token_idxs_list]\n",
    "        \n",
    "    sent_l = np.array([len(sent_idxs) for sent_idxs in feed_input_token_idxs_list], np.int32)\n",
    "    batch_l = len(sent_l)\n",
    "    \n",
    "    feed_input_token_idxs = pad_sequences(feed_input_token_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_input_idxs = pad_sequences(feed_dec_input_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_target_idxs = pad_sequences(feed_dec_target_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    \n",
    "    if assertion:\n",
    "        index = 0\n",
    "        for instance in batch:\n",
    "            for line_idxs in instance.token_idxs:\n",
    "                assert feed_input_token_idxs_list[index] == line_idxs\n",
    "                index += 1\n",
    "        assert feed_input_token_idxs.shape[1] == np.max(sent_l)\n",
    "        assert feed_dec_input_idxs.shape[1] == np.max(sent_l) + 1\n",
    "        assert feed_dec_target_idxs.shape[1] == np.max(sent_l) + 1\n",
    "    \n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "\n",
    "    feed_dict = {\n",
    "                t_variables['bow']: bow, \n",
    "                t_variables['batch_l']: batch_l, t_variables['doc_l']: doc_l, t_variables['sent_l']: sent_l, \n",
    "                t_variables['input_token_idxs']: feed_input_token_idxs, t_variables['dec_input_idxs']: feed_dec_input_idxs, t_variables['dec_target_idxs']: feed_dec_target_idxs, \n",
    "                t_variables['keep_prob']: keep_prob\n",
    "    }\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     10,
     24,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables):\n",
    "    sample_batch = dev_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "def check_shape(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()\n",
    "    \n",
    "def check_value(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()    \n",
    "    \n",
    "# sent_loss_kl_categ_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, tf_log(prob_topic_infer/prob_topic_sents)), 1))\n",
    "# debug_value([sent_loss_kl_categ, sent_loss_kl_categ_tmp])\n",
    "# sent_loss_kl_gauss_tmp = 0.5 * tf.reduce_sum(tf.exp(logvars_topic_infer-logvars_topic) + tf.square(means_topic - means_topic_infer) / tf.exp(logvars_topic) - 1 + (logvars_topic - logvars_topic_infer), -1)\n",
    "# sent_loss_kl_gmm_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss_tmp), -1))\n",
    "# debug_value([sent_loss_kl_gmm_tmp, sent_loss_kl_gmm])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encode bow\n",
    "with tf.variable_scope('topic/enc', reuse=False):\n",
    "    hidden_bow_ = tf.layers.Dense(units=config.dim_hidden_bow, activation=tf.nn.relu, name='hidden_bow')(t_variables['bow'])\n",
    "    hidden_bow = tf.layers.Dropout(t_variables['keep_prob'])(hidden_bow_)\n",
    "    means_bow = tf.layers.Dense(units=config.dim_latent_bow, name='mean_bow')(hidden_bow)\n",
    "    logvars_bow = tf.layers.Dense(units=config.dim_latent_bow, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_bow)\n",
    "    latents_bow = sample_latents(means_bow, logvars_bow) # sample latent vectors\n",
    "\n",
    "    prob_topic = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic')(latents_bow) # inference of topic probabilities\n",
    "\n",
    "# decode bow\n",
    "with tf.variable_scope('shared', reuse=False):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "bow_embeddings = tf.nn.embedding_lookup(embeddings, bow_idxs) # embeddings of each bow features\n",
    "\n",
    "with tf.variable_scope('topic/dec', reuse=False):\n",
    "    topic_embeddings = tf.get_variable('topic_emb', [config.n_topic, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of topics\n",
    "\n",
    "    topic_bow = tf.nn.softmax(tf.matmul(topic_embeddings, bow_embeddings, transpose_b=True), 1) # bow vectors for each topic\n",
    "    logits_bow = tf_log(tf.matmul(prob_topic, topic_bow)) # predicted bow distribution\n",
    "\n",
    "    # prior of each gaussian distribution (computed for each topic)\n",
    "    hidden_topic = tf.layers.Dense(units=config.dim_hidden_topic, activation=tf.nn.relu, name='hidden_topic')(topic_bow)\n",
    "    means_topic = tf.layers.Dense(units=config.dim_latent, name='mean_topic')(hidden_topic)\n",
    "    logvars_topic = tf.layers.Dense(units=config.dim_latent, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_topic)\n",
    "    sigma_topic = tf.exp(0.5 * logvars_topic)\n",
    "    gauss_topic = tfd.Normal(loc=means_topic, scale=sigma_topic)    \n",
    "    \n",
    "# define losses\n",
    "topic_losses_recon = -tf.reduce_sum(tf.multiply(t_variables['bow'], logits_bow), 1)\n",
    "topic_loss_recon = tf.reduce_mean(topic_losses_recon) # negative log likelihood of each words\n",
    "\n",
    "topic_loss_kl = compute_kl_loss(means_bow, logvars_bow) # KL divergence b/w latent dist & gaussian std\n",
    "\n",
    "topic_bow_norm = topic_bow / tf.norm(topic_bow, axis=1, keepdims=True)\n",
    "topic_dots = tf.clip_by_value(tf.matmul(topic_bow_norm, tf.transpose(topic_bow_norm)), -1., 1.)\n",
    "topic_loss_reg = tf.reduce_mean(tf.square(topic_dots - tf.eye(config.n_topic)))\n",
    "# topic_angles = tf.acos(topic_dots)\n",
    "# topic_angles_mean = tf.reduce_mean(topic_angles)\n",
    "# topic_angles_vars = tf.reduce_mean(tf.square(topic_angles - topic_angles_mean))\n",
    "# topic_loss_reg = tf.exp(topic_angles_vars - topic_angles_mean)\n",
    "\n",
    "# monitor\n",
    "n_bow = tf.reduce_sum(t_variables['bow'], 1)\n",
    "topic_ppls = tf.divide(topic_losses_recon, tf.maximum(1e-5, n_bow))\n",
    "topics_freq_bow_indices = tf.nn.top_k(topic_bow, 10, name='topic_freq_bow').indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "input_token_idxs = t_variables['input_token_idxs']\n",
    "batch_l = t_variables['batch_l']\n",
    "sent_l = t_variables['sent_l']\n",
    "max_sent_l = tf.reduce_max(sent_l)\n",
    "\n",
    "with tf.variable_scope('sent/enc', reuse=False):\n",
    "    # get word embedding\n",
    "    enc_input = tf.nn.embedding_lookup(embeddings, input_token_idxs)\n",
    "\n",
    "    # get sentence embedding\n",
    "    _, enc_state = dynamic_bi_rnn(enc_input, sent_l, config.dim_hidden, t_variables['keep_prob'])\n",
    "\n",
    "    # TODO House Holder flow\n",
    "    hidden_topic_infer =  tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='hidden_topic_infer')(enc_state)\n",
    "    prob_topic_infer = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic_infer')(hidden_topic_infer)\n",
    "\n",
    "    w_mean_topic_infer = tf.get_variable('mean_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32)\n",
    "    b_mean_topic_infer = tf.get_variable('mean_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32)\n",
    "    means_topic_infer = tf.tensordot(enc_state, w_mean_topic_infer, axes=[[1], [1]]) + b_mean_topic_infer\n",
    "    \n",
    "    w_logvar_topic_infer = tf.get_variable('logvar_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    b_logvar_topic_infer = tf.get_variable('logvar_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    logvars_topic_infer = tf.tensordot(enc_state, w_logvar_topic_infer, axes=[[1], [1]]) + b_logvar_topic_infer\n",
    "    sigma_topic_infer = tf.exp(0.5 * logvars_topic_infer)\n",
    "    gauss_topic_infer = tfd.Normal(loc=means_topic_infer, scale=sigma_topic_infer)\n",
    "    \n",
    "    # latent vectors from each gaussian dist.\n",
    "    latents_topic_infer = sample_latents(means_topic_infer, logvars_topic_infer) \n",
    "    # latent vector from gaussian mixture\n",
    "    latents_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), latents_topic_infer, transpose_a=True)\n",
    "    \n",
    "    # for beam search\n",
    "    means_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), means_topic_infer, transpose_a=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for decoding\n",
    "dec_sent_l = tf.add(sent_l, 1)\n",
    "dec_input_idxs = t_variables['dec_input_idxs']\n",
    "dec_input = tf.nn.embedding_lookup(embeddings, dec_input_idxs)\n",
    "\n",
    "dec_latents_input = tf.tile(latents_input, [1, tf.shape(dec_input)[1], 1])\n",
    "dec_concat_input = tf.concat([dec_input, dec_latents_input], -1)\n",
    "\n",
    "# decode for training\n",
    "with tf.variable_scope('sent/dec/rnn', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32, reuse=False):\n",
    "    dec_cell = tf.contrib.rnn.GRUCell(config.dim_hidden)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob = t_variables['keep_prob'])\n",
    "\n",
    "    dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(latents_input, 1))\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_concat_input, sequence_length=dec_sent_l)\n",
    "\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=dec_cell,\n",
    "        helper=helper,\n",
    "        initial_state=dec_initial_state)\n",
    "\n",
    "    dec_outputs, _, output_sent_l = tf.contrib.seq2seq.dynamic_decode(train_decoder)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(config.n_vocab, use_bias=False, name='out')\n",
    "    output_logits = output_layer(dec_outputs.rnn_output)\n",
    "    \n",
    "    output_token_idxs = tf.argmax(output_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tokens = tf.fill([batch_l], config.BOS_IDX)\n",
    "end_token = config.EOS_IDX\n",
    "\n",
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    infer_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(means_input, 1))\n",
    "    beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(infer_dec_initial_state, multiplier=config.beam_width)\n",
    "    beam_latents_input = tf.contrib.seq2seq.tile_batch(tf.squeeze(means_input, 1), multiplier=config.beam_width) # added\n",
    "    \n",
    "    beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=beam_latents_input)\n",
    "\n",
    "    beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    beam_output_token_idxs = beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    inter_means_input = tf.placeholder(tf.float32, [None, config.dim_latent])\n",
    "    \n",
    "    inter_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(inter_means_input)\n",
    "    inter_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(inter_dec_initial_state, multiplier=config.beam_width)\n",
    "    inter_beam_latents_input = tf.contrib.seq2seq.tile_batch(inter_means_input, multiplier=config.beam_width) # added\n",
    "    \n",
    "    inter_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=inter_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=inter_beam_latents_input)\n",
    "\n",
    "    inter_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inter_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    inter_beam_output_token_idxs = inter_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    topic_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic)\n",
    "    topic_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(topic_dec_initial_state, multiplier=config.beam_width)\n",
    "    topic_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic, multiplier=config.beam_width) # added\n",
    "    \n",
    "    topic_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=topic_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=topic_beam_latents_input)\n",
    "\n",
    "    topic_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        topic_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    topic_beam_output_token_idxs = topic_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    means_topic_summary = tf.reduce_mean(means_topic_infer, 0)\n",
    "    \n",
    "    summary_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic_summary)\n",
    "    summary_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(summary_dec_initial_state, multiplier=config.beam_width)\n",
    "    summary_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic_summary, multiplier=config.beam_width) # added\n",
    "    \n",
    "    summary_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=summary_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width,\n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=summary_beam_latents_input)\n",
    "\n",
    "    summary_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        summary_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    summary_beam_output_token_idxs = summary_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language modeling cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target and mask\n",
    "dec_target_idxs = t_variables['dec_target_idxs']\n",
    "dec_mask_tokens = tf.sequence_mask(dec_sent_l, maxlen=max_sent_l+1, dtype=tf.float32)\n",
    "\n",
    "# nll for each token (averaged over batch & sentence)\n",
    "sent_loss_recon = tf.contrib.seq2seq.sequence_loss(output_logits, dec_target_idxs, dec_mask_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_l = t_variables['doc_l']\n",
    "mask_sents = tf.sequence_mask(doc_l)\n",
    "mask_sents_flatten = tf.reshape(mask_sents, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1]])\n",
    "\n",
    "prob_topic_tiled = tf.tile(tf.expand_dims(prob_topic, 1), [1, tf.shape(mask_sents)[1], 1])\n",
    "prob_topic_flatten = tf.reshape(prob_topic_tiled, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1], config.n_topic])\n",
    "prob_topic_sents = tf.boolean_mask(prob_topic_flatten, mask_sents_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferred mixture probabilities (computed for each sentence)\n",
    "categ_topic_infer = tfd.Categorical(probs=prob_topic_infer)\n",
    "\n",
    "# prior of mixture probabilities (computed for each document, tiled for each sentence)\n",
    "categ_topic = tfd.Categorical(probs=prob_topic_sents)\n",
    "\n",
    "sent_loss_kl_categ = tf.reduce_mean(tfd.kl_divergence(categ_topic_infer, categ_topic))\n",
    "\n",
    "# inference of each gaussian gaussribution (computed for each sentence)\n",
    "\n",
    "sent_loss_kl_gauss = tf.reduce_sum(tfd.kl_divergence(gauss_topic_infer, gauss_topic), -1)\n",
    "sent_loss_kl_gmm = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss), -1))\n",
    "\n",
    "sent_loss_kl = sent_loss_kl_categ + sent_loss_kl_gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "tau = tf.cast(tf.divide(tf.mod(global_step, tf.constant(config.cycle_steps)), tf.constant(config.cycle_steps)), dtype=tf.float32)\n",
    "beta = tf.minimum(1., tau/config.r_cycle)\n",
    "\n",
    "sent_loss = sent_loss_recon + beta * sent_loss_kl\n",
    "\n",
    "topic_loss = topic_loss_recon + topic_loss_kl + config.reg * topic_loss_reg\n",
    "loss = topic_loss + sent_loss\n",
    "\n",
    "# define optimizer\n",
    "if config.opt == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif config.opt == 'Adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "    \n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_sents(token_idxs, config, idx_to_word):\n",
    "    sents = []\n",
    "    for sent_idxs in token_idxs:\n",
    "        tokens = []\n",
    "        for idx in sent_idxs:\n",
    "            if idx == config.EOS_IDX: break\n",
    "            tokens.append(idx_to_word[idx])\n",
    "        sent = ' '.join(tokens)\n",
    "        sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, ppls_batch \\\n",
    "            = sess.run([loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, topic_ppls], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "    loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, sent_loss_recon_mean, sent_loss_kl_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    return loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, sent_loss_recon_mean, sent_loss_kl_mean, ppl_mean\n",
    "\n",
    "def get_all_losses(sess, batches):\n",
    "    losses = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch = \\\n",
    "        sess.run([loss, topic_loss_recon, topic_loss_kl, sent_loss_recon, sent_loss_kl], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "    print('LOSS %.2f | TM NLL: %.2f, KL: %.4f | LM NLL: %.2f, KL: %.4f' %  np.mean(losses, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_batch):\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    pred_token_idxs = sess.run(output_token_idxs, feed_dict = feed_dict)\n",
    "    true_token_idxs = [sent_idxs for instance in sample_batch for sent_idxs in instance.token_idxs]\n",
    "    \n",
    "    assert len(pred_token_idxs) == len(true_token_idxs)\n",
    "    \n",
    "    pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "    true_sents = idxs_to_sents(true_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    for i, (true_sent, pred_sent) in enumerate(zip(true_sents, pred_sents)):        \n",
    "        print(i, 'TRUE: %s' % true_sent)\n",
    "        print(i, 'PRED: %s' % pred_sent)\n",
    "\n",
    "def print_topic_sample():\n",
    "    pred_topics_freq_bow_indices, pred_topic_token_idxs = sess.run([topics_freq_bow_indices, topic_beam_output_token_idxs], \n",
    "                                                                                                           feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "    pred_topic_sents = idxs_to_sents(pred_topic_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Topic Samples-----------')\n",
    "    for i, (topic_freq_bow_idxs, pred_topic_sent) in enumerate(zip(topics_freq_bow_idxs, pred_topic_sents)):\n",
    "        print(i, ' BOW:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' SENTENCE:', pred_topic_sent)\n",
    "        \n",
    "def print_summary(test_batch):\n",
    "    feed_dict = get_feed_dict(test_batch)\n",
    "    feed_dict[t_variables['batch_l']] = config.n_topic\n",
    "    feed_dict[t_variables['keep_prob']] = 1.\n",
    "    pred_topics_freq_bow_indices, pred_summary_token_idxs = sess.run([topics_freq_bow_indices, summary_beam_output_token_idxs], feed_dict=feed_dict)\n",
    "    pred_summary_sents = idxs_to_sents(pred_summary_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Output sentences for each topic-----------')\n",
    "    print('Item idx:', test_batch[0].item_idx)\n",
    "    for i, (topic_freq_bow_idxs, pred_summary_sent) in enumerate(zip(topics_freq_bow_idxs, pred_summary_sents)):\n",
    "        print(i, ' BOW:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' SENTENCE:', pred_summary_sent)\n",
    "        \n",
    "    print('-----------Summaries-----------')\n",
    "    for i, summary in enumerate(test_batch[0].summaries):\n",
    "        print('SUMMARY %i :'%i, '\\n', summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "saver = tf.train.Saver(max_to_keep=10)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','TM','','','','LM','','VALID:','TM','','','','LM','', ''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL','REG','NLL','KL','LOSS','PPL','NLL','KL','REG','NLL','KL', 'Beta']]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>LM</th>\n",
       "      <th></th>\n",
       "      <th>VALID:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>LM</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>128.53</td>\n",
       "      <td>1036</td>\n",
       "      <td>118.02</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.90</td>\n",
       "      <td>9.12</td>\n",
       "      <td>1.49</td>\n",
       "      <td>128.62</td>\n",
       "      <td>1017</td>\n",
       "      <td>115.86</td>\n",
       "      <td>2.74</td>\n",
       "      <td>0.90</td>\n",
       "      <td>9.12</td>\n",
       "      <td>1.64</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>121.50</td>\n",
       "      <td>602</td>\n",
       "      <td>114.68</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.36</td>\n",
       "      <td>6.08</td>\n",
       "      <td>3.51</td>\n",
       "      <td>111.73</td>\n",
       "      <td>550</td>\n",
       "      <td>105.85</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.22</td>\n",
       "      <td>5.47</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>120.08</td>\n",
       "      <td>586</td>\n",
       "      <td>113.88</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.24</td>\n",
       "      <td>5.73</td>\n",
       "      <td>2.03</td>\n",
       "      <td>110.66</td>\n",
       "      <td>536</td>\n",
       "      <td>105.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>5.11</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>1500</td>\n",
       "      <td>119.25</td>\n",
       "      <td>578</td>\n",
       "      <td>113.40</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.53</td>\n",
       "      <td>1.36</td>\n",
       "      <td>110.23</td>\n",
       "      <td>532</td>\n",
       "      <td>105.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.91</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>118.88</td>\n",
       "      <td>573</td>\n",
       "      <td>113.25</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.13</td>\n",
       "      <td>5.39</td>\n",
       "      <td>1.03</td>\n",
       "      <td>110.08</td>\n",
       "      <td>531</td>\n",
       "      <td>105.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.79</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>118.93</td>\n",
       "      <td>572</td>\n",
       "      <td>113.38</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "      <td>5.33</td>\n",
       "      <td>0.90</td>\n",
       "      <td>110.12</td>\n",
       "      <td>537</td>\n",
       "      <td>105.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.73</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2776</th>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>118.63</td>\n",
       "      <td>569</td>\n",
       "      <td>113.22</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.09</td>\n",
       "      <td>5.24</td>\n",
       "      <td>0.74</td>\n",
       "      <td>109.90</td>\n",
       "      <td>531</td>\n",
       "      <td>105.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.65</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3276</th>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>118.54</td>\n",
       "      <td>567</td>\n",
       "      <td>113.22</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.16</td>\n",
       "      <td>0.63</td>\n",
       "      <td>109.81</td>\n",
       "      <td>530</td>\n",
       "      <td>105.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.59</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3776</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>118.35</td>\n",
       "      <td>567</td>\n",
       "      <td>113.11</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.07</td>\n",
       "      <td>5.10</td>\n",
       "      <td>0.55</td>\n",
       "      <td>109.83</td>\n",
       "      <td>532</td>\n",
       "      <td>105.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.53</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4276</th>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>118.30</td>\n",
       "      <td>566</td>\n",
       "      <td>113.12</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.06</td>\n",
       "      <td>5.05</td>\n",
       "      <td>0.48</td>\n",
       "      <td>109.70</td>\n",
       "      <td>531</td>\n",
       "      <td>105.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.47</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4551</th>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>118.30</td>\n",
       "      <td>565</td>\n",
       "      <td>113.15</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.06</td>\n",
       "      <td>5.02</td>\n",
       "      <td>0.46</td>\n",
       "      <td>109.67</td>\n",
       "      <td>530</td>\n",
       "      <td>105.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5051</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>118.30</td>\n",
       "      <td>564</td>\n",
       "      <td>113.20</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.41</td>\n",
       "      <td>109.64</td>\n",
       "      <td>530</td>\n",
       "      <td>105.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5551</th>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>118.23</td>\n",
       "      <td>564</td>\n",
       "      <td>113.19</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4.94</td>\n",
       "      <td>0.37</td>\n",
       "      <td>109.57</td>\n",
       "      <td>528</td>\n",
       "      <td>105.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6051</th>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>1500</td>\n",
       "      <td>118.12</td>\n",
       "      <td>563</td>\n",
       "      <td>113.12</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4.90</td>\n",
       "      <td>0.34</td>\n",
       "      <td>109.64</td>\n",
       "      <td>533</td>\n",
       "      <td>105.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6551</th>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>118.04</td>\n",
       "      <td>562</td>\n",
       "      <td>113.08</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4.87</td>\n",
       "      <td>0.32</td>\n",
       "      <td>109.52</td>\n",
       "      <td>529</td>\n",
       "      <td>105.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6826</th>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>118.00</td>\n",
       "      <td>562</td>\n",
       "      <td>113.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4.86</td>\n",
       "      <td>0.31</td>\n",
       "      <td>109.53</td>\n",
       "      <td>528</td>\n",
       "      <td>105.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7326</th>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>117.96</td>\n",
       "      <td>562</td>\n",
       "      <td>113.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4.83</td>\n",
       "      <td>0.28</td>\n",
       "      <td>109.56</td>\n",
       "      <td>533</td>\n",
       "      <td>105.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7826</th>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>117.92</td>\n",
       "      <td>561</td>\n",
       "      <td>113.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4.80</td>\n",
       "      <td>0.27</td>\n",
       "      <td>109.49</td>\n",
       "      <td>532</td>\n",
       "      <td>105.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8326</th>\n",
       "      <td>65</td>\n",
       "      <td>3</td>\n",
       "      <td>1500</td>\n",
       "      <td>117.87</td>\n",
       "      <td>561</td>\n",
       "      <td>113.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4.77</td>\n",
       "      <td>0.25</td>\n",
       "      <td>109.36</td>\n",
       "      <td>527</td>\n",
       "      <td>105.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8826</th>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>117.85</td>\n",
       "      <td>561</td>\n",
       "      <td>113.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.75</td>\n",
       "      <td>0.24</td>\n",
       "      <td>109.42</td>\n",
       "      <td>529</td>\n",
       "      <td>105.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9101</th>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>117.82</td>\n",
       "      <td>561</td>\n",
       "      <td>113.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.74</td>\n",
       "      <td>0.23</td>\n",
       "      <td>109.46</td>\n",
       "      <td>531</td>\n",
       "      <td>105.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9601</th>\n",
       "      <td>54</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "      <td>117.76</td>\n",
       "      <td>560</td>\n",
       "      <td>112.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.72</td>\n",
       "      <td>0.22</td>\n",
       "      <td>109.38</td>\n",
       "      <td>528</td>\n",
       "      <td>105.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10101</th>\n",
       "      <td>65</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>117.73</td>\n",
       "      <td>560</td>\n",
       "      <td>112.97</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.70</td>\n",
       "      <td>0.21</td>\n",
       "      <td>109.33</td>\n",
       "      <td>528</td>\n",
       "      <td>105.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10601</th>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>1500</td>\n",
       "      <td>117.69</td>\n",
       "      <td>560</td>\n",
       "      <td>112.95</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.68</td>\n",
       "      <td>0.20</td>\n",
       "      <td>109.26</td>\n",
       "      <td>527</td>\n",
       "      <td>105.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11101</th>\n",
       "      <td>55</td>\n",
       "      <td>4</td>\n",
       "      <td>2000</td>\n",
       "      <td>117.69</td>\n",
       "      <td>559</td>\n",
       "      <td>112.97</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.66</td>\n",
       "      <td>0.19</td>\n",
       "      <td>109.39</td>\n",
       "      <td>532</td>\n",
       "      <td>105.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11376</th>\n",
       "      <td>31</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>117.69</td>\n",
       "      <td>559</td>\n",
       "      <td>112.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.65</td>\n",
       "      <td>0.18</td>\n",
       "      <td>109.34</td>\n",
       "      <td>530</td>\n",
       "      <td>105.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11876</th>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>117.69</td>\n",
       "      <td>559</td>\n",
       "      <td>113.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.64</td>\n",
       "      <td>0.18</td>\n",
       "      <td>109.31</td>\n",
       "      <td>529</td>\n",
       "      <td>105.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12376</th>\n",
       "      <td>56</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>117.66</td>\n",
       "      <td>559</td>\n",
       "      <td>112.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.62</td>\n",
       "      <td>0.17</td>\n",
       "      <td>109.30</td>\n",
       "      <td>529</td>\n",
       "      <td>105.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12876</th>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>1500</td>\n",
       "      <td>117.62</td>\n",
       "      <td>559</td>\n",
       "      <td>112.96</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0.16</td>\n",
       "      <td>109.34</td>\n",
       "      <td>532</td>\n",
       "      <td>105.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13376</th>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>117.59</td>\n",
       "      <td>559</td>\n",
       "      <td>112.95</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.59</td>\n",
       "      <td>0.16</td>\n",
       "      <td>109.31</td>\n",
       "      <td>531</td>\n",
       "      <td>105.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100101</th>\n",
       "      <td>30</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>nan</td>\n",
       "      <td>639339200</td>\n",
       "      <td>361.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.76</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>390.49</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.60</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100601</th>\n",
       "      <td>54</td>\n",
       "      <td>44</td>\n",
       "      <td>500</td>\n",
       "      <td>nan</td>\n",
       "      <td>647449472</td>\n",
       "      <td>361.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.77</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>390.49</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.61</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101101</th>\n",
       "      <td>54</td>\n",
       "      <td>44</td>\n",
       "      <td>1000</td>\n",
       "      <td>nan</td>\n",
       "      <td>655542656</td>\n",
       "      <td>361.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.77</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>390.49</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.61</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101601</th>\n",
       "      <td>54</td>\n",
       "      <td>44</td>\n",
       "      <td>1500</td>\n",
       "      <td>nan</td>\n",
       "      <td>663655936</td>\n",
       "      <td>361.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.77</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>390.50</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.62</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102101</th>\n",
       "      <td>54</td>\n",
       "      <td>44</td>\n",
       "      <td>2000</td>\n",
       "      <td>nan</td>\n",
       "      <td>671828608</td>\n",
       "      <td>361.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.77</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>390.50</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.62</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102376</th>\n",
       "      <td>30</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>nan</td>\n",
       "      <td>676324672</td>\n",
       "      <td>362.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.77</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>390.30</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.63</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102876</th>\n",
       "      <td>54</td>\n",
       "      <td>45</td>\n",
       "      <td>500</td>\n",
       "      <td>nan</td>\n",
       "      <td>684525440</td>\n",
       "      <td>362.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.77</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>390.31</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103376</th>\n",
       "      <td>54</td>\n",
       "      <td>45</td>\n",
       "      <td>1000</td>\n",
       "      <td>nan</td>\n",
       "      <td>692751616</td>\n",
       "      <td>362.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.77</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>390.31</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103876</th>\n",
       "      <td>54</td>\n",
       "      <td>45</td>\n",
       "      <td>1500</td>\n",
       "      <td>nan</td>\n",
       "      <td>700949632</td>\n",
       "      <td>362.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.77</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>390.32</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104376</th>\n",
       "      <td>53</td>\n",
       "      <td>45</td>\n",
       "      <td>2000</td>\n",
       "      <td>nan</td>\n",
       "      <td>709187904</td>\n",
       "      <td>362.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.77</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>390.32</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104651</th>\n",
       "      <td>29</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>nan</td>\n",
       "      <td>713706624</td>\n",
       "      <td>362.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.77</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>390.33</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105151</th>\n",
       "      <td>54</td>\n",
       "      <td>46</td>\n",
       "      <td>500</td>\n",
       "      <td>nan</td>\n",
       "      <td>721962560</td>\n",
       "      <td>363.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.77</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>390.33</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105651</th>\n",
       "      <td>53</td>\n",
       "      <td>46</td>\n",
       "      <td>1000</td>\n",
       "      <td>nan</td>\n",
       "      <td>730209536</td>\n",
       "      <td>363.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.77</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>390.40</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.66</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106151</th>\n",
       "      <td>53</td>\n",
       "      <td>46</td>\n",
       "      <td>1500</td>\n",
       "      <td>nan</td>\n",
       "      <td>738561984</td>\n",
       "      <td>363.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.77</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>390.46</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.67</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106651</th>\n",
       "      <td>53</td>\n",
       "      <td>46</td>\n",
       "      <td>2000</td>\n",
       "      <td>nan</td>\n",
       "      <td>746884608</td>\n",
       "      <td>363.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.77</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>390.48</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.67</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106926</th>\n",
       "      <td>30</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>nan</td>\n",
       "      <td>751431360</td>\n",
       "      <td>363.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.77</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>390.47</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.67</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107426</th>\n",
       "      <td>53</td>\n",
       "      <td>47</td>\n",
       "      <td>500</td>\n",
       "      <td>nan</td>\n",
       "      <td>759709184</td>\n",
       "      <td>364.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.77</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>390.51</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.68</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107926</th>\n",
       "      <td>54</td>\n",
       "      <td>47</td>\n",
       "      <td>1000</td>\n",
       "      <td>nan</td>\n",
       "      <td>768082560</td>\n",
       "      <td>364.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.77</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>390.53</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.68</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108426</th>\n",
       "      <td>56</td>\n",
       "      <td>47</td>\n",
       "      <td>1500</td>\n",
       "      <td>nan</td>\n",
       "      <td>776431232</td>\n",
       "      <td>364.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.77</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>390.53</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.69</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108926</th>\n",
       "      <td>54</td>\n",
       "      <td>47</td>\n",
       "      <td>2000</td>\n",
       "      <td>nan</td>\n",
       "      <td>784822784</td>\n",
       "      <td>364.68</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>390.54</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.69</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109201</th>\n",
       "      <td>31</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>nan</td>\n",
       "      <td>789454400</td>\n",
       "      <td>364.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>390.54</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.70</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109701</th>\n",
       "      <td>55</td>\n",
       "      <td>48</td>\n",
       "      <td>500</td>\n",
       "      <td>nan</td>\n",
       "      <td>797836032</td>\n",
       "      <td>364.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>390.55</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.70</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110201</th>\n",
       "      <td>59</td>\n",
       "      <td>48</td>\n",
       "      <td>1000</td>\n",
       "      <td>nan</td>\n",
       "      <td>806234304</td>\n",
       "      <td>365.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>390.55</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.71</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110701</th>\n",
       "      <td>59</td>\n",
       "      <td>48</td>\n",
       "      <td>1500</td>\n",
       "      <td>nan</td>\n",
       "      <td>814637120</td>\n",
       "      <td>365.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>390.55</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.71</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111201</th>\n",
       "      <td>59</td>\n",
       "      <td>48</td>\n",
       "      <td>2000</td>\n",
       "      <td>nan</td>\n",
       "      <td>823085120</td>\n",
       "      <td>365.51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>390.56</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.72</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111476</th>\n",
       "      <td>35</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>nan</td>\n",
       "      <td>827731008</td>\n",
       "      <td>365.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>390.55</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.72</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111976</th>\n",
       "      <td>59</td>\n",
       "      <td>49</td>\n",
       "      <td>500</td>\n",
       "      <td>nan</td>\n",
       "      <td>836198336</td>\n",
       "      <td>365.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>390.56</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.73</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112476</th>\n",
       "      <td>58</td>\n",
       "      <td>49</td>\n",
       "      <td>1000</td>\n",
       "      <td>nan</td>\n",
       "      <td>844674944</td>\n",
       "      <td>366.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>390.56</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.73</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112976</th>\n",
       "      <td>58</td>\n",
       "      <td>49</td>\n",
       "      <td>1500</td>\n",
       "      <td>nan</td>\n",
       "      <td>853115456</td>\n",
       "      <td>366.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>390.56</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.74</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113476</th>\n",
       "      <td>58</td>\n",
       "      <td>49</td>\n",
       "      <td>2000</td>\n",
       "      <td>nan</td>\n",
       "      <td>861572928</td>\n",
       "      <td>366.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>390.57</td>\n",
       "      <td>8555976704</td>\n",
       "      <td>382.77</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.74</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows  18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       TRAIN:         TM                        LM        \\\n",
       "       Time  Ep    Ct    LOSS        PPL     NLL    KL   REG   NLL    KL   \n",
       "1        14   0     0  128.53       1036  118.02  0.48  0.90  9.12  1.49   \n",
       "501      79   0   500  121.50        602  114.68  0.24  0.36  6.08  3.51   \n",
       "1001     63   0  1000  120.08        586  113.88  0.13  0.24  5.73  2.03   \n",
       "1501     65   0  1500  119.25        578  113.40  0.09  0.17  5.53  1.36   \n",
       "2001     62   0  2000  118.88        573  113.25  0.07  0.13  5.39  1.03   \n",
       "2276     29   1     0  118.93        572  113.38  0.06  0.11  5.33  0.90   \n",
       "2776     72   1   500  118.63        569  113.22  0.05  0.09  5.24  0.74   \n",
       "3276     71   1  1000  118.54        567  113.22  0.04  0.08  5.16  0.63   \n",
       "3776     55   1  1500  118.35        567  113.11  0.04  0.07  5.10  0.55   \n",
       "4276     65   1  2000  118.30        566  113.12  0.04  0.06  5.05  0.48   \n",
       "4551     41   2     0  118.30        565  113.15  0.04  0.06  5.02  0.46   \n",
       "5051     64   2   500  118.30        564  113.20  0.04  0.06  4.98  0.41   \n",
       "5551     65   2  1000  118.23        564  113.19  0.03  0.05  4.94  0.37   \n",
       "6051     55   2  1500  118.12        563  113.12  0.03  0.05  4.90  0.34   \n",
       "6551     75   2  2000  118.04        562  113.08  0.03  0.04  4.87  0.32   \n",
       "6826     30   3     0  118.00        562  113.06  0.03  0.04  4.86  0.31   \n",
       "7326     55   3   500  117.96        562  113.05  0.03  0.04  4.83  0.28   \n",
       "7826     64   3  1000  117.92        561  113.04  0.03  0.04  4.80  0.27   \n",
       "8326     65   3  1500  117.87        561  113.02  0.02  0.04  4.77  0.25   \n",
       "8826     55   3  2000  117.85        561  113.02  0.02  0.03  4.75  0.24   \n",
       "9101     31   4     0  117.82        561  113.01  0.02  0.03  4.74  0.23   \n",
       "9601     54   4   500  117.76        560  112.98  0.02  0.03  4.72  0.22   \n",
       "10101    65   4  1000  117.73        560  112.97  0.02  0.03  4.70  0.21   \n",
       "10601    64   4  1500  117.69        560  112.95  0.02  0.03  4.68  0.20   \n",
       "11101    55   4  2000  117.69        559  112.97  0.02  0.03  4.66  0.19   \n",
       "11376    31   5     0  117.69        559  112.98  0.02  0.03  4.65  0.18   \n",
       "11876    55   5   500  117.69        559  113.00  0.02  0.03  4.64  0.18   \n",
       "12376    56   5  1000  117.66        559  112.98  0.02  0.03  4.62  0.17   \n",
       "12876    55   5  1500  117.62        559  112.96  0.02  0.03  4.60  0.16   \n",
       "13376    55   5  2000  117.59        559  112.95  0.01  0.02  4.59  0.16   \n",
       "...     ...  ..   ...     ...        ...     ...   ...   ...   ...   ...   \n",
       "100101   30  44     0     nan  639339200  361.03  0.00  0.76   nan   nan   \n",
       "100601   54  44   500     nan  647449472  361.26  0.00  0.77   nan   nan   \n",
       "101101   54  44  1000     nan  655542656  361.47  0.00  0.77   nan   nan   \n",
       "101601   54  44  1500     nan  663655936  361.70  0.00  0.77   nan   nan   \n",
       "102101   54  44  2000     nan  671828608  361.91  0.00  0.77   nan   nan   \n",
       "102376   30  45     0     nan  676324672  362.04  0.00  0.77   nan   nan   \n",
       "102876   54  45   500     nan  684525440  362.25  0.00  0.77   nan   nan   \n",
       "103376   54  45  1000     nan  692751616  362.46  0.00  0.77   nan   nan   \n",
       "103876   54  45  1500     nan  700949632  362.67  0.00  0.77   nan   nan   \n",
       "104376   53  45  2000     nan  709187904  362.88  0.00  0.77   nan   nan   \n",
       "104651   29  46     0     nan  713706624  362.99  0.00  0.77   nan   nan   \n",
       "105151   54  46   500     nan  721962560  363.19  0.00  0.77   nan   nan   \n",
       "105651   53  46  1000     nan  730209536  363.38  0.00  0.77   nan   nan   \n",
       "106151   53  46  1500     nan  738561984  363.59  0.00  0.77   nan   nan   \n",
       "106651   53  46  2000     nan  746884608  363.79  0.00  0.77   nan   nan   \n",
       "106926   30  47     0     nan  751431360  363.91  0.00  0.77   nan   nan   \n",
       "107426   53  47   500     nan  759709184  364.10  0.00  0.77   nan   nan   \n",
       "107926   54  47  1000     nan  768082560  364.30  0.00  0.77   nan   nan   \n",
       "108426   56  47  1500     nan  776431232  364.49  0.00  0.77   nan   nan   \n",
       "108926   54  47  2000     nan  784822784  364.68  0.00  0.78   nan   nan   \n",
       "109201   31  48     0     nan  789454400  364.79  0.00  0.78   nan   nan   \n",
       "109701   55  48   500     nan  797836032  364.96  0.00  0.78   nan   nan   \n",
       "110201   59  48  1000     nan  806234304  365.15  0.00  0.78   nan   nan   \n",
       "110701   59  48  1500     nan  814637120  365.33  0.00  0.78   nan   nan   \n",
       "111201   59  48  2000     nan  823085120  365.51  0.00  0.78   nan   nan   \n",
       "111476   35  49     0     nan  827731008  365.63  0.00  0.78   nan   nan   \n",
       "111976   59  49   500     nan  836198336  365.82  0.00  0.78   nan   nan   \n",
       "112476   58  49  1000     nan  844674944  366.00  0.00  0.78   nan   nan   \n",
       "112976   58  49  1500     nan  853115456  366.17  0.00  0.78   nan   nan   \n",
       "113476   58  49  2000     nan  861572928  366.35  0.00  0.78   nan   nan   \n",
       "\n",
       "        VALID:          TM                        LM               \n",
       "          LOSS         PPL     NLL    KL   REG   NLL    KL   Beta  \n",
       "1       128.62        1017  115.86  2.74  0.90  9.12  1.64  0.000  \n",
       "501     111.73         550  105.85  0.01  0.22  5.47  2.06  0.088  \n",
       "1001    110.66         536  105.49  0.00  0.04  5.11  0.07  0.176  \n",
       "1501    110.23         532  105.31  0.00  0.01  4.91  0.02  0.264  \n",
       "2001    110.08         531  105.28  0.00  0.01  4.79  0.01  0.352  \n",
       "2276    110.12         537  105.38  0.00  0.01  4.73  0.01  0.400  \n",
       "2776    109.90         531  105.24  0.00  0.01  4.65  0.01  0.488  \n",
       "3276    109.81         530  105.21  0.00  0.01  4.59  0.01  0.576  \n",
       "3776    109.83         532  105.29  0.00  0.01  4.53  0.01  0.664  \n",
       "4276    109.70         531  105.21  0.00  0.01  4.47  0.01  0.752  \n",
       "4551    109.67         530  105.20  0.00  0.01  4.46  0.00  0.800  \n",
       "5051    109.64         530  105.21  0.00  0.01  4.42  0.00  0.888  \n",
       "5551    109.57         528  105.19  0.00  0.01  4.38  0.00  0.976  \n",
       "6051    109.64         533  105.28  0.00  0.01  4.35  0.00  1.000  \n",
       "6551    109.52         529  105.19  0.00  0.01  4.32  0.00  1.000  \n",
       "6826    109.53         528  105.21  0.00  0.01  4.31  0.00  1.000  \n",
       "7326    109.56         533  105.29  0.00  0.01  4.26  0.00  1.000  \n",
       "7826    109.49         532  105.24  0.00  0.01  4.24  0.00  1.000  \n",
       "8326    109.36         527  105.12  0.00  0.01  4.22  0.00  1.000  \n",
       "8826    109.42         529  105.20  0.00  0.01  4.22  0.00  1.000  \n",
       "9101    109.46         531  105.26  0.00  0.01  4.19  0.00  1.000  \n",
       "9601    109.38         528  105.19  0.00  0.01  4.18  0.00  1.000  \n",
       "10101   109.33         528  105.17  0.00  0.01  4.15  0.00  1.000  \n",
       "10601   109.26         527  105.12  0.00  0.01  4.14  0.00  1.000  \n",
       "11101   109.39         532  105.25  0.00  0.01  4.13  0.00  1.000  \n",
       "11376   109.34         530  105.23  0.00  0.01  4.10  0.00  0.000  \n",
       "11876   109.31         529  105.21  0.00  0.01  4.10  0.01  0.088  \n",
       "12376   109.30         529  105.20  0.00  0.01  4.10  0.00  0.176  \n",
       "12876   109.34         532  105.26  0.00  0.01  4.07  0.00  0.264  \n",
       "13376   109.31         531  105.23  0.00  0.01  4.07  0.00  0.352  \n",
       "...        ...         ...     ...   ...   ...   ...   ...    ...  \n",
       "100101  390.49  8555976704  382.77  0.00  0.90  6.60  0.21  1.000  \n",
       "100601  390.49  8555976704  382.77  0.00  0.90  6.61  0.21  1.000  \n",
       "101101  390.49  8555976704  382.77  0.00  0.90  6.61  0.21  1.000  \n",
       "101601  390.50  8555976704  382.77  0.00  0.90  6.62  0.20  1.000  \n",
       "102101  390.50  8555976704  382.77  0.00  0.90  6.62  0.20  1.000  \n",
       "102376  390.30  8555976704  382.77  0.00  0.90  6.63  0.20  0.000  \n",
       "102876  390.31  8555976704  382.77  0.00  0.90  6.63  0.00  0.088  \n",
       "103376  390.31  8555976704  382.77  0.00  0.90  6.64  0.00  0.176  \n",
       "103876  390.32  8555976704  382.77  0.00  0.90  6.64  0.00  0.264  \n",
       "104376  390.32  8555976704  382.77  0.00  0.90  6.65  0.00  0.352  \n",
       "104651  390.33  8555976704  382.77  0.00  0.90  6.65  0.00  0.400  \n",
       "105151  390.33  8555976704  382.77  0.00  0.90  6.66  0.00  0.488  \n",
       "105651  390.40  8555976704  382.77  0.00  0.90  6.66  0.11  0.576  \n",
       "106151  390.46  8555976704  382.77  0.00  0.90  6.67  0.18  0.664  \n",
       "106651  390.48  8555976704  382.77  0.00  0.90  6.67  0.18  0.752  \n",
       "106926  390.47  8555976704  382.77  0.00  0.90  6.67  0.16  0.800  \n",
       "107426  390.51  8555976704  382.77  0.00  0.90  6.68  0.18  0.888  \n",
       "107926  390.53  8555976704  382.77  0.00  0.90  6.68  0.17  0.976  \n",
       "108426  390.53  8555976704  382.77  0.00  0.90  6.69  0.17  1.000  \n",
       "108926  390.54  8555976704  382.77  0.00  0.90  6.69  0.17  1.000  \n",
       "109201  390.54  8555976704  382.77  0.00  0.90  6.70  0.17  1.000  \n",
       "109701  390.55  8555976704  382.77  0.00  0.90  6.70  0.17  1.000  \n",
       "110201  390.55  8555976704  382.77  0.00  0.90  6.71  0.17  1.000  \n",
       "110701  390.55  8555976704  382.77  0.00  0.90  6.71  0.17  1.000  \n",
       "111201  390.56  8555976704  382.77  0.00  0.90  6.72  0.17  1.000  \n",
       "111476  390.55  8555976704  382.77  0.00  0.90  6.72  0.16  1.000  \n",
       "111976  390.56  8555976704  382.77  0.00  0.90  6.73  0.16  1.000  \n",
       "112476  390.56  8555976704  382.77  0.00  0.90  6.73  0.16  1.000  \n",
       "112976  390.56  8555976704  382.77  0.00  0.90  6.74  0.16  1.000  \n",
       "113476  390.57  8555976704  382.77  0.00  0.90  6.74  0.15  1.000  \n",
       "\n",
       "[250 rows x 18 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Output sentences for each topic-----------\n",
      "Item idx: B000VB7EFW\n",
      "0  BOW: cover ! $ % & ' 'd 'll 'm 're\n",
      "0  SENTENCE: \n",
      "1  BOW: cover ! $ % & ' 'd 'll 'm 're\n",
      "1  SENTENCE: \n",
      "2  BOW: cover ; - ! $ % & ' 'd 'll\n",
      "2  SENTENCE: \n",
      "3  BOW: cover ! $ % & ' 'd 'll 'm 're\n",
      "3  SENTENCE: \n",
      "4  BOW: cover ! $ % & ' 'd 'll 'm 're\n",
      "4  SENTENCE: \n",
      "5  BOW: cover ! $ % & ' 'd 'll 'm 're\n",
      "5  SENTENCE: \n",
      "6  BOW: cover ! $ % & ' 'd 'll 'm 're\n",
      "6  SENTENCE: \n",
      "7  BOW: cover ! $ % & ' 'd 'll 'm 're\n",
      "7  SENTENCE: \n",
      "8  BOW: cover ! $ % & ' 'd 'll 'm 're\n",
      "8  SENTENCE: \n",
      "9  BOW: cover ! $ % & ' 'd 'll 'm 're\n",
      "9  SENTENCE: \n",
      "-----------Summaries-----------\n",
      "SUMMARY 0 : \n",
      " This is a very well made bag, nice construction, lots of pockets.\n",
      "the straps are very comfortable.\n",
      "and protects everything inside.\n",
      "It says\n",
      "it fits a 17inch notebook,\n",
      "however it did not.\n",
      "after using the pack for less than a month,\n",
      "it is ripping out already.\n",
      "SUMMARY 1 : \n",
      " This is a very well made bag, nice construction, lots of pockets.\n",
      "The quality is excellent\n",
      "and it is very durable.\n",
      "the straps are very comfortable.\n",
      "and protects everything inside.\n",
      "The color is a true red\n",
      "and it fits nicely.\n",
      "it is ripping out already.\n",
      "and it doesnt fit.\n",
      "It's just not the lightest backpack\n",
      "because some zipper teeth were not aligned.\n",
      "SUMMARY 2 : \n",
      " The quality is excellent\n",
      "and I love all the pockets and compartments.\n",
      "and it is very durable.\n",
      "and can't beleive the price\n",
      "and protects everything inside.\n",
      "The laptop\n",
      "doesn't fit in it.\n",
      "it is ripping out already.\n",
      "It's just not the lightest backpack\n",
      "0 TRUE: i use this bag for school , to carry my laptop and accessories , as well as books and notebooks for one class\n",
      "0 PRED: \n",
      "1 TRUE: it works great\n",
      "1 PRED: \n",
      "2 TRUE: i could also see using it as a carry-on for travel ; it could easily hold my laptop , kindle , and flight necessities\n",
      "2 PRED: \n",
      "3 TRUE: this bag has handy compartments under the flap for pens , cell phone and other electronics , etc .\n",
      "3 PRED: \n",
      "4 TRUE: it also has a <unk> laptop section\n",
      "4 PRED: \n",
      "5 TRUE: as a short woman , i appreciate the adjustable strap , and as someone who carries this bag around a lot , i appreciate the padding on the strap\n",
      "5 PRED: \n",
      "6 TRUE: really , my only complaint is that the color is boring\n",
      "6 PRED: \n",
      "7 TRUE: this bag has held up really well so far\n",
      "7 PRED: \n",
      "8 TRUE: i 'm very happy with it\n",
      "8 PRED: \n",
      "9 TRUE: i really like this case\n",
      "9 PRED: \n",
      "10 TRUE: it 's not bulky , it provides some protection without being a full hardcase , and it has room for a few things in addition to my # & # # ; mbp\n",
      "10 PRED: \n",
      "11 TRUE: the problem is that it 's falling apart now\n",
      "11 PRED: \n",
      "12 TRUE: it should last more than # months , i think\n",
      "12 PRED: \n",
      "13 TRUE: one end of one handle strap is coming out of the seam , and part of the zipper seam is falling apart\n",
      "13 PRED: \n",
      "14 TRUE: i 'm trying to get it replaced by lacie but they 're telling me to go through amazon\n",
      "14 PRED: \n",
      "15 TRUE: amazon 's return window has expired\n",
      "15 PRED: \n",
      "16 TRUE: hopefully lacie will do the right thing\n",
      "16 PRED: \n",
      "17 TRUE: it 's got all the right pockets in all the right places , and the padding right where it needs to be\n",
      "17 PRED: \n",
      "18 TRUE: i have an over-sized # `` <unk> laptop with accessories , and fits really well in this case , with plenty of room to spare\n",
      "18 PRED: \n",
      "19 TRUE: it is very sturdy -lrb- targus has a good reputation for that -rrb-\n",
      "19 PRED: \n",
      "20 TRUE: i 've had it over a year and there are absolutely no signs of wear and tear , despite being on the road for a good bit of that time\n",
      "20 PRED: \n",
      "21 TRUE: update : after two years now , and constant traveling , the case is still holding strong\n",
      "21 PRED: \n",
      "22 TRUE: it looks like it 's in mint condition with absolutely no wear\n",
      "22 PRED: \n",
      "23 TRUE: i now carry the laptop , presentation materials , a small projector and marketing brochures everywhere\n",
      "23 PRED: \n",
      "24 TRUE: i bought this thinking it was very cute and unique\n",
      "24 PRED: \n",
      "25 TRUE: i recieved it in the mail a few days later and it was all i had hoped for\n",
      "25 PRED: \n",
      "26 TRUE: the two <unk> were\n",
      "26 PRED: \n",
      "27 TRUE: a rather odd picture on the back including bull <unk>\n",
      "27 PRED: \n",
      "28 TRUE: and i really have to fight to get my cord/charger into any of the pockets\n",
      "28 PRED: \n",
      "29 TRUE: aside from those two negatives i am still pleased with the product and have decided to keep it and continue to use it , and would still recomend it to others\n",
      "29 PRED: \n",
      "30 TRUE: i ordered this brenthaven sleeve because the company had a reputation for making high quality products , but i definitely did not expect a `` sleeve `` to have such a solid construction\n",
      "30 PRED: \n",
      "31 TRUE: this sleeve totally blew my expectations away\n",
      "31 PRED: \n",
      "32 TRUE: there is no flex in the sleeve , and the inside is heavily padded\n",
      "32 PRED: \n",
      "33 TRUE: i would not have to worry if i accidentally dropped my laptop from # ft\n",
      "33 PRED: \n",
      "34 TRUE: i highly recommend this product to anyone who is looking for a thin and stylish case without sacrificing laptop protection\n",
      "34 PRED: in\n",
      "35 TRUE: this is the type of case i have been looking for to use with my macbook pro\n",
      "35 PRED: \n",
      "36 TRUE: i have been unable to find one\n",
      "36 PRED: \n",
      "37 TRUE: i found this one just after i got the computer and love it\n",
      "37 PRED: \n",
      "38 TRUE: it protects my computer without adding a lot of bulk or weight\n",
      "38 PRED: \n",
      "39 TRUE: it looks great in the pink color\n",
      "39 PRED: \n",
      "40 TRUE: nice ... no instructions but the two plastic shells fit well on the new # air\n",
      "40 PRED: \n",
      "41 TRUE: they eventually snap on and the color looks great\n",
      "41 PRED: \n",
      "42 TRUE: my issue with the case is that the weight of the lcd cover -lrb- top cover -rrb- is so much that it will open or close the cover if you are holding the laptop at an angle\n",
      "42 PRED: \n",
      "43 TRUE: sometimes i like laying in bed looking at the laptop and now with the cover attached , the cover will close on me\n",
      "43 PRED: \n",
      "44 TRUE: this is a good sleeve , but a little big for the # -inch macbook air because the laptop is so thin\n",
      "44 PRED: \n",
      "45 TRUE: the outer pockets are really only good for thin items -lrb- headphones , usb drive , etc . .\n",
      "45 PRED: \n",
      "46 TRUE: needless to say , i ca n't carry my charger in the sleeve , but it 's not such a big deal for me\n",
      "46 PRED: \n",
      "47 TRUE: the quality of the sleeve is really good and you ca n't beat the price\n",
      "47 PRED: \n",
      "48 TRUE: caught this on an amazon deal and it 's perfect\n",
      "48 PRED: \n",
      "49 TRUE: especially at the price i got it for\n",
      "49 PRED: \n",
      "50 TRUE: may never need another backpack again\n",
      "50 PRED: \n",
      "51 TRUE: super durable , roomy , tons of pockets , and comfortable to wear\n",
      "51 PRED: \n",
      "52 TRUE: i got this for my # & # # ; macbook air\n",
      "52 PRED: \n",
      "53 TRUE: not very heavily padded , but as im careful w my things that isnt a concern for me\n",
      "53 PRED: \n",
      "54 TRUE: i get a lot of compliments when i bring it out\n",
      "54 PRED: \n",
      "55 TRUE: the product has a good quality and matches the description\n",
      "55 PRED: \n",
      "56 TRUE: i bought the black one and it is really slim and feels comfortable when you typing or carrying your mac pro out\n",
      "56 PRED: \n",
      "57 TRUE: deserve the price\n",
      "57 PRED: \n",
      "58 TRUE: i needed a <unk> yet <unk> but definitely more protective than a silicone cover for my new -lrb- # -rrb- macbook air # inch\n",
      "58 PRED: \n",
      "59 TRUE: i am very happy with this case and get lots of compliments\n",
      "59 PRED: \n",
      "60 TRUE: it is very very functional and the price was reasonable\n",
      "60 PRED: \n",
      "61 TRUE: it 's durable , it 's been relatively weather resistant , looks good , and is comfortable -lrb- can be adjusted to wear in several configurations -rrb-\n",
      "61 PRED: \n",
      "62 TRUE: my only complaint is that there are no feet for stability on the bottom if you want set up upright , but the more stuff you have in it the easier it is to stand it up\n",
      "62 PRED: \n",
      "63 TRUE: this was a wonderful case for my laptop ; it does just what i needed , it keeps my laptop from sliding off of my bed , helps me hold on to my laptop when carrying it\n",
      "63 PRED: \n",
      "64 TRUE: great colors too\n",
      "64 PRED: \n"
     ]
    }
   ],
   "source": [
    "if len(log_df) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "while epoch < config.epochs:\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = get_feed_dict(batch)\n",
    "\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, sent_loss_kl_categ_batch, sent_loss_kl_gmm_batch, ppls_batch = \\\n",
    "        sess.run([opt, loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, sent_loss_kl_categ, sent_loss_kl_gmm, topic_ppls], feed_dict = feed_dict)\n",
    "   \n",
    "        if sent_loss_kl_batch == np.inf:\n",
    "            print('Nan occured')\n",
    "            ckpt = tf.train.get_checkpoint_state(config.modeldir)\n",
    "            model_checkpoint_path = ckpt.all_model_checkpoint_paths[-1]\n",
    "            saver.restore(sess, model_checkpoint_path)            \n",
    "            break\n",
    "            \n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if ct%config.log_period==0:\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train, sent_loss_recon_train, sent_loss_kl_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, sent_loss_recon_dev, sent_loss_kl_dev, ppl_dev = get_loss(sess, dev_batches)\n",
    "            global_step_log, beta_eval = sess.run([tf.train.get_global_step(), beta])\n",
    "            \n",
    "            if loss_dev < loss_min:\n",
    "                loss_min = loss_dev\n",
    "                saver.save(sess, config.modelpath, global_step=global_step_log)\n",
    "\n",
    "            clear_output()\n",
    "    \n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_reg_train, '%.2f'%sent_loss_recon_train, '%.2f'%sent_loss_kl_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_reg_dev, '%.2f'%sent_loss_recon_dev, '%.2f'%sent_loss_kl_dev,  '%.3f'%beta_eval],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "\n",
    "            print_summary(test_batches[1][1])\n",
    "            print_sample(batch)\n",
    "            \n",
    "            time_start = time.time()\n",
    "            \n",
    "    epoch += 1\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "_prob_topic, _prob_topic_sents, _prob_topic_infer, _means_topic_infer = debug_value([prob_topic, prob_topic_sents, prob_topic_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.6868487e-05, 8.6784212e-06, 9.9986529e-01, 1.4388333e-05,\n",
       "        1.7386952e-05, 1.4713172e-05, 7.6274541e-06, 1.7465694e-05,\n",
       "        1.1872702e-05, 1.5650594e-05], dtype=float32),\n",
       " array([1.4476807e-11, 1.0054337e-11, 1.0000000e+00, 1.1890730e-11,\n",
       "        1.2296763e-11, 1.2778320e-11, 1.0013915e-11, 1.2678735e-11,\n",
       "        1.2906035e-11, 1.3451982e-11], dtype=float32))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_i = 4\n",
    "_prob_topic_sents[batch_i], _prob_topic_infer[batch_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.8682381e+01, -1.7612494e+01, -1.7935837e+01, -1.7858501e+01],\n",
       "       [-1.8674824e+01, -1.8183336e+01, -1.8666691e+01, -1.8586023e+01],\n",
       "       [-3.4380314e-01, -2.0471608e-02,  1.2687216e-02,  9.8747285e-03],\n",
       "       [-1.8649149e+01, -1.8267252e+01, -1.8531384e+01, -1.8472548e+01],\n",
       "       [-1.8672371e+01, -1.7965401e+01, -1.8521042e+01, -1.8361164e+01],\n",
       "       [-1.8543634e+01, -1.7945541e+01, -1.8386732e+01, -1.8298574e+01],\n",
       "       [-1.8830236e+01, -1.8141018e+01, -1.8735767e+01, -1.8616648e+01],\n",
       "       [-1.8482653e+01, -1.7899456e+01, -1.8424776e+01, -1.8251898e+01],\n",
       "       [-1.8605921e+01, -1.8031254e+01, -1.8298433e+01, -1.8395790e+01],\n",
       "       [-1.8475525e+01, -1.8080276e+01, -1.8305523e+01, -1.8298229e+01]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_means_topic_infer[0][:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_means_topic, b_means_topic = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"topic/dec/mean_topic\")\n",
    "\n",
    "pred_topic_embeddings, pred_topic_bow, pred_means_topic, pred_logvars_topic, pred_token_idxs, _w_means_topic, _b_means_topic, _w_mean_topic_infer = \\\n",
    "                                sess.run([topic_embeddings, topic_bow, means_topic, logvars_topic, topic_beam_output_token_idxs, w_means_topic, b_means_topic, w_mean_topic_infer], \n",
    "                                         feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "\n",
    "pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "\n",
    "pred_topics_freq_bow_indices = np.argsort(pred_topic_bow, 1)[:, ::-1][:, :10]\n",
    "pred_topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cover', 'zips', 'floor', 'flexible', 'flaw', 'flat', 'flash', 'flap', 'fix', 'fitting']\n",
      "['cover', 'zips', 'floor', 'flexible', 'flaw', 'flat', 'flash', 'flap', 'fix', 'fitting']\n",
      "['cover', ';', '-', 'fix', 'fine', 'finger', 'fingerprints', 'finish', 'fitting', 'zips']\n",
      "['cover', 'zips', 'floor', 'flexible', 'flaw', 'flat', 'flash', 'flap', 'fix', 'fitting']\n",
      "['cover', 'zips', 'floor', 'flexible', 'flaw', 'flat', 'flash', 'flap', 'fix', 'fitting']\n",
      "['cover', 'zips', 'floor', 'flexible', 'flaw', 'flat', 'flash', 'flap', 'fix', 'fitting']\n",
      "['cover', 'zips', 'floor', 'flexible', 'flaw', 'flat', 'flash', 'flap', 'fix', 'fitting']\n",
      "['cover', 'zips', 'floor', 'flexible', 'flaw', 'flat', 'flash', 'flap', 'fix', 'fitting']\n",
      "['cover', 'zips', 'floor', 'flexible', 'flaw', 'flat', 'flash', 'flap', 'fix', 'fitting']\n",
      "['cover', 'zips', 'floor', 'flexible', 'flaw', 'flat', 'flash', 'flap', 'fix', 'fitting']\n"
     ]
    }
   ],
   "source": [
    "for idxs in pred_topics_freq_bow_idxs:\n",
    "    print([idx_to_word[idx] for idx in idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.836687  , -2.4325309 , -2.962067  , -2.1210825 , -2.6124496 ,\n",
       "        -1.8852538 , -2.4223897 , -2.7951393 , -2.2931848 , -2.1283157 ],\n",
       "       [-1.6798272 , -2.2147717 , -1.8608545 , -1.6574732 , -1.8061484 ,\n",
       "        -2.1520698 , -1.5335383 , -1.9913623 , -1.3796672 , -2.0616167 ],\n",
       "       [-1.0150758 , -0.89505154, -1.0576841 , -1.0169678 , -0.9941106 ,\n",
       "        -1.0553619 , -0.9833162 , -0.9620681 , -0.9419456 , -0.9740358 ],\n",
       "       [-2.4011621 , -2.0774152 , -1.6468724 , -2.3599555 , -2.3003724 ,\n",
       "        -2.1366048 , -2.1995428 , -2.0477817 , -2.3054452 , -2.083294  ],\n",
       "       [-1.9486684 , -1.5919771 , -2.2330887 , -1.8404497 , -1.6795243 ,\n",
       "        -1.7537042 , -1.8309035 , -2.172012  , -1.9229968 , -1.5811139 ],\n",
       "       [-1.7979274 , -2.3900738 , -2.2278528 , -2.05863   , -1.8887968 ,\n",
       "        -1.8150214 , -1.9893633 , -2.0921412 , -1.8070822 , -2.0015564 ],\n",
       "       [-1.9307586 , -2.6086934 , -2.179174  , -2.474641  , -2.6866994 ,\n",
       "        -3.1212592 , -1.8104733 , -1.8549055 , -2.0995457 , -2.6680553 ],\n",
       "       [-1.5515674 , -1.8104458 , -2.1408725 , -2.1475892 , -1.8703393 ,\n",
       "        -1.8393112 , -2.1167963 , -1.5572557 , -1.9509026 , -2.456197  ],\n",
       "       [-2.2025871 , -2.7079618 , -1.6625471 , -2.0059726 , -2.2501829 ,\n",
       "        -2.368145  , -2.2579024 , -1.9573809 , -2.1523035 , -2.3313115 ],\n",
       "       [-2.0786552 , -1.732205  , -2.3874724 , -1.968615  , -1.8172389 ,\n",
       "        -2.164403  , -2.3647726 , -2.4559977 , -2.1528792 , -2.2694247 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_topic_embeddings[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_topic_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0739732 , -0.08790135,  0.03179525, ..., -0.06434678,\n",
       "         0.0469898 , -0.01484643],\n",
       "       [ 0.12066822, -0.07779175,  0.0811064 , ...,  0.00998305,\n",
       "         0.02352253, -0.06851349],\n",
       "       [-0.18138495,  0.8433418 ,  0.61981755, ..., -0.11070126,\n",
       "        -0.24917184,  0.6417571 ],\n",
       "       ...,\n",
       "       [-0.07755507,  0.08136196, -0.03300147, ..., -0.03827026,\n",
       "        -0.01169072,  0.04148794],\n",
       "       [ 0.2552175 ,  0.77502584,  0.5159265 , ...,  0.26267377,\n",
       "         0.12818944,  0.3866017 ],\n",
       "       [ 0.06512748,  0.06922452, -0.01005569, ...,  0.10908195,\n",
       "        -0.07925141,  0.00807684]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_w_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.34380314, -0.0219513 ,  0.01438296,  0.00777078, -0.34469175,\n",
       "       -0.3049803 , -0.02120364, -0.41466707,  0.01747918, -0.00686976,\n",
       "        0.01272351, -0.02183504,  0.04956156,  0.03889449,  0.01130024,\n",
       "       -0.32727343,  0.01677949, -0.41357547,  0.01243312, -0.00792332,\n",
       "       -0.34260193, -0.41951048,  0.05819419, -0.30284566, -0.31787518,\n",
       "       -0.31931284, -0.32922003, -0.3255996 , -0.07046406, -0.29471713,\n",
       "       -0.3527893 ,  0.00778683], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.34380314, -0.0219513 ,  0.01438296,  0.00777078, -0.34469175,\n",
       "        -0.3049803 , -0.02120364, -0.41466707,  0.01747918, -0.00686976,\n",
       "         0.01272351, -0.02183504,  0.04956156,  0.03889449,  0.01130024,\n",
       "        -0.32727343,  0.01677949, -0.41357547,  0.01243312, -0.00792332,\n",
       "        -0.34260193, -0.41951048,  0.05819419, -0.30284566, -0.31787518,\n",
       "        -0.31931284, -0.32922003, -0.3255996 , -0.07046406, -0.29471713,\n",
       "        -0.3527893 ,  0.00778683],\n",
       "       [-0.34380314, -0.0219513 ,  0.01438296,  0.00777078, -0.34469175,\n",
       "        -0.3049803 , -0.02120364, -0.41466707,  0.01747918, -0.00686976,\n",
       "         0.01272351, -0.02183504,  0.04956156,  0.03889449,  0.01130024,\n",
       "        -0.32727343,  0.01677949, -0.41357547,  0.01243312, -0.00792332,\n",
       "        -0.34260193, -0.41951048,  0.05819419, -0.30284566, -0.31787518,\n",
       "        -0.31931284, -0.32922003, -0.3255996 , -0.07046406, -0.29471713,\n",
       "        -0.3527893 ,  0.00778683],\n",
       "       [-0.34380314, -0.0219513 ,  0.01438296,  0.00777078, -0.34469175,\n",
       "        -0.3049803 , -0.02120364, -0.41466707,  0.01747918, -0.00686976,\n",
       "         0.01272351, -0.02183504,  0.04956156,  0.03889449,  0.01130024,\n",
       "        -0.32727343,  0.01677949, -0.41357547,  0.01243312, -0.00792332,\n",
       "        -0.34260193, -0.41951048,  0.05819419, -0.30284566, -0.31787518,\n",
       "        -0.31931284, -0.32922003, -0.3255996 , -0.07046406, -0.29471713,\n",
       "        -0.3527893 ,  0.00778683],\n",
       "       [-0.34380314, -0.0219513 ,  0.01438296,  0.00777078, -0.34469175,\n",
       "        -0.3049803 , -0.02120364, -0.41466707,  0.01747918, -0.00686976,\n",
       "         0.01272351, -0.02183504,  0.04956156,  0.03889449,  0.01130024,\n",
       "        -0.32727343,  0.01677949, -0.41357547,  0.01243312, -0.00792332,\n",
       "        -0.34260193, -0.41951048,  0.05819419, -0.30284566, -0.31787518,\n",
       "        -0.31931284, -0.32922003, -0.3255996 , -0.07046406, -0.29471713,\n",
       "        -0.3527893 ,  0.00778683],\n",
       "       [-0.34380314, -0.0219513 ,  0.01438296,  0.00777078, -0.34469175,\n",
       "        -0.3049803 , -0.02120364, -0.41466707,  0.01747918, -0.00686976,\n",
       "         0.01272351, -0.02183504,  0.04956156,  0.03889449,  0.01130024,\n",
       "        -0.32727343,  0.01677949, -0.41357547,  0.01243312, -0.00792332,\n",
       "        -0.34260193, -0.41951048,  0.05819419, -0.30284566, -0.31787518,\n",
       "        -0.31931284, -0.32922003, -0.3255996 , -0.07046406, -0.29471713,\n",
       "        -0.3527893 ,  0.00778683],\n",
       "       [-0.34380314, -0.0219513 ,  0.01438296,  0.00777078, -0.34469175,\n",
       "        -0.3049803 , -0.02120364, -0.41466707,  0.01747918, -0.00686976,\n",
       "         0.01272351, -0.02183504,  0.04956156,  0.03889449,  0.01130024,\n",
       "        -0.32727343,  0.01677949, -0.41357547,  0.01243312, -0.00792332,\n",
       "        -0.34260193, -0.41951048,  0.05819419, -0.30284566, -0.31787518,\n",
       "        -0.31931284, -0.32922003, -0.3255996 , -0.07046406, -0.29471713,\n",
       "        -0.3527893 ,  0.00778683],\n",
       "       [-0.34380314, -0.0219513 ,  0.01438296,  0.00777078, -0.34469175,\n",
       "        -0.3049803 , -0.02120364, -0.41466707,  0.01747918, -0.00686976,\n",
       "         0.01272351, -0.02183504,  0.04956156,  0.03889449,  0.01130024,\n",
       "        -0.32727343,  0.01677949, -0.41357547,  0.01243312, -0.00792332,\n",
       "        -0.34260193, -0.41951048,  0.05819419, -0.30284566, -0.31787518,\n",
       "        -0.31931284, -0.32922003, -0.3255996 , -0.07046406, -0.29471713,\n",
       "        -0.3527893 ,  0.00778683],\n",
       "       [-0.34380314, -0.0219513 ,  0.01438296,  0.00777078, -0.34469175,\n",
       "        -0.3049803 , -0.02120364, -0.41466707,  0.01747918, -0.00686976,\n",
       "         0.01272351, -0.02183504,  0.04956156,  0.03889449,  0.01130024,\n",
       "        -0.32727343,  0.01677949, -0.41357547,  0.01243312, -0.00792332,\n",
       "        -0.34260193, -0.41951048,  0.05819419, -0.30284566, -0.31787518,\n",
       "        -0.31931284, -0.32922003, -0.3255996 , -0.07046406, -0.29471713,\n",
       "        -0.3527893 ,  0.00778683],\n",
       "       [-0.34380314, -0.0219513 ,  0.01438296,  0.00777078, -0.34469175,\n",
       "        -0.3049803 , -0.02120364, -0.41466707,  0.01747918, -0.00686976,\n",
       "         0.01272351, -0.02183504,  0.04956156,  0.03889449,  0.01130024,\n",
       "        -0.32727343,  0.01677949, -0.41357547,  0.01243312, -0.00792332,\n",
       "        -0.34260193, -0.41951048,  0.05819419, -0.30284566, -0.31787518,\n",
       "        -0.31931284, -0.32922003, -0.3255996 , -0.07046406, -0.29471713,\n",
       "        -0.3527893 ,  0.00778683],\n",
       "       [-0.34380314, -0.0219513 ,  0.01438296,  0.00777078, -0.34469175,\n",
       "        -0.3049803 , -0.02120364, -0.41466707,  0.01747918, -0.00686976,\n",
       "         0.01272351, -0.02183504,  0.04956156,  0.03889449,  0.01130024,\n",
       "        -0.32727343,  0.01677949, -0.41357547,  0.01243312, -0.00792332,\n",
       "        -0.34260193, -0.41951048,  0.05819419, -0.30284566, -0.31787518,\n",
       "        -0.31931284, -0.32922003, -0.3255996 , -0.07046406, -0.29471713,\n",
       "        -0.3527893 ,  0.00778683]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-35.833862, -35.247192, -35.53736 , -35.848877, -35.871967,\n",
       "        -35.88059 , -35.82204 , -35.93227 , -35.87375 , -35.87809 ],\n",
       "       [-35.57662 , -35.536495, -35.588844, -35.550194, -35.547264,\n",
       "        -35.56824 , -35.578804, -35.54832 , -35.5852  , -35.59411 ],\n",
       "       [-35.929325, -35.76501 , -36.037468, -35.967644, -35.233234,\n",
       "        -35.940414, -35.025246, -35.227554, -36.04773 , -36.170334],\n",
       "       [-35.57632 , -35.555447, -35.552483, -35.575047, -35.55795 ,\n",
       "        -35.547314, -35.56661 , -35.556694, -35.56401 , -35.54191 ],\n",
       "       [-35.57836 , -35.568584, -35.568565, -35.581158, -35.557106,\n",
       "        -35.572052, -35.54077 , -35.57486 , -35.5603  , -35.53741 ],\n",
       "       [-35.577534, -35.566864, -35.581524, -35.540504, -35.580524,\n",
       "        -35.55489 , -35.56971 , -35.58291 , -35.572197, -35.550014],\n",
       "       [-35.55864 , -35.556496, -35.552464, -35.559177, -35.542934,\n",
       "        -35.56165 , -35.583477, -35.56518 , -35.544052, -35.56943 ],\n",
       "       [-35.533333, -35.554634, -35.582344, -35.554573, -35.541954,\n",
       "        -35.54299 , -35.539135, -35.539368, -35.572212, -35.567078],\n",
       "       [-35.55204 , -35.56797 , -35.5471  , -35.51749 , -35.552086,\n",
       "        -35.538033, -35.515873, -35.569008, -35.517666, -35.526566],\n",
       "       [-35.54879 , -35.5959  , -35.55086 , -35.547096, -35.571575,\n",
       "        -35.579014, -35.57492 , -35.54526 , -35.574085, -35.573845]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_w_mean_topic_infer[:, :10, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.34380314, -0.0219513 ,  0.01438296,  0.00777078, -0.34469175,\n",
       "       -0.3049803 , -0.02120364, -0.41466707,  0.01747918, -0.00686976,\n",
       "        0.01272351, -0.02183504,  0.04956156,  0.03889449,  0.01130024,\n",
       "       -0.32727343,  0.01677949, -0.41357547,  0.01243312, -0.00792332,\n",
       "       -0.34260193, -0.41951048,  0.05819419, -0.30284566, -0.31787518,\n",
       "       -0.31931284, -0.32922003, -0.3255996 , -0.07046406, -0.29471713,\n",
       "       -0.3527893 ,  0.00778683], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enc_state_infer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-ee7c3cd147b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_enc_state_infer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_means_topic_infer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdebug_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menc_state_infer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans_topic_infer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'enc_state_infer' is not defined"
     ]
    }
   ],
   "source": [
    "_enc_state_infer, _means_topic_infer = debug_value([enc_state_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_enc_state_infer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_means_topic_infer[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
