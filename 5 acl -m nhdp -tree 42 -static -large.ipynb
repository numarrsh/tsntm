{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from data_structure import get_batches\n",
    "from hntm import HierarchicalNeuralTopicModel\n",
    "from tree import get_descendant_idxs\n",
    "from evaluation import validate, get_hierarchical_affinity, get_topic_specialization, print_topic_sample\n",
    "from configure import get_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(nb_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu\n",
    "np.random.seed(config.seed)\n",
    "random.seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.path_data,'rb'))\n",
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)\n",
    "config.dim_bow = len(bow_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     10
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables, model):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, model, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    return _variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = []\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "ppl_min = np.inf\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','','','','','VALID:','','','','','TEST:','', 'SPEC:', '', '', 'HIER:', ''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL','REG','LOSS','PPL','NLL','KL','REG','LOSS','PPL', '1', '2', '3', 'CHILD', 'OTHER']]))))\n",
    "\n",
    "cmd_rm = 'rm -r %s' % config.dir_model\n",
    "res = subprocess.call(cmd_rm.split())\n",
    "cmd_mk = 'mkdir %s' % config.dir_model\n",
    "res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "def update_checkpoint(config, checkpoint, global_step):\n",
    "    checkpoint.append(config.path_model + '-%i' % global_step)\n",
    "    if len(checkpoint) > config.max_to_keep:\n",
    "        path_model = checkpoint.pop(0) + '.*'\n",
    "        for p in glob.glob(path_model):\n",
    "            os.remove(p)\n",
    "    cPickle.dump(checkpoint, open(config.path_checkpoint, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "model = HierarchicalNeuralTopicModel(config)\n",
    "sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(max_to_keep=config.max_to_keep)\n",
    "update_tree_flg = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train & validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th colspan=\"4\" halign=\"left\"></th>\n",
       "      <th>VALID:</th>\n",
       "      <th colspan=\"4\" halign=\"left\"></th>\n",
       "      <th>TEST:</th>\n",
       "      <th></th>\n",
       "      <th>SPEC:</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "      <th>HIER:</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>CHILD</th>\n",
       "      <th>OTHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>89</td>\n",
       "      <td>8</td>\n",
       "      <td>463</td>\n",
       "      <td>9162.94</td>\n",
       "      <td>1857</td>\n",
       "      <td>9141.54</td>\n",
       "      <td>21.24</td>\n",
       "      <td>0.15</td>\n",
       "      <td>9160.31</td>\n",
       "      <td>1796</td>\n",
       "      <td>9138.45</td>\n",
       "      <td>21.75</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9160.37</td>\n",
       "      <td>1796</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>82</td>\n",
       "      <td>17</td>\n",
       "      <td>360</td>\n",
       "      <td>9130.95</td>\n",
       "      <td>1808</td>\n",
       "      <td>9109.57</td>\n",
       "      <td>21.27</td>\n",
       "      <td>0.13</td>\n",
       "      <td>9148.71</td>\n",
       "      <td>1780</td>\n",
       "      <td>9128.07</td>\n",
       "      <td>20.53</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9150.20</td>\n",
       "      <td>1781</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>81</td>\n",
       "      <td>26</td>\n",
       "      <td>257</td>\n",
       "      <td>9117.10</td>\n",
       "      <td>1787</td>\n",
       "      <td>9095.83</td>\n",
       "      <td>21.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9145.18</td>\n",
       "      <td>1774</td>\n",
       "      <td>9124.39</td>\n",
       "      <td>20.70</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9145.59</td>\n",
       "      <td>1774</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>80</td>\n",
       "      <td>35</td>\n",
       "      <td>154</td>\n",
       "      <td>9109.00</td>\n",
       "      <td>1776</td>\n",
       "      <td>9087.75</td>\n",
       "      <td>21.01</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9143.37</td>\n",
       "      <td>1772</td>\n",
       "      <td>9122.77</td>\n",
       "      <td>20.50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9142.21</td>\n",
       "      <td>1770</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>80</td>\n",
       "      <td>44</td>\n",
       "      <td>51</td>\n",
       "      <td>9103.41</td>\n",
       "      <td>1768</td>\n",
       "      <td>9082.33</td>\n",
       "      <td>20.91</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9142.93</td>\n",
       "      <td>1771</td>\n",
       "      <td>9122.50</td>\n",
       "      <td>20.33</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9142.54</td>\n",
       "      <td>1770</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>80</td>\n",
       "      <td>52</td>\n",
       "      <td>515</td>\n",
       "      <td>9099.50</td>\n",
       "      <td>1762</td>\n",
       "      <td>9078.56</td>\n",
       "      <td>20.82</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9142.96</td>\n",
       "      <td>1771</td>\n",
       "      <td>9122.67</td>\n",
       "      <td>20.19</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9142.51</td>\n",
       "      <td>1770</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35000</th>\n",
       "      <td>80</td>\n",
       "      <td>61</td>\n",
       "      <td>412</td>\n",
       "      <td>9096.81</td>\n",
       "      <td>1758</td>\n",
       "      <td>9075.89</td>\n",
       "      <td>20.74</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9141.70</td>\n",
       "      <td>1769</td>\n",
       "      <td>9121.42</td>\n",
       "      <td>20.20</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9141.16</td>\n",
       "      <td>1768</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40000</th>\n",
       "      <td>82</td>\n",
       "      <td>70</td>\n",
       "      <td>309</td>\n",
       "      <td>9094.63</td>\n",
       "      <td>1755</td>\n",
       "      <td>9073.75</td>\n",
       "      <td>20.67</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9141.14</td>\n",
       "      <td>1768</td>\n",
       "      <td>9120.99</td>\n",
       "      <td>20.06</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9141.76</td>\n",
       "      <td>1769</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45000</th>\n",
       "      <td>80</td>\n",
       "      <td>79</td>\n",
       "      <td>206</td>\n",
       "      <td>9092.70</td>\n",
       "      <td>1752</td>\n",
       "      <td>9071.90</td>\n",
       "      <td>20.60</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9140.66</td>\n",
       "      <td>1768</td>\n",
       "      <td>9120.56</td>\n",
       "      <td>20.01</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9141.76</td>\n",
       "      <td>1769</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000</th>\n",
       "      <td>78</td>\n",
       "      <td>88</td>\n",
       "      <td>103</td>\n",
       "      <td>9091.04</td>\n",
       "      <td>1750</td>\n",
       "      <td>9070.32</td>\n",
       "      <td>20.54</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9140.12</td>\n",
       "      <td>1767</td>\n",
       "      <td>9120.04</td>\n",
       "      <td>19.99</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9140.82</td>\n",
       "      <td>1768</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55000</th>\n",
       "      <td>72</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>9089.55</td>\n",
       "      <td>1748</td>\n",
       "      <td>9068.91</td>\n",
       "      <td>20.49</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9141.03</td>\n",
       "      <td>1768</td>\n",
       "      <td>9121.12</td>\n",
       "      <td>19.82</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9140.82</td>\n",
       "      <td>1768</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60000</th>\n",
       "      <td>75</td>\n",
       "      <td>105</td>\n",
       "      <td>464</td>\n",
       "      <td>9088.38</td>\n",
       "      <td>1746</td>\n",
       "      <td>9067.80</td>\n",
       "      <td>20.44</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9142.01</td>\n",
       "      <td>1768</td>\n",
       "      <td>9122.19</td>\n",
       "      <td>19.73</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9140.82</td>\n",
       "      <td>1768</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65000</th>\n",
       "      <td>73</td>\n",
       "      <td>114</td>\n",
       "      <td>361</td>\n",
       "      <td>9087.46</td>\n",
       "      <td>1744</td>\n",
       "      <td>9066.87</td>\n",
       "      <td>20.40</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9139.41</td>\n",
       "      <td>1766</td>\n",
       "      <td>9119.69</td>\n",
       "      <td>19.64</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9140.06</td>\n",
       "      <td>1767</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70000</th>\n",
       "      <td>74</td>\n",
       "      <td>123</td>\n",
       "      <td>258</td>\n",
       "      <td>9086.54</td>\n",
       "      <td>1743</td>\n",
       "      <td>9065.96</td>\n",
       "      <td>20.36</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9140.72</td>\n",
       "      <td>1768</td>\n",
       "      <td>9120.89</td>\n",
       "      <td>19.74</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9140.06</td>\n",
       "      <td>1767</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75000</th>\n",
       "      <td>75</td>\n",
       "      <td>132</td>\n",
       "      <td>155</td>\n",
       "      <td>9085.67</td>\n",
       "      <td>1742</td>\n",
       "      <td>9065.12</td>\n",
       "      <td>20.33</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9139.00</td>\n",
       "      <td>1766</td>\n",
       "      <td>9119.23</td>\n",
       "      <td>19.68</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80000</th>\n",
       "      <td>74</td>\n",
       "      <td>141</td>\n",
       "      <td>52</td>\n",
       "      <td>9084.85</td>\n",
       "      <td>1741</td>\n",
       "      <td>9064.33</td>\n",
       "      <td>20.29</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9139.90</td>\n",
       "      <td>1766</td>\n",
       "      <td>9120.12</td>\n",
       "      <td>19.69</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85000</th>\n",
       "      <td>75</td>\n",
       "      <td>149</td>\n",
       "      <td>516</td>\n",
       "      <td>9084.15</td>\n",
       "      <td>1740</td>\n",
       "      <td>9063.67</td>\n",
       "      <td>20.26</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9139.87</td>\n",
       "      <td>1766</td>\n",
       "      <td>9120.17</td>\n",
       "      <td>19.62</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90000</th>\n",
       "      <td>70</td>\n",
       "      <td>158</td>\n",
       "      <td>413</td>\n",
       "      <td>9083.58</td>\n",
       "      <td>1739</td>\n",
       "      <td>9063.12</td>\n",
       "      <td>20.22</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9139.67</td>\n",
       "      <td>1766</td>\n",
       "      <td>9119.95</td>\n",
       "      <td>19.64</td>\n",
       "      <td>0.08</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95000</th>\n",
       "      <td>75</td>\n",
       "      <td>167</td>\n",
       "      <td>310</td>\n",
       "      <td>9083.07</td>\n",
       "      <td>1738</td>\n",
       "      <td>9062.63</td>\n",
       "      <td>20.19</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9139.82</td>\n",
       "      <td>1766</td>\n",
       "      <td>9120.20</td>\n",
       "      <td>19.53</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>74</td>\n",
       "      <td>176</td>\n",
       "      <td>207</td>\n",
       "      <td>9082.55</td>\n",
       "      <td>1738</td>\n",
       "      <td>9062.12</td>\n",
       "      <td>20.15</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9139.97</td>\n",
       "      <td>1767</td>\n",
       "      <td>9120.30</td>\n",
       "      <td>19.58</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105000</th>\n",
       "      <td>72</td>\n",
       "      <td>185</td>\n",
       "      <td>104</td>\n",
       "      <td>9082.06</td>\n",
       "      <td>1737</td>\n",
       "      <td>9061.66</td>\n",
       "      <td>20.12</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9139.31</td>\n",
       "      <td>1766</td>\n",
       "      <td>9119.80</td>\n",
       "      <td>19.43</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110000</th>\n",
       "      <td>76</td>\n",
       "      <td>194</td>\n",
       "      <td>1</td>\n",
       "      <td>9081.55</td>\n",
       "      <td>1736</td>\n",
       "      <td>9061.18</td>\n",
       "      <td>20.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9139.94</td>\n",
       "      <td>1767</td>\n",
       "      <td>9120.58</td>\n",
       "      <td>19.27</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115000</th>\n",
       "      <td>78</td>\n",
       "      <td>202</td>\n",
       "      <td>465</td>\n",
       "      <td>9081.14</td>\n",
       "      <td>1736</td>\n",
       "      <td>9060.80</td>\n",
       "      <td>20.05</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9140.19</td>\n",
       "      <td>1766</td>\n",
       "      <td>9120.92</td>\n",
       "      <td>19.19</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120000</th>\n",
       "      <td>77</td>\n",
       "      <td>211</td>\n",
       "      <td>362</td>\n",
       "      <td>9080.82</td>\n",
       "      <td>1735</td>\n",
       "      <td>9060.53</td>\n",
       "      <td>20.01</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9139.92</td>\n",
       "      <td>1767</td>\n",
       "      <td>9120.64</td>\n",
       "      <td>19.19</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125000</th>\n",
       "      <td>74</td>\n",
       "      <td>220</td>\n",
       "      <td>259</td>\n",
       "      <td>9080.51</td>\n",
       "      <td>1735</td>\n",
       "      <td>9060.31</td>\n",
       "      <td>19.98</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9139.67</td>\n",
       "      <td>1766</td>\n",
       "      <td>9120.45</td>\n",
       "      <td>19.14</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130000</th>\n",
       "      <td>77</td>\n",
       "      <td>229</td>\n",
       "      <td>156</td>\n",
       "      <td>9080.20</td>\n",
       "      <td>1734</td>\n",
       "      <td>9060.09</td>\n",
       "      <td>19.95</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9139.51</td>\n",
       "      <td>1766</td>\n",
       "      <td>9120.15</td>\n",
       "      <td>19.27</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135000</th>\n",
       "      <td>73</td>\n",
       "      <td>238</td>\n",
       "      <td>53</td>\n",
       "      <td>9079.87</td>\n",
       "      <td>1734</td>\n",
       "      <td>9059.85</td>\n",
       "      <td>19.92</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9141.20</td>\n",
       "      <td>1768</td>\n",
       "      <td>9121.94</td>\n",
       "      <td>19.17</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140000</th>\n",
       "      <td>71</td>\n",
       "      <td>246</td>\n",
       "      <td>517</td>\n",
       "      <td>9079.58</td>\n",
       "      <td>1733</td>\n",
       "      <td>9059.65</td>\n",
       "      <td>19.89</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9141.29</td>\n",
       "      <td>1768</td>\n",
       "      <td>9122.06</td>\n",
       "      <td>19.13</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145000</th>\n",
       "      <td>71</td>\n",
       "      <td>255</td>\n",
       "      <td>414</td>\n",
       "      <td>9079.35</td>\n",
       "      <td>1733</td>\n",
       "      <td>9059.49</td>\n",
       "      <td>19.86</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9139.66</td>\n",
       "      <td>1766</td>\n",
       "      <td>9120.53</td>\n",
       "      <td>19.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150000</th>\n",
       "      <td>72</td>\n",
       "      <td>264</td>\n",
       "      <td>311</td>\n",
       "      <td>9079.13</td>\n",
       "      <td>1732</td>\n",
       "      <td>9059.35</td>\n",
       "      <td>19.84</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9140.45</td>\n",
       "      <td>1767</td>\n",
       "      <td>9121.38</td>\n",
       "      <td>18.98</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155000</th>\n",
       "      <td>72</td>\n",
       "      <td>273</td>\n",
       "      <td>208</td>\n",
       "      <td>9078.90</td>\n",
       "      <td>1732</td>\n",
       "      <td>9059.19</td>\n",
       "      <td>19.81</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9140.77</td>\n",
       "      <td>1767</td>\n",
       "      <td>9121.62</td>\n",
       "      <td>19.06</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160000</th>\n",
       "      <td>73</td>\n",
       "      <td>282</td>\n",
       "      <td>105</td>\n",
       "      <td>9078.68</td>\n",
       "      <td>1732</td>\n",
       "      <td>9059.02</td>\n",
       "      <td>19.79</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9139.90</td>\n",
       "      <td>1766</td>\n",
       "      <td>9120.84</td>\n",
       "      <td>18.97</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165000</th>\n",
       "      <td>72</td>\n",
       "      <td>291</td>\n",
       "      <td>2</td>\n",
       "      <td>9078.42</td>\n",
       "      <td>1731</td>\n",
       "      <td>9058.83</td>\n",
       "      <td>19.76</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9140.55</td>\n",
       "      <td>1767</td>\n",
       "      <td>9121.53</td>\n",
       "      <td>18.94</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170000</th>\n",
       "      <td>71</td>\n",
       "      <td>299</td>\n",
       "      <td>466</td>\n",
       "      <td>9078.22</td>\n",
       "      <td>1731</td>\n",
       "      <td>9058.69</td>\n",
       "      <td>19.74</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9140.64</td>\n",
       "      <td>1767</td>\n",
       "      <td>9121.61</td>\n",
       "      <td>18.94</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175000</th>\n",
       "      <td>70</td>\n",
       "      <td>308</td>\n",
       "      <td>363</td>\n",
       "      <td>9078.06</td>\n",
       "      <td>1731</td>\n",
       "      <td>9058.59</td>\n",
       "      <td>19.72</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9140.67</td>\n",
       "      <td>1767</td>\n",
       "      <td>9121.74</td>\n",
       "      <td>18.85</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180000</th>\n",
       "      <td>71</td>\n",
       "      <td>317</td>\n",
       "      <td>260</td>\n",
       "      <td>9077.88</td>\n",
       "      <td>1730</td>\n",
       "      <td>9058.46</td>\n",
       "      <td>19.70</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9140.70</td>\n",
       "      <td>1767</td>\n",
       "      <td>9121.82</td>\n",
       "      <td>18.79</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185000</th>\n",
       "      <td>69</td>\n",
       "      <td>326</td>\n",
       "      <td>157</td>\n",
       "      <td>9077.69</td>\n",
       "      <td>1730</td>\n",
       "      <td>9058.31</td>\n",
       "      <td>19.67</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9139.95</td>\n",
       "      <td>1766</td>\n",
       "      <td>9121.00</td>\n",
       "      <td>18.87</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190000</th>\n",
       "      <td>73</td>\n",
       "      <td>335</td>\n",
       "      <td>54</td>\n",
       "      <td>9077.48</td>\n",
       "      <td>1730</td>\n",
       "      <td>9058.16</td>\n",
       "      <td>19.65</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9140.62</td>\n",
       "      <td>1767</td>\n",
       "      <td>9121.65</td>\n",
       "      <td>18.89</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195000</th>\n",
       "      <td>75</td>\n",
       "      <td>343</td>\n",
       "      <td>518</td>\n",
       "      <td>9077.30</td>\n",
       "      <td>1729</td>\n",
       "      <td>9058.03</td>\n",
       "      <td>19.63</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9141.52</td>\n",
       "      <td>1768</td>\n",
       "      <td>9122.62</td>\n",
       "      <td>18.82</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200000</th>\n",
       "      <td>74</td>\n",
       "      <td>352</td>\n",
       "      <td>415</td>\n",
       "      <td>9077.17</td>\n",
       "      <td>1729</td>\n",
       "      <td>9057.93</td>\n",
       "      <td>19.61</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9140.18</td>\n",
       "      <td>1766</td>\n",
       "      <td>9121.39</td>\n",
       "      <td>18.70</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205000</th>\n",
       "      <td>81</td>\n",
       "      <td>361</td>\n",
       "      <td>312</td>\n",
       "      <td>9077.04</td>\n",
       "      <td>1729</td>\n",
       "      <td>9057.83</td>\n",
       "      <td>19.59</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9139.99</td>\n",
       "      <td>1766</td>\n",
       "      <td>9121.12</td>\n",
       "      <td>18.78</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210000</th>\n",
       "      <td>75</td>\n",
       "      <td>370</td>\n",
       "      <td>209</td>\n",
       "      <td>9076.89</td>\n",
       "      <td>1729</td>\n",
       "      <td>9057.73</td>\n",
       "      <td>19.57</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9140.43</td>\n",
       "      <td>1767</td>\n",
       "      <td>9121.49</td>\n",
       "      <td>18.85</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215000</th>\n",
       "      <td>76</td>\n",
       "      <td>379</td>\n",
       "      <td>106</td>\n",
       "      <td>9076.74</td>\n",
       "      <td>1728</td>\n",
       "      <td>9057.61</td>\n",
       "      <td>19.55</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9139.59</td>\n",
       "      <td>1766</td>\n",
       "      <td>9120.86</td>\n",
       "      <td>18.64</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220000</th>\n",
       "      <td>71</td>\n",
       "      <td>388</td>\n",
       "      <td>3</td>\n",
       "      <td>9076.57</td>\n",
       "      <td>1728</td>\n",
       "      <td>9057.48</td>\n",
       "      <td>19.53</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9141.70</td>\n",
       "      <td>1769</td>\n",
       "      <td>9122.88</td>\n",
       "      <td>18.73</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225000</th>\n",
       "      <td>74</td>\n",
       "      <td>396</td>\n",
       "      <td>467</td>\n",
       "      <td>9076.44</td>\n",
       "      <td>1728</td>\n",
       "      <td>9057.38</td>\n",
       "      <td>19.51</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9141.45</td>\n",
       "      <td>1768</td>\n",
       "      <td>9122.76</td>\n",
       "      <td>18.60</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230000</th>\n",
       "      <td>81</td>\n",
       "      <td>405</td>\n",
       "      <td>364</td>\n",
       "      <td>9076.34</td>\n",
       "      <td>1728</td>\n",
       "      <td>9057.31</td>\n",
       "      <td>19.49</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9140.06</td>\n",
       "      <td>1767</td>\n",
       "      <td>9121.39</td>\n",
       "      <td>18.58</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235000</th>\n",
       "      <td>72</td>\n",
       "      <td>414</td>\n",
       "      <td>261</td>\n",
       "      <td>9076.23</td>\n",
       "      <td>1728</td>\n",
       "      <td>9057.22</td>\n",
       "      <td>19.47</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9141.18</td>\n",
       "      <td>1768</td>\n",
       "      <td>9122.57</td>\n",
       "      <td>18.52</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240000</th>\n",
       "      <td>73</td>\n",
       "      <td>423</td>\n",
       "      <td>158</td>\n",
       "      <td>9076.02</td>\n",
       "      <td>1727</td>\n",
       "      <td>9057.02</td>\n",
       "      <td>19.45</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9140.27</td>\n",
       "      <td>1767</td>\n",
       "      <td>9121.60</td>\n",
       "      <td>18.58</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245000</th>\n",
       "      <td>72</td>\n",
       "      <td>432</td>\n",
       "      <td>55</td>\n",
       "      <td>9075.76</td>\n",
       "      <td>1727</td>\n",
       "      <td>9056.74</td>\n",
       "      <td>19.43</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9141.11</td>\n",
       "      <td>1768</td>\n",
       "      <td>9122.45</td>\n",
       "      <td>18.57</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9138.49</td>\n",
       "      <td>1765</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        TRAIN:                               VALID:        \\\n",
       "       Time   Ep   Ct     LOSS   PPL      NLL     KL   REG     LOSS   PPL   \n",
       "5000     89    8  463  9162.94  1857  9141.54  21.24  0.15  9160.31  1796   \n",
       "10000    82   17  360  9130.95  1808  9109.57  21.27  0.13  9148.71  1780   \n",
       "15000    81   26  257  9117.10  1787  9095.83  21.12  0.12  9145.18  1774   \n",
       "20000    80   35  154  9109.00  1776  9087.75  21.01  0.12  9143.37  1772   \n",
       "25000    80   44   51  9103.41  1768  9082.33  20.91  0.11  9142.93  1771   \n",
       "30000    80   52  515  9099.50  1762  9078.56  20.82  0.11  9142.96  1771   \n",
       "35000    80   61  412  9096.81  1758  9075.89  20.74  0.11  9141.70  1769   \n",
       "40000    82   70  309  9094.63  1755  9073.75  20.67  0.10  9141.14  1768   \n",
       "45000    80   79  206  9092.70  1752  9071.90  20.60  0.10  9140.66  1768   \n",
       "50000    78   88  103  9091.04  1750  9070.32  20.54  0.10  9140.12  1767   \n",
       "55000    72   97    0  9089.55  1748  9068.91  20.49  0.10  9141.03  1768   \n",
       "60000    75  105  464  9088.38  1746  9067.80  20.44  0.10  9142.01  1768   \n",
       "65000    73  114  361  9087.46  1744  9066.87  20.40  0.10  9139.41  1766   \n",
       "70000    74  123  258  9086.54  1743  9065.96  20.36  0.10  9140.72  1768   \n",
       "75000    75  132  155  9085.67  1742  9065.12  20.33  0.10  9139.00  1766   \n",
       "80000    74  141   52  9084.85  1741  9064.33  20.29  0.10  9139.90  1766   \n",
       "85000    75  149  516  9084.15  1740  9063.67  20.26  0.10  9139.87  1766   \n",
       "90000    70  158  413  9083.58  1739  9063.12  20.22  0.09  9139.67  1766   \n",
       "95000    75  167  310  9083.07  1738  9062.63  20.19  0.09  9139.82  1766   \n",
       "100000   74  176  207  9082.55  1738  9062.12  20.15  0.09  9139.97  1767   \n",
       "105000   72  185  104  9082.06  1737  9061.66  20.12  0.09  9139.31  1766   \n",
       "110000   76  194    1  9081.55  1736  9061.18  20.08  0.09  9139.94  1767   \n",
       "115000   78  202  465  9081.14  1736  9060.80  20.05  0.09  9140.19  1766   \n",
       "120000   77  211  362  9080.82  1735  9060.53  20.01  0.09  9139.92  1767   \n",
       "125000   74  220  259  9080.51  1735  9060.31  19.98  0.09  9139.67  1766   \n",
       "130000   77  229  156  9080.20  1734  9060.09  19.95  0.09  9139.51  1766   \n",
       "135000   73  238   53  9079.87  1734  9059.85  19.92  0.09  9141.20  1768   \n",
       "140000   71  246  517  9079.58  1733  9059.65  19.89  0.09  9141.29  1768   \n",
       "145000   71  255  414  9079.35  1733  9059.49  19.86  0.09  9139.66  1766   \n",
       "150000   72  264  311  9079.13  1732  9059.35  19.84  0.09  9140.45  1767   \n",
       "155000   72  273  208  9078.90  1732  9059.19  19.81  0.09  9140.77  1767   \n",
       "160000   73  282  105  9078.68  1732  9059.02  19.79  0.09  9139.90  1766   \n",
       "165000   72  291    2  9078.42  1731  9058.83  19.76  0.09  9140.55  1767   \n",
       "170000   71  299  466  9078.22  1731  9058.69  19.74  0.09  9140.64  1767   \n",
       "175000   70  308  363  9078.06  1731  9058.59  19.72  0.09  9140.67  1767   \n",
       "180000   71  317  260  9077.88  1730  9058.46  19.70  0.09  9140.70  1767   \n",
       "185000   69  326  157  9077.69  1730  9058.31  19.67  0.09  9139.95  1766   \n",
       "190000   73  335   54  9077.48  1730  9058.16  19.65  0.09  9140.62  1767   \n",
       "195000   75  343  518  9077.30  1729  9058.03  19.63  0.09  9141.52  1768   \n",
       "200000   74  352  415  9077.17  1729  9057.93  19.61  0.09  9140.18  1766   \n",
       "205000   81  361  312  9077.04  1729  9057.83  19.59  0.09  9139.99  1766   \n",
       "210000   75  370  209  9076.89  1729  9057.73  19.57  0.09  9140.43  1767   \n",
       "215000   76  379  106  9076.74  1728  9057.61  19.55  0.09  9139.59  1766   \n",
       "220000   71  388    3  9076.57  1728  9057.48  19.53  0.09  9141.70  1769   \n",
       "225000   74  396  467  9076.44  1728  9057.38  19.51  0.09  9141.45  1768   \n",
       "230000   81  405  364  9076.34  1728  9057.31  19.49  0.09  9140.06  1767   \n",
       "235000   72  414  261  9076.23  1728  9057.22  19.47  0.09  9141.18  1768   \n",
       "240000   73  423  158  9076.02  1727  9057.02  19.45  0.09  9140.27  1767   \n",
       "245000   72  432   55  9075.76  1727  9056.74  19.43  0.09  9141.11  1768   \n",
       "\n",
       "                                TEST:       SPEC:             HIER:        \n",
       "            NLL     KL   REG     LOSS   PPL     1     2     3 CHILD OTHER  \n",
       "5000    9138.45  21.75  0.11  9160.37  1796  0.36  0.43  0.45  0.32  0.25  \n",
       "10000   9128.07  20.53  0.11  9150.20  1781  0.38  0.43  0.46  0.29  0.26  \n",
       "15000   9124.39  20.70  0.10  9145.59  1774  0.37  0.44  0.46  0.28  0.26  \n",
       "20000   9122.77  20.50  0.10  9142.21  1770  0.38  0.44  0.47  0.27  0.25  \n",
       "25000   9122.50  20.33  0.09  9142.54  1770  0.38  0.44  0.46  0.27  0.26  \n",
       "30000   9122.67  20.19  0.09  9142.51  1770  0.38  0.44  0.46  0.28  0.26  \n",
       "35000   9121.42  20.20  0.09  9141.16  1768  0.37  0.44  0.46  0.26  0.26  \n",
       "40000   9120.99  20.06  0.09  9141.76  1769  0.37  0.44  0.46  0.26  0.26  \n",
       "45000   9120.56  20.01  0.09  9141.76  1769  0.37  0.44  0.47  0.26  0.26  \n",
       "50000   9120.04  19.99  0.09  9140.82  1768  0.38  0.44  0.47  0.26  0.26  \n",
       "55000   9121.12  19.82  0.09  9140.82  1768  0.38  0.44  0.46  0.27  0.26  \n",
       "60000   9122.19  19.73  0.09  9140.82  1768  0.37  0.43  0.46  0.27  0.27  \n",
       "65000   9119.69  19.64  0.09  9140.06  1767  0.38  0.43  0.46  0.26  0.27  \n",
       "70000   9120.89  19.74  0.09  9140.06  1767  0.38  0.44  0.46  0.26  0.27  \n",
       "75000   9119.23  19.68  0.09  9138.49  1765  0.38  0.44  0.46  0.26  0.26  \n",
       "80000   9120.12  19.69  0.09  9138.49  1765  0.38  0.44  0.47  0.26  0.26  \n",
       "85000   9120.17  19.62  0.09  9138.49  1765  0.38  0.44  0.46  0.26  0.27  \n",
       "90000   9119.95  19.64  0.08  9138.49  1765  0.37  0.44  0.46  0.25  0.26  \n",
       "95000   9120.20  19.53  0.09  9138.49  1765  0.37  0.44  0.46  0.25  0.26  \n",
       "100000  9120.30  19.58  0.09  9138.49  1765  0.37  0.44  0.46  0.26  0.26  \n",
       "105000  9119.80  19.43  0.09  9138.49  1765  0.38  0.44  0.46  0.26  0.27  \n",
       "110000  9120.58  19.27  0.09  9138.49  1765  0.38  0.44  0.46  0.26  0.27  \n",
       "115000  9120.92  19.19  0.09  9138.49  1765  0.37  0.44  0.46  0.26  0.27  \n",
       "120000  9120.64  19.19  0.09  9138.49  1765  0.38  0.44  0.46  0.25  0.27  \n",
       "125000  9120.45  19.14  0.09  9138.49  1765  0.38  0.44  0.46  0.25  0.27  \n",
       "130000  9120.15  19.27  0.09  9138.49  1765  0.38  0.44  0.47  0.25  0.26  \n",
       "135000  9121.94  19.17  0.09  9138.49  1765  0.38  0.44  0.47  0.25  0.26  \n",
       "140000  9122.06  19.13  0.09  9138.49  1765  0.38  0.44  0.46  0.26  0.27  \n",
       "145000  9120.53  19.04  0.09  9138.49  1765  0.37  0.44  0.47  0.25  0.27  \n",
       "150000  9121.38  18.98  0.09  9138.49  1765  0.37  0.44  0.47  0.25  0.26  \n",
       "155000  9121.62  19.06  0.09  9138.49  1765  0.37  0.44  0.47  0.25  0.26  \n",
       "160000  9120.84  18.97  0.09  9138.49  1765  0.38  0.44  0.46  0.25  0.27  \n",
       "165000  9121.53  18.94  0.09  9138.49  1765  0.38  0.44  0.47  0.25  0.27  \n",
       "170000  9121.61  18.94  0.09  9138.49  1765  0.37  0.44  0.46  0.26  0.27  \n",
       "175000  9121.74  18.85  0.09  9138.49  1765  0.38  0.44  0.47  0.25  0.27  \n",
       "180000  9121.82  18.79  0.09  9138.49  1765  0.38  0.44  0.47  0.25  0.27  \n",
       "185000  9121.00  18.87  0.09  9138.49  1765  0.38  0.45  0.47  0.25  0.26  \n",
       "190000  9121.65  18.89  0.09  9138.49  1765  0.38  0.44  0.46  0.25  0.27  \n",
       "195000  9122.62  18.82  0.09  9138.49  1765  0.38  0.44  0.46  0.26  0.27  \n",
       "200000  9121.39  18.70  0.09  9138.49  1765  0.37  0.44  0.47  0.25  0.27  \n",
       "205000  9121.12  18.78  0.09  9138.49  1765  0.37  0.44  0.46  0.25  0.27  \n",
       "210000  9121.49  18.85  0.09  9138.49  1765  0.37  0.44  0.47  0.25  0.26  \n",
       "215000  9120.86  18.64  0.09  9138.49  1765  0.38  0.44  0.46  0.25  0.27  \n",
       "220000  9122.88  18.73  0.09  9138.49  1765  0.38  0.44  0.47  0.25  0.27  \n",
       "225000  9122.76  18.60  0.09  9138.49  1765  0.37  0.44  0.46  0.25  0.27  \n",
       "230000  9121.39  18.58  0.09  9138.49  1765  0.38  0.44  0.47  0.25  0.27  \n",
       "235000  9122.57  18.52  0.09  9138.49  1765  0.37  0.44  0.47  0.25  0.27  \n",
       "240000  9121.60  18.58  0.09  9138.49  1765  0.38  0.45  0.47  0.25  0.26  \n",
       "245000  9122.45  18.57  0.09  9138.49  1765  0.38  0.44  0.47  0.25  0.27  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 R: 1.000 P: 0.152 semantic verb syntactic lexical structure verbs noun type object rules\n",
      "   1 R: 0.227 P: 0.059 tree parsing dependency parser node trees grammar parse nodes rules\n",
      "     11 R: 0.108 P: 0.108 training models features algorithm performance probability feature learning test method\n",
      "     12 R: 0.059 P: 0.059 models neural network training embeddings layer vector representations input representation\n",
      "   2 R: 0.221 P: 0.078 similarity sense semantic context method vector wordnet senses models pairs\n",
      "     21 R: 0.056 P: 0.056 translation english source alignment target phrase sentences translations pairs parallel\n",
      "     22 R: 0.087 P: 0.087 features classification feature sentiment classifier topic positive analysis negative training\n",
      "   3 R: 0.191 P: 0.064 entity features entities training extraction relation performance patterns relations precision\n",
      "     31 R: 0.079 P: 0.079 languages english pos errors morphological character chinese tags tag error\n",
      "     32 R: 0.048 P: 0.048 annotation event discourse relations annotated events annotations relation temporal argument\n",
      "   4 R: 0.210 P: 0.054 document query documents question sentences topic answer questions terms term\n",
      "     41 R: 0.083 P: 0.083 user resources web research project tools tool database linguistic texts\n",
      "     42 R: 0.073 P: 0.073 speech dialogue user utterance human speaker utterances recognition systems spoken\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "while epoch < config.n_epochs:\n",
    "    # train\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = model.get_feed_dict(batch)\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, ppls_batch, global_step_log = \\\n",
    "        sess.run([model.opt, model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_reg, model.topic_ppls, tf.train.get_global_step()], feed_dict = feed_dict)\n",
    "\n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if global_step_log % config.log_period == 0:\n",
    "            # validate\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "\n",
    "            # test\n",
    "            if ppl_dev < ppl_min:\n",
    "                ppl_min = ppl_dev\n",
    "                loss_test, _, _, _, ppl_test, _ = validate(sess, test_batches, model)\n",
    "                saver.save(sess, config.path_model, global_step=global_step_log)\n",
    "                cPickle.dump(config, open(config.path_config % global_step_log, 'wb'))\n",
    "                update_checkpoint(config, checkpoint, global_step_log)\n",
    "            \n",
    "            # visualize topic\n",
    "            topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "            topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "            topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "            topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "            descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "            recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "            \n",
    "            depth_specs = get_topic_specialization(sess, model, instances_test)\n",
    "            hierarchical_affinities = get_hierarchical_affinity(sess, model)\n",
    "            \n",
    "            # log\n",
    "            clear_output()\n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_reg_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_reg_dev, \\\n",
    "                    '%.2f'%loss_test, '%.0f'%ppl_test, \\\n",
    "                    '%.2f'%depth_specs[1], '%.2f'%depth_specs[2], '%.2f'%depth_specs[3], \\\n",
    "                    '%.2f'%hierarchical_affinities[0], '%.2f'%hierarchical_affinities[1]],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "            cPickle.dump(log_df, open(os.path.join(config.path_log), 'wb'))\n",
    "            print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)\n",
    "\n",
    "            # update tree\n",
    "            if not config.static:\n",
    "                config.tree_idxs, update_tree_flg = model.update_tree(topic_prob_topic, recur_prob_topic)\n",
    "                if update_tree_flg:\n",
    "                    print(config.tree_idxs)\n",
    "                    name_variables = {tensor.name: variable for tensor, variable in zip(tf.global_variables(), sess.run(tf.global_variables()))} # store paremeters\n",
    "                    if 'sess' in globals(): sess.close()\n",
    "                    model = HierarchicalNeuralTopicModel(config)\n",
    "                    sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "                    name_tensors = {tensor.name: tensor for tensor in tf.global_variables()}\n",
    "                    sess.run([name_tensors[name].assign(variable) for name, variable in name_variables.items()]) # restore parameters\n",
    "                    saver = tf.train.Saver(max_to_keep=1)\n",
    "                \n",
    "            time_start = time.time()\n",
    "\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "    epoch += 1\n",
    "\n",
    "loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "display(log_df)\n",
    "print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_year():\n",
    "    probs_topics = []\n",
    "    years = []\n",
    "    for i, train_batch in train_batches:\n",
    "        probs_topics_batch = sess.run(model.prob_topic, feed_dict=model.get_feed_dict(train_batch, mode='test'))\n",
    "        years_batch = [instance.year for instance in train_batch]\n",
    "        probs_topics += [probs_topics_batch]\n",
    "        years += years_batch\n",
    "    probs_topics = np.concatenate(probs_topics)\n",
    "    years = np.array(years)\n",
    "\n",
    "    topic_years = years.dot(probs_topics) / np.sum(probs_topics, 0)\n",
    "    topic_year = {model.topic_idxs[i]: year for i, year in enumerate(topic_years)}\n",
    "    return topic_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36288, 16), (36288,))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_topics.shape, years.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 2006.1405287293783,\n",
       " 1: 2015.073065684627,\n",
       " 2: 2006.6358226095088,\n",
       " 3: 2010.2605308626469,\n",
       " 4: 2011.1153424999409,\n",
       " 5: 2010.2201559271937,\n",
       " 11: 2010.638231762466,\n",
       " 12: 2013.6412160983643,\n",
       " 21: 2009.3432210176409,\n",
       " 22: 2003.0377479786164,\n",
       " 31: 2010.7845425693845,\n",
       " 32: 2007.4959372377405,\n",
       " 41: 2008.9745451250799,\n",
       " 42: 2009.024079592822,\n",
       " 51: 2007.963976828227,\n",
       " 52: 2008.7701026307714}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topic_sample(sess, model, topic_prob_topic=None, recur_prob_topic=None, topic_freq_tokens=None, topic_year=None, parent_idx=0, depth=0):\n",
    "    if depth == 0: # print root\n",
    "        assert len(topic_prob_topic) == len(recur_prob_topic) == len(topic_freq_tokens) == len(topic_year)\n",
    "        freq_tokens = topic_freq_tokens[parent_idx]\n",
    "        recur_topic = recur_prob_topic[parent_idx]\n",
    "        prob_topic = topic_prob_topic[parent_idx]\n",
    "        year = topic_year[parent_idx]\n",
    "        print(parent_idx, 'Avg Year: %i' % year, ' '.join(freq_tokens))\n",
    "    \n",
    "    child_idxs = model.tree_idxs[parent_idx]\n",
    "    depth += 1\n",
    "    for child_idx in child_idxs:\n",
    "        freq_tokens = topic_freq_tokens[child_idx]\n",
    "        recur_topic = recur_prob_topic[child_idx]\n",
    "        prob_topic = topic_prob_topic[child_idx]\n",
    "        year = topic_year[child_idx]\n",
    "        print('  '*depth, child_idx, 'Avg Year: %i' % year, ' '.join(freq_tokens))\n",
    "        \n",
    "        if child_idx in model.tree_idxs: \n",
    "            print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens, topic_year=topic_year, parent_idx=child_idx, depth=depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Avg Year: 2006 user dialogue knowledge utterance generation domain action state systems human\n",
      "   1 Avg Year: 2015 models embeddings neural vector network training vectors representations embedding representation\n",
      "     11 Avg Year: 2010 document documents question topic query sentences questions answer method similarity\n",
      "     12 Avg Year: 2013 sentiment features tweets negative positive classification analysis social opinion polarity\n",
      "   2 Avg Year: 2006 tree parsing dependency parser grammar node trees parse rules nodes\n",
      "     21 Avg Year: 2009 event relations annotation relation discourse events argument semantic annotated temporal\n",
      "     22 Avg Year: 2003 verb semantic syntactic structure noun lexical verbs rules grammar phrase\n",
      "   3 Avg Year: 2010 training models algorithm probability features feature function learning performance distribution\n",
      "     31 Avg Year: 2010 features feature training performance entity learning classification classifier entities class\n",
      "     32 Avg Year: 2007 errors pos character chinese error segmentation tag tags tagging method\n",
      "   4 Avg Year: 2011 translation source target alignment phrase english pairs training parallel sentences\n",
      "     41 Avg Year: 2008 sense semantic similarity wordnet lexical senses terms context relations method\n",
      "     42 Avg Year: 2009 languages english morphological translation arabic lexicon dictionary forms form spanish\n",
      "   5 Avg Year: 2010 evaluation systems human sentences scores test quality score metrics mt\n",
      "     51 Avg Year: 2007 annotation web resources tools user research project tool linguistic processing\n",
      "     52 Avg Year: 2008 speech speakers speaker study features spoken recognition models native participants\n"
     ]
    }
   ],
   "source": [
    "print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens, topic_year=topic_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
