{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from data_structure import get_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "from topic_model import TopicModel\n",
    "\n",
    "from topic_beam_search_decoder import BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '2', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/apnews/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/tglm_vae', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'apnews', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 1000, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 500, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.1, 'lr')\n",
    "flags.DEFINE_float('reg', 0.1, 'regularization term')\n",
    "flags.DEFINE_float('beta', 0.001, 'initial value of beta')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_integer('warmup', 5000, 'warmup period for KL')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 10, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_topic', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('PAD_IDX', word_to_idx[PAD], 'PAD_IDX')\n",
    "flags.DEFINE_integer('UNK_IDX', word_to_idx[UNK], 'UNK_IDX')\n",
    "flags.DEFINE_integer('BOS_IDX', word_to_idx[BOS], 'BOS_IDX')\n",
    "flags.DEFINE_integer('EOS_IDX', word_to_idx[EOS], 'EOS_IDX')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "maximum_iterations = max([max([instance.max_sent_l for instance in batch]) for ct, batch in dev_batches])\n",
    "flags.DEFINE_integer('maximum_iterations', maximum_iterations, 'maximum_iterations')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build language model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['bow'] = tf.placeholder(tf.float32, [None, config.dim_bow], name='bow')\n",
    "t_variables['input_token_idxs'] = tf.placeholder(tf.int32, [None, None], name='input_token_idxs')\n",
    "t_variables['dec_input_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_input_idxs')\n",
    "t_variables['dec_target_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_target_idxs')\n",
    "t_variables['batch_l'] = tf.placeholder(tf.int32, name='batch_l')\n",
    "t_variables['doc_l'] = tf.placeholder(tf.int32, [None], name='doc_l')\n",
    "t_variables['sent_l'] = tf.placeholder(tf.int32, [None], name='sent_l')\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train', assertion=False):\n",
    "    def token_dropout(sent_idxs):\n",
    "        sent_idxs_dropout = np.asarray(sent_idxs)\n",
    "        sent_idxs_dropout[np.random.rand(len(sent_idxs)) > config.word_keep_prob] = config.UNK_IDX\n",
    "        return list(sent_idxs_dropout)\n",
    "\n",
    "    bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "    \n",
    "    doc_l = np.array([len(instance.token_idxs) for instance in batch])\n",
    "    \n",
    "    feed_input_token_idxs_list = [sent_idxs for instance in batch for sent_idxs in instance.token_idxs]\n",
    "    feed_dec_input_idxs_list = [[config.BOS_IDX] + token_dropout(sent_idxs) for sent_idxs in feed_input_token_idxs_list]\n",
    "    feed_dec_target_idxs_list = [sent_idxs + [config.EOS_IDX]  for sent_idxs in feed_input_token_idxs_list]\n",
    "        \n",
    "    sent_l = np.array([len(sent_idxs) for sent_idxs in feed_input_token_idxs_list], np.int32)\n",
    "    batch_l = len(sent_l)\n",
    "    \n",
    "    feed_input_token_idxs = pad_sequences(feed_input_token_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_input_idxs = pad_sequences(feed_dec_input_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_target_idxs = pad_sequences(feed_dec_target_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    \n",
    "    if assertion:\n",
    "        index = 0\n",
    "        for instance in batch:\n",
    "            for line_idxs in instance.token_idxs:\n",
    "                assert feed_input_token_idxs_list[index] == line_idxs\n",
    "                index += 1\n",
    "        assert feed_input_token_idxs.shape[1] == np.max(sent_l)\n",
    "        assert feed_dec_input_idxs.shape[1] == np.max(sent_l) + 1\n",
    "        assert feed_dec_target_idxs.shape[1] == np.max(sent_l) + 1\n",
    "    \n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "\n",
    "    feed_dict = {\n",
    "                t_variables['bow']: bow, \n",
    "                t_variables['batch_l']: batch_l, t_variables['doc_l']: doc_l, t_variables['sent_l']: sent_l, \n",
    "                t_variables['input_token_idxs']: feed_input_token_idxs, t_variables['dec_input_idxs']: feed_dec_input_idxs, t_variables['dec_target_idxs']: feed_dec_target_idxs, \n",
    "                t_variables['keep_prob']: keep_prob\n",
    "    }\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "def check_shape(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encode bow\n",
    "with tf.variable_scope('topic/enc', reuse=False):\n",
    "    hidden_bow_ = tf.layers.Dense(units=config.dim_hidden_bow, activation=tf.nn.relu, name='hidden_bow')(t_variables['bow'])\n",
    "    hidden_bow = tf.layers.Dropout(t_variables['keep_prob'])(hidden_bow_)\n",
    "    means_bow = tf.layers.Dense(units=config.dim_latent_topic, name='mean_bow')(hidden_bow)\n",
    "    logvars_bow = tf.layers.Dense(units=config.dim_latent_topic, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_bow)\n",
    "    latents_bow = sample_latents(means_bow, logvars_bow) # sample latent vectors\n",
    "\n",
    "    prob_topic = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob')(latents_bow) # inference of topic probabilities\n",
    "\n",
    "# decode bow\n",
    "with tf.variable_scope('shared', reuse=False):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "bow_embeddings = tf.nn.embedding_lookup(embeddings, bow_idxs) # embeddings of each bow features\n",
    "\n",
    "with tf.variable_scope('topic/dec', reuse=False):\n",
    "    topic_embeddings = tf.get_variable('topic_emb', [config.n_topic, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of topics\n",
    "\n",
    "    topic_bow = tf.nn.softmax(tf.matmul(topic_embeddings, bow_embeddings, transpose_b=True), 1) # bow vectors for each topic\n",
    "    logits_bow = tf_log(tf.matmul(prob_topic, topic_bow)) # predicted bow distribution\n",
    "\n",
    "# prior of each gaussian distribution (computed for each topic)\n",
    "    means_topic = tf.layers.Dense(units=config.dim_latent, name='mean_topic')(topic_bow)\n",
    "    logvars_topic = tf.layers.Dense(units=config.dim_latent, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(topic_bow)\n",
    "sigma_topic = tf.exp(0.5 * logvars_topic)\n",
    "gauss_topic = tfd.Normal(loc=means_topic, scale=sigma_topic)    \n",
    "    \n",
    "# define losses\n",
    "topic_losses_recon = -tf.reduce_sum(tf.multiply(t_variables['bow'], logits_bow), 1)\n",
    "topic_loss_recon = tf.reduce_mean(topic_losses_recon) # negative log likelihood of each words\n",
    "\n",
    "topic_loss_kl = compute_kl_loss(means_bow, logvars_bow) # KL divergence b/w latent dist & gaussian std\n",
    "\n",
    "topic_embeddings_norm = topic_embeddings / tf.norm(topic_embeddings, axis=1, keepdims=True)\n",
    "topic_angles = tf.matmul(topic_embeddings_norm, tf.transpose(topic_embeddings_norm))\n",
    "topic_angles_mean = tf.reduce_mean(topic_angles, keepdims=True)\n",
    "topic_angles_vars = tf.reduce_mean(tf.square(topic_angles - topic_angles_mean))\n",
    "topic_loss_reg = topic_angles_vars - tf.squeeze(topic_angles_mean)\n",
    "\n",
    "# monitor\n",
    "n_bow = tf.reduce_sum(t_variables['bow'], 1)\n",
    "topic_ppls = tf.divide(topic_losses_recon, n_bow)\n",
    "topics_freq_bow_indices = tf.nn.top_k(topic_bow, 10, name='topic_freq_bow').indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "input_token_idxs = t_variables['input_token_idxs']\n",
    "batch_l = t_variables['batch_l']\n",
    "sent_l = t_variables['sent_l']\n",
    "max_sent_l = tf.reduce_max(sent_l)\n",
    "\n",
    "with tf.variable_scope('sent/enc', reuse=False):\n",
    "    # get word embedding\n",
    "    enc_input = tf.nn.embedding_lookup(embeddings, input_token_idxs)\n",
    "\n",
    "    # get sentence embedding\n",
    "    _, enc_state = dynamic_rnn(enc_input, sent_l, config.dim_hidden, t_variables['keep_prob'])\n",
    "\n",
    "    # TODO House Holder flow\n",
    "    prob_topic_infer = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax)(enc_state)\n",
    "    \n",
    "    # inference of each gaussian dist. parameter\n",
    "    enc_state_infer = tf.tile(tf.expand_dims(enc_state, 1), [1, config.n_topic, 1]) # tile over topics\n",
    "    means_topic_infer = tf.layers.Dense(units=config.dim_latent, name='mean_topic_infer')(enc_state_infer)\n",
    "    logvars_topic_infer = tf.layers.Dense(units=config.dim_latent, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic_infer')(enc_state_infer)\n",
    "    sigma_topic_infer = tf.exp(0.5 * logvars_topic_infer)\n",
    "    gauss_topic_infer = tfd.Normal(loc=means_topic_infer, scale=sigma_topic_infer)\n",
    "\n",
    "    # latent vectors from each gaussian dist.\n",
    "    latents_topic_infer = sample_latents(means_topic_infer, logvars_topic_infer) \n",
    "    # latent vector from gaussian mixture    \n",
    "    latents_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), latents_topic_infer, transpose_a=True)\n",
    "    \n",
    "    # for beam search\n",
    "    means_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), means_topic_infer, transpose_a=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for decoding\n",
    "dec_sent_l = tf.add(sent_l, 1)\n",
    "dec_input_idxs = t_variables['dec_input_idxs']\n",
    "dec_input = tf.nn.embedding_lookup(embeddings, dec_input_idxs)\n",
    "\n",
    "dec_latents_input = tf.tile(latents_input, [1, tf.shape(dec_input)[1], 1])\n",
    "dec_concat_input = tf.concat([dec_input, dec_latents_input], -1)\n",
    "\n",
    "# decode for training\n",
    "with tf.variable_scope('sent/dec/rnn', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32, reuse=False):\n",
    "    dec_cell = tf.contrib.rnn.GRUCell(config.dim_hidden)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob = t_variables['keep_prob'])\n",
    "\n",
    "    dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(latents_input, 1))\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_concat_input, sequence_length=dec_sent_l)\n",
    "\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=dec_cell,\n",
    "        helper=helper,\n",
    "        initial_state=dec_initial_state)\n",
    "\n",
    "    dec_outputs, _, output_sent_l = tf.contrib.seq2seq.dynamic_decode(train_decoder)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(config.n_vocab, use_bias=False, name='out')\n",
    "    output_logits = output_layer(dec_outputs.rnn_output)\n",
    "    \n",
    "    output_token_idxs = tf.argmax(output_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tokens = tf.fill([batch_l], config.BOS_IDX)\n",
    "end_token = config.EOS_IDX\n",
    "\n",
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    infer_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(means_input, 1))\n",
    "    beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(infer_dec_initial_state, multiplier=config.beam_width)\n",
    "    beam_latents_input = tf.contrib.seq2seq.tile_batch(tf.squeeze(means_input, 1), multiplier=config.beam_width) # added\n",
    "    \n",
    "    beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=beam_latents_input)\n",
    "\n",
    "    beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    beam_output_token_idxs = beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    inter_means_input = tf.placeholder(tf.float32, [None, config.dim_latent])\n",
    "    \n",
    "    inter_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(inter_means_input)\n",
    "    inter_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(inter_dec_initial_state, multiplier=config.beam_width)\n",
    "    inter_beam_latents_input = tf.contrib.seq2seq.tile_batch(inter_means_input, multiplier=config.beam_width) # added\n",
    "    \n",
    "    inter_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=inter_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=inter_beam_latents_input)\n",
    "\n",
    "    inter_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inter_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    inter_beam_output_token_idxs = inter_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language modeling cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target and mask\n",
    "dec_target_idxs = t_variables['dec_target_idxs']\n",
    "dec_mask_tokens = tf.sequence_mask(dec_sent_l, maxlen=max_sent_l+1, dtype=tf.float32)\n",
    "\n",
    "# nll for each token (averaged over batch & sentence)\n",
    "sent_loss_recon = tf.contrib.seq2seq.sequence_loss(output_logits, dec_target_idxs, dec_mask_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_l = t_variables['doc_l']\n",
    "mask_sents = tf.sequence_mask(doc_l)\n",
    "mask_sents_flatten = tf.reshape(mask_sents, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1]])\n",
    "\n",
    "prob_topic_tiled = tf.tile(tf.expand_dims(prob_topic, 1), [1, tf.shape(mask_sents)[1], 1])\n",
    "prob_topic_flatten = tf.reshape(prob_topic_tiled, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1], config.n_topic])\n",
    "prob_topic_sents = tf.boolean_mask(prob_topic_flatten, mask_sents_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferred mixture probabilities (computed for each sentence)\n",
    "categ_topic_infer = tfd.Categorical(probs=prob_topic_infer)\n",
    "\n",
    "# prior of mixture probabilities (computed for each document, tiled for each sentence)\n",
    "categ_topic = tfd.Categorical(probs=prob_topic_sents)\n",
    "\n",
    "sent_loss_kl_categ = tf.reduce_mean(tfd.kl_divergence(categ_topic_infer, categ_topic))\n",
    "\n",
    "# inference of each gaussian gaussribution (computed for each sentence)\n",
    "\n",
    "sent_loss_kl_gauss = tf.reduce_sum(tfd.kl_divergence(gauss_topic_infer, gauss_topic), -1)\n",
    "sent_loss_kl_gmm = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss), -1))\n",
    "\n",
    "sent_loss_kl = sent_loss_kl_categ + sent_loss_kl_gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "beta = tf.Variable(config.beta, name='beta', trainable=False) if config.warmup > 0 else tf.constant(1., name='beta')\n",
    "update_beta = tf.assign_add(beta, 1./(config.warmup*len(train_batches)))\n",
    "sent_loss = sent_loss_recon + beta * sent_loss_kl\n",
    "\n",
    "topic_loss = topic_loss_recon + topic_loss_kl + config.reg * topic_loss_reg\n",
    "loss = topic_loss + sent_loss\n",
    "\n",
    "# define optimizer\n",
    "if config.opt == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif config.opt == 'Adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "    \n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_sents(token_idxs, config, idx_to_word):\n",
    "    sents = []\n",
    "    for sent_idxs in token_idxs:\n",
    "        tokens = []\n",
    "        for idx in sent_idxs:\n",
    "            if idx == config.EOS_IDX: break\n",
    "            tokens.append(idx_to_word[idx])\n",
    "        sent = ' '.join(tokens)\n",
    "        sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_batch, sent_loss_batch, ppls_batch = sess.run([loss, topic_loss, sent_loss, topic_ppls], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_batch, sent_loss_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "    loss_mean, topic_loss_mean, sent_loss_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    return loss_mean, topic_loss_mean, sent_loss_mean, ppl_mean\n",
    "\n",
    "def get_all_losses(sess, batches):\n",
    "    losses = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch = \\\n",
    "        sess.run([loss, topic_loss_recon, topic_loss_kl, sent_loss_recon, sent_loss_kl], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "    print('LOSS %.2f | TM NLL: %.2f, KL: %.4f | LM NLL: %.2f, KL: %.4f' %  np.mean(losses, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_batch):\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    pred_token_idxs = sess.run(output_token_idxs, feed_dict = feed_dict)\n",
    "    true_token_idxs = [sent_idxs for instance in sample_batch for sent_idxs in instance.token_idxs]\n",
    "    \n",
    "    assert len(pred_token_idxs) == len(true_token_idxs)\n",
    "    \n",
    "    pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "    true_sents = idxs_to_sents(true_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    for true_sent, pred_sent in zip(true_sents, pred_sents):        \n",
    "        print('True: %s' % true_sent)\n",
    "        print('Pred: %s' % pred_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "logs = []\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "saver = tf.train.Saver(max_to_keep=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "028[s], 26[s], Ep: 00, Ct: 00000|TR LOSS: 386, PPL: 2661|TM NLL: 375, KL: 0.89 | LM NLL: 10.34, KL: 0.78|DE LOSS: 347, PPL: 2657, TM: 337, LM: 10.34|BETA: 0.001000\n",
      "True: a providence man has been sentenced to seven years in prison for his role in a counterfeit check scheme prosecutors say cost local banks around $ #\n",
      "Pred: kvly jenison tumlin emancipation shore tumlin pont impressed judgments u.n. charlton impressive upscale judgments sentiments irene pont irene autism upscale infrastructure handles upscale downturn career congressmen impressive impressive impressive mississippians judgments impressive impressed impressive irene impressive grandchildren impressive impressive\n",
      "True: at his sentencing tuesday , a judge also ordered # year old ernest kar to pay more than $ # in restitution\n",
      "Pred: bice whitefish creve vulnerability giovanna lamprecht whitefish vulnerability surrogate mortuary lamprecht episode favored marisa vulnerability marik saves marik favored marik marik wsfa wsfa knifepoint marik marik marik marik marik clibborn marik fumes chelsea marik fumes marik augustus expo marik\n",
      "True: kar was convicted in february of bank fraud and conspiracy to commit bank fraud\n",
      "Pred: moammar stolen democrats deducting regardless pagans antwerp dusseau pagans paulo integrate necessity eskimo pruning rusty eskimo explosions eskimo eskimo dusseau sheffield eskimo eskimo eskimo upshaw pagans eskimo eskimo payette hobgood attached overcharged spanaway eskimo eskimo eskimo eskimo eskimo wickard\n",
      "True: prosecutors said kar and others created counterfeit checks between # and # , and recruited people who allowed the fake checks to be deposited in their accounts and then made cash withdrawals for a cut\n",
      "Pred: shealy shealy achampong tense tense competitiveness montana montana hargis hargis montana khon brackets montana bladen generated generated cellblock baisden montana replace hargis nineteen palcohol constable fabregas ramifications montana montana replace montana lanza hargis hargis candlelight palcohol montana montana simers\n",
      "True: many of the fake checks were drawn on legitimate businesses\n",
      "Pred: olbert alienate thune clinch unsuccessfully cottonwood domestically unwillingness judgeships domestically morganza hayhurst domestically morganza ragley commissioners analysts domestically analysts morganza mullins unsuccessfully domestically bouvay domestically bouvay analysts wed. bouvay unannounced domestically unwillingness berkshire snapped reindeer reindeer atomic mullins domestically\n",
      "True: kar was arrested last year after warwick police found check making materials and fake checks in his car and motel room\n",
      "Pred: absurd coming danone orphanage kafka pianos lockney voucher //bit.ly/u barring burnie recruiter recruiter calling pinned recruiter recruiter recruiter recruiter recruiter danone recruiter recruiter recruiter calling recruiter recruiter recruiter recruiter recruiter broadway recruiter recruiter barbecue recruiter drinks arraigned pinecrest recruiter\n",
      "True: four co defendants are serving federal prison terms\n",
      "Pred: employment transmissions wausau ended odlin ribbons sovereign sen lafourche sen sen pitches snowboarders elderts quaker soileau colbert congresswoman snowboarders sen hormel tevin edition flash pitches cookeville sovereign bosnia thornburg banner depart kirby councilmen contributions nov. cinematic killed sen dismembering\n",
      "True: four are being prosecuted on state charges\n",
      "Pred: bacardi ripped runs hunting hunting batter balloonists gk blackburn marilyn gk residential symbolize blackburn lineman grandchildren grandchildren gk plott batter bowser plott spokeswoman felons gk grandchildren gk felons plott impressed dominguez batter blackburn batter cementation dominguez blackburn ctfastrak gk\n",
      "True: country crossing casino lobbyist jarrod massey has apologized to a defense lawyer for comments he made in alabama 's gambling corruption trial\n",
      "Pred: teague laundering spokesman frederick feed sundown nov. champaign feed scouring skydiving specially scouring pocketed pankey lineman inscribed scouring skydiving sting algorithms pocketed gena pocketed scouring specially scouring solution worthy scouring siting interpretive scouring skydiving skydiving groundwork debuted cockfighting replicate\n",
      "True: the trial resumed wednesday with massey telling a lawyer for casino owner milton mcgregor that he regretted comments he made about the lawyer during his testimony on tuesday\n",
      "Pred: nightclub indicates oats heavily oats therapeutic strip crisscrossing undisclosed hamski doggett bolster strip broome gasification strip strip ladd taxi strip mixing strip taxi strip taxi strip strip taxi taxi neuroinvasive ladd taxi restrictive taxi kochanowski spreading undisclosed taxi arcadia\n",
      "True: massey had told attorney bobby segall that he did n't like him and he enjoyed annoying him\n",
      "Pred: becomes intensified intensified lotteries extra grow grow extra ppov extra fewest grubb grow regents madagascar extra cansler phillipos masked pensacola cornstalk beastie regents bonanza definitely extra extra judon isakson extra grow extra extra newsome tpc motorcyclists masked primarily jsu\n",
      "True: segall accepted the apology and said he <unk> massey\n",
      "Pred: absurd railcars mccool helmet coroners guerra dean guerra leads counseled berkshire mccool berkshire counseled irritate berkshire berkshire counseled smurfberries counseled berkshire counseled robins nba newsrooms counseled orchards coil lofton berkshire berkshire gop embezzled lisle exhumed counseled compounds berkshire berkshire\n",
      "True: but he insisted that massey offer the apology without the jury in the courtroom\n",
      "Pred: voters wailuku gardipee lyle hendersonville germans mcrath emission paroles paroles emission langley emission delbert remembers bbc langley langley duped duped emission emission emission langley emission mcrath print emission tumwater tumwater dubai washed thunder patterson print respond emission langley b.\n",
      "True: u.s. district judge myron thompson agreed to have the jury leave before massey apologized\n",
      "Pred: flu borders hopkins success picasso pogue wpvi epps restricting pogue benitez restricting mckibben embrace spirits representative representative pogue art musk art alberto writes restricting remove dominguez art dominguez art remove art pogue mckibben mckibben art art appointing mckibben remove\n",
      "True: massey is on the witness stand for the fifth day\n",
      "Pred: hollingsworth quoting quoting quoting clutching dioguardi roar overboard roar sopuch roar havens lamar flandreau rat quoting remembers schuyler remembers humphrey progressive remembers gomez vandermeer remembers dickens progressive asia homestead lamar janklow eliot humphrey wrongdoing vandermeer lamar remembers iraqi quoting\n",
      "True: lawyers predict his testimony will continue thursday\n",
      "Pred: sprenger hypothermic hypothermic monongahela pets montague noguez trail pets complexes pets pets complexes tva trespassing tire pets complexes tuatagaloa restructuring pets pets tray urges complexes trail complexes hailed complexes trespassing trail legally jewels trespassing trespassing mdoc pfc trespassing pets\n",
      "True: maricopa county authorities say a <unk> woman has been arrested on animal cruelty charges\n",
      "Pred: gondolas unresponsive safe paragliding paragliding recruiter embezzled trimet sponsors scarfo follow midwife scarfo schnickel embezzled sebelius recruiter commercially scarfo encounter gop dismissal sponsors scarfo scarfo pitre resume divisive gop dismissal recruiter gop gop sponsors bryce proclaiming gop divisive divisive\n",
      "True: county sheriff 's officials said thursday that charlene scott was booked into the downtown phoenix jail on suspicion of # counts of animal cruelty and failure to provide necessary medical attention\n",
      "Pred: clipped hanged proof proof proof constable caterpillars constable constable constable constable constable fielded constable cushman constable cushman cushman hiker neel constable constable constable sub constable alpine constable email legislatures neel khon khon email constable freeways harlan broadening constable fielded\n",
      "True: authorities have responded to scott 's residence west of phoenix since november about neglected dogs suffering from chronic tick infestation causing hair loss and inflammation of the skin\n",
      "Pred: collin gibson collin rebut kxmc rebut element offsite clearing offsite offsite offsite offsite offsite offsite rebut haired exhibiting rebut offsite offsite rabid eden element offsite tac offsite offsite fan offsite expanded offsite urs rabid tac pathogens rabid promotional offsite\n",
      "True: they say a search warrant wednesday found a puppy in need of emergency medical treatment\n",
      "Pred: beltran hhs guaranty acn brink brink margarine sturken margarine guaranty margarine guaranty hooded resigned acn falashmura politics lighthouses politics sustain irritate overlook margarine gov stateside stateside lighthouses stateside doak morley lighthouses lighthouses stateside sustain sustain irritate sustain irritate stateside\n",
      "True: in the process of seizing the dogs and puppies , authorities say one dead puppy was found\n",
      "Pred: farmers meritorious initiating prom bourne openly munk stoneham newborns receivers newborns stoneham ppov newborns eternal valve munk munk eternal rayner novella munk munk ppov malaise munk munk munk ppov munk ppov munk eternal munk novella munk munk reflecting munk\n",
      "True: they say five adult dogs and nine puppies were seized and transported to receive medical attention\n",
      "Pred: rims adelphonse wingfield reporting bureaucracy hire wandering wrongfully wrongfully wrongfully westbrook wrongfully upsetting pelican pest tax wrongfully wandering upsetting pest pest pest pelican upsetting wrongfully biospecifics pelican wrongfully westbrook negotiation wrongfully upsetting wandering upsetting revive upsetting pelican virtually biospecifics\n",
      "True: sheriff 's officials could n't immediately say whether scott has legal representation yet\n",
      "Pred: alyssa lotteries proof ashland wingfield neel neel shuler lotteries travels parkston specify ludescher ludescher travels travels neel neel midweek parkston midweek monona travels monona midweek sturken parkston parkston specify ashland travels darrington sturken midweek ludescher scottsboro midweek pinner midweek\n",
      "True: michigan gubernatorial candidates rick snyder and virg bernero have addressed a gathering of business leaders in detroit\n",
      "Pred: guitars guitars reynaud lithium athletic allowances dwyer caregivers dannelly scheme scheme aldi smack mindy strategically respondents boas dannelly albanian shiite elsewhere strategically wftv dwyer underemployed hatched strategically manger smack consolidate potti smack underemployed affiliate mullins kerri surfacing affiliate caregivers\n",
      "True: thursday 's event was n't a debate , though\n",
      "Pred: belief silver menno shaker shaker dishon doxer hussey canned terrence events canned terrence rockies kelso ralston canned giggey kelso kelso kelso terrence kelso commitments integral ira ralston kadyrbayev akorn barraza ira listen ira kelso kelso kelso teaching blount barraza\n",
      "True: bernero , the democratic mayor of lansing , and his republican opponent , ann arbor businessman snyder , gave opening remarks before answering a series of questions sent in by the audience\n",
      "Pred: lilly projecting hindering minerals aiona amison minerals aiona waylon minerals flaming dimasi strangely expensive strangely expensive expensive expensive waylon expensive strangely renew expensive expensive strangely neugebauer stewart expensive rigged neugebauer stewart stewart expensive expensive neugebauer expensive neugebauer //hrld.us/ expensive\n",
      "True: neither was on stage at the same time except when they shook hands at the event 's conclusion\n",
      "Pred: montes carta receptive collection aramark similarities tom aramark collection syria collection zubko collection collection advanced aramark higher promotional incorporate zubko grayling advanced fe refrigeration vanderbilt price them collection zubko equality promotional promotional collection advanced advanced zubko syria syria collection\n",
      "True: the sides have agreed to only one debate , on sunday in <unk>\n",
      "Pred: minnesotans overcharged dispatchers overseas reverses resident nose olenchock funds claudia funds frisco usc operated earned frisco operated heather claudia operated claudia klug frisco operated frisco claudia frisco claudia frisco operated abide frisco operated claudia upscale molest claudia operated funds\n",
      "True: they also spoke separately sept. # to the west michigan policy forum in grand rapids\n",
      "Pred: taggart scholarship primarily meixueiro simsbury snow lijana lijana island island simsbury irs island pleaded dee sowela suites astorga batter unicredit han batter filmed irs sowela dee salvation batter privately batter dee irs accosted clusters regan batter batter batter has\n",
      "True: on oct. # they will be the keynote speakers at the american arab chamber of commerce 's # th annual building economic bridges banquet in dearborn\n",
      "Pred: rhode mcallister mcallister eversource mcallister vr viacom ahu ahu villanova ahu faked villanova eggers eggers mcallister rebounds rebounds paradise nov. nov. eggers ankle rebounds eggers nov. eggers rebounds nov. eggers rebounds eggers ahu ahu ahu ankle eggers bridenstine rebounds\n",
      "True: backers of an initiative to raise nevada mining industry taxes say they do n't have enough signatures to put the measure on the november ballot\n",
      "Pred: protracted declaring tusd warwick detrimental warwick udot checkpoint details germans checkpoint abbie bendelladj obici voorhees henri loader championed match checkpoint evicted distributed declaring checkpoint moorman ppc krantz jung varieties checkpoint checkpoint cadick bendelladj evicted details warwick germans bendelladj folk\n",
      "True: but initiative proponent bob fulkerson of the progressive leadership alliance of nevada said monday the group will press state legislators to take action themselves next year\n",
      "Pred: iva grass indictment traveled combing occupancy occupancy aguirre traditionally aguirre occupancy traditionally marketers traditionally traditionally traditionally novels hanged hanged hanged traditionally hanged occupancy marketers morley occupancy occupancy morley liar marketers hanged traditionally marketers traditionally morley traditionally occupancy hanged marketers\n",
      "True: fulkerson says organizers collected only about two thirds of the # signatures required to put the question to voters this year\n",
      "Pred: required siloam backhoe weekday degiacomo fahrenholtz shooter bloody ap bloody bloody grubbs bloody richest grubbs biddle hosier premiere marquee foreigners bloody bloody grubbs affiliates ap edmunds bloody grubbs affiliates jorgensen bloody marquee planted planted kershaw bloody affiliates grubbs underwriters\n",
      "True: the deadline was tuesday\n",
      "Pred: flag flag flag becker stowe plus northern require slammed slammed becker physically geographically rufus rufus restructuring becker becker clergy orth sen negotiation becker mccausland becker treat maturity becker orth becker becker becker becker orth ucla becker portray negotiation dann\n",
      "True: fulkerson says whether the group will make another try in # depends on what the state legislature does in its next session\n",
      "Pred: done newcomer carley heritage heritage uo stilwell published heritage uo pu lb ventilation heritage stahl smoker stahl heritage heritage heritage findings carmona directions uo plead heritage villaraigosa notify heritage heritage heritage heritage directions ventilation sang directions forced desire heritage\n",
      "True: the nevada supreme court has n't ruled yet after hearing oral arguments last week from attorneys for mining interests and plan on whether the initiative was legally flawed and should n't be on the ballot\n",
      "Pred: subject darlington tongue guests guests guests guests guests guests guests guests guests guests guests guests guests guests guests guests guests tomahawk guests guests guests guests guests guests guests guests tomahawk guests guests guests guests guests guests guests tomahawk guests\n",
      "True: a new report says the average student loan debt for graduates of indiana 's public , four year universities last year rose to $ # , or more than $ # higher than the national average\n",
      "Pred: thrift proof proof proof proof proof proof mountainsides mountainsides store debuted mountainsides plant mountainsides rawlings hv mountainsides diamondhead alpine mountainsides zhejiang rawlings hv rawlings hv stepping rawlings mountainsides blues parrot sacrifice dells stepping tight rawlings zhejiang store alpine dells\n",
      "True: a report by the advocacy group project on student debt ranks indiana # th highest in the nation for student debt\n",
      "Pred: biologic vestas lockwood biologic entertainer peabody keurig keurig hair //on. ferries ferries smuggling hair ridiculous ferries keurig staying staying ferries tallies bottled ferries inspirato staying inspirato uso bottled wrap uso ridiculous bottled ferries tokens bottled //on. fans occupy bottled\n",
      "True: it says # percent of the state 's # graduating class had debt , slightly below the national average\n",
      "Pred: stratford farthing measurements stratford collierville well student parrot parrot parrot parrot parrot parrot basf volleyball basf parrot parrot parrot parrot parrot parrot parrot parrot parrot parrot mm volleyball parrot bomb silpada volleyball parrot parrot parrot parrot parrot parrot dementia\n",
      "True: indiana higher education commissioner teresa <unk> tells the journal & courier of lafayette ( http : <unk> ) the debt has a chilling effect on how people decide on going to college\n",
      "Pred: helmick bumpy profits bumpy tale steadily tale forever tale tale lamprecht newburyport tale newburyport lama newburyport newburyport hoy newburyport newburyport tale han widows newburyport harrisonburg newburyport newburyport event androli dede newburyport newburyport schermerhorn betancourt waist newburyport dede androli hoy\n",
      "True: the institution with the highest loan payback was indiana university northwest at nearly $ # while students at the university of southern indiana owe $ # on average\n",
      "Pred: apples slaughter maturity bee beam beam maturity pocketing eddie surpassing doubled northside doubled northside northside kokomo frog maturity maturity koontz defied koontz maturity maturity maturity kenergy armistead doubled kenergy maturity frog maturity doubled maturity maturity kenergy comment surpassing kenergy\n",
      "True: indiana university came in at $ # and purdue at $ #\n",
      "Pred: dimes susceptible babbott rhoden nongame moses caf materials spies pozen allowance allowance devote advantage mckibben reardon affect gatherers moses fj moses moses charities affect charities moses pozen guests pozen charities disproportionately rowe guests whole fj allowance charities customer klapmeier\n",
      "True: state officials have acknowledged that social workers with the tennessee department of children 's services had been involved with a mother and infant before the # year old died in november\n",
      "Pred: standridge standridge retraction defender defender defender figoski guilford retraction figoski loeser retraction buckman corcoran absentia participant aliens exporters greeted defender figoski gigantic defender describe describe corcoran reforms boathouse retraction describe barnegat retraction corcoran participant retraction participant aliens brickman mumbai\n",
      "True: the mother now stands accused of killing her child by endangerment and neglect\n",
      "Pred: hy zoo mathias nationalist sculpting troubling faiths lesser vincent troubling troubling harn genealogical kightlinger visionary sunk fdny caucasus sync troubling uproar menzies jailhouse td fdny mindy faiths troubling fdny mortuary fdny visionary kaleb kaleb pcp menzies paperwork windshield td\n",
      "True: a spokeswoman for the agency said dcs had contact with the mother one time before the child died\n",
      "Pred: est nation nation germans hanna granville millinocket schnickel alzheimer ortiz oliva ortiz partisan paralysis tait plunk buffett incentives sunlight shoreline schnickel ozone hanna biotech clashes sunlight shoreline ortiz ortiz paves buffett marik joke s.c. pencil ucla plunk debate marik\n",
      "True: police say the mother tested positive for drugs the day the child died\n",
      "Pred: pecos goulart crow runs illness unresponsive classrooms bronx bronx bronx bronx bronx bronx bronx bronx bronx bonjour bronx lijana acid eves trunk bronx bronx terpening trunk coahoma liquefied antekeier trunk bronx sculpting bronx avera polito eves bronx coahoma strickland\n",
      "True: the revelation , which was reported by the daily news journal http : <unk> # hv , comes amid an uproar at dcs involving the deaths of children the agency was supposed to protect\n",
      "Pred: pollock barrier mindy wjon mcneese eckstein portraits bipartisan physicist lobsterman lobsterman physicist physicist conservative conservative conservative conservative conservative conservative recovered tutsis lobsterman subsidized haik suzanne lobsterman subsidized conservative conservative conservative konawa bobb conservative compost kema allergic conservative subsidized trespassing\n",
      "True: several media outlets have filed a lawsuit to force the agency to release detailed records on # cases , including the deaths of # children that had been monitored by dcs\n",
      "Pred: psychology dominoes committee dedicated dedicated mickey rancher dedicated spring revamp dedicated repatriated spring inscribed eyman blakeley rancher cota web rancher eyman mckennan blakeley spring eyman eyman mckennan mess eyman rancher mckennan //townta.lk/ schellin eyman rancher volunteer web fritzen pu\n",
      "True: heart of america medical center in rugby has bought johnson clinic , with sites in rugby , <unk> , <unk> and towner\n",
      "Pred: henri kwan kiley hendersonville harassed wielding wielding kimbrough seems harassed gaynor official xm seems rabid oro whitefield entering discord seems mcclatchy oro thus seems meigs hariri seems xm sacrifice humming reformer gregor started mastodon summers lothspeich skydiving irish oro\n",
      "True: under the agreement , johnson clinic will change its name to the heart of america johnson clinic\n",
      "Pred: spices plaquemines rhonda black borders borders crisscrossing former marketed debit borders design borders bagola design debit healthnet cayman borders bagola reapportionment j.d collect bushee debit medtronic cp noodles noodles bagola unnecessary noodles allowance borders absurd misappropriating descent noodles absurd\n",
      "True: plans to integrate the two systems have been ongoing since early #\n",
      "Pred: depression unofficial hilton khalil lamprecht famed tomb tomb lamprecht lamprecht tomb famed influencing lamprecht tomb cordell lamprecht tomb tomb lamprecht tomb lamprecht lamprecht allotted tomb tomb tomb tomb famed tomb famed tomb tomb tomb tomb famed tomb tomb allotted\n",
      "True: the official agreement was signed aug. #\n",
      "Pred: eastside handle lotteries srp handle deet handle retina mays sail midpoint midpoint lacs sail midpoint minimal influence sail paiva advancing tubes payette liability influence handle transcripts treasurers treasurers midpoint sail futures spokesmen handle nextgen futures stabilization repairs spokesmen midpoint\n",
      "True: heart of america ceo jeff <unk> told the minot daily news that the integration will help in recruitment efforts and enable the hospital and clinic to be financially sound\n",
      "Pred: swamped gondolas carrera carrera khalil khalil khalil griego cotney khalil khalil kochanowski addington khalil pile griego brass turf cotney similarities sustain sylmar mcclendon sylmar italy sorely poizner griego boyland boyland chamber cotney doling cotney boyland slam approve griego cotney\n",
      "True: interstate # between iowa and south dakota has reopened a day after a levee was built on top of it to protect nearby homes from high water\n",
      "Pred: valenta championed skeptics renner commanding illustration ceremonies skeptics biometric grates skeptics trunk brainchild provocative ceremonies tonya redeeming brainchild grates grates physically casillas prospects brainchild tournaments settles casillas partridge farook grates exhibit adoption grates settles tournaments gamma breakers brainchild grates\n",
      "True: national guard soldiers and south dakota transportation workers dismantled the berm friday and traffic started flowing again between sioux city , iowa , and sioux falls , south dakota , around # p.m.\n",
      "Pred: owen clarify status nevada drum sentiments wed. nicolas disappeared custody gustavus selling appealing longshore nicolas selling corroded //goo.gl/ thal speculate herndon generator appealing appealing complied speculate rockaway nts ige recyclable recyclable subsidies bejar congregation nts kaczynski devil gustavus figoski\n",
      "True: the national weather service says the big sioux river where iowa , nebraska and south dakota meet crested overnight at a level lower than expected\n",
      "Pred: schell laser warren shaker khon contributions showboat quaker contributions quaker contributions contributions physically transform contributions lobby contributions oliver contributions mccargo contributions shaker oliver tod lobby crocs contributions physically employing isadore lobby pigrum visionary northbound physically mojave quaker northbound shaker\n",
      "True: the i # dike forced motorists onto detours\n",
      "Pred: cadets rubin creve petersburg graduated dannel magraff petersburg cn stretch underwriting dannel invalidate dayton magraff biovista backs magraff magraff stretch overflowed magraff labord tumwater backs magraff magraff magraff dannel prevents magraff counterterrorism overflowed foley dayton biovista user magraff magraff\n",
      "True: floodwaters also blocked many other roads connecting south dakota and iowa between sioux falls and sioux city\n",
      "Pred: woolsey allens reporting republicans fly taggart fly bombing nah nah tributaries tributaries voiced bombing fawns spouses remark nah tributaries tributaries tributaries substantial nah tributaries remark gilbrech luge collectors nah fly alexanyan tributaries art tributaries materials substantial tribe bombing substantial\n",
      "True: a man who state police are calling `` the most prolific trader of child <unk> in louisiana `` has been arrested\n",
      "Pred: tigard exams corvette exams rollback wausau feet wausau affiliation retailer retailer retailer rollback affiliation brawl retailer //stjr.nl/ retailer delay renaming sends marcella retailer brawl rollback glimpse destroy retailer delay renaming retailer recreational bargained retailer brawl brawl paypal paypal teenage\n",
      "True: state troopers said on tuesday that a joint investigation by louisiana state police detectives , the desoto parish sheriff 's office and the pennsylvania state police led to the arrest of # year old gregory blackburn of mansfield\n",
      "Pred: coursework rumors coeur paramus ef gonorrhea investigates affidavits ef soukup ef inadequately magnifying farmhouse soukup affidavits parties affidavits kirsten nerheim gonorrhea huenefeld ef soukup kirsten kirsten soukup ef affidavits liquid rubbish cellophane hp quail baltic kirsten competitions historian arts\n",
      "True: on monday , troopers discovered at blackburn 's home several computer hard drives containing thousands of images of child pornography that had been downloaded from the internet\n",
      "Pred: detailing supposedly spying underemployed reassign locker sokha sokha although reassign sokha sokha sokha fidelity mastroianni sokha corn hancock fidelity sokha nehls itw sokha floatplane fidelity sokha sokha passes sokha fidelity dean haugh fleet itw sokha nehls sokha sokha sokha\n",
      "True: blackburn was booked into the desoto parish jail on charges of pornography involving juveniles and possession of schedule i drugs\n",
      "Pred: prater late fatality climate wexford liable chula commute antidepressants liable liable liable sentence liable sentence logged logged logged logged casts logged logged sentence logged logged sentence logged logged logged nevada sentence logged logged liable logged logged logged logged haiti\n",
      "True: he is being held without bond\n",
      "Pred: extending rubin jogging nba coughing grids constructed coughing absurd dodson imf occupants dodson allegheny brainchild jung robertson adamant dodson prosser residing dodson hawke term worsened atomic sigala lunchtime dodson stakeholders dodson prosser cousins pastures prosser dodson cousins troubled prosser\n",
      "0 : checks prosecutors tuesday prison fraud check charges wood bank escape\n",
      "1 : heart snyder nevada michigan debt group event indiana west johnson\n",
      "2 : sioux south dakota clinic iowa burned medical honored contacted reopened\n",
      "3 : clinic america child children johnson mother heart initiative news medical\n",
      "4 : south dakota sioux iowa mother agency child deaths national died\n",
      "5 : mining initiative south nevada raise put plan signatures make principal\n",
      "6 : indiana debt average student university highest higher loan national southern\n",
      "7 : lawyer made trial jury child judge comments testimony told tuesday\n",
      "8 : jail child sex parish checks charges joint pornography burns arrested\n",
      "9 : medical scott authorities dogs sheriff animal phoenix testimony clinic transported\n"
     ]
    }
   ],
   "source": [
    "if len(logs) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "while epoch < config.epochs:\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = get_feed_dict(batch)\n",
    "        if config.warmup > 0 and beta_eval < 1.0: sess.run(update_beta)\n",
    "\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch, ppls_batch = \\\n",
    "        sess.run([opt, loss, topic_loss_recon, topic_loss_kl, sent_loss_recon, sent_loss_kl, topic_ppls], feed_dict = feed_dict)\n",
    "        \n",
    "        if sent_loss_kl_batch == np.inf:\n",
    "            pdb.set_trace()\n",
    "\n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if ct%config.log_period==0:\n",
    "            time_dev = time.time()\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, sent_loss_recon_train, sent_loss_kl_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_dev, sent_loss_dev, ppl_dev = get_loss(sess, dev_batches)\n",
    "            \n",
    "            if loss_dev < loss_min:\n",
    "                loss_min = loss_dev\n",
    "                saver.save(sess, config.modelpath, global_step=epoch*10000+ct)\n",
    "\n",
    "            if config.warmup > 0: beta_eval = beta.eval(session=sess)\n",
    "\n",
    "            clear_output()\n",
    "            time_finish = time.time()\n",
    "            time_log = int(time_finish - time_start)\n",
    "            time_log_dev = int(time_finish - time_dev)\n",
    "            logs += [(time_log, time_log_dev, epoch, ct, loss_train, ppl_train, topic_loss_recon_train, topic_loss_kl_train, sent_loss_recon_train, sent_loss_kl_train, loss_dev, ppl_dev, topic_loss_dev, sent_loss_dev, beta_eval)]\n",
    "            for log in logs:\n",
    "                print('%03d[s], %02d[s], Ep: %02d, Ct: %05d|TR LOSS: %.0f, PPL: %.0f|TM NLL: %.0f, KL: %.2f | LM NLL: %.2f, KL: %.2f|DE LOSS: %.0f, PPL: %.0f, TM: %.0f, LM: %.2f|BETA: %.6f' %  log)\n",
    "\n",
    "            print_sample(batch)\n",
    "\n",
    "            time_start = time.time()\n",
    "            \n",
    "            # visualize topic\n",
    "            topics_freq_bow_idxs = bow_idxs[sess.run(topics_freq_bow_indices)]\n",
    "            for topic, topic_freq_bow_idxs in enumerate(topics_freq_bow_idxs):\n",
    "                print(topic, ':', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "                \n",
    "    epoch += 1\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2883"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([i for i, batch in ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-24bb357ae5d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while next(train_iter):\n",
    "    i += 1\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2884"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "007[s], 06[s], Ep: 00, Ct: 00000|TR LOSS: 398, PPL: 2663|TM NLL: 387, KL: 0.75 | LM NLL: 10.34, KL: 0.60|DE LOSS: 347, PPL: 2656, TM: 337, LM: 10.34|BETA: 0.001000\n",
      "065[s], 06[s], Ep: 00, Ct: 00500|TR LOSS: 326, PPL: 1675|TM NLL: 318, KL: 0.60 | LM NLL: 7.41, KL: 19.54|DE LOSS: 319, PPL: 1471, TM: 312, LM: 6.83|BETA: 0.001035\n",
      "065[s], 06[s], Ep: 00, Ct: 01000|TR LOSS: 324, PPL: 1530|TM NLL: 315, KL: 1.43 | LM NLL: 7.12, KL: 16.75|DE LOSS: 316, PPL: 1349, TM: 309, LM: 6.76|BETA: 0.001069\n",
      "066[s], 06[s], Ep: 00, Ct: 01500|TR LOSS: 323, PPL: 1468|TM NLL: 314, KL: 1.79 | LM NLL: 7.01, KL: 14.90|DE LOSS: 315, PPL: 1321, TM: 309, LM: 6.67|BETA: 0.001104\n",
      "066[s], 06[s], Ep: 00, Ct: 02000|TR LOSS: 322, PPL: 1421|TM NLL: 313, KL: 2.04 | LM NLL: 6.93, KL: 13.92|DE LOSS: 313, PPL: 1252, TM: 307, LM: 6.58|BETA: 0.001139\n",
      "066[s], 06[s], Ep: 00, Ct: 02500|TR LOSS: inf, PPL: 1387|TM NLL: 312, KL: 2.29 | LM NLL: 6.89, KL: inf|DE LOSS: 313, PPL: 1246, TM: 306, LM: 6.66|BETA: 0.001173\n",
      "052[s], 06[s], Ep: 01, Ct: 00000|TR LOSS: inf, PPL: 1361|TM NLL: 311, KL: 2.42 | LM NLL: 6.86, KL: inf|DE LOSS: 312, PPL: 1203, TM: 305, LM: 6.65|BETA: 0.001200\n",
      "066[s], 06[s], Ep: 01, Ct: 00500|TR LOSS: inf, PPL: 1333|TM NLL: 310, KL: 2.56 | LM NLL: 6.84, KL: inf|DE LOSS: 311, PPL: 1159, TM: 304, LM: 6.60|BETA: 0.001235\n",
      "066[s], 06[s], Ep: 01, Ct: 01000|TR LOSS: inf, PPL: 1308|TM NLL: 309, KL: 2.68 | LM NLL: 6.81, KL: inf|DE LOSS: 310, PPL: 1141, TM: 303, LM: 6.57|BETA: 0.001269\n",
      "066[s], 06[s], Ep: 01, Ct: 01500|TR LOSS: inf, PPL: 1288|TM NLL: 309, KL: 2.76 | LM NLL: 6.79, KL: inf|DE LOSS: 309, PPL: 1121, TM: 302, LM: 6.51|BETA: 0.001304\n",
      "066[s], 06[s], Ep: 01, Ct: 02000|TR LOSS: inf, PPL: 1269|TM NLL: 308, KL: 2.82 | LM NLL: 6.76, KL: inf|DE LOSS: 308, PPL: 1104, TM: 302, LM: 6.46|BETA: 0.001339\n",
      "066[s], 06[s], Ep: 01, Ct: 02500|TR LOSS: inf, PPL: 1251|TM NLL: 307, KL: 2.90 | LM NLL: 6.74, KL: inf|DE LOSS: 308, PPL: 1090, TM: 301, LM: 6.42|BETA: 0.001374\n",
      "052[s], 06[s], Ep: 02, Ct: 00000|TR LOSS: inf, PPL: 1239|TM NLL: 307, KL: 2.93 | LM NLL: 6.72, KL: inf|DE LOSS: 308, PPL: 1099, TM: 301, LM: 6.37|BETA: 0.001400\n",
      "066[s], 06[s], Ep: 02, Ct: 00500|TR LOSS: inf, PPL: 1226|TM NLL: 307, KL: 2.97 | LM NLL: 6.69, KL: inf|DE LOSS: 307, PPL: 1071, TM: 301, LM: 6.34|BETA: 0.001435\n",
      "066[s], 06[s], Ep: 02, Ct: 01000|TR LOSS: inf, PPL: 1214|TM NLL: 306, KL: 3.00 | LM NLL: 6.67, KL: inf|DE LOSS: 307, PPL: 1077, TM: 301, LM: 6.29|BETA: 0.001470\n",
      "066[s], 06[s], Ep: 02, Ct: 01500|TR LOSS: inf, PPL: 1205|TM NLL: 306, KL: 3.03 | LM NLL: 6.65, KL: inf|DE LOSS: 307, PPL: 1070, TM: 300, LM: 6.24|BETA: 0.001504\n",
      "066[s], 06[s], Ep: 02, Ct: 02000|TR LOSS: inf, PPL: 1196|TM NLL: 306, KL: 3.05 | LM NLL: 6.62, KL: inf|DE LOSS: 307, PPL: 1063, TM: 300, LM: 6.20|BETA: 0.001539\n",
      "066[s], 06[s], Ep: 02, Ct: 02500|TR LOSS: inf, PPL: 1186|TM NLL: 305, KL: 3.08 | LM NLL: 6.60, KL: inf|DE LOSS: 306, PPL: 1064, TM: 300, LM: 6.16|BETA: 0.001574\n",
      "052[s], 06[s], Ep: 03, Ct: 00000|TR LOSS: inf, PPL: 1180|TM NLL: 305, KL: 3.09 | LM NLL: 6.58, KL: inf|DE LOSS: 306, PPL: 1062, TM: 300, LM: 6.14|BETA: 0.001600\n",
      "066[s], 06[s], Ep: 03, Ct: 00500|TR LOSS: inf, PPL: 1173|TM NLL: 305, KL: 3.11 | LM NLL: 6.56, KL: inf|DE LOSS: 306, PPL: 1051, TM: 300, LM: 6.11|BETA: 0.001635\n",
      "066[s], 06[s], Ep: 03, Ct: 01000|TR LOSS: inf, PPL: 1166|TM NLL: 304, KL: 3.13 | LM NLL: 6.54, KL: inf|DE LOSS: 306, PPL: 1056, TM: 300, LM: 6.10|BETA: 0.001670\n",
      "066[s], 06[s], Ep: 03, Ct: 01500|TR LOSS: inf, PPL: 1161|TM NLL: 304, KL: 3.15 | LM NLL: 6.52, KL: inf|DE LOSS: 306, PPL: 1044, TM: 300, LM: 6.03|BETA: 0.001704\n",
      "066[s], 06[s], Ep: 03, Ct: 02000|TR LOSS: inf, PPL: 1155|TM NLL: 304, KL: 3.17 | LM NLL: 6.50, KL: inf|DE LOSS: 306, PPL: 1050, TM: 300, LM: 6.00|BETA: 0.001739\n",
      "066[s], 06[s], Ep: 03, Ct: 02500|TR LOSS: inf, PPL: 1149|TM NLL: 304, KL: 3.19 | LM NLL: 6.48, KL: inf|DE LOSS: 306, PPL: 1045, TM: 300, LM: 5.97|BETA: 0.001774\n",
      "052[s], 06[s], Ep: 04, Ct: 00000|TR LOSS: inf, PPL: 1145|TM NLL: 304, KL: 3.20 | LM NLL: 6.46, KL: inf|DE LOSS: 306, PPL: 1044, TM: 300, LM: 5.95|BETA: 0.001800\n",
      "067[s], 06[s], Ep: 04, Ct: 00500|TR LOSS: inf, PPL: 1141|TM NLL: 304, KL: 3.21 | LM NLL: 6.44, KL: inf|DE LOSS: 306, PPL: 1041, TM: 300, LM: 5.92|BETA: 0.001835\n",
      "066[s], 06[s], Ep: 04, Ct: 01000|TR LOSS: inf, PPL: 1136|TM NLL: 303, KL: 3.23 | LM NLL: 6.42, KL: inf|DE LOSS: 305, PPL: 1043, TM: 300, LM: 5.90|BETA: 0.001870\n",
      "068[s], 06[s], Ep: 04, Ct: 01500|TR LOSS: inf, PPL: 1133|TM NLL: 303, KL: 3.24 | LM NLL: 6.41, KL: inf|DE LOSS: 305, PPL: 1038, TM: 299, LM: 5.88|BETA: 0.001904\n",
      "070[s], 07[s], Ep: 04, Ct: 02000|TR LOSS: inf, PPL: 1129|TM NLL: 303, KL: 3.25 | LM NLL: 6.39, KL: inf|DE LOSS: 305, PPL: 1037, TM: 299, LM: 5.85|BETA: 0.001939\n",
      "070[s], 07[s], Ep: 04, Ct: 02500|TR LOSS: inf, PPL: 1125|TM NLL: 303, KL: 3.27 | LM NLL: 6.37, KL: inf|DE LOSS: 305, PPL: 1036, TM: 299, LM: 5.82|BETA: 0.001974\n",
      "055[s], 07[s], Ep: 05, Ct: 00000|TR LOSS: inf, PPL: 1122|TM NLL: 303, KL: 3.28 | LM NLL: 6.36, KL: inf|DE LOSS: 305, PPL: 1038, TM: 300, LM: 5.81|BETA: 0.002001\n",
      "072[s], 07[s], Ep: 05, Ct: 00500|TR LOSS: inf, PPL: 1119|TM NLL: 303, KL: 3.29 | LM NLL: 6.34, KL: inf|DE LOSS: 305, PPL: 1032, TM: 299, LM: 5.81|BETA: 0.002035\n",
      "071[s], 07[s], Ep: 05, Ct: 01000|TR LOSS: inf, PPL: 1115|TM NLL: 303, KL: 3.30 | LM NLL: 6.32, KL: inf|DE LOSS: 305, PPL: 1033, TM: 299, LM: 5.79|BETA: 0.002070\n",
      "072[s], 07[s], Ep: 05, Ct: 01500|TR LOSS: inf, PPL: 1113|TM NLL: 303, KL: 3.31 | LM NLL: 6.31, KL: inf|DE LOSS: 305, PPL: 1031, TM: 299, LM: 5.74|BETA: 0.002105\n",
      "071[s], 06[s], Ep: 05, Ct: 02000|TR LOSS: inf, PPL: 1110|TM NLL: 302, KL: 3.32 | LM NLL: 6.29, KL: inf|DE LOSS: 305, PPL: 1026, TM: 299, LM: 5.73|BETA: 0.002139\n",
      "072[s], 07[s], Ep: 05, Ct: 02500|TR LOSS: inf, PPL: 1107|TM NLL: 302, KL: 3.33 | LM NLL: 6.28, KL: inf|DE LOSS: 305, PPL: 1031, TM: 299, LM: 5.71|BETA: 0.002174\n",
      "057[s], 07[s], Ep: 06, Ct: 00000|TR LOSS: inf, PPL: 1105|TM NLL: 302, KL: 3.34 | LM NLL: 6.27, KL: inf|DE LOSS: 305, PPL: 1032, TM: 299, LM: 5.69|BETA: 0.002201\n",
      "076[s], 07[s], Ep: 06, Ct: 00500|TR LOSS: inf, PPL: 1102|TM NLL: 302, KL: 3.35 | LM NLL: 6.25, KL: inf|DE LOSS: 305, PPL: 1023, TM: 299, LM: 5.67|BETA: 0.002235\n",
      "077[s], 07[s], Ep: 06, Ct: 01000|TR LOSS: inf, PPL: 1100|TM NLL: 302, KL: 3.36 | LM NLL: 6.24, KL: inf|DE LOSS: 305, PPL: 1026, TM: 299, LM: 5.66|BETA: 0.002270\n",
      "077[s], 07[s], Ep: 06, Ct: 01500|TR LOSS: inf, PPL: 1098|TM NLL: 302, KL: 3.37 | LM NLL: 6.22, KL: inf|DE LOSS: 305, PPL: 1020, TM: 299, LM: 5.64|BETA: 0.002305\n",
      "077[s], 07[s], Ep: 06, Ct: 02000|TR LOSS: inf, PPL: 1096|TM NLL: 302, KL: 3.38 | LM NLL: 6.21, KL: inf|DE LOSS: 305, PPL: 1018, TM: 299, LM: 5.62|BETA: 0.002339\n",
      "076[s], 08[s], Ep: 06, Ct: 02500|TR LOSS: inf, PPL: 1093|TM NLL: 302, KL: 3.38 | LM NLL: 6.20, KL: inf|DE LOSS: 304, PPL: 1021, TM: 299, LM: 5.61|BETA: 0.002374\n",
      "061[s], 07[s], Ep: 07, Ct: 00000|TR LOSS: inf, PPL: 1091|TM NLL: 302, KL: 3.39 | LM NLL: 6.19, KL: inf|DE LOSS: 305, PPL: 1024, TM: 299, LM: 5.60|BETA: 0.002401\n",
      "077[s], 07[s], Ep: 07, Ct: 00500|TR LOSS: inf, PPL: 1089|TM NLL: 302, KL: 3.40 | LM NLL: 6.17, KL: inf|DE LOSS: 305, PPL: 1019, TM: 299, LM: 5.59|BETA: 0.002435\n",
      "077[s], 07[s], Ep: 07, Ct: 01000|TR LOSS: inf, PPL: 1087|TM NLL: 302, KL: 3.41 | LM NLL: 6.16, KL: inf|DE LOSS: 304, PPL: 1017, TM: 299, LM: 5.57|BETA: 0.002470\n",
      "077[s], 07[s], Ep: 07, Ct: 01500|TR LOSS: inf, PPL: 1086|TM NLL: 302, KL: 3.42 | LM NLL: 6.15, KL: inf|DE LOSS: 304, PPL: 1014, TM: 299, LM: 5.56|BETA: 0.002505\n",
      "077[s], 07[s], Ep: 07, Ct: 02000|TR LOSS: inf, PPL: 1084|TM NLL: 301, KL: 3.42 | LM NLL: 6.14, KL: inf|DE LOSS: 304, PPL: 1017, TM: 299, LM: 5.54|BETA: 0.002539\n",
      "077[s], 07[s], Ep: 07, Ct: 02500|TR LOSS: inf, PPL: 1082|TM NLL: 301, KL: 3.43 | LM NLL: 6.12, KL: inf|DE LOSS: 304, PPL: 1017, TM: 299, LM: 5.54|BETA: 0.002574\n",
      "060[s], 07[s], Ep: 08, Ct: 00000|TR LOSS: inf, PPL: 1081|TM NLL: 301, KL: 3.44 | LM NLL: 6.12, KL: inf|DE LOSS: 304, PPL: 1021, TM: 299, LM: 5.53|BETA: 0.002601\n",
      "077[s], 07[s], Ep: 08, Ct: 00500|TR LOSS: inf, PPL: 1079|TM NLL: 301, KL: 3.44 | LM NLL: 6.10, KL: inf|DE LOSS: 304, PPL: 1011, TM: 299, LM: 5.51|BETA: 0.002636\n",
      "077[s], 07[s], Ep: 08, Ct: 01000|TR LOSS: inf, PPL: 1077|TM NLL: 301, KL: 3.45 | LM NLL: 6.09, KL: inf|DE LOSS: 304, PPL: 1015, TM: 299, LM: 5.51|BETA: 0.002670\n",
      "077[s], 07[s], Ep: 08, Ct: 01500|TR LOSS: inf, PPL: 1076|TM NLL: 301, KL: 3.46 | LM NLL: 6.08, KL: inf|DE LOSS: 304, PPL: 1015, TM: 299, LM: 5.49|BETA: 0.002705\n",
      "077[s], 07[s], Ep: 08, Ct: 02000|TR LOSS: inf, PPL: 1075|TM NLL: 301, KL: 3.46 | LM NLL: 6.07, KL: inf|DE LOSS: 304, PPL: 1015, TM: 299, LM: 5.48|BETA: 0.002740\n",
      "077[s], 07[s], Ep: 08, Ct: 02500|TR LOSS: inf, PPL: 1073|TM NLL: 301, KL: 3.47 | LM NLL: 6.06, KL: inf|DE LOSS: 304, PPL: 1014, TM: 299, LM: 5.47|BETA: 0.002774\n",
      "061[s], 07[s], Ep: 09, Ct: 00000|TR LOSS: inf, PPL: 1072|TM NLL: 301, KL: 3.47 | LM NLL: 6.05, KL: inf|DE LOSS: 304, PPL: 1013, TM: 299, LM: 5.46|BETA: 0.002801\n",
      "077[s], 07[s], Ep: 09, Ct: 00500|TR LOSS: inf, PPL: 1070|TM NLL: 301, KL: 3.48 | LM NLL: 6.04, KL: inf|DE LOSS: 304, PPL: 1013, TM: 299, LM: 5.45|BETA: 0.002836\n",
      "077[s], 07[s], Ep: 09, Ct: 01000|TR LOSS: inf, PPL: 1069|TM NLL: 301, KL: 3.49 | LM NLL: 6.03, KL: inf|DE LOSS: 304, PPL: 1011, TM: 299, LM: 5.44|BETA: 0.002870\n",
      "077[s], 07[s], Ep: 09, Ct: 01500|TR LOSS: inf, PPL: 1068|TM NLL: 301, KL: 3.49 | LM NLL: 6.02, KL: inf|DE LOSS: 304, PPL: 1012, TM: 299, LM: 5.43|BETA: 0.002905\n",
      "077[s], 07[s], Ep: 09, Ct: 02000|TR LOSS: inf, PPL: 1067|TM NLL: 301, KL: 3.50 | LM NLL: 6.01, KL: inf|DE LOSS: 304, PPL: 1011, TM: 299, LM: 5.42|BETA: 0.002940\n",
      "076[s], 07[s], Ep: 09, Ct: 02500|TR LOSS: inf, PPL: 1065|TM NLL: 301, KL: 3.50 | LM NLL: 6.00, KL: inf|DE LOSS: 304, PPL: 1009, TM: 299, LM: 5.42|BETA: 0.002974\n",
      "061[s], 07[s], Ep: 10, Ct: 00000|TR LOSS: inf, PPL: 1064|TM NLL: 301, KL: 3.51 | LM NLL: 6.00, KL: inf|DE LOSS: 304, PPL: 1009, TM: 299, LM: 5.41|BETA: 0.003001\n",
      "077[s], 07[s], Ep: 10, Ct: 00500|TR LOSS: inf, PPL: 1063|TM NLL: 301, KL: 3.51 | LM NLL: 5.99, KL: inf|DE LOSS: 304, PPL: 1012, TM: 299, LM: 5.40|BETA: 0.003036\n",
      "077[s], 07[s], Ep: 10, Ct: 01000|TR LOSS: inf, PPL: 1062|TM NLL: 301, KL: 3.52 | LM NLL: 5.98, KL: inf|DE LOSS: 304, PPL: 1008, TM: 299, LM: 5.39|BETA: 0.003070\n",
      "077[s], 07[s], Ep: 10, Ct: 01500|TR LOSS: inf, PPL: 1061|TM NLL: 301, KL: 3.52 | LM NLL: 5.97, KL: inf|DE LOSS: 304, PPL: 1010, TM: 299, LM: 5.38|BETA: 0.003105\n",
      "077[s], 07[s], Ep: 10, Ct: 02000|TR LOSS: inf, PPL: 1060|TM NLL: 301, KL: 3.53 | LM NLL: 5.96, KL: inf|DE LOSS: 304, PPL: 1006, TM: 298, LM: 5.37|BETA: 0.003140\n",
      "077[s], 07[s], Ep: 10, Ct: 02500|TR LOSS: inf, PPL: 1059|TM NLL: 300, KL: 3.53 | LM NLL: 5.95, KL: inf|DE LOSS: 304, PPL: 1011, TM: 299, LM: 5.37|BETA: 0.003174\n",
      "061[s], 08[s], Ep: 11, Ct: 00000|TR LOSS: inf, PPL: 1058|TM NLL: 300, KL: 3.54 | LM NLL: 5.94, KL: inf|DE LOSS: 304, PPL: 1010, TM: 299, LM: 5.37|BETA: 0.003201\n",
      "077[s], 07[s], Ep: 11, Ct: 00500|TR LOSS: inf, PPL: 1057|TM NLL: 300, KL: 3.54 | LM NLL: 5.94, KL: inf|DE LOSS: 304, PPL: 1004, TM: 299, LM: 5.36|BETA: 0.003236\n",
      "077[s], 07[s], Ep: 11, Ct: 01000|TR LOSS: inf, PPL: 1056|TM NLL: 300, KL: 3.54 | LM NLL: 5.93, KL: inf|DE LOSS: 304, PPL: 1007, TM: 299, LM: 5.35|BETA: 0.003271\n",
      "077[s], 07[s], Ep: 11, Ct: 01500|TR LOSS: inf, PPL: 1056|TM NLL: 300, KL: 3.55 | LM NLL: 5.92, KL: inf|DE LOSS: 304, PPL: 1005, TM: 298, LM: 5.34|BETA: 0.003305\n",
      "077[s], 07[s], Ep: 11, Ct: 02000|TR LOSS: inf, PPL: 1055|TM NLL: 300, KL: 3.55 | LM NLL: 5.91, KL: inf|DE LOSS: 304, PPL: 1008, TM: 299, LM: 5.33|BETA: 0.003340\n",
      "077[s], 07[s], Ep: 11, Ct: 02500|TR LOSS: inf, PPL: 1054|TM NLL: 300, KL: 3.56 | LM NLL: 5.90, KL: inf|DE LOSS: 304, PPL: 1006, TM: 298, LM: 5.33|BETA: 0.003375\n",
      "060[s], 07[s], Ep: 12, Ct: 00000|TR LOSS: inf, PPL: 1053|TM NLL: 300, KL: 3.56 | LM NLL: 5.90, KL: inf|DE LOSS: 304, PPL: 1011, TM: 299, LM: 5.33|BETA: 0.003401\n",
      "077[s], 07[s], Ep: 12, Ct: 00500|TR LOSS: inf, PPL: 1052|TM NLL: 300, KL: 3.56 | LM NLL: 5.89, KL: inf|DE LOSS: 304, PPL: 1006, TM: 298, LM: 5.31|BETA: 0.003436\n",
      "077[s], 07[s], Ep: 12, Ct: 01000|TR LOSS: inf, PPL: 1051|TM NLL: 300, KL: 3.57 | LM NLL: 5.88, KL: inf|DE LOSS: 304, PPL: 1004, TM: 298, LM: 5.31|BETA: 0.003471\n",
      "077[s], 07[s], Ep: 12, Ct: 01500|TR LOSS: inf, PPL: 1050|TM NLL: 300, KL: 3.57 | LM NLL: 5.87, KL: inf|DE LOSS: 304, PPL: 1008, TM: 299, LM: 5.31|BETA: 0.003505\n",
      "077[s], 07[s], Ep: 12, Ct: 02000|TR LOSS: inf, PPL: 1050|TM NLL: 300, KL: 3.57 | LM NLL: 5.87, KL: inf|DE LOSS: 304, PPL: 1007, TM: 298, LM: 5.30|BETA: 0.003540\n",
      "077[s], 07[s], Ep: 12, Ct: 02500|TR LOSS: inf, PPL: 1049|TM NLL: 300, KL: 3.58 | LM NLL: 5.86, KL: inf|DE LOSS: 304, PPL: 1007, TM: 298, LM: 5.29|BETA: 0.003575\n",
      "061[s], 07[s], Ep: 13, Ct: 00000|TR LOSS: inf, PPL: 1048|TM NLL: 300, KL: 3.58 | LM NLL: 5.85, KL: inf|DE LOSS: 304, PPL: 1005, TM: 298, LM: 5.29|BETA: 0.003601\n",
      "077[s], 07[s], Ep: 13, Ct: 00500|TR LOSS: inf, PPL: 1047|TM NLL: 300, KL: 3.58 | LM NLL: 5.85, KL: inf|DE LOSS: 304, PPL: 1004, TM: 298, LM: 5.29|BETA: 0.003636\n",
      "077[s], 07[s], Ep: 13, Ct: 01000|TR LOSS: inf, PPL: 1047|TM NLL: 300, KL: 3.59 | LM NLL: 5.84, KL: inf|DE LOSS: 304, PPL: 1004, TM: 298, LM: 5.28|BETA: 0.003671\n",
      "077[s], 07[s], Ep: 13, Ct: 01500|TR LOSS: inf, PPL: 1046|TM NLL: 300, KL: 3.59 | LM NLL: 5.83, KL: inf|DE LOSS: 304, PPL: 1008, TM: 298, LM: 5.27|BETA: 0.003705\n",
      "077[s], 07[s], Ep: 13, Ct: 02000|TR LOSS: inf, PPL: 1045|TM NLL: 300, KL: 3.59 | LM NLL: 5.83, KL: inf|DE LOSS: 304, PPL: 1007, TM: 298, LM: 5.26|BETA: 0.003740\n",
      "077[s], 08[s], Ep: 13, Ct: 02500|TR LOSS: inf, PPL: 1044|TM NLL: 300, KL: 3.60 | LM NLL: 5.82, KL: inf|DE LOSS: 304, PPL: 1006, TM: 298, LM: 5.26|BETA: 0.003775\n",
      "061[s], 08[s], Ep: 14, Ct: 00000|TR LOSS: inf, PPL: 1044|TM NLL: 300, KL: 3.60 | LM NLL: 5.81, KL: inf|DE LOSS: 304, PPL: 1008, TM: 299, LM: 5.25|BETA: 0.003801\n",
      "077[s], 07[s], Ep: 14, Ct: 00500|TR LOSS: inf, PPL: 1043|TM NLL: 300, KL: 3.60 | LM NLL: 5.81, KL: inf|DE LOSS: 304, PPL: 1002, TM: 298, LM: 5.24|BETA: 0.003836\n",
      "077[s], 07[s], Ep: 14, Ct: 01000|TR LOSS: inf, PPL: 1043|TM NLL: 300, KL: 3.60 | LM NLL: 5.80, KL: inf|DE LOSS: 304, PPL: 1003, TM: 298, LM: 5.24|BETA: 0.003871\n",
      "077[s], 07[s], Ep: 14, Ct: 01500|TR LOSS: inf, PPL: 1042|TM NLL: 300, KL: 3.61 | LM NLL: 5.80, KL: inf|DE LOSS: 304, PPL: 1003, TM: 298, LM: 5.24|BETA: 0.003906\n",
      "077[s], 07[s], Ep: 14, Ct: 02000|TR LOSS: inf, PPL: 1042|TM NLL: 300, KL: 3.61 | LM NLL: 5.79, KL: inf|DE LOSS: 304, PPL: 1001, TM: 298, LM: 5.23|BETA: 0.003940\n",
      "077[s], 08[s], Ep: 14, Ct: 02500|TR LOSS: inf, PPL: 1041|TM NLL: 300, KL: 3.61 | LM NLL: 5.78, KL: inf|DE LOSS: 304, PPL: 1007, TM: 299, LM: 5.23|BETA: 0.003975\n",
      "061[s], 07[s], Ep: 15, Ct: 00000|TR LOSS: inf, PPL: 1040|TM NLL: 300, KL: 3.61 | LM NLL: 5.78, KL: inf|DE LOSS: 304, PPL: 1009, TM: 299, LM: 5.23|BETA: 0.004002\n",
      "077[s], 07[s], Ep: 15, Ct: 00500|TR LOSS: inf, PPL: 1040|TM NLL: 300, KL: 3.62 | LM NLL: 5.77, KL: inf|DE LOSS: 304, PPL: 1003, TM: 298, LM: 5.22|BETA: 0.004036\n",
      "077[s], 07[s], Ep: 15, Ct: 01000|TR LOSS: inf, PPL: 1039|TM NLL: 300, KL: 3.62 | LM NLL: 5.77, KL: inf|DE LOSS: 304, PPL: 1005, TM: 298, LM: 5.22|BETA: 0.004071\n",
      "077[s], 08[s], Ep: 15, Ct: 01500|TR LOSS: inf, PPL: 1039|TM NLL: 300, KL: 3.62 | LM NLL: 5.76, KL: inf|DE LOSS: 304, PPL: 1002, TM: 298, LM: 5.20|BETA: 0.004106\n",
      "077[s], 07[s], Ep: 15, Ct: 02000|TR LOSS: inf, PPL: 1038|TM NLL: 300, KL: 3.62 | LM NLL: 5.75, KL: inf|DE LOSS: 304, PPL: 1004, TM: 298, LM: 5.21|BETA: 0.004140\n",
      "075[s], 07[s], Ep: 15, Ct: 02500|TR LOSS: inf, PPL: 1037|TM NLL: 300, KL: 3.63 | LM NLL: 5.75, KL: inf|DE LOSS: 304, PPL: 1005, TM: 298, LM: 5.20|BETA: 0.004175\n",
      "056[s], 07[s], Ep: 16, Ct: 00000|TR LOSS: inf, PPL: 1037|TM NLL: 300, KL: 3.63 | LM NLL: 5.74, KL: inf|DE LOSS: 304, PPL: 1006, TM: 298, LM: 5.20|BETA: 0.004202\n",
      "072[s], 07[s], Ep: 16, Ct: 00500|TR LOSS: inf, PPL: 1036|TM NLL: 300, KL: 3.63 | LM NLL: 5.74, KL: inf|DE LOSS: 303, PPL: 1001, TM: 298, LM: 5.19|BETA: 0.004236\n",
      "071[s], 07[s], Ep: 16, Ct: 01000|TR LOSS: inf, PPL: 1036|TM NLL: 300, KL: 3.63 | LM NLL: 5.73, KL: inf|DE LOSS: 303, PPL: 999, TM: 298, LM: 5.19|BETA: 0.004271\n",
      "072[s], 07[s], Ep: 16, Ct: 01500|TR LOSS: inf, PPL: 1035|TM NLL: 300, KL: 3.64 | LM NLL: 5.73, KL: inf|DE LOSS: 304, PPL: 1004, TM: 298, LM: 5.19|BETA: 0.004306\n",
      "072[s], 07[s], Ep: 16, Ct: 02000|TR LOSS: inf, PPL: 1035|TM NLL: 300, KL: 3.64 | LM NLL: 5.72, KL: inf|DE LOSS: 304, PPL: 1005, TM: 298, LM: 5.18|BETA: 0.004340\n",
      "072[s], 07[s], Ep: 16, Ct: 02500|TR LOSS: inf, PPL: 1034|TM NLL: 299, KL: 3.64 | LM NLL: 5.72, KL: inf|DE LOSS: 304, PPL: 1002, TM: 298, LM: 5.18|BETA: 0.004375\n",
      "057[s], 07[s], Ep: 17, Ct: 00000|TR LOSS: inf, PPL: 1034|TM NLL: 299, KL: 3.64 | LM NLL: 5.71, KL: inf|DE LOSS: 304, PPL: 1005, TM: 298, LM: 5.18|BETA: 0.004402\n",
      "072[s], 07[s], Ep: 17, Ct: 00500|TR LOSS: inf, PPL: 1033|TM NLL: 299, KL: 3.64 | LM NLL: 5.71, KL: inf|DE LOSS: 303, PPL: 1000, TM: 298, LM: 5.17|BETA: 0.004436\n",
      "072[s], 07[s], Ep: 17, Ct: 01000|TR LOSS: inf, PPL: 1033|TM NLL: 299, KL: 3.65 | LM NLL: 5.70, KL: inf|DE LOSS: 303, PPL: 1002, TM: 298, LM: 5.16|BETA: 0.004471\n",
      "072[s], 07[s], Ep: 17, Ct: 01500|TR LOSS: inf, PPL: 1033|TM NLL: 299, KL: 3.65 | LM NLL: 5.70, KL: inf|DE LOSS: 304, PPL: 1004, TM: 298, LM: 5.16|BETA: 0.004506\n",
      "072[s], 07[s], Ep: 17, Ct: 02000|TR LOSS: inf, PPL: 1032|TM NLL: 299, KL: 3.65 | LM NLL: 5.69, KL: inf|DE LOSS: 303, PPL: 1000, TM: 298, LM: 5.16|BETA: 0.004541\n",
      "072[s], 07[s], Ep: 17, Ct: 02500|TR LOSS: inf, PPL: 1032|TM NLL: 299, KL: 3.65 | LM NLL: 5.69, KL: inf|DE LOSS: 303, PPL: 1002, TM: 298, LM: 5.16|BETA: 0.004575\n",
      "057[s], 07[s], Ep: 18, Ct: 00000|TR LOSS: inf, PPL: 1031|TM NLL: 299, KL: 3.65 | LM NLL: 5.68, KL: inf|DE LOSS: 304, PPL: 1004, TM: 298, LM: 5.16|BETA: 0.004602\n",
      "072[s], 07[s], Ep: 18, Ct: 00500|TR LOSS: inf, PPL: 1031|TM NLL: 299, KL: 3.66 | LM NLL: 5.68, KL: inf|DE LOSS: 304, PPL: 1003, TM: 298, LM: 5.15|BETA: 0.004637\n",
      "072[s], 07[s], Ep: 18, Ct: 01000|TR LOSS: inf, PPL: 1030|TM NLL: 299, KL: 3.66 | LM NLL: 5.67, KL: inf|DE LOSS: 303, PPL: 1001, TM: 298, LM: 5.14|BETA: 0.004671\n",
      "071[s], 07[s], Ep: 18, Ct: 01500|TR LOSS: inf, PPL: 1030|TM NLL: 299, KL: 3.66 | LM NLL: 5.67, KL: inf|DE LOSS: 303, PPL: 1001, TM: 298, LM: 5.14|BETA: 0.004706\n",
      "072[s], 07[s], Ep: 18, Ct: 02000|TR LOSS: inf, PPL: 1030|TM NLL: 299, KL: 3.66 | LM NLL: 5.66, KL: inf|DE LOSS: 303, PPL: 1002, TM: 298, LM: 5.13|BETA: 0.004741\n",
      "072[s], 07[s], Ep: 18, Ct: 02500|TR LOSS: inf, PPL: 1029|TM NLL: 299, KL: 3.66 | LM NLL: 5.66, KL: inf|DE LOSS: 304, PPL: 1002, TM: 298, LM: 5.14|BETA: 0.004775\n",
      "056[s], 07[s], Ep: 19, Ct: 00000|TR LOSS: inf, PPL: 1029|TM NLL: 299, KL: 3.66 | LM NLL: 5.65, KL: inf|DE LOSS: 303, PPL: 1002, TM: 298, LM: 5.13|BETA: 0.004802\n",
      "072[s], 07[s], Ep: 19, Ct: 00500|TR LOSS: inf, PPL: 1028|TM NLL: 299, KL: 3.67 | LM NLL: 5.65, KL: inf|DE LOSS: 303, PPL: 1000, TM: 298, LM: 5.12|BETA: 0.004837\n",
      "072[s], 07[s], Ep: 19, Ct: 01000|TR LOSS: inf, PPL: 1028|TM NLL: 299, KL: 3.67 | LM NLL: 5.64, KL: inf|DE LOSS: 303, PPL: 1003, TM: 298, LM: 5.12|BETA: 0.004871\n",
      "072[s], 07[s], Ep: 19, Ct: 01500|TR LOSS: inf, PPL: 1028|TM NLL: 299, KL: 3.67 | LM NLL: 5.64, KL: inf|DE LOSS: 303, PPL: 999, TM: 298, LM: 5.12|BETA: 0.004906\n",
      "073[s], 07[s], Ep: 19, Ct: 02000|TR LOSS: inf, PPL: 1027|TM NLL: 299, KL: 3.67 | LM NLL: 5.63, KL: inf|DE LOSS: 303, PPL: 1003, TM: 298, LM: 5.11|BETA: 0.004941\n",
      "076[s], 07[s], Ep: 19, Ct: 02500|TR LOSS: inf, PPL: 1027|TM NLL: 299, KL: 3.67 | LM NLL: 5.63, KL: inf|DE LOSS: 303, PPL: 1001, TM: 298, LM: 5.11|BETA: 0.004975\n",
      "060[s], 07[s], Ep: 20, Ct: 00000|TR LOSS: inf, PPL: 1026|TM NLL: 299, KL: 3.67 | LM NLL: 5.63, KL: inf|DE LOSS: 303, PPL: 1001, TM: 298, LM: 5.11|BETA: 0.005002\n",
      "076[s], 07[s], Ep: 20, Ct: 00500|TR LOSS: inf, PPL: 1026|TM NLL: 299, KL: 3.68 | LM NLL: 5.62, KL: inf|DE LOSS: 303, PPL: 999, TM: 298, LM: 5.11|BETA: 0.005037\n",
      "075[s], 07[s], Ep: 20, Ct: 01000|TR LOSS: inf, PPL: 1026|TM NLL: 299, KL: 3.68 | LM NLL: 5.62, KL: inf|DE LOSS: 303, PPL: 1003, TM: 298, LM: 5.10|BETA: 0.005071\n",
      "076[s], 07[s], Ep: 20, Ct: 01500|TR LOSS: inf, PPL: 1025|TM NLL: 299, KL: 3.68 | LM NLL: 5.61, KL: inf|DE LOSS: 303, PPL: 997, TM: 298, LM: 5.10|BETA: 0.005106\n",
      "075[s], 07[s], Ep: 20, Ct: 02000|TR LOSS: inf, PPL: 1025|TM NLL: 299, KL: 3.68 | LM NLL: 5.61, KL: inf|DE LOSS: 303, PPL: 1001, TM: 298, LM: 5.10|BETA: 0.005141\n",
      "072[s], 07[s], Ep: 20, Ct: 02500|TR LOSS: inf, PPL: 1025|TM NLL: 299, KL: 3.68 | LM NLL: 5.60, KL: inf|DE LOSS: 303, PPL: 1002, TM: 298, LM: 5.09|BETA: 0.005176\n",
      "057[s], 07[s], Ep: 21, Ct: 00000|TR LOSS: inf, PPL: 1024|TM NLL: 299, KL: 3.68 | LM NLL: 5.60, KL: inf|DE LOSS: 303, PPL: 1000, TM: 298, LM: 5.09|BETA: 0.005202\n",
      "072[s], 07[s], Ep: 21, Ct: 00500|TR LOSS: inf, PPL: 1024|TM NLL: 299, KL: 3.69 | LM NLL: 5.60, KL: inf|DE LOSS: 303, PPL: 995, TM: 298, LM: 5.09|BETA: 0.005237\n",
      "074[s], 07[s], Ep: 21, Ct: 01000|TR LOSS: inf, PPL: 1024|TM NLL: 299, KL: 3.69 | LM NLL: 5.59, KL: inf|DE LOSS: 303, PPL: 1000, TM: 298, LM: 5.09|BETA: 0.005272\n",
      "076[s], 07[s], Ep: 21, Ct: 01500|TR LOSS: inf, PPL: 1023|TM NLL: 299, KL: 3.69 | LM NLL: 5.59, KL: inf|DE LOSS: 303, PPL: 1000, TM: 298, LM: 5.08|BETA: 0.005306\n",
      "076[s], 07[s], Ep: 21, Ct: 02000|TR LOSS: inf, PPL: 1023|TM NLL: 299, KL: 3.69 | LM NLL: 5.58, KL: inf|DE LOSS: 303, PPL: 1001, TM: 298, LM: 5.07|BETA: 0.005341\n",
      "073[s], 07[s], Ep: 21, Ct: 02500|TR LOSS: inf, PPL: 1023|TM NLL: 299, KL: 3.69 | LM NLL: 5.58, KL: inf|DE LOSS: 303, PPL: 1002, TM: 298, LM: 5.07|BETA: 0.005376\n",
      "057[s], 07[s], Ep: 22, Ct: 00000|TR LOSS: inf, PPL: 1022|TM NLL: 299, KL: 3.69 | LM NLL: 5.58, KL: inf|DE LOSS: 303, PPL: 997, TM: 298, LM: 5.08|BETA: 0.005402\n",
      "073[s], 07[s], Ep: 22, Ct: 00500|TR LOSS: inf, PPL: 1022|TM NLL: 299, KL: 3.69 | LM NLL: 5.57, KL: inf|DE LOSS: 303, PPL: 999, TM: 298, LM: 5.07|BETA: 0.005437\n",
      "073[s], 07[s], Ep: 22, Ct: 01000|TR LOSS: inf, PPL: 1022|TM NLL: 299, KL: 3.70 | LM NLL: 5.57, KL: inf|DE LOSS: 303, PPL: 999, TM: 298, LM: 5.06|BETA: 0.005472\n",
      "073[s], 07[s], Ep: 22, Ct: 01500|TR LOSS: inf, PPL: 1021|TM NLL: 299, KL: 3.70 | LM NLL: 5.56, KL: inf|DE LOSS: 303, PPL: 1002, TM: 298, LM: 5.06|BETA: 0.005506\n",
      "072[s], 07[s], Ep: 22, Ct: 02000|TR LOSS: inf, PPL: 1021|TM NLL: 299, KL: 3.70 | LM NLL: 5.56, KL: inf|DE LOSS: 303, PPL: 997, TM: 298, LM: 5.06|BETA: 0.005541\n",
      "071[s], 07[s], Ep: 22, Ct: 02500|TR LOSS: inf, PPL: 1021|TM NLL: 299, KL: 3.70 | LM NLL: 5.56, KL: inf|DE LOSS: 303, PPL: 998, TM: 298, LM: 5.06|BETA: 0.005576\n",
      "057[s], 07[s], Ep: 23, Ct: 00000|TR LOSS: inf, PPL: 1021|TM NLL: 299, KL: 3.70 | LM NLL: 5.55, KL: inf|DE LOSS: 303, PPL: 1000, TM: 298, LM: 5.06|BETA: 0.005602\n",
      "072[s], 07[s], Ep: 23, Ct: 00500|TR LOSS: inf, PPL: 1020|TM NLL: 299, KL: 3.70 | LM NLL: 5.55, KL: inf|DE LOSS: 303, PPL: 998, TM: 298, LM: 5.05|BETA: 0.005637\n",
      "072[s], 07[s], Ep: 23, Ct: 01000|TR LOSS: inf, PPL: 1020|TM NLL: 299, KL: 3.70 | LM NLL: 5.55, KL: inf|DE LOSS: 303, PPL: 997, TM: 298, LM: 5.05|BETA: 0.005672\n",
      "072[s], 07[s], Ep: 23, Ct: 01500|TR LOSS: inf, PPL: 1020|TM NLL: 299, KL: 3.71 | LM NLL: 5.54, KL: inf|DE LOSS: 303, PPL: 997, TM: 298, LM: 5.05|BETA: 0.005706\n",
      "072[s], 07[s], Ep: 23, Ct: 02000|TR LOSS: inf, PPL: 1019|TM NLL: 299, KL: 3.71 | LM NLL: 5.54, KL: inf|DE LOSS: 303, PPL: 1001, TM: 298, LM: 5.04|BETA: 0.005741\n",
      "072[s], 07[s], Ep: 23, Ct: 02500|TR LOSS: inf, PPL: 1019|TM NLL: 299, KL: 3.71 | LM NLL: 5.53, KL: inf|DE LOSS: 303, PPL: 1001, TM: 298, LM: 5.05|BETA: 0.005776\n",
      "057[s], 07[s], Ep: 24, Ct: 00000|TR LOSS: inf, PPL: 1019|TM NLL: 299, KL: 3.71 | LM NLL: 5.53, KL: inf|DE LOSS: 303, PPL: 998, TM: 298, LM: 5.04|BETA: 0.005802\n",
      "072[s], 07[s], Ep: 24, Ct: 00500|TR LOSS: inf, PPL: 1019|TM NLL: 299, KL: 3.71 | LM NLL: 5.53, KL: inf|DE LOSS: 303, PPL: 994, TM: 298, LM: 5.04|BETA: 0.005837\n",
      "072[s], 07[s], Ep: 24, Ct: 01000|TR LOSS: inf, PPL: 1018|TM NLL: 299, KL: 3.71 | LM NLL: 5.52, KL: inf|DE LOSS: 303, PPL: 999, TM: 298, LM: 5.04|BETA: 0.005872\n",
      "071[s], 06[s], Ep: 24, Ct: 01500|TR LOSS: inf, PPL: 1018|TM NLL: 299, KL: 3.71 | LM NLL: 5.52, KL: inf|DE LOSS: 303, PPL: 997, TM: 298, LM: 5.04|BETA: 0.005907\n",
      "073[s], 07[s], Ep: 24, Ct: 02000|TR LOSS: inf, PPL: 1018|TM NLL: 299, KL: 3.71 | LM NLL: 5.52, KL: inf|DE LOSS: 303, PPL: 998, TM: 298, LM: 5.03|BETA: 0.005941\n",
      "071[s], 07[s], Ep: 24, Ct: 02500|TR LOSS: inf, PPL: 1017|TM NLL: 299, KL: 3.72 | LM NLL: 5.51, KL: inf|DE LOSS: 303, PPL: 1000, TM: 298, LM: 5.03|BETA: 0.005976\n",
      "056[s], 07[s], Ep: 25, Ct: 00000|TR LOSS: inf, PPL: 1017|TM NLL: 299, KL: 3.72 | LM NLL: 5.51, KL: inf|DE LOSS: 303, PPL: 997, TM: 298, LM: 5.03|BETA: 0.006003\n",
      "071[s], 06[s], Ep: 25, Ct: 00500|TR LOSS: inf, PPL: 1017|TM NLL: 299, KL: 3.72 | LM NLL: 5.51, KL: inf|DE LOSS: 303, PPL: 996, TM: 298, LM: 5.03|BETA: 0.006037\n",
      "072[s], 07[s], Ep: 25, Ct: 01000|TR LOSS: inf, PPL: 1017|TM NLL: 299, KL: 3.72 | LM NLL: 5.50, KL: inf|DE LOSS: 303, PPL: 997, TM: 298, LM: 5.02|BETA: 0.006072\n",
      "072[s], 07[s], Ep: 25, Ct: 01500|TR LOSS: inf, PPL: 1017|TM NLL: 299, KL: 3.72 | LM NLL: 5.50, KL: inf|DE LOSS: 303, PPL: 998, TM: 298, LM: 5.02|BETA: 0.006107\n",
      "072[s], 07[s], Ep: 25, Ct: 02000|TR LOSS: inf, PPL: 1016|TM NLL: 299, KL: 3.72 | LM NLL: 5.50, KL: inf|DE LOSS: 303, PPL: 995, TM: 298, LM: 5.02|BETA: 0.006141\n",
      "071[s], 07[s], Ep: 25, Ct: 02500|TR LOSS: inf, PPL: 1016|TM NLL: 299, KL: 3.72 | LM NLL: 5.49, KL: inf|DE LOSS: 303, PPL: 999, TM: 298, LM: 5.01|BETA: 0.006176\n",
      "057[s], 07[s], Ep: 26, Ct: 00000|TR LOSS: inf, PPL: 1016|TM NLL: 299, KL: 3.72 | LM NLL: 5.49, KL: inf|DE LOSS: 303, PPL: 995, TM: 298, LM: 5.02|BETA: 0.006203\n",
      "072[s], 07[s], Ep: 26, Ct: 00500|TR LOSS: inf, PPL: 1015|TM NLL: 299, KL: 3.73 | LM NLL: 5.49, KL: inf|DE LOSS: 303, PPL: 997, TM: 298, LM: 5.02|BETA: 0.006237\n",
      "071[s], 07[s], Ep: 26, Ct: 01000|TR LOSS: inf, PPL: 1015|TM NLL: 299, KL: 3.73 | LM NLL: 5.48, KL: inf|DE LOSS: 303, PPL: 995, TM: 298, LM: 5.01|BETA: 0.006272\n",
      "073[s], 07[s], Ep: 26, Ct: 01500|TR LOSS: inf, PPL: 1015|TM NLL: 299, KL: 3.73 | LM NLL: 5.48, KL: inf|DE LOSS: 303, PPL: 999, TM: 298, LM: 5.01|BETA: 0.006307\n",
      "072[s], 07[s], Ep: 26, Ct: 02000|TR LOSS: inf, PPL: 1015|TM NLL: 299, KL: 3.73 | LM NLL: 5.48, KL: inf|DE LOSS: 303, PPL: 996, TM: 298, LM: 5.01|BETA: 0.006341\n",
      "072[s], 07[s], Ep: 26, Ct: 02500|TR LOSS: inf, PPL: 1015|TM NLL: 299, KL: 3.73 | LM NLL: 5.47, KL: inf|DE LOSS: 303, PPL: 999, TM: 298, LM: 5.00|BETA: 0.006376\n",
      "057[s], 07[s], Ep: 27, Ct: 00000|TR LOSS: inf, PPL: 1014|TM NLL: 299, KL: 3.73 | LM NLL: 5.47, KL: inf|DE LOSS: 303, PPL: 994, TM: 298, LM: 5.00|BETA: 0.006403\n",
      "072[s], 07[s], Ep: 27, Ct: 00500|TR LOSS: inf, PPL: 1014|TM NLL: 299, KL: 3.73 | LM NLL: 5.47, KL: inf|DE LOSS: 303, PPL: 996, TM: 298, LM: 5.00|BETA: 0.006437\n",
      "072[s], 07[s], Ep: 27, Ct: 01000|TR LOSS: inf, PPL: 1014|TM NLL: 299, KL: 3.73 | LM NLL: 5.46, KL: inf|DE LOSS: 303, PPL: 996, TM: 298, LM: 5.00|BETA: 0.006472\n",
      "072[s], 07[s], Ep: 27, Ct: 01500|TR LOSS: inf, PPL: 1014|TM NLL: 299, KL: 3.73 | LM NLL: 5.46, KL: inf|DE LOSS: 303, PPL: 999, TM: 298, LM: 4.99|BETA: 0.006507\n",
      "072[s], 07[s], Ep: 27, Ct: 02000|TR LOSS: inf, PPL: 1014|TM NLL: 299, KL: 3.74 | LM NLL: 5.46, KL: inf|DE LOSS: 303, PPL: 993, TM: 298, LM: 4.99|BETA: 0.006542\n",
      "071[s], 07[s], Ep: 27, Ct: 02500|TR LOSS: inf, PPL: 1013|TM NLL: 299, KL: 3.74 | LM NLL: 5.45, KL: inf|DE LOSS: 303, PPL: 999, TM: 298, LM: 4.99|BETA: 0.006576\n",
      "055[s], 07[s], Ep: 28, Ct: 00000|TR LOSS: inf, PPL: 1013|TM NLL: 299, KL: 3.74 | LM NLL: 5.45, KL: inf|DE LOSS: 303, PPL: 998, TM: 298, LM: 4.99|BETA: 0.006603\n",
      "070[s], 07[s], Ep: 28, Ct: 00500|TR LOSS: inf, PPL: 1013|TM NLL: 299, KL: 3.74 | LM NLL: 5.45, KL: inf|DE LOSS: 303, PPL: 995, TM: 298, LM: 4.98|BETA: 0.006638\n",
      "071[s], 07[s], Ep: 28, Ct: 01000|TR LOSS: inf, PPL: 1013|TM NLL: 299, KL: 3.74 | LM NLL: 5.45, KL: inf|DE LOSS: 303, PPL: 995, TM: 298, LM: 4.99|BETA: 0.006672\n",
      "073[s], 07[s], Ep: 28, Ct: 01500|TR LOSS: inf, PPL: 1012|TM NLL: 299, KL: 3.74 | LM NLL: 5.44, KL: inf|DE LOSS: 303, PPL: 996, TM: 298, LM: 4.98|BETA: 0.006707\n",
      "073[s], 07[s], Ep: 28, Ct: 02000|TR LOSS: inf, PPL: 1012|TM NLL: 299, KL: 3.74 | LM NLL: 5.44, KL: inf|DE LOSS: 303, PPL: 997, TM: 298, LM: 4.98|BETA: 0.006742\n"
     ]
    }
   ],
   "source": [
    "for log in logs:\n",
    "    print('%03d[s], %02d[s], Ep: %02d, Ct: %05d|TR LOSS: %.0f, PPL: %.0f|TM NLL: %.0f, KL: %.2f | LM NLL: %.2f, KL: %.2f|DE LOSS: %.0f, PPL: %.0f, TM: %.0f, LM: %.2f|BETA: %.6f' %  log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : republican senate democratic house election party campaign governor republicans gov\n",
      "1 : court judge federal u.s. attorney lawsuit filed case district law\n",
      "2 : million company http water project reports federal department u.s. plant\n",
      "3 : vehicle car crash driver truck died driving killed hit struck\n",
      "4 : department http people reports health office law officers week security\n",
      "5 : percent million company cents share revenue billion rate shares average\n",
      "6 : national saturday event museum died day years u.s. center president\n",
      "7 : bill tax budget million health lawmakers house measure gov approved\n",
      "8 : school university students board education program president college schools district\n",
      "9 : fire firefighters area reported authorities home people sunday miles blaze\n",
      "10 : found home officers authorities shot woman sheriff shooting arrested body\n",
      "11 : charged guilty prosecutors charges court years prison pleaded attorney murder\n",
      "12 : service weather power national storm snow tuesday expected rain customers\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
