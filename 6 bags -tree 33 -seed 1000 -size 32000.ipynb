{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from data_structure import get_batches\n",
    "from hntm import HierarchicalNeuralTopicModel\n",
    "from tree import get_descendant_idxs\n",
    "from evaluation import validate, get_hierarchical_affinity, get_topic_specialization, print_topic_sample\n",
    "from configure import get_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(nb_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu\n",
    "np.random.seed(config.seed)\n",
    "random.seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train_tmp, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.path_data,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(instances_train_tmp) > config.size:\n",
    "    instances_train = np.random.choice(instances_train_tmp, config.size, replace=False)\n",
    "else:\n",
    "    instances_train = instances_train_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)\n",
    "config.dim_bow = len(bow_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0,
     10
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables, model):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, model, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    return _variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = []\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "ppl_min = np.inf\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','','','','','VALID:','','','','','TEST:','', 'SPEC:', '', '', 'HIER:', ''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL','REG','LOSS','PPL','NLL','KL','REG','LOSS','PPL', '1', '2', '3', 'CHILD', 'OTHER']]))))\n",
    "\n",
    "cmd_rm = 'rm -r %s' % config.dir_model\n",
    "res = subprocess.call(cmd_rm.split())\n",
    "cmd_mk = 'mkdir %s' % config.dir_model\n",
    "res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "def update_checkpoint(config, checkpoint, global_step):\n",
    "    checkpoint.append(config.path_model + '-%i' % global_step)\n",
    "    if len(checkpoint) > config.max_to_keep:\n",
    "        path_model = checkpoint.pop(0) + '.*'\n",
    "        for p in glob.glob(path_model):\n",
    "            os.remove(p)\n",
    "    cPickle.dump(checkpoint, open(config.path_checkpoint, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "model = HierarchicalNeuralTopicModel(config)\n",
    "sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(max_to_keep=config.max_to_keep)\n",
    "update_tree_flg = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train & validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th colspan=\"4\" halign=\"left\"></th>\n",
       "      <th>VALID:</th>\n",
       "      <th colspan=\"4\" halign=\"left\"></th>\n",
       "      <th>TEST:</th>\n",
       "      <th></th>\n",
       "      <th>SPEC:</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "      <th>HIER:</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>CHILD</th>\n",
       "      <th>OTHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>8.898845</td>\n",
       "      <td>0</td>\n",
       "      <td>498</td>\n",
       "      <td>113.08</td>\n",
       "      <td>577</td>\n",
       "      <td>112.69</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.01</td>\n",
       "      <td>105.65</td>\n",
       "      <td>540.212036</td>\n",
       "      <td>105.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.97</td>\n",
       "      <td>541.149658</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5489</th>\n",
       "      <td>4.772600</td>\n",
       "      <td>10</td>\n",
       "      <td>498</td>\n",
       "      <td>111.44</td>\n",
       "      <td>638</td>\n",
       "      <td>110.41</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.85</td>\n",
       "      <td>490.031158</td>\n",
       "      <td>102.50</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>102.34</td>\n",
       "      <td>491.967865</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10479</th>\n",
       "      <td>4.848753</td>\n",
       "      <td>20</td>\n",
       "      <td>498</td>\n",
       "      <td>110.97</td>\n",
       "      <td>715</td>\n",
       "      <td>109.61</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.56</td>\n",
       "      <td>482.333649</td>\n",
       "      <td>101.93</td>\n",
       "      <td>1.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>102.00</td>\n",
       "      <td>482.993958</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15469</th>\n",
       "      <td>5.457107</td>\n",
       "      <td>30</td>\n",
       "      <td>498</td>\n",
       "      <td>110.73</td>\n",
       "      <td>752</td>\n",
       "      <td>109.21</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.42</td>\n",
       "      <td>478.754700</td>\n",
       "      <td>101.69</td>\n",
       "      <td>1.72</td>\n",
       "      <td>0.00</td>\n",
       "      <td>101.83</td>\n",
       "      <td>478.241547</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20459</th>\n",
       "      <td>8.327302</td>\n",
       "      <td>40</td>\n",
       "      <td>498</td>\n",
       "      <td>110.59</td>\n",
       "      <td>763</td>\n",
       "      <td>108.97</td>\n",
       "      <td>1.62</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.37</td>\n",
       "      <td>478.389587</td>\n",
       "      <td>101.56</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>101.72</td>\n",
       "      <td>475.683380</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25449</th>\n",
       "      <td>10.272416</td>\n",
       "      <td>50</td>\n",
       "      <td>498</td>\n",
       "      <td>110.48</td>\n",
       "      <td>766</td>\n",
       "      <td>108.77</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.39</td>\n",
       "      <td>478.722961</td>\n",
       "      <td>101.52</td>\n",
       "      <td>1.86</td>\n",
       "      <td>0.00</td>\n",
       "      <td>101.72</td>\n",
       "      <td>475.683380</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30439</th>\n",
       "      <td>6.430791</td>\n",
       "      <td>60</td>\n",
       "      <td>498</td>\n",
       "      <td>110.41</td>\n",
       "      <td>762</td>\n",
       "      <td>108.65</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.33</td>\n",
       "      <td>476.254883</td>\n",
       "      <td>101.47</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>101.69</td>\n",
       "      <td>474.521210</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35429</th>\n",
       "      <td>7.495747</td>\n",
       "      <td>70</td>\n",
       "      <td>498</td>\n",
       "      <td>110.34</td>\n",
       "      <td>754</td>\n",
       "      <td>108.54</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.25</td>\n",
       "      <td>475.038727</td>\n",
       "      <td>101.34</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>101.57</td>\n",
       "      <td>472.259857</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40419</th>\n",
       "      <td>7.042968</td>\n",
       "      <td>80</td>\n",
       "      <td>498</td>\n",
       "      <td>110.28</td>\n",
       "      <td>747</td>\n",
       "      <td>108.44</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.30</td>\n",
       "      <td>476.182648</td>\n",
       "      <td>101.36</td>\n",
       "      <td>1.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>101.57</td>\n",
       "      <td>472.259857</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45409</th>\n",
       "      <td>6.841849</td>\n",
       "      <td>90</td>\n",
       "      <td>498</td>\n",
       "      <td>110.24</td>\n",
       "      <td>740</td>\n",
       "      <td>108.36</td>\n",
       "      <td>1.87</td>\n",
       "      <td>0.01</td>\n",
       "      <td>103.24</td>\n",
       "      <td>475.345093</td>\n",
       "      <td>101.28</td>\n",
       "      <td>1.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>101.57</td>\n",
       "      <td>472.259857</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50399</th>\n",
       "      <td>5.352785</td>\n",
       "      <td>100</td>\n",
       "      <td>498</td>\n",
       "      <td>110.20</td>\n",
       "      <td>734</td>\n",
       "      <td>108.29</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.22</td>\n",
       "      <td>474.457397</td>\n",
       "      <td>101.24</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0.00</td>\n",
       "      <td>101.55</td>\n",
       "      <td>471.871338</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55389</th>\n",
       "      <td>6.259919</td>\n",
       "      <td>110</td>\n",
       "      <td>498</td>\n",
       "      <td>110.16</td>\n",
       "      <td>728</td>\n",
       "      <td>108.23</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>103.21</td>\n",
       "      <td>473.993622</td>\n",
       "      <td>101.21</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>101.52</td>\n",
       "      <td>470.839905</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            TRAIN:                           VALID:  \\\n",
       "            Time   Ep   Ct    LOSS  PPL     NLL    KL   REG    LOSS   \n",
       "499     8.898845    0  498  113.08  577  112.69  0.38  0.01  105.65   \n",
       "5489    4.772600   10  498  111.44  638  110.41  1.02  0.01  103.85   \n",
       "10479   4.848753   20  498  110.97  715  109.61  1.35  0.00  103.56   \n",
       "15469   5.457107   30  498  110.73  752  109.21  1.51  0.00  103.42   \n",
       "20459   8.327302   40  498  110.59  763  108.97  1.62  0.01  103.37   \n",
       "25449  10.272416   50  498  110.48  766  108.77  1.70  0.01  103.39   \n",
       "30439   6.430791   60  498  110.41  762  108.65  1.75  0.01  103.33   \n",
       "35429   7.495747   70  498  110.34  754  108.54  1.80  0.01  103.25   \n",
       "40419   7.042968   80  498  110.28  747  108.44  1.84  0.01  103.30   \n",
       "45409   6.841849   90  498  110.24  740  108.36  1.87  0.01  103.24   \n",
       "50399   5.352785  100  498  110.20  734  108.29  1.90  0.00  103.22   \n",
       "55389   6.259919  110  498  110.16  728  108.23  1.93  0.00  103.21   \n",
       "\n",
       "                                        TEST:             SPEC:              \\\n",
       "              PPL     NLL    KL   REG    LOSS         PPL     1     2     3   \n",
       "499    540.212036  105.32  0.32  0.00  103.97  541.149658  0.14  0.21  0.24   \n",
       "5489   490.031158  102.50  1.35  0.00  102.34  491.967865  0.25  0.57  0.51   \n",
       "10479  482.333649  101.93  1.62  0.00  102.00  482.993958  0.27  0.59  0.55   \n",
       "15469  478.754700  101.69  1.72  0.00  101.83  478.241547  0.28  0.60  0.55   \n",
       "20459  478.389587  101.56  1.81  0.00  101.72  475.683380  0.30  0.63  0.53   \n",
       "25449  478.722961  101.52  1.86  0.00  101.72  475.683380  0.30  0.64  0.54   \n",
       "30439  476.254883  101.47  1.85  0.00  101.69  474.521210  0.30  0.61  0.54   \n",
       "35429  475.038727  101.34  1.91  0.00  101.57  472.259857  0.31  0.63  0.53   \n",
       "40419  476.182648  101.36  1.94  0.00  101.57  472.259857  0.30  0.63  0.53   \n",
       "45409  475.345093  101.28  1.96  0.00  101.57  472.259857  0.31  0.64  0.54   \n",
       "50399  474.457397  101.24  1.97  0.00  101.55  471.871338  0.30  0.64  0.55   \n",
       "55389  473.993622  101.21  2.00  0.00  101.52  470.839905  0.30  0.63  0.54   \n",
       "\n",
       "      HIER:        \n",
       "      CHILD OTHER  \n",
       "499    0.86  0.73  \n",
       "5489   0.40  0.19  \n",
       "10479  0.32  0.13  \n",
       "15469  0.33  0.15  \n",
       "20459  0.27  0.14  \n",
       "25449  0.27  0.16  \n",
       "30439  0.29  0.10  \n",
       "35429  0.29  0.16  \n",
       "40419  0.27  0.14  \n",
       "45409  0.28  0.15  \n",
       "50399  0.27  0.12  \n",
       "55389  0.27  0.14  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 R: 1.000 P: 0.422 carry pockets pocket room nice small perfect lot hold back\n",
      "   1 R: 0.264 P: 0.129 & ; sleeve protection inside inch air snug ipad nice\n",
      "     11 R: 0.072 P: 0.072 ! love color perfect perfectly recommend cute highly absolutely price\n",
      "     13 R: 0.046 P: 0.046 smell price $ amazon quality reviews size item ... buy\n",
      "     12 R: 0.017 P: 0.017 price quality made bought 'm - pro material air nice\n",
      "   3 R: 0.087 P: 0.041 zipper months broke strap handle zippers top open - plastic\n",
      "     33 R: 0.046 P: 0.046 return bought ... back quality ! time money buy item\n",
      "   5 R: 0.100 P: 0.056 protection hard cover scratches protect air mac pro nice easy\n",
      "     51 R: 0.030 P: 0.030 color love cover mac blue ! easy purple keyboard perfectly\n",
      "     52 R: 0.014 P: 0.014 pro price air retina perfectly apple $ buy bought cheap\n",
      "   2 R: 0.126 P: 0.071 bottom top cover plastic back part piece corners feet rubber\n",
      "     21 R: 0.055 P: 0.055 cover keyboard color ! protector screen picture love hard 'm\n",
      "{0: [1, 5, 2, 4], 1: [11, 14], 5: [53], 2: [21, 22], 4: [41]}\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "while epoch < config.n_epochs:\n",
    "    # train\n",
    "    time_start = time.time()\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = model.get_feed_dict(batch)\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, ppls_batch, global_step_log = \\\n",
    "        sess.run([model.opt, model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_reg, model.topic_ppls, tf.train.get_global_step()], feed_dict = feed_dict)\n",
    "\n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        time_log = float(time.time() - time_start)\n",
    "        \n",
    "        # validate\n",
    "        loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train = np.mean(losses_train, 0)\n",
    "        ppl_train = np.exp(np.mean(ppls_train))\n",
    "        loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "\n",
    "        # test\n",
    "        if ppl_dev < ppl_min:\n",
    "            ppl_min = ppl_dev\n",
    "            loss_test, _, _, _, ppl_test, _ = validate(sess, test_batches, model)\n",
    "            saver.save(sess, config.path_model, global_step=global_step_log)\n",
    "            cPickle.dump(config, open(config.path_config % global_step_log, 'wb'))\n",
    "            update_checkpoint(config, checkpoint, global_step_log)\n",
    "\n",
    "        # visualize topic\n",
    "        topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "        topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "        topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "        topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "        descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "        recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "\n",
    "        depth_specs = get_topic_specialization(sess, model, instances_test)\n",
    "        hierarchical_affinities = get_hierarchical_affinity(sess, model)\n",
    "\n",
    "        # log\n",
    "        clear_output()\n",
    "        log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_reg_train, \\\n",
    "                '%.2f'%loss_dev, ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_reg_dev, \\\n",
    "                '%.2f'%loss_test, ppl_test, \\\n",
    "                '%.2f'%depth_specs[1], '%.2f'%depth_specs[2], '%.2f'%depth_specs[3], \\\n",
    "                '%.2f'%hierarchical_affinities[0], '%.2f'%hierarchical_affinities[1]],\n",
    "                index=log_df.columns)\n",
    "        log_df.loc[global_step_log] = log_series\n",
    "        display(log_df)\n",
    "        cPickle.dump(log_df, open(os.path.join(config.path_log), 'wb'))\n",
    "        print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)\n",
    "\n",
    "        # update tree\n",
    "        if not config.static:\n",
    "            config.tree_idxs, update_tree_flg = model.update_tree(topic_prob_topic, recur_prob_topic)\n",
    "            if update_tree_flg:\n",
    "                print(config.tree_idxs)\n",
    "                name_variables = {tensor.name: variable for tensor, variable in zip(tf.global_variables(), sess.run(tf.global_variables()))} # store paremeters\n",
    "                if 'sess' in globals(): sess.close()\n",
    "                model = HierarchicalNeuralTopicModel(config)\n",
    "                sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "                name_tensors = {tensor.name: tensor for tensor in tf.global_variables()}\n",
    "                sess.run([name_tensors[name].assign(variable) for name, variable in name_variables.items()]) # restore parameters\n",
    "                saver = tf.train.Saver(max_to_keep=1)\n",
    "\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "    epoch += 1\n",
    "\n",
    "loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "display(log_df)\n",
    "print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
