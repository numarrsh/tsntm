{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "import _pickle as cPickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from scipy.special import gammaln\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/bags/instances.pkl', 'path of data')\n",
    "\n",
    "flags.DEFINE_integer('n_depth', 3, 'depth of tree')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "config = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <_io.FileIO name='data/bags/instances.pkl' mode='rb' closefd=True>\n",
      "ResourceWarning: unclosed file <_io.BufferedReader name='data/bags/instances.pkl'>\n"
     ]
    }
   ],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))\n",
    "docs_bow = [instance.bow for instance in instances_train]\n",
    "docs_raw = [[[bow_index]*int(doc_bow[bow_index]) for bow_index in np.where(doc_bow > 0)[0]] for doc_bow in docs_bow]\n",
    "docs_words = [[idx for idxs in doc for idx in idxs] for doc in docs_raw]\n",
    "words = [word for doc_words in docs_words for word in doc_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs_bow = [instance.bow for instance in instances_test]\n",
    "test_docs_raw = [[[bow_index]*int(test_doc_bow[bow_index]) for bow_index in np.where(test_doc_bow > 0)[0]] for test_doc_bow in test_docs_bow]\n",
    "test_docs_words = [[idx for idxs in doc for idx in idxs] for doc in test_docs_raw][:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31943, 1035, 568401)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_doc = len(docs_words)\n",
    "n_vocab = len(np.unique(words))\n",
    "n_words = len(words)\n",
    "assert n_vocab == len(bow_idxs)\n",
    "n_doc, n_vocab, n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assign docs to tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Topic:\n",
    "    def __init__(self, idx, sibling_idx, parent, depth, n_doc, n_vocab):\n",
    "        self.idx = idx\n",
    "        self.sibling_idx = sibling_idx\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.depth = depth\n",
    "        self.cnt_doc = 0\n",
    "        self.n_doc = n_doc\n",
    "        self.n_vocab = n_vocab\n",
    "        self.cnt_words = np.zeros(n_vocab) # Number of Words over Documents\n",
    "        self.set_prob_words()\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def sample_child(self, doc, train=True):\n",
    "        s_child_prior = self.get_s_child_prior(gam)\n",
    "        s_child_likelihood = self.get_s_child_likelihood(doc, eta)\n",
    "        p_child = np.array(s_child_prior * s_child_likelihood) / np.sum(s_child_prior * s_child_likelihood)\n",
    "        \n",
    "        child_index = np.random.multinomial(1, p_child).argmax()\n",
    "        if verbose: print('Depth: ', self.depth, 'p_child: ', p_child, 'selected:', child_index)\n",
    "        \n",
    "        if child_index < len(self.children):\n",
    "            child = self.children[child_index]\n",
    "        else:\n",
    "            child = self.get_new_child()\n",
    "            if train: self.children += [child]\n",
    "        return child\n",
    "    \n",
    "    def init_sample_child(self, train=True):\n",
    "        s_child_prior = self.get_s_child_prior(gam)\n",
    "        p_child = np.array(s_child_prior) / np.sum(s_child_prior)\n",
    "        \n",
    "        child_index = np.random.multinomial(1, p_child).argmax()\n",
    "        if verbose: print('Depth: ', self.depth, 'p_child: ', p_child, 'selected:', child_index)\n",
    "\n",
    "        if child_index < len(self.children):\n",
    "            child = self.children[child_index]\n",
    "        else:\n",
    "            child = self.get_new_child()\n",
    "            if train: self.children += [child]\n",
    "        return child\n",
    "    \n",
    "    def get_probs_child(self, doc):\n",
    "        s_child_prior = self.get_s_child_prior(gam)\n",
    "        s_child_likelihood = self.get_s_child_likelihood(doc, eta)\n",
    "        p_child = np.array(s_child_prior * s_child_likelihood) / np.sum(s_child_prior * s_child_likelihood)\n",
    "        return p_child\n",
    "    \n",
    "    def get_s_child_prior(self, gam):\n",
    "        s_child_prior = [child.cnt_doc for child in self.children]\n",
    "        s_child_prior += [gam]\n",
    "        return s_child_prior\n",
    "    \n",
    "    def get_s_child_likelihood(self, doc, eta):\n",
    "        if len(self.children) > 0:\n",
    "            children_cnt_words = np.concatenate([np.array([child.cnt_words for child in self.children]), np.zeros([1, self.n_vocab])], 0) # (Children+1) x Vocabulary\n",
    "        else:\n",
    "            children_cnt_words = np.zeros([1, self.n_vocab]) # (Children+1) x Vocabulary\n",
    "        \n",
    "        cnt_words_doc = doc.cnt_words[None, :] # 1 x Vocabulary\n",
    "\n",
    "        logits_likelihood = gammaln(np.sum(children_cnt_words, -1) + n_vocab*eta) \\\n",
    "                            - np.sum(gammaln(children_cnt_words + eta), -1) \\\n",
    "                            - gammaln(np.sum(children_cnt_words + cnt_words_doc, -1) + n_vocab*eta) \\\n",
    "                            + np.sum(gammaln(children_cnt_words + cnt_words_doc + eta), -1)\n",
    "        s_child_likelihood = np.exp(logits_likelihood)\n",
    "        return s_child_likelihood\n",
    "    \n",
    "    def get_new_child(self):\n",
    "        sibling_idx = max([child.sibling_idx for child in self.children]) + 1 if len(self.children) > 0 else 1\n",
    "        idx = self.idx + '-' + str(sibling_idx)\n",
    "        depth = self.depth+1\n",
    "        child = Topic(idx=idx, sibling_idx=sibling_idx, parent=self, depth=depth, n_doc=self.n_doc, n_vocab=self.n_vocab)        \n",
    "        return child\n",
    "        \n",
    "    def get_children(self):\n",
    "        child = self.get_new_child()\n",
    "        children = self.children + [child]\n",
    "        return children\n",
    "    \n",
    "    def delete_topic(self):\n",
    "        self.parent.children.remove(self)\n",
    "        \n",
    "    def set_prob_words(self):\n",
    "        cnt_words = self.cnt_words + eta\n",
    "        self.prob_words = cnt_words / np.sum(cnt_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc:\n",
    "    def __init__(self, idx, words, bow, n_depth):\n",
    "        self.idx = idx\n",
    "        self.words = words\n",
    "        self.cnt_words = bow\n",
    "        assert len(words) == np.sum(bow)\n",
    "        \n",
    "        self.topics = [] # Depth\n",
    "        self.word_depths = [] # Word Indices\n",
    "        self.depth_cnt_words = np.zeros([n_depth, n_vocab])\n",
    "                \n",
    "    def get_probs_depth(self, word_idx):\n",
    "        s_docs = np.sum(self.depth_cnt_words, -1) + alpha # Depth\n",
    "        s_words = np.array([topic.cnt_words[word_idx] for topic in self.topics]) + eta # Depth\n",
    "        z_words = np.array([np.sum(topic.cnt_words) for topic in self.topics]) + n_vocab*eta # Depth\n",
    "        assert s_docs.shape == s_words.shape == z_words.shape\n",
    "\n",
    "        s_depths = s_docs*s_words/z_words\n",
    "        p_depths = s_depths/np.sum(s_depths) # Depth\n",
    "        return p_depths\n",
    "    \n",
    "    def sample_depth(self, word_idx):\n",
    "        prob_depths = self.get_probs_depth(word_idx)\n",
    "        word_depth = np.argmax(np.random.multinomial(1, prob_depths))\n",
    "        return word_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample doc path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p({\\bf c}_{m}\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf w}, {\\bf c}_{-m}, {\\bf z})\\propto p({\\bf w}_{m}\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf c}, {\\bf w}_{-m}, {\\bf z})\\cdot p({\\bf c}_{m}\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf c}_{-m})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p({\\bf w}_{m}\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf c}, {\\bf w}_{-m}, {\\bf z})=\\prod_{\\ell=1}^{L}\\left(\\frac{\\Gamma(n_{c_{m,\\ell},-m}^{(\\cdot)}+W\\eta)}{\\prod_{w}\\Gamma(n_{c_{m,\\ell},-m}^{(w)}+\\eta)}\\frac{\\prod_{w}\\Gamma(n_{c_{m,\\ell},-m}^{(w)}+n_{c_{m,\\ell},m}^{(w)}+\\eta)}{\\Gamma(n_{c_{m,\\ell},-m}^{(\\cdot)}+n_{c_{m,\\ell},m}^{(\\cdot)}+W\\eta)}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_doc_topics(docs, topic_root, train=True):\n",
    "    for doc in docs:\n",
    "        topic = topic_root\n",
    "        doc.topics = [topic]\n",
    "        if train: topic.cnt_doc += 1 # increment count of docs\n",
    "\n",
    "        for depth in range(1, n_depth):\n",
    "            topic = topic.init_sample_child(train=train)\n",
    "            doc.topics += [topic]\n",
    "            if train: topic.cnt_doc += 1 # increment count of docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_doc_topics(docs, topic_root, train=True):\n",
    "    for doc in docs:\n",
    "        if train:\n",
    "            for depth in range(1, n_depth):\n",
    "                topic = doc.topics[depth]\n",
    "                topic.cnt_doc -= 1 # decrement count of docs\n",
    "                assert topic.cnt_doc >= 0\n",
    "                topic.cnt_words -= doc.depth_cnt_words[depth] # decrement count of words\n",
    "                assert np.min(topic.cnt_words) >= 0\n",
    "\n",
    "                if topic.cnt_doc == 0: \n",
    "                    topic.delete_topic()\n",
    "                    assert np.sum(topic.cnt_words) == 0\n",
    "\n",
    "        topic = topic_root\n",
    "        doc.topics = [topic]\n",
    "        for depth in range(1, n_depth):\n",
    "            topic = topic.sample_child(doc, train=train)\n",
    "            doc.topics += [topic]\n",
    "            if train: topic.cnt_doc += 1 # increment count of docs\n",
    "            if train: topic.cnt_words += doc.depth_cnt_words[depth] # increment count of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assign words to topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "p(z_{i}=j\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf z}_{-i},{\\bf w})\\propto (n_{-i,j}^{(d_{i})}+\\alpha)\\frac{n_{-i,j}^{(w_{i})}+\\eta}{n_{-i,j}^{(\\cdot)}+W\\eta}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_word_topics(docs, train=True):\n",
    "    for doc in docs:\n",
    "        if doc.idx % 10000 == 0: print(doc.idx, end=' ')\n",
    "        for word_index, word_idx in enumerate(doc.words):\n",
    "            # sample depth of word\n",
    "            new_depth = doc.sample_depth(word_idx)\n",
    "            new_topic = doc.topics[new_depth]\n",
    "            \n",
    "            # increment count of words\n",
    "            doc.depth_cnt_words[new_depth, word_idx] += 1\n",
    "            if train: new_topic.cnt_words[word_idx] += 1\n",
    "            doc.word_depths.append(new_depth) # for reference when sampling\n",
    "            \n",
    "        assert len(doc.word_depths) == len(doc.words) == np.sum(doc.depth_cnt_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_word_topics(docs, train=True):\n",
    "    for doc in docs:\n",
    "        if doc.idx % 10000 == 0: print(doc.idx, end=' ')\n",
    "        for word_index, word_idx in enumerate(doc.words):\n",
    "            # refer depth of word\n",
    "            old_depth = doc.word_depths[word_index]\n",
    "            old_topic = doc.topics[old_depth]\n",
    "            \n",
    "            # decrement count of words\n",
    "            doc.depth_cnt_words[old_depth, word_idx] -= 1\n",
    "            if train: old_topic.cnt_words[word_idx] -= 1            \n",
    "            \n",
    "            # sample depth of word\n",
    "            new_depth = doc.sample_depth(word_idx)\n",
    "            new_topic = doc.topics[new_depth]\n",
    "            \n",
    "            # increment count of words\n",
    "            doc.depth_cnt_words[new_depth, word_idx] += 1\n",
    "            if train: new_topic.cnt_words[word_idx] += 1\n",
    "            doc.word_depths[word_index] = new_depth # for sample\n",
    "            \n",
    "        assert len(doc.word_depths) == len(doc.words) == np.sum(doc.depth_cnt_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recur_cnt_words(topic):\n",
    "    cnt_words = np.sum(topic.cnt_words)\n",
    "    for child in topic.children:\n",
    "        cnt_words += recur_cnt_words(child)\n",
    "    return cnt_words\n",
    "    \n",
    "def assert_sum_cnt_words(topic_root):\n",
    "    sum_cnt_words = recur_cnt_words(topic_root)\n",
    "    assert sum_cnt_words == sum([len(doc.words) for doc in docs])\n",
    "    \n",
    "def nearly_equal(val, thre):\n",
    "    return (val > thre-1e-5) and (val < thre+1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.array([10., 5., 1.])\n",
    "gam = 0.01\n",
    "eta = 1.\n",
    "n_depth = 3\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_sample = 100\n",
    "docs = [Doc(idx=doc_idx, words=doc_words, bow=doc_bow, n_depth=config.n_depth) for doc_idx, (doc_words, doc_bow) in enumerate(zip(docs_words, docs_bow))]\n",
    "topic_root = Topic(idx='0', sibling_idx=0, parent=None, depth=0, n_doc=n_doc, n_vocab=n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_train(docs, topic_root):\n",
    "    init_doc_topics(docs=docs, topic_root=topic_root)\n",
    "    init_word_topics(docs=docs)\n",
    "    assert_sum_cnt_words(topic_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_train(docs, topic_root):\n",
    "    sample_doc_topics(docs=docs, topic_root=topic_root)\n",
    "    sample_word_topics(docs=docs)\n",
    "    assert_sum_cnt_words(topic_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_test(test_docs, topic_root):\n",
    "    init_doc_topics(docs=test_docs, topic_root=topic_root, train=False)\n",
    "    init_word_topics(docs=test_docs, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_test(test_docs, topic_root):\n",
    "    sample_doc_topics(docs=test_docs, topic_root=topic_root, train=False)\n",
    "    sample_word_topics(docs=test_docs, train=False)\n",
    "    assert_sum_cnt_words(topic_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_child_idxs(topic):\n",
    "    topic_freq_words = [idx_to_word[bow_idxs[bow_index]] for bow_index in np.argsort(topic.cnt_words)[::-1][:10]]\n",
    "    print('  '*topic.depth, topic.idx, ':', [child.idx for child in topic.children], topic.cnt_doc, np.sum(topic.cnt_words), topic_freq_words)\n",
    "    for topic in topic.children:\n",
    "        print_child_idxs(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perplexity(docs, topic_root):\n",
    "    def set_prob_words(topic):\n",
    "        topic.set_prob_words()\n",
    "        for topic_child in topic.children:\n",
    "            set_prob_words(topic_child)\n",
    "            \n",
    "    # set Probabilty of Words\n",
    "    set_prob_words(topic_root)\n",
    "\n",
    "    logit_docs = []\n",
    "    for doc in docs:\n",
    "        # Path Probability for each document\n",
    "        topic = topic_root\n",
    "        probs_paths= [{topic: 1.}]\n",
    "        for depth in range(1, n_depth):\n",
    "            probs_path = {}\n",
    "            for topic, prob_path in probs_paths[-1].items():\n",
    "                topics_child = topic.get_children()\n",
    "                probs_child = topic.get_probs_child(doc)\n",
    "                probs_path_child = prob_path * probs_child\n",
    "                for topic_child, prob_path_child in zip(topics_child, probs_path_child):\n",
    "                    probs_path[topic_child] = prob_path_child\n",
    "            probs_paths.append(probs_path)    \n",
    "\n",
    "        all_topics = []\n",
    "        for probs_path in probs_paths:\n",
    "            all_topics += list(probs_path.keys())\n",
    "\n",
    "        # topic probability for each depth\n",
    "        probs_depths_topics = np.array([[probs_path[topic] if topic in probs_path else 0. for topic in all_topics] for probs_path in probs_paths])\n",
    "        # depth probability for each word\n",
    "        probs_words_depths = np.array([doc.get_probs_depth(word_idx) for word_idx in doc.words])\n",
    "        # topic probability for each depth\n",
    "        probs_words_topics = probs_words_depths.dot(probs_depths_topics) # n_doc x n_topic\n",
    "\n",
    "        probs_topics_words = np.array([topic.prob_words for topic in all_topics]) # n_topic x n_vocab\n",
    "        probs_words_bow = probs_words_topics.dot(probs_topics_words) # n_doc x n_vocab\n",
    "        logit_words = np.log(probs_words_bow[np.arange(len(doc.words)), doc.words])\n",
    "        logit_doc = np.mean(logit_words)\n",
    "        logit_docs.append(logit_doc)\n",
    "\n",
    "    perplexity = np.exp(-np.mean(logit_docs))\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 100\n",
    "alpha = np.array([10., 5., 1.])\n",
    "gam = 0.01\n",
    "eta = 1.\n",
    "n_depth = 3\n",
    "verbose = False\n",
    "topic_root = Topic(idx='0', sibling_idx=0, parent=None, depth=0, n_doc=n_doc, n_vocab=n_vocab)\n",
    "docs = [Doc(idx=doc_idx, words=doc_words, bow=doc_bow, n_depth=config.n_depth) for doc_idx, (doc_words, doc_bow) in enumerate(zip(docs_words, docs_bow))]\n",
    "test_docs = [Doc(idx=doc_idx, words=doc_words, bow=doc_bow, n_depth=config.n_depth) for doc_idx, (doc_words, doc_bow) in enumerate(zip(test_docs_words, test_docs_bow))]\n",
    "\n",
    "# n_sample=3\n",
    "# docs = docs[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10000 20000 30000 0 Perplexity: 390.60\n",
      "0 10000 20000 30000 0 Perplexity: 333.98\n",
      "0 10000 20000 30000 0 Perplexity: 310.07\n",
      "0 10000 20000 30000 0 Perplexity: 305.00\n",
      "0 10000 20000 30000 0 Perplexity: 301.75\n",
      "0 10000 20000 30000 0 Perplexity: 298.30\n",
      "0 10000 20000 30000 0 Perplexity: 289.41\n",
      "0 10000 20000 30000 0 Perplexity: 289.85\n",
      "0 10000 20000 30000 0 Perplexity: 286.04\n",
      "0 10000 20000 30000 0 Perplexity: 279.60\n",
      "0 10000 20000 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-459ce7a1ac07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0massert_sum_cnt_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0msample_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0msample_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0massert_sum_cnt_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-fb78074103c0>\u001b[0m in \u001b[0;36msample_train\u001b[0;34m(docs, topic_root)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msample_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msample_doc_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopic_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msample_word_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0massert_sum_cnt_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-1cfb337ef324>\u001b[0m in \u001b[0;36msample_word_topics\u001b[0;34m(docs, train)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m# sample depth of word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mnew_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_depth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mnew_topic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_depth\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-a6b225296396>\u001b[0m in \u001b[0;36msample_depth\u001b[0;34m(self, word_idx)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample_depth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mprob_depths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_probs_depth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mword_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob_depths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mword_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-a6b225296396>\u001b[0m in \u001b[0;36mget_probs_depth\u001b[0;34m(self, word_idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_probs_depth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0ms_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth_cnt_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;31m# Depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0ms_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnt_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meta\u001b[0m \u001b[0;31m# Depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mz_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnt_words\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_vocab\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meta\u001b[0m \u001b[0;31m# Depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[1;32m   2074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 2076\u001b[0;31m                           initial=initial)\n\u001b[0m\u001b[1;32m   2077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(n_sample):\n",
    "    if i == 0:\n",
    "        init_train(docs, topic_root)\n",
    "        init_test(test_docs, topic_root)        \n",
    "        assert_sum_cnt_words(topic_root)\n",
    "    else:\n",
    "        sample_train(docs, topic_root)\n",
    "        sample_test(test_docs, topic_root)                \n",
    "        assert_sum_cnt_words(topic_root)\n",
    "        \n",
    "    perplexity = get_perplexity(test_docs, topic_root)\n",
    "    print('Perplexity: %.2f' % perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 : ['0-1', '0-2', '0-3', '0-4', '0-5', '0-6'] 5000 54010.0 ['!', 'carry', 'sleeve', 'bought', 'nice', 'room', 'pockets', 'quality', 'love', 'price']\n",
      "   0-1 : ['0-1-1', '0-1-2', '0-1-3'] 1663 10348.0 ['!', 'pro', 'pocket', 'perfectly', 'netbook', ';', '&', 'perfect', 'protect', 'power']\n",
      "     0-1-1 : [] 480 548.0 ['smell', 'thick', 'snug', 'fast', 'fine', 'cover', 'minor', 'sleeve', 'barely', 'zipper']\n",
      "     0-1-2 : [] 962 1006.0 ['charger', 'mouse', 'cord', 'chromebook', 'perfectly', 'zipper', 'zippered', 'inside', 'snug', 'vaio']\n",
      "     0-1-3 : [] 221 182.0 ['dvd', 'simply', 'stands', 'give', 'investment', 'cloth', 'perfectly', 'opens', 'literally', 'pain']\n",
      "   0-2 : ['0-2-1', '0-2-2', '0-2-3', '0-2-4'] 1970 12040.0 ['!', 'sturdy', 'handle', 'hold', 'pack', 'school', 'years', 'long', 'year', 'lot']\n",
      "     0-2-1 : [] 785 937.0 ['books', 'back', 'rolling', 'shoulders', 'handle', 'carrying', 'addition', 'pictures', 'difficult', 'places']\n",
      "     0-2-2 : [] 687 746.0 ['backpacks', 'lots', 'gift', 'christmas', 'ample', 'school', 'issues', 'construction', 'clothes', 'abuse']\n",
      "     0-2-3 : [] 100 108.0 ['nicer', 'sort', 'water', 'size', 'carry', 'reviewer', 'handle', 'sufficient', 'closure', 'stated']\n",
      "     0-2-4 : [] 398 413.0 ['quick', 'huge', 'seat', 'pictures', 'daily', 'essentials', 'supply', 'son', 'length', 'electronics']\n",
      "   0-3 : ['0-3-1', '0-3-2'] 546 3527.0 ['-', 'bottom', 'zipper', 'side', 'sturdy', 'top', 'find', 'part', 'pro', 'handle']\n",
      "     0-3-1 : [] 321 402.0 ['shoulders', 'weekend', 'zipper', 'tear', 'lid', 'interior', 'velcro', 'extremely', 'broke', 'metal']\n",
      "     0-3-2 : [] 225 336.0 ['shows', 'pro', 'electronics', 'mcover', 'wear', 'closure', 'smoothly', 'sufficient', 'bottom', 'roll']\n",
      "   0-4 : ['0-4-1'] 307 2697.0 ['cover', 'usb', 'pro', 'card', 'hard', 'shell', 'ports', 'works', 'apple', 'bottom']\n",
      "     0-4-1 : [] 307 461.0 ['apple', 'cover', 'angle', 'difficult', 'card', 'cd', 'pc', 'drive', 'color', 'adapter']\n",
      "   0-5 : ['0-5-1', '0-5-2'] 436 2878.0 ['pro', 'sleeves', 'protection', 'buy', '--', 'leather', 'drive', 'reviews', 'purchased', 'memory']\n",
      "     0-5-1 : [] 419 686.0 ['leather', 'pictures', 'pc', 'flash', 'toshiba', 'review', 'rating', 'recently', 'online', 'additional']\n",
      "     0-5-2 : [] 17 25.0 ['version', 'buying', 'display', 'stick', 'items', 'large', 'tight', 'model', 'removable', 'forward']\n",
      "   0-6 : ['0-6-1'] 78 533.0 ['choice', 'airport', 'tsa', 'security', 'neoprene', 'bit', 'pc', 'person', 'airports', 'trips']\n",
      "     0-6-1 : [] 78 126.0 ['...', 'friendly', '?', 'asus', 'airports', 'point', 'ipad', 'hole', 'plain', 'usb']\n"
     ]
    }
   ],
   "source": [
    "print_child_idxs(topic_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "366.97786105014274"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_perplexity(test_docs, topic_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_freq_tokens(topic, topics_freq_tokens):\n",
    "    topic_freq_tokens = ' '.join([idx_to_word[bow_idxs[bow_index]] for bow_index in np.argsort(topic.cnt_words)[::-1][:10]])\n",
    "    topics_freq_tokens.append(topic_freq_tokens)\n",
    "    for child in topic.children:\n",
    "        add_freq_tokens(child, topics_freq_tokens)\n",
    "\n",
    "topics_freq_tokens = []\n",
    "add_freq_tokens(topic_root, topics_freq_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_coherence = 'npmi/data/bags/cgs.txt'\n",
    "with open(path_coherence, 'w') as f:\n",
    "    f.write('\\n'.join(topics_freq_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get specialization score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_bow = np.sum([instance.bow for instance in instances_train], 0)\n",
    "norm_vec = norm_bow / np.linalg.norm(norm_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spec(topic, depth_specs=None):\n",
    "    if depth_specs is None: depth_specs = defaultdict(list)\n",
    "    topic_vec = topic.prob_words / np.linalg.norm(topic.prob_words)\n",
    "    topic_spec = 1 - topic_vec.dot(norm_vec)\n",
    "    depth_specs[topic.depth].append(topic_spec)\n",
    "    for child in topic.children:\n",
    "        depth_specs = add_spec(child, depth_specs)\n",
    "    return depth_specs\n",
    "\n",
    "depth_specs = add_spec(topic_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0749372385946312]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth_specs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0749372385946312\n",
      "1 0.6226005207697187\n",
      "2 0.49158613401802365\n",
      "3 nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "for depth, specs in depth_specs.items():\n",
    "    spec = np.mean(specs)\n",
    "    print(depth, spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_arc(topic, depth_arcs=None):\n",
    "    if depth_arcs is None: depth_arcs = defaultdict(list)\n",
    "    topic_vec = topic.prob_words / np.linalg.norm(topic.prob_words)\n",
    "    topic_arc = np.arccos(topic_vec.dot(norm_vec))\n",
    "    depth_arcs[topic.depth].append(topic_arc)\n",
    "    for child in topic.children:\n",
    "        depth_arcs = add_arc(child, depth_arcs)\n",
    "    return depth_arcs\n",
    "\n",
    "depth_arcs = add_arc(topic_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0749372385946312\n",
      "1 0.6196820668592284\n",
      "2 0.4913526867429402\n"
     ]
    }
   ],
   "source": [
    "for depth, arcs in depth_arcs.items():\n",
    "    spec = 1 - np.cos(np.mean(arcs))\n",
    "    print(depth, spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
