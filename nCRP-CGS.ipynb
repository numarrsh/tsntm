{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "import _pickle as cPickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from scipy.special import gammaln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "# flags.DEFINE_string('data_path', 'data/synthetic/instances_ncrp_9.pkl', 'path of data')\n",
    "flags.DEFINE_string('data_path', 'data/bags/instances.pkl', 'path of data')\n",
    "\n",
    "flags.DEFINE_integer('n_depth', 3, 'depth of tree')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "config = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))\n",
    "docs_bow = [instance.bow for instance in instances_train]\n",
    "docs_raw = [[[bow_index]*int(doc_bow[bow_index]) for bow_index in np.where(doc_bow > 0)[0]] for doc_bow in docs_bow]\n",
    "docs_words = [[idx for idxs in doc for idx in idxs] for doc in docs_raw][:5000]\n",
    "words = [word for doc_words in docs_words for word in doc_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs_bow = [instance.bow for instance in instances_test]\n",
    "test_docs_raw = [[[bow_index]*int(test_doc_bow[bow_index]) for bow_index in np.where(test_doc_bow > 0)[0]] for test_doc_bow in test_docs_bow]\n",
    "test_docs_words = [[idx for idxs in doc for idx in idxs] for doc in test_docs_raw][:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1035, 92009)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_doc = len(docs_words)\n",
    "n_vocab = len(np.unique(words))\n",
    "n_words = len(words)\n",
    "assert n_vocab == len(bow_idxs)\n",
    "n_doc, n_vocab, n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assign docs to tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Topic:\n",
    "    def __init__(self, idx, sibling_idx, parent, depth, n_doc, n_vocab):\n",
    "        self.idx = idx\n",
    "        self.sibling_idx = sibling_idx\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.depth = depth\n",
    "        self.cnt_doc = 0\n",
    "        self.n_doc = n_doc\n",
    "        self.n_vocab = n_vocab\n",
    "        self.cnt_words = np.zeros(n_vocab) # Number of Words over Documents\n",
    "        self.set_prob_words()\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def sample_child(self, doc, train=True):\n",
    "        s_child_prior = self.get_s_child_prior(gam)\n",
    "        s_child_likelihood = self.get_s_child_likelihood(doc, eta)\n",
    "        p_child = np.array(s_child_prior * s_child_likelihood) / np.sum(s_child_prior * s_child_likelihood)\n",
    "        \n",
    "        child_index = np.random.multinomial(1, p_child).argmax()\n",
    "        if verbose: print('Depth: ', self.depth, 'p_child: ', p_child, 'selected:', child_index)\n",
    "        \n",
    "        if child_index < len(self.children):\n",
    "            child = self.children[child_index]\n",
    "        else:\n",
    "            child = self.get_new_child()\n",
    "            if train: self.children += [child]\n",
    "        return child\n",
    "    \n",
    "    def init_sample_child(self, train=True):\n",
    "        s_child_prior = self.get_s_child_prior(gam)\n",
    "        p_child = np.array(s_child_prior) / np.sum(s_child_prior)\n",
    "        \n",
    "        child_index = np.random.multinomial(1, p_child).argmax()\n",
    "        if verbose: print('Depth: ', self.depth, 'p_child: ', p_child, 'selected:', child_index)\n",
    "\n",
    "        if child_index < len(self.children):\n",
    "            child = self.children[child_index]\n",
    "        else:\n",
    "            child = self.get_new_child()\n",
    "            if train: self.children += [child]\n",
    "        return child\n",
    "    \n",
    "    def get_probs_child(self, doc):\n",
    "        s_child_prior = self.get_s_child_prior(gam)\n",
    "        s_child_likelihood = self.get_s_child_likelihood(doc, eta)\n",
    "        p_child = np.array(s_child_prior * s_child_likelihood) / np.sum(s_child_prior * s_child_likelihood)\n",
    "        return p_child\n",
    "    \n",
    "    def get_s_child_prior(self, gam):\n",
    "        s_child_prior = [child.cnt_doc for child in self.children]\n",
    "        s_child_prior += [gam]\n",
    "        return s_child_prior\n",
    "    \n",
    "    def get_s_child_likelihood(self, doc, eta):\n",
    "        if len(self.children) > 0:\n",
    "            children_cnt_words = np.concatenate([np.array([child.cnt_words for child in self.children]), np.zeros([1, self.n_vocab])], 0) # (Children+1) x Vocabulary\n",
    "        else:\n",
    "            children_cnt_words = np.zeros([1, self.n_vocab]) # (Children+1) x Vocabulary\n",
    "        \n",
    "        cnt_words_doc = doc.cnt_words[None, :] # 1 x Vocabulary\n",
    "\n",
    "        logits_likelihood = gammaln(np.sum(children_cnt_words, -1) + n_vocab*eta) \\\n",
    "                            - np.sum(gammaln(children_cnt_words + eta), -1) \\\n",
    "                            - gammaln(np.sum(children_cnt_words + cnt_words_doc, -1) + n_vocab*eta) \\\n",
    "                            + np.sum(gammaln(children_cnt_words + cnt_words_doc + eta), -1)\n",
    "        s_child_likelihood = np.exp(logits_likelihood)\n",
    "        return s_child_likelihood\n",
    "    \n",
    "    def get_new_child(self):\n",
    "        sibling_idx = max([child.sibling_idx for child in self.children]) + 1 if len(self.children) > 0 else 1\n",
    "        idx = self.idx + '-' + str(sibling_idx)\n",
    "        depth = self.depth+1\n",
    "        child = Topic(idx=idx, sibling_idx=sibling_idx, parent=self, depth=depth, n_doc=self.n_doc, n_vocab=self.n_vocab)        \n",
    "        return child\n",
    "        \n",
    "    def get_children(self):\n",
    "        child = self.get_new_child()\n",
    "        children = self.children + [child]\n",
    "        return children\n",
    "    \n",
    "    def delete_topic(self):\n",
    "        self.parent.children.remove(self)\n",
    "        \n",
    "    def set_prob_words(self):\n",
    "        cnt_words = self.cnt_words + eta\n",
    "        self.prob_words = cnt_words / np.sum(cnt_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc:\n",
    "    def __init__(self, idx, words, bow, n_depth):\n",
    "        self.idx = idx\n",
    "        self.words = words\n",
    "        self.cnt_words = bow\n",
    "        assert len(words) == np.sum(bow)\n",
    "        \n",
    "        self.topics = [] # Depth\n",
    "        self.word_depths = [] # Word Indices\n",
    "        self.depth_cnt_words = np.zeros([n_depth, n_vocab])\n",
    "                \n",
    "    def get_probs_depth(self, word_idx):\n",
    "        s_docs = np.sum(self.depth_cnt_words, -1) + alpha # Depth\n",
    "        s_words = np.array([topic.cnt_words[word_idx] for topic in self.topics]) + eta # Depth\n",
    "        z_words = np.array([np.sum(topic.cnt_words) for topic in self.topics]) + n_vocab*eta # Depth\n",
    "        assert s_docs.shape == s_words.shape == z_words.shape\n",
    "\n",
    "        s_depths = s_docs*s_words/z_words\n",
    "        p_depths = s_depths/np.sum(s_depths) # Depth\n",
    "        return p_depths\n",
    "    \n",
    "    def sample_depth(self, word_idx):\n",
    "        prob_depths = self.get_probs_depth(word_idx)\n",
    "        word_depth = np.argmax(np.random.multinomial(1, prob_depths))\n",
    "        return word_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample doc path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p({\\bf c}_{m}\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf w}, {\\bf c}_{-m}, {\\bf z})\\propto p({\\bf w}_{m}\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf c}, {\\bf w}_{-m}, {\\bf z})\\cdot p({\\bf c}_{m}\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf c}_{-m})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p({\\bf w}_{m}\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf c}, {\\bf w}_{-m}, {\\bf z})=\\prod_{\\ell=1}^{L}\\left(\\frac{\\Gamma(n_{c_{m,\\ell},-m}^{(\\cdot)}+W\\eta)}{\\prod_{w}\\Gamma(n_{c_{m,\\ell},-m}^{(w)}+\\eta)}\\frac{\\prod_{w}\\Gamma(n_{c_{m,\\ell},-m}^{(w)}+n_{c_{m,\\ell},m}^{(w)}+\\eta)}{\\Gamma(n_{c_{m,\\ell},-m}^{(\\cdot)}+n_{c_{m,\\ell},m}^{(\\cdot)}+W\\eta)}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_doc_topics(docs, topic_root, train=True):\n",
    "    for doc in docs:\n",
    "        topic = topic_root\n",
    "        doc.topics = [topic]\n",
    "        if train: topic.cnt_doc += 1 # increment count of docs\n",
    "\n",
    "        for depth in range(1, n_depth):\n",
    "            topic = topic.init_sample_child(train=train)\n",
    "            doc.topics += [topic]\n",
    "            if train: topic.cnt_doc += 1 # increment count of docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_doc_topics(docs, topic_root, train=True):\n",
    "    for doc in docs:\n",
    "        if train:\n",
    "            for depth in range(1, n_depth):\n",
    "                topic = doc.topics[depth]\n",
    "                topic.cnt_doc -= 1 # decrement count of docs\n",
    "                assert topic.cnt_doc >= 0\n",
    "                topic.cnt_words -= doc.depth_cnt_words[depth] # decrement count of words\n",
    "                assert np.min(topic.cnt_words) >= 0\n",
    "\n",
    "                if topic.cnt_doc == 0: \n",
    "                    topic.delete_topic()\n",
    "                    assert np.sum(topic.cnt_words) == 0\n",
    "\n",
    "        topic = topic_root\n",
    "        doc.topics = [topic]\n",
    "        for depth in range(1, n_depth):\n",
    "            topic = topic.sample_child(doc, train=train)\n",
    "            doc.topics += [topic]\n",
    "            if train: topic.cnt_doc += 1 # increment count of docs\n",
    "            if train: topic.cnt_words += doc.depth_cnt_words[depth] # increment count of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assign words to topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "p(z_{i}=j\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf z}_{-i},{\\bf w})\\propto (n_{-i,j}^{(d_{i})}+\\alpha)\\frac{n_{-i,j}^{(w_{i})}+\\eta}{n_{-i,j}^{(\\cdot)}+W\\eta}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_word_topics(docs, train=True):\n",
    "    for doc in docs:\n",
    "        if doc.idx % 1000 == 0: print(doc.idx, end=' ')\n",
    "        for word_index, word_idx in enumerate(doc.words):\n",
    "            # sample depth of word\n",
    "            new_depth = doc.sample_depth(word_idx)\n",
    "            new_topic = doc.topics[new_depth]\n",
    "            \n",
    "            # increment count of words\n",
    "            doc.depth_cnt_words[new_depth, word_idx] += 1\n",
    "            if train: new_topic.cnt_words[word_idx] += 1\n",
    "            doc.word_depths.append(new_depth) # for reference when sampling\n",
    "            \n",
    "        assert len(doc.word_depths) == len(doc.words) == np.sum(doc.depth_cnt_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_word_topics(docs, train=True):\n",
    "    for doc in docs:\n",
    "        if doc.idx % 1000 == 0: print(doc.idx, end=' ')\n",
    "        for word_index, word_idx in enumerate(doc.words):\n",
    "            # refer depth of word\n",
    "            old_depth = doc.word_depths[word_index]\n",
    "            old_topic = doc.topics[old_depth]\n",
    "            \n",
    "            # decrement count of words\n",
    "            doc.depth_cnt_words[old_depth, word_idx] -= 1\n",
    "            if train: old_topic.cnt_words[word_idx] -= 1            \n",
    "            \n",
    "            # sample depth of word\n",
    "            new_depth = doc.sample_depth(word_idx)\n",
    "            new_topic = doc.topics[new_depth]\n",
    "            \n",
    "            # increment count of words\n",
    "            doc.depth_cnt_words[new_depth, word_idx] += 1\n",
    "            if train: new_topic.cnt_words[word_idx] += 1\n",
    "            doc.word_depths[word_index] = new_depth # for sample\n",
    "            \n",
    "        assert len(doc.word_depths) == len(doc.words) == np.sum(doc.depth_cnt_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recur_cnt_words(topic):\n",
    "    cnt_words = np.sum(topic.cnt_words)\n",
    "    for child in topic.children:\n",
    "        cnt_words += recur_cnt_words(child)\n",
    "    return cnt_words\n",
    "    \n",
    "def assert_sum_cnt_words(topic_root):\n",
    "    sum_cnt_words = recur_cnt_words(topic_root)\n",
    "    assert sum_cnt_words == sum([len(doc.words) for doc in docs])\n",
    "    \n",
    "def nearly_equal(val, thre):\n",
    "    return (val > thre-1e-5) and (val < thre+1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.array([10., 5., 1.])\n",
    "gam = 0.01\n",
    "eta = 1.\n",
    "n_depth = 3\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_sample = 100\n",
    "docs = [Doc(idx=doc_idx, words=doc_words, bow=doc_bow, n_depth=config.n_depth) for doc_idx, (doc_words, doc_bow) in enumerate(zip(docs_words, docs_bow))]\n",
    "topic_root = Topic(idx='0', sibling_idx=0, parent=None, depth=0, n_doc=n_doc, n_vocab=n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_train(docs, topic_root):\n",
    "    init_doc_topics(docs=docs, topic_root=topic_root)\n",
    "    init_word_topics(docs=docs)\n",
    "    assert_sum_cnt_words(topic_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_train(docs, topic_root):\n",
    "    sample_doc_topics(docs=docs, topic_root=topic_root)\n",
    "    sample_word_topics(docs=docs)\n",
    "    assert_sum_cnt_words(topic_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1000 2000 3000 4000 0 1000 2000 3000 4000 "
     ]
    }
   ],
   "source": [
    "init_train(docs, topic_root)\n",
    "sample_train(docs, topic_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_perplexity(docs, topic_root):\n",
    "    def set_prob_words(topic):\n",
    "        topic.set_prob_words()\n",
    "        for topic_child in topic.children:\n",
    "            set_prob_words(topic_child)\n",
    "            \n",
    "    # set Probabilty of Words\n",
    "    set_prob_words(topic_root)\n",
    "    \n",
    "    logit_docs, n_words = 0, 0\n",
    "    for doc in docs:\n",
    "        # Path Probability for each document\n",
    "        topic = topic_root\n",
    "        probs_paths= [{topic: 1.}]\n",
    "        for depth in range(1, n_depth):\n",
    "            probs_path = {}\n",
    "            for topic, prob_path in probs_paths[-1].items():\n",
    "                topics_child = topic.get_children()\n",
    "                probs_child = topic.get_probs_child(doc)\n",
    "                probs_path_child = prob_path * probs_child\n",
    "                for topic_child, prob_path_child in zip(topics_child, probs_path_child):\n",
    "                    probs_path[topic_child] = prob_path_child\n",
    "            probs_paths.append(probs_path)    \n",
    "            \n",
    "        assert nearly_equal(np.sum([sum(probs_path.values()) for probs_path in probs_paths]), n_depth)        \n",
    "\n",
    "        # Depth Probability for Each Word\n",
    "        probs_depths = []\n",
    "        for word_index, word_idx in enumerate(doc.words):\n",
    "            probs_depth = doc.get_probs_depth(word_idx)\n",
    "            probs_depths.append(probs_depth)\n",
    "            \n",
    "        assert nearly_equal(np.sum(probs_depths), len(doc.words))\n",
    "    \n",
    "        # Likelihood of Doc\n",
    "        assert len(probs_depths) == len(doc.words)\n",
    "        logit_doc = 0\n",
    "        for prob_depths, word_idx in zip(probs_depths, doc.words):\n",
    "#             prob_topics, prob_word_topics = [], []\n",
    "            prob_word = 0\n",
    "            for prob_paths, prob_depth in zip(probs_paths, prob_depths):\n",
    "                for topic, prob_path in prob_paths.items():\n",
    "                    prob_topic = prob_path * prob_depth # scalar\n",
    "                    prob_word_topic = topic.prob_words[word_idx] # scalar\n",
    "#                     prob_topics.append(prob_topic)\n",
    "#                     prob_word_topics.append(prob_word_topic)\n",
    "                    prob_word += prob_topic * prob_word_topic\n",
    "            logit_word = np.log(prob_word)\n",
    "            logit_doc += logit_word\n",
    "        logit_docs += logit_doc\n",
    "        n_words += len(doc.words)\n",
    "#         assert nearly_equal(sum(prob_topics), 1.)\n",
    "        \n",
    "    perplexity = np.exp(-logit_docs/n_words)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs = [Doc(idx=doc_idx, words=doc_words, bow=doc_bow, n_depth=config.n_depth) for doc_idx, (doc_words, doc_bow) in enumerate(zip(test_docs_words, test_docs_bow))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_test(test_docs, topic_root):\n",
    "    init_doc_topics(docs=test_docs, topic_root=topic_root, train=False)\n",
    "    init_word_topics(docs=test_docs, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_test(test_docs, topic_root):\n",
    "    sample_doc_topics(docs=test_docs, topic_root=topic_root, train=False)\n",
    "    sample_word_topics(docs=test_docs, train=False)\n",
    "    assert_sum_cnt_words(topic_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 "
     ]
    }
   ],
   "source": [
    "init_test(test_docs, topic_root)\n",
    "sample_test(test_docs, topic_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382.9140862263253"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_perplexity(test_docs, topic_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 : ['0-1', '0-2', '0-3', '0-4', '0-5', '0-6', '0-7', '0-8', '0-9', '0-10', '0-11', '0-12', '0-13', '0-14', '0-15'] 5000 47528.0 ['!', 'bought', 'nice', 'carry', 'quality', 'price', 'love', 'made', 'perfect', 'recommend']\n",
      "   0-1 : ['0-1-1', '0-1-2', '0-1-3'] 480 3397.0 ['!', 'mac', 'pro', 'cover', 'perfect', 'love', 'sleeve', 'perfectly', 'apple', 'color']\n",
      "     0-1-1 : [] 81 55.0 ['earlier', 'padded', 'protective', 'holder', 'cost', 'metal', 'unibody', 'highly', 'expensive', 'expecting']\n",
      "     0-1-2 : [] 203 140.0 ['cool', '?', 'aspire', 'ordered', 'external', 'awesome', 'waste', 'style', 'air', 'long']\n",
      "     0-1-3 : [] 196 140.0 ['online', 'shipping', 'colors', 'completely', 'spot', 'snaps', 'pleased', 'green', 'short', 'paper']\n",
      "   0-2 : ['0-2-1', '0-2-2', '0-2-3'] 602 4360.0 ['!', 'back', 'years', 'school', 'wheels', 'books', 'heavy', 'year', 'handle', 'pack']\n",
      "     0-2-1 : [] 147 117.0 ['college', 'load', 'adding', 'company', '+', 'travels', 'extremely', 'son', 'year', 'completely']\n",
      "     0-2-2 : [] 227 175.0 ['port', 'thick', 'samsonite', 'wont', 'hoped', 'excellent', 'plane', 'design', 'wear', 'usage']\n",
      "     0-2-3 : [] 228 185.0 ['prefer', 'luggage', 'comments', 'material', 'tear', 'tend', 'began', 'weird', 'products', 'find']\n",
      "   0-3 : ['0-3-1', '0-3-2'] 300 2800.0 ['strap', 'broke', 'months', 'handle', 'shoulder', 'zipper', 'started', 'straps', 'time', 'stitching']\n",
      "     0-3-1 : [] 121 127.0 ['day', 'heavier', 'charge', 'important', 'happen', 'plug', 'rugged', 'fall', 'anymore', \"'ll\"]\n",
      "     0-3-2 : [] 179 152.0 ['break', 'fast', 'keys', \"'ll\", 'compartments', 'normal', 'late', 'cheaply', 'textbooks', 'sewn']\n",
      "   0-4 : ['0-4-1'] 219 2215.0 ['cover', 'pro', 'bottom', 'mac', 'shell', 'scratches', 'clear', 'top', 'apple', 'plastic']\n",
      "     0-4-1 : [] 219 179.0 ['pro', 'sleek', 'protective', 'bought', 'book', 'owned', 'order', 'open', 'hands', 'slightly']\n",
      "   0-5 : ['0-5-1', '0-5-2'] 402 3522.0 ['sleeve', 'pro', 'zipper', 'protection', 'pocket', 'inside', 'material', 'snug', 'protect', 'neoprene']\n",
      "     0-5-1 : [] 152 154.0 ['offer', 'screen', 'older', 'finish', 'careful', 'rubber', 'side', '/', 'store', 'returning']\n",
      "     0-5-2 : [] 250 261.0 ['pro', '-inch', 'build', 'pull', 'damage', 'key', 'unit', 'make', 'zip', 'snug']\n",
      "   0-6 : ['0-6-1'] 83 970.0 ['usb', 'card', 'drive', 'power', 'drives', 'ports', 'slot', 'works', 'external', 'hard']\n",
      "     0-6-1 : [] 83 56.0 ['stay', 'son', 'difference', 'unit', 'difficult', 'check', 'foam', 'video', 'keyboard', 'key']\n",
      "   0-7 : ['0-7-1', '0-7-2'] 454 3805.0 ['pocket', 'power', 'netbook', 'sleeve', 'mouse', 'cord', 'dvd', 'inch', 'room', 'player']\n",
      "     0-7-1 : [] 209 166.0 ['issue', 'acer', 'convenient', 'sony', 'vaio', 'card', 'charger', 'added', 'wireless', 'constantly']\n",
      "     0-7-2 : [] 245 186.0 ['space', 'feels', 'extra', 'paperwork', 'bought', 'appears', 'chromebook', 'personal', 'nicely', 'check']\n",
      "   0-8 : ['0-8-1', '0-8-2'] 405 2990.0 ['sleeve', 'smell', 'netbook', 'color', 'strap', 'inside', 'neoprene', 'cute', 'protection', 'cover']\n",
      "     0-8-1 : [] 210 199.0 ['return', 'fabric', 'attach', 'hope', 'beautiful', 'system', 'hand', 'longer', 'amazon', 'zippers']\n",
      "     0-8-2 : [] 195 157.0 ['protecting', 'cover', 'dell', 'protection', 'blue', 'fact', 'orange', 'tear', 'save', 'dirty']\n",
      "   0-9 : ['0-9-1', '0-9-2', '0-9-3'] 345 3525.0 ['pocket', '-', 'pockets', 'strap', 'front', 'compartment', ':', 'shoulder', 'small', 'large']\n",
      "     0-9-1 : [] 54 63.0 ['break', 'cable', 'samsonite', 'sharp', 'commute', 'comments', 'coming', 'inspiron', 'area', 'fitting']\n",
      "     0-9-2 : [] 129 186.0 ['lap', 'easier', 'sliding', 'double', 'main', 'accessible', 'ipod', 'width', 'quick', 'wheels']\n",
      "     0-9-3 : [] 162 173.0 ['pros', 'bit', 'phone', 'difficult', 'plane', 'samsonite', 'external', 'roomy', 'makes', 'ac']\n",
      "   0-10 : ['0-10-1', '0-10-2'] 90 752.0 ['feet', '$', 'rubber', 'bottom', 'surface', 'metal', 'protective', 'flat', \"'d\", 'screen']\n",
      "     0-10-1 : [] 38 32.0 ['breaking', 'extremely', 'careful', 'aluminum', 'tab', 'tabs', 'plastic', 'everyday', 'high', 'ordering']\n",
      "     0-10-2 : [] 52 86.0 ['-', 'photo', 'desk', 'giving', 'suggest', 'warranty', 'made', 'alot', 'scratch', 'trouble']\n",
      "   0-11 : ['0-11-1', '0-11-2'] 152 1242.0 ['&', ';', '...', 'zipper', 'wheels', 'file', 'works', 'asus', 'broken', 'folders']\n",
      "     0-11-1 : [] 104 103.0 ['...', 'targus', 'corners', ';', 'corner', 'pad', 'finish', 'gave', 'pen', 'pro']\n",
      "     0-11-2 : [] 48 51.0 ['close', 'barely', 'wearing', 'player', 'wear', 'happen', 'cheaply', 'end', 'entire', 'sleeve']\n",
      "   0-12 : ['0-12-1', '0-12-2', '0-12-3'] 823 6167.0 ['pockets', 'carry', 'room', 'books', 'plenty', 'pack', 'lots', 'comfortable', 'space', '&']\n",
      "     0-12-1 : [] 418 360.0 ['camera', 'interior', 'change', 'smoothly', 'walk', 'lenses', 'constructed', 'area', 'lightweight', 'thinner']\n",
      "     0-12-2 : [] 200 153.0 ['sits', 'world', 'commute', 'opening', 'portable', 'back', 'rolls', 'giving', 'wireless', 'essentials']\n",
      "     0-12-3 : [] 205 212.0 ['bags', 'number', 'airport', 'shows', 'pockets', 'buy', 'numerous', 'cables', 'luggage', 'plane']\n",
      "   0-13 : ['0-13-1', '0-13-2'] 264 1728.0 ['item', '$', 'amazon', 'dvd', 'money', 'player', 'price', 'sleeves', 'box', '...']\n",
      "     0-13-1 : [] 172 138.0 ['price', 'delivery', 'delivered', 'service', 'description', 'bags', 'purchase', 'rolls', 'saved', 'sewn']\n",
      "     0-13-2 : [] 92 87.0 ['targus', 'excellent', 'exact', 'closed', 'clear', 'christmas', 'comment', 'heavy', 'takes', 'finish']\n",
      "   0-14 : ['0-14-1'] 187 1222.0 ['color', \"'\", 'pink', 'leather', '`', 'pockets', 'comfortable', 'wanted', 'tote', 'colors']\n",
      "     0-14-1 : [] 187 176.0 ['roller', 'tall', 'hand', 'sewn', 'version', 'pretty', 'negative', 'cords', 'annoying', 'place']\n",
      "   0-15 : ['0-15-1'] 193 1284.0 ['leather', 'swissgear', 'inch', 'camera', 'nylon', 'top', 'based', 'received', 'side', 'corners']\n",
      "     0-15-1 : [] 193 228.0 ['pack', 'amazing', 'inches', 'snug', 'logo', 'high', 'mp', 'construction', 'designed', 'bigger']\n"
     ]
    }
   ],
   "source": [
    "def print_child_idxs(topic):\n",
    "    topic_freq_words = [idx_to_word[bow_idxs[bow_index]] for bow_index in np.argsort(topic.cnt_words)[::-1][:10]]\n",
    "    print('  '*topic.depth, topic.idx, ':', [child.idx for child in topic.children], topic.cnt_doc, np.sum(topic.cnt_words), topic_freq_words)\n",
    "    for topic in topic.children:\n",
    "        print_child_idxs(topic)\n",
    "print_child_idxs(topic_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 100\n",
    "alpha = np.array([10., 5., 1.])\n",
    "gam = 0.01\n",
    "eta = 1.\n",
    "n_depth = 3\n",
    "verbose = False\n",
    "topic_root = Topic(idx='0', sibling_idx=0, parent=None, depth=0, n_doc=n_doc, n_vocab=n_vocab)\n",
    "docs = [Doc(idx=doc_idx, words=doc_words, bow=doc_bow, n_depth=config.n_depth) for doc_idx, (doc_words, doc_bow) in enumerate(zip(docs_words, docs_bow))]\n",
    "test_docs = [Doc(idx=doc_idx, words=doc_words, bow=doc_bow, n_depth=config.n_depth) for doc_idx, (doc_words, doc_bow) in enumerate(zip(test_docs_words, test_docs_bow))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1000 2000 3000 4000 0 Perplexity: 405.33\n",
      "0 1000 2000 3000 4000 0 Perplexity: 380.17\n",
      "0 1000 2000 3000 4000 0 Perplexity: 359.57\n",
      "0 1000 2000 3000 4000 0 Perplexity: 348.04\n",
      "0 1000 2000 3000 4000 0 Perplexity: 340.89\n",
      "0 1000 2000 3000 4000 0 Perplexity: 338.02\n",
      "0 1000 2000 3000 4000 0 Perplexity: 338.59\n",
      "0 1000 2000 3000 4000 0 Perplexity: 336.91\n",
      "0 1000 2000 3000 4000 0 Perplexity: 333.46\n",
      "0 1000 2000 3000 4000 0 Perplexity: 327.39\n",
      "0 1000 2000 3000 4000 0 Perplexity: 325.36\n",
      "0 1000 2000 3000 4000 0 Perplexity: 324.81\n",
      "0 1000 2000 3000 4000 0 Perplexity: 324.92\n",
      "0 1000 2000 3000 4000 0 Perplexity: 324.69\n",
      "0 1000 2000 3000 4000 0 Perplexity: 326.62\n",
      "0 1000 2000 3000 4000 0 Perplexity: 322.96\n",
      "0 1000 2000 3000 4000 0 Perplexity: 322.50\n",
      "0 1000 2000 3000 4000 0 Perplexity: 320.78\n",
      "0 1000 2000 3000 4000 0 Perplexity: 325.07\n",
      "0 1000 2000 3000 4000 0 Perplexity: 320.36\n",
      "0 1000 2000 3000 4000 0 Perplexity: 320.18\n",
      "0 1000 2000 3000 4000 0 Perplexity: 319.27\n",
      "0 1000 2000 3000 4000 0 Perplexity: 315.50\n",
      "0 1000 2000 3000 4000 0 Perplexity: 320.01\n",
      "0 1000 2000 3000 4000 0 Perplexity: 321.17\n",
      "0 1000 2000 3000 4000 0 Perplexity: 318.51\n",
      "0 1000 2000 3000 4000 0 Perplexity: 321.79\n",
      "0 1000 2000 3000 4000 0 Perplexity: 320.90\n",
      "0 1000 2000 3000 4000 0 Perplexity: 320.98\n",
      "0 1000 2000 3000 4000 0 Perplexity: 320.83\n",
      "0 1000 2000 3000 4000 0 Perplexity: 320.56\n",
      "0 1000 2000 3000 4000 0 Perplexity: 318.14\n",
      "0 1000 2000 3000 4000 0 Perplexity: 318.99\n",
      "0 1000 2000 3000 4000 0 Perplexity: 318.90\n",
      "0 1000 2000 3000 4000 0 Perplexity: 316.01\n",
      "0 1000 2000 3000 4000 0 Perplexity: 314.81\n",
      "0 1000 2000 3000 4000 0 Perplexity: 314.78\n",
      "0 1000 2000 3000 4000 0 Perplexity: 315.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-ed78a62630ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0massert_sum_cnt_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0msample_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0msample_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0massert_sum_cnt_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-fb78074103c0>\u001b[0m in \u001b[0;36msample_train\u001b[0;34m(docs, topic_root)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msample_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msample_doc_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopic_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0msample_word_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0massert_sum_cnt_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-a7ea03ec0997>\u001b[0m in \u001b[0;36msample_doc_topics\u001b[0;34m(docs, topic_root, train)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mtopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopics\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnt_doc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# increment count of docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-7e95814a13c3>\u001b[0m in \u001b[0;36msample_child\u001b[0;34m(self, doc, train)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0ms_child_prior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_s_child_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0ms_child_likelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_s_child_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mp_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_child_prior\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ms_child_likelihood\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_child_prior\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ms_child_likelihood\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-7e95814a13c3>\u001b[0m in \u001b[0;36mget_s_child_likelihood\u001b[0;34m(self, doc, eta)\u001b[0m\n\u001b[1;32m     64\u001b[0m                             \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgammaln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchildren_cnt_words\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                             \u001b[0;34m-\u001b[0m \u001b[0mgammaln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchildren_cnt_words\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcnt_words_doc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_vocab\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                             \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgammaln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchildren_cnt_words\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcnt_words_doc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0ms_child_likelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_likelihood\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ms_child_likelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(n_sample):\n",
    "    if i == 0:\n",
    "        init_train(docs, topic_root)\n",
    "        init_test(test_docs, topic_root)        \n",
    "        assert_sum_cnt_words(topic_root)\n",
    "    else:\n",
    "        sample_train(docs, topic_root)\n",
    "        sample_test(test_docs, topic_root)                \n",
    "        assert_sum_cnt_words(topic_root)\n",
    "        \n",
    "    perplexity = eval_perplexity(test_docs, topic_root)\n",
    "    print('Perplexity: %.2f' % perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-10-96b81c9e9ae2>\u001b[0m(14)\u001b[0;36minit_word_topics\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     10 \u001b[0;31m            \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth_cnt_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     11 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_topic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnt_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m            \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_depths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_depth\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# for reference when sampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     13 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 14 \u001b[0;31m        \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_depths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth_cnt_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> np.sum(doc.depth_cnt_words)\n",
      "20.0\n",
      "ipdb> len(doc.words)\n",
      "10\n",
      "ipdb> len\n",
      "<built-in function len>\n",
      "ipdb> len(doc.word_depths)\n",
      "20\n",
      "ipdb> doc.word_depths\n",
      "[0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 2, 0, 0, 0, 2, 0, 0, 1]\n",
      "ipdb> train\n",
      "False\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
