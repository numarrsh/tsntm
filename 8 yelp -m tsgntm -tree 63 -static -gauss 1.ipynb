{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from data_structure import get_batches\n",
    "from hntm import HierarchicalNeuralTopicModel\n",
    "from nhdp import nestedHierarchicalNeuralTopicModel\n",
    "from tsgntm import TreeStructuredGaussianNeuralTopicModel\n",
    "from tree import get_descendant_idxs\n",
    "from evaluation import get_hierarchical_affinity, get_topic_specialization, print_topic_sample, print_topic_year\n",
    "from configure import get_config\n",
    "\n",
    "pd.set_option('display.max_columns', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(nb_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu\n",
    "np.random.seed(config.seed)\n",
    "random.seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.path_data,'rb'))\n",
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)\n",
    "config.dim_bow = len(bow_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     6
    ]
   },
   "outputs": [],
   "source": [
    "def debug(variable, sample_batch=None):\n",
    "    if sample_batch is None: sample_batch = test_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch, mode='test')\n",
    "    _variable = sess.run(variable, feed_dict=feed_dict)\n",
    "    return _variable\n",
    "\n",
    "def check(variable):\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sample_batch = test_batches[0]\n",
    "    feed_dict = model.get_feed_dict(sample_batch, mode='test', assertion=True)\n",
    "    _variable = sess.run(variable, feed_dict=feed_dict)\n",
    "    return _variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "TopicModels = {'hntm': HierarchicalNeuralTopicModel, 'nhdp': nestedHierarchicalNeuralTopicModel, 'tsgntm': TreeStructuredGaussianNeuralTopicModel}\n",
    "TopicModel = TopicModels[config.model]\n",
    "model = TopicModel(config)\n",
    "sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(max_to_keep=config.max_to_keep)\n",
    "update_tree_flg = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     24
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = []\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "ppl_min = np.inf\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','','','','','','VALID:','','','','','','TEST:','', 'SPEC:', '', '', 'HIER:', ''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL', 'GAUSS', 'REG','LOSS','PPL','NLL','KL', 'GAUSS','REG','LOSS','PPL', '1', '2', '3', 'CHILD', 'OTHER']]))))\n",
    "\n",
    "cmd_rm = 'rm -r %s' % config.dir_model\n",
    "res = subprocess.call(cmd_rm.split())\n",
    "cmd_mk = 'mkdir %s' % config.dir_model\n",
    "res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "def update_checkpoint(config, checkpoint, global_step):\n",
    "    checkpoint.append(config.path_model + '-%i' % global_step)\n",
    "    if len(checkpoint) > config.max_to_keep:\n",
    "        path_model = checkpoint.pop(0) + '.*'\n",
    "        for p in glob.glob(path_model):\n",
    "            os.remove(p)\n",
    "    cPickle.dump(checkpoint, open(config.path_checkpoint, 'wb'))\n",
    "    \n",
    "def validate(sess, batches, model):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    prob_topic_list = []\n",
    "    n_bow_list = []\n",
    "    n_topics_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = model.get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch, ppls_batch, prob_topic_batch, n_bow_batch, n_topics_batch \\\n",
    "            = sess.run([model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_gauss, model.topic_loss_reg, model.topic_ppls, model.prob_topic, model.n_bow, model.n_topics], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "        prob_topic_list.append(prob_topic_batch)\n",
    "        n_bow_list.append(n_bow_batch)\n",
    "        n_topics_list.append(n_topics_batch)\n",
    "    loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_gauss_mean, topic_loss_reg_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    \n",
    "    probs_topic = np.concatenate(prob_topic_list, 0)\n",
    "    \n",
    "    n_bow = np.concatenate(n_bow_list, 0)\n",
    "    n_topics = np.concatenate(n_topics_list, 0)\n",
    "    probs_topic_mean = np.sum(n_topics, 0) / np.sum(n_bow)\n",
    "    \n",
    "    return loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_gauss_mean, topic_loss_reg_mean, ppl_mean, probs_topic_mean    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train & validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th colspan=\"5\" halign=\"left\"></th>\n",
       "      <th>VALID:</th>\n",
       "      <th colspan=\"5\" halign=\"left\"></th>\n",
       "      <th>TEST:</th>\n",
       "      <th></th>\n",
       "      <th>SPEC:</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "      <th>HIER:</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>GAUSS</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>GAUSS</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>CHILD</th>\n",
       "      <th>OTHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>999</td>\n",
       "      <td>928.91</td>\n",
       "      <td>2691</td>\n",
       "      <td>923.50</td>\n",
       "      <td>1.60</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.24</td>\n",
       "      <td>888.19</td>\n",
       "      <td>2519</td>\n",
       "      <td>881.96</td>\n",
       "      <td>1.91</td>\n",
       "      <td>4.08</td>\n",
       "      <td>0.24</td>\n",
       "      <td>885.92</td>\n",
       "      <td>2467</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>1999</td>\n",
       "      <td>921.96</td>\n",
       "      <td>2538</td>\n",
       "      <td>916.06</td>\n",
       "      <td>2.13</td>\n",
       "      <td>3.53</td>\n",
       "      <td>0.24</td>\n",
       "      <td>880.70</td>\n",
       "      <td>2365</td>\n",
       "      <td>874.29</td>\n",
       "      <td>3.13</td>\n",
       "      <td>2.93</td>\n",
       "      <td>0.35</td>\n",
       "      <td>879.34</td>\n",
       "      <td>2362</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>295</td>\n",
       "      <td>919.36</td>\n",
       "      <td>2468</td>\n",
       "      <td>913.31</td>\n",
       "      <td>2.42</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.24</td>\n",
       "      <td>878.76</td>\n",
       "      <td>2315</td>\n",
       "      <td>871.83</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.54</td>\n",
       "      <td>0.24</td>\n",
       "      <td>878.41</td>\n",
       "      <td>2325</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>1295</td>\n",
       "      <td>916.82</td>\n",
       "      <td>2422</td>\n",
       "      <td>910.56</td>\n",
       "      <td>2.62</td>\n",
       "      <td>3.40</td>\n",
       "      <td>0.23</td>\n",
       "      <td>873.13</td>\n",
       "      <td>2193</td>\n",
       "      <td>865.86</td>\n",
       "      <td>3.58</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.19</td>\n",
       "      <td>877.21</td>\n",
       "      <td>2311</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>2295</td>\n",
       "      <td>914.61</td>\n",
       "      <td>2385</td>\n",
       "      <td>908.19</td>\n",
       "      <td>2.77</td>\n",
       "      <td>3.43</td>\n",
       "      <td>0.23</td>\n",
       "      <td>872.92</td>\n",
       "      <td>2201</td>\n",
       "      <td>865.64</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3.59</td>\n",
       "      <td>0.20</td>\n",
       "      <td>877.21</td>\n",
       "      <td>2311</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170000</th>\n",
       "      <td>69</td>\n",
       "      <td>62</td>\n",
       "      <td>2351</td>\n",
       "      <td>895.10</td>\n",
       "      <td>1999</td>\n",
       "      <td>885.58</td>\n",
       "      <td>4.53</td>\n",
       "      <td>4.83</td>\n",
       "      <td>0.15</td>\n",
       "      <td>857.76</td>\n",
       "      <td>1905</td>\n",
       "      <td>847.83</td>\n",
       "      <td>4.70</td>\n",
       "      <td>5.05</td>\n",
       "      <td>0.17</td>\n",
       "      <td>859.45</td>\n",
       "      <td>1954</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171000</th>\n",
       "      <td>69</td>\n",
       "      <td>63</td>\n",
       "      <td>647</td>\n",
       "      <td>895.10</td>\n",
       "      <td>1998</td>\n",
       "      <td>885.57</td>\n",
       "      <td>4.53</td>\n",
       "      <td>4.84</td>\n",
       "      <td>0.15</td>\n",
       "      <td>856.93</td>\n",
       "      <td>1891</td>\n",
       "      <td>847.07</td>\n",
       "      <td>4.73</td>\n",
       "      <td>5.01</td>\n",
       "      <td>0.12</td>\n",
       "      <td>859.45</td>\n",
       "      <td>1954</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172000</th>\n",
       "      <td>67</td>\n",
       "      <td>63</td>\n",
       "      <td>1647</td>\n",
       "      <td>895.08</td>\n",
       "      <td>1998</td>\n",
       "      <td>885.54</td>\n",
       "      <td>4.54</td>\n",
       "      <td>4.84</td>\n",
       "      <td>0.15</td>\n",
       "      <td>856.88</td>\n",
       "      <td>1891</td>\n",
       "      <td>846.96</td>\n",
       "      <td>4.73</td>\n",
       "      <td>5.05</td>\n",
       "      <td>0.13</td>\n",
       "      <td>859.45</td>\n",
       "      <td>1954</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173000</th>\n",
       "      <td>68</td>\n",
       "      <td>63</td>\n",
       "      <td>2647</td>\n",
       "      <td>895.06</td>\n",
       "      <td>1998</td>\n",
       "      <td>885.52</td>\n",
       "      <td>4.54</td>\n",
       "      <td>4.84</td>\n",
       "      <td>0.15</td>\n",
       "      <td>856.71</td>\n",
       "      <td>1886</td>\n",
       "      <td>846.82</td>\n",
       "      <td>4.73</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>859.45</td>\n",
       "      <td>1954</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174000</th>\n",
       "      <td>69</td>\n",
       "      <td>64</td>\n",
       "      <td>943</td>\n",
       "      <td>895.04</td>\n",
       "      <td>1997</td>\n",
       "      <td>885.50</td>\n",
       "      <td>4.54</td>\n",
       "      <td>4.84</td>\n",
       "      <td>0.15</td>\n",
       "      <td>858.14</td>\n",
       "      <td>1919</td>\n",
       "      <td>848.33</td>\n",
       "      <td>4.69</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>859.45</td>\n",
       "      <td>1954</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       TRAIN:                                  VALID:        \\\n",
       "       Time  Ep    Ct    LOSS   PPL     NLL    KL GAUSS   REG    LOSS   PPL   \n",
       "1000     85   0   999  928.91  2691  923.50  1.60  3.57  0.24  888.19  2519   \n",
       "2000     71   0  1999  921.96  2538  916.06  2.13  3.53  0.24  880.70  2365   \n",
       "3000     65   1   295  919.36  2468  913.31  2.42  3.39  0.24  878.76  2315   \n",
       "4000     67   1  1295  916.82  2422  910.56  2.62  3.40  0.23  873.13  2193   \n",
       "5000     66   1  2295  914.61  2385  908.19  2.77  3.43  0.23  872.92  2201   \n",
       "...     ...  ..   ...     ...   ...     ...   ...   ...   ...     ...   ...   \n",
       "170000   69  62  2351  895.10  1999  885.58  4.53  4.83  0.15  857.76  1905   \n",
       "171000   69  63   647  895.10  1998  885.57  4.53  4.84  0.15  856.93  1891   \n",
       "172000   67  63  1647  895.08  1998  885.54  4.54  4.84  0.15  856.88  1891   \n",
       "173000   68  63  2647  895.06  1998  885.52  4.54  4.84  0.15  856.71  1886   \n",
       "174000   69  64   943  895.04  1997  885.50  4.54  4.84  0.15  858.14  1919   \n",
       "\n",
       "                                   TEST:       SPEC:             HIER:        \n",
       "           NLL    KL GAUSS   REG    LOSS   PPL     1     2     3 CHILD OTHER  \n",
       "1000    881.96  1.91  4.08  0.24  885.92  2467  0.25  0.22  0.20  0.94  0.86  \n",
       "2000    874.29  3.13  2.93  0.35  879.34  2362  0.33  0.22  0.22  0.94  0.84  \n",
       "3000    871.83  3.15  3.54  0.24  878.41  2325  0.17  0.23  0.24  0.93  0.81  \n",
       "4000    865.86  3.58  3.50  0.19  877.21  2311  0.24  0.22  0.24  0.93  0.79  \n",
       "5000    865.64  3.50  3.59  0.20  877.21  2311  0.18  0.21  0.25  0.93  0.84  \n",
       "...        ...   ...   ...   ...     ...   ...   ...   ...   ...   ...   ...  \n",
       "170000  847.83  4.70  5.05  0.17  859.45  1954  0.20  0.25  0.31  0.83  0.68  \n",
       "171000  847.07  4.73  5.01  0.12  859.45  1954  0.22  0.24  0.30  0.85  0.69  \n",
       "172000  846.96  4.73  5.05  0.13  859.45  1954  0.22  0.25  0.32  0.84  0.72  \n",
       "173000  846.82  4.73  5.00  0.16  859.45  1954  0.21  0.22  0.31  0.85  0.72  \n",
       "174000  848.33  4.69  5.00  0.12  859.45  1954  0.19  0.22  0.31  0.82  0.69  \n",
       "\n",
       "[174 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 R: 1.000 P: 0.039 meat beef fried portions soup cooked fish steak waitress dishes\n",
      "   1 R: 0.355 P: 0.024 meat fish dishes shrimp fried dish pork steak enjoyed fantastic\n",
      "     11 R: 0.172 P: 0.172 rice thai mexican tacos chinese pho soup rolls spicy beef\n",
      "     12 R: 0.061 P: 0.061 rice dishes meat shrimp dish portions pork fish flavor beef\n",
      "     13 R: 0.098 P: 0.098 steak wine wonderful enjoyed fantastic cooked bread attentive husband italian\n",
      "   2 R: 0.210 P: 0.007 coffee cream ice free options inside tea wonderful found kind\n",
      "     21 R: 0.004 P: 0.004 sandwich coffee free options huge line bread stop open large\n",
      "     22 R: 0.104 P: 0.104 coffee cream ice tea chocolate cake flavors donuts shop flavor\n",
      "     23 R: 0.096 P: 0.096 sandwich burgers sandwiches bacon coffee eggs toast bread brunch pancakes\n",
      "   3 R: 0.171 P: 0.028 check years music cool asked told free decent room money\n",
      "     31 R: 0.001 P: 0.001 check asked free years cool manager decent found helpful told\n",
      "     32 R: 0.007 P: 0.007 wings manager check asked waitress cool decent years told music\n",
      "     33 R: 0.135 P: 0.135 wings beers waitress bartender music patio burgers game bartenders manager\n",
      "   4 R: 0.002 P: 0.001 found helpful years asked wanted today check told things wo\n",
      "     41 R: 0.000 P: 0.000 wonderful years free found check fantastic absolutely wanted care job\n",
      "     42 R: 0.000 P: 0.000 years found huge decent free full asked options check today\n",
      "     43 R: 0.001 P: 0.001 sandwich bread steak meat huge fantastic husband cooked wonderful large\n",
      "   5 R: 0.001 P: 0.000 helpful years found check things wonderful items money free asked\n",
      "     51 R: 0.000 P: 0.000 check cool told asked manager care years music helpful free\n",
      "     52 R: 0.000 P: 0.000 asked found items today helpful told manager wanted years business\n",
      "     53 R: 0.000 P: 0.000 free helpful large huge check items found wonderful enjoyed years\n",
      "   6 R: 0.222 P: 0.002 years car helpful room check money free care job found\n",
      "     61 R: 0.066 P: 0.066 room show pool hotel rooms stay music theater play kids\n",
      "     62 R: 0.044 P: 0.044 store helpful stores buy shopping employees line parking check shop\n",
      "     63 R: 0.110 P: 0.110 car job nails salon professional nail appointment care hair company\n",
      "[ -6.8735294 -24.7944    -14.273019  -12.744328   -6.5592575  -6.501658\n",
      "  -6.9204035 -45.763798  -32.74842   -41.96528   -14.480558  -30.48465\n",
      " -32.446068  -12.36282   -13.02422   -23.111229   -6.5422974  -6.5604925\n",
      "  -6.5972185  -6.5047803  -6.537445   -6.519479  -17.786057  -16.396152\n",
      " -25.845116 ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-01d99103f841>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mrecur_prob_topic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mparent_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic_prob_topic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchild_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchild_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecur_child_idxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparent_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecur_child_idxs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdescendant_idxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mdepth_specs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_topic_specialization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstances_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mhierarchical_affinities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_hierarchical_affinity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tsntm/evaluation.py\u001b[0m in \u001b[0;36mget_topic_specialization\u001b[0;34m(sess, model, instances, verbose)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_topic_specialization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mtopics_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_bow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mtopics_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_bow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "while epoch < config.n_epochs:\n",
    "    # train\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = model.get_feed_dict(batch)\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch, ppls_batch, global_step_log = \\\n",
    "        sess.run([model.opt, model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_gauss, model.topic_loss_reg, model.topic_ppls, tf.train.get_global_step()], feed_dict = feed_dict)\n",
    "\n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "#         if global_step_log % config.log_period == 0:\n",
    "        if global_step_log % 1000 == 0:\n",
    "            # validate\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_gauss_train, topic_loss_reg_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_gauss_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "\n",
    "            # test\n",
    "            if ppl_dev < ppl_min:\n",
    "                ppl_min = ppl_dev\n",
    "                loss_test, _, _, _, _, ppl_test, _ = validate(sess, test_batches, model)\n",
    "                saver.save(sess, config.path_model, global_step=global_step_log)\n",
    "                cPickle.dump(config, open(config.path_config % global_step_log, 'wb'))\n",
    "                update_checkpoint(config, checkpoint, global_step_log)\n",
    "            \n",
    "            # visualize topic\n",
    "            topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "            topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "            topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "            topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "            descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "            recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "            \n",
    "            depth_specs = get_topic_specialization(sess, model, instances_test)\n",
    "            hierarchical_affinities = get_hierarchical_affinity(sess, model)\n",
    "            \n",
    "            # log\n",
    "            clear_output()\n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_gauss_train, '%.2f'%topic_loss_reg_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_gauss_dev, '%.2f'%topic_loss_reg_dev, \\\n",
    "                    '%.2f'%loss_test, '%.0f'%ppl_test, \\\n",
    "                    '%.2f'%depth_specs[1], '%.2f'%depth_specs[2], '%.2f'%depth_specs[3], \\\n",
    "                    '%.2f'%hierarchical_affinities[0], '%.2f'%hierarchical_affinities[1]],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "            cPickle.dump(log_df, open(os.path.join(config.path_log), 'wb'))\n",
    "            print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)\n",
    "            print(np.sum(debug(model.topic_logvars), 1))\n",
    "            \n",
    "            # update tree\n",
    "            if not config.static:\n",
    "                config.tree_idxs, update_tree_flg = model.update_tree(topic_prob_topic, recur_prob_topic)\n",
    "                if update_tree_flg:\n",
    "                    print(config.tree_idxs)\n",
    "                    name_variables = {tensor.name: variable for tensor, variable in zip(tf.global_variables(), sess.run(tf.global_variables()))} # store paremeters\n",
    "                    if 'sess' in globals(): sess.close()\n",
    "                    model = HierarchicalNeuralTopicModel(config)\n",
    "                    sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "                    name_tensors = {tensor.name: tensor for tensor in tf.global_variables()}\n",
    "                    sess.run([name_tensors[name].assign(variable) for name, variable in name_variables.items()]) # restore parameters\n",
    "                    saver = tf.train.Saver(max_to_keep=1)\n",
    "                \n",
    "            time_start = time.time()\n",
    "\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "    epoch += 1\n",
    "\n",
    "loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "display(log_df)\n",
    "print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'topic/enc/hidden_bow/kernel:0' shape=(6177, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/enc/hidden_bow/bias:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/enc/mean_bow/kernel:0' shape=(256, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/enc/mean_bow/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/enc/logvar_bow/kernel:0' shape=(256, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/enc/logvar_bow/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/enc/sticks_path/init_state_parent:0' shape=(1, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/enc/sticks_path/input/ancestral/kernel:0' shape=(32, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/enc/sticks_path/input/ancestral/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/enc/sticks_path/input/fraternal/kernel:0' shape=(32, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/enc/sticks_path/input/fraternal/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/enc/sticks_path/output/hidden/kernel:0' shape=(32, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/enc/sticks_path/output/hidden/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/enc/sticks_depth/init_state_parent:0' shape=(1, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/enc/sticks_depth/input/ancestral/kernel:0' shape=(32, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/enc/sticks_depth/input/ancestral/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/enc/sticks_depth/input/fraternal/kernel:0' shape=(32, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/enc/sticks_depth/input/fraternal/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/enc/sticks_depth/output/hidden/kernel:0' shape=(32, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/enc/sticks_depth/output/hidden/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'shared/emb:0' shape=(6177, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/dec/emb_topic/init_state_parent:0' shape=(1, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/dec/emb_topic/input/ancestral/kernel:0' shape=(256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/dec/emb_topic/input/ancestral/bias:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/dec/emb_topic/input/fraternal/kernel:0' shape=(256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/dec/emb_topic/input/fraternal/bias:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/dec/emb_topic/output/hidden/kernel:0' shape=(256, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/dec/emb_topic/output/hidden/bias:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/dec/topic_means/kernel:0' shape=(256, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/dec/topic_means/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/dec/topic_logvars/kernel:0' shape=(256, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/dec/topic_logvars/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/dec/topic_bow/kernel:0' shape=(32, 6177) dtype=float32_ref>,\n",
       " <tf.Variable 'topic/dec/topic_bow/bias:0' shape=(6177,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_year(sample_batches):\n",
    "    probs_topics = []\n",
    "    years = []\n",
    "    for i, sample_batch in sample_batches:\n",
    "        probs_topics_batch = sess.run(model.prob_topic, feed_dict=model.get_feed_dict(sample_batch, mode='test'))\n",
    "        years_batch = [instance.year for instance in sample_batch]\n",
    "        probs_topics += [probs_topics_batch]\n",
    "        years += years_batch\n",
    "    probs_topics = np.concatenate(probs_topics)\n",
    "    years = np.array(years)\n",
    "\n",
    "    topic_years = years.dot(probs_topics) / np.sum(probs_topics, 0)\n",
    "    topic_year = {model.topic_idxs[i]: year for i, year in enumerate(topic_years)}\n",
    "    return topic_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Avg Year: 2005 level analysis english systems domain lexical linguistic test process lexicon\n",
      "   1 Avg Year: 2000 grammar structure rules semantic form representation rule lexical type structures\n",
      "     11 Avg Year: 2006 verb semantic syntactic verbs event relations discourse argument relation arguments\n",
      "     12 Avg Year: 2007 tree parsing dependency parser grammar trees node parse rules syntactic\n",
      "   2 Avg Year: 2008 english languages dictionary errors morphological chinese lexical rules corpora pos\n",
      "     21 Avg Year: 2009 entity sense relations relation semantic entities wordnet senses lexical patterns\n",
      "     22 Avg Year: 2010 translation english source alignment phrase target languages sentences systems parallel\n",
      "   3 Avg Year: 2008 models probability training algorithm probabilities search parameters sequence segmentation gram\n",
      "     31 Avg Year: 2011 features feature training performance learning classifier classification accuracy test class\n",
      "     32 Avg Year: 2015 models network vector embeddings neural training vectors representations embedding input\n",
      "   4 Avg Year: 2007 knowledge semantic terms term natural values context type representation concept\n",
      "     41 Avg Year: 2009 question query terms documents answer questions document domain term web\n",
      "     42 Avg Year: 2010 similarity topic document sentences method score scores measure clustering pairs\n",
      "   5 Avg Year: 2007 user annotation resources project tools tool database research interface linguistic\n",
      "     51 Avg Year: 2007 speech dialogue user speaker utterance utterances recognition spoken human speakers\n",
      "     52 Avg Year: 2013 sentiment tweets polarity twitter negative opinion positive social emotion annotators\n"
     ]
    }
   ],
   "source": [
    "sample_batches = get_batches(instances_train, config.batch_size)\n",
    "topic_year = get_topic_year(sample_batches)\n",
    "print_topic_year(sess, model, topic_freq_tokens=topic_freq_tokens, topic_year=topic_year)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
