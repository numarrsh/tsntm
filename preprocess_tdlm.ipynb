{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import _pickle as cPickle\n",
    "from collections import OrderedDict, defaultdict, Counter\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "from data_structure import Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('train_path', 'data/apnews/apnews50k_train.txt', 'path of output data')\n",
    "flags.DEFINE_string('valid_path', 'data/apnews/apnews50k_valid.txt', 'path of input data')\n",
    "flags.DEFINE_string('test_path', 'data/apnews/apnews50k_test.txt', 'path of input data')\n",
    "\n",
    "flags.DEFINE_string('stopwords_path', 'data/stopwords_mallet.txt', 'path of input data')\n",
    "\n",
    "flags.DEFINE_string('output_path', 'data/apnews/instances.pkl', 'path of output data')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', 50000, 'size of vocab')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "config = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special tokens\n",
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences\n",
    "dummy_tokens = [PAD, UNK, BOS, EOS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(data_path):\n",
    "    data_dict = {}\n",
    "    with open(data_path, 'r') as f:\n",
    "        txt = f.readlines()\n",
    "        tokens_list = []\n",
    "        doc_list = []\n",
    "        for doc in txt:\n",
    "            doc = re.sub(r'-', ' ', doc)\n",
    "            doc = re.sub(r'[0-9]+.[0-9]+|[0-9]+,[0-9]+|[0-9]+', '<num>', doc)\n",
    "            lines = doc.split('\\t')\n",
    "            doc = ' '.join(lines)\n",
    "            doc_list.append(doc)\n",
    "            tokens = [word_tokenize(line)[:-1] for line in lines]\n",
    "            tokens_list.append(tokens)\n",
    "            doc_l = len(tokens)\n",
    "            max_sent_l = max([len(line) for line in tokens])\n",
    "        data_dict['doc'] = doc_list\n",
    "        data_dict['tokens'] = tokens_list\n",
    "        data_dict['doc_l'] = doc_l\n",
    "        data_dict['max_sent_l'] = max_sent_l\n",
    "    data_df = pd.DataFrame(data_dict)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = get_df(config.train_path)\n",
    "valid_df = get_df(config.valid_path)\n",
    "test_df = get_df(config.test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2000, 2000)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df), len(valid_df), len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build vocab for language modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_cnt_dict(train_df, min_tf=None):\n",
    "    # create vocab of words\n",
    "    word_cnt_dict = defaultdict(int)\n",
    "    word_cnt_dict['.'] = np.inf\n",
    "    word_cnt_dict[EOS] = np.inf\n",
    "    word_cnt_dict[BOS] = np.inf\n",
    "    word_cnt_dict[UNK] = np.inf\n",
    "    word_cnt_dict[PAD] = np.inf\n",
    "    \n",
    "    tokens_list = []\n",
    "    for doc in train_df.tokens:\n",
    "        tokens_list.extend(doc)\n",
    "    \n",
    "    for tokens in tokens_list:\n",
    "        for word in tokens:\n",
    "            word_cnt_dict[word] += 1\n",
    "    word_cnt_dict = sorted(word_cnt_dict.items(), key=lambda x: x[1])[::-1]\n",
    "    \n",
    "    if type(min_tf) is int:\n",
    "        word_cnt_dict = list(filter(lambda x: x[1] > min_tf, word_cnt_dict))\n",
    "    elif type(min_tf) is float:\n",
    "        word_cnt_dict = word_cnt_dict[:int(min_tf*len(word_cnt_dict))]\n",
    "    return word_cnt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29662\n"
     ]
    }
   ],
   "source": [
    "word_cnt_dict = get_word_cnt_dict(train_df, min_tf=10)\n",
    "print(len(word_cnt_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_word = {idx: word for idx, (word, cnt) in enumerate(word_cnt_dict)}\n",
    "word_to_idx = {word: idx for idx, word in idx_to_word.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build bow vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_word_cnt_dict = get_word_cnt_dict(train_df, min_tf=0.001)\n",
    "len(stop_word_cnt_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(config.stopwords_path, 'r') as f:\n",
    "    stop_words_mallet = [w.replace('\\n', '') for w in f.readlines()]\n",
    "len(stop_words_mallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "642"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stop_words_mallet + [w_cnt[0] for w_cnt in stop_word_cnt_dict if w_cnt[0] not in dummy_tokens]\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['u.s'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6734\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=100, max_df=1.0, stop_words=stop_words, tokenizer=word_tokenize, norm='l1', use_idf=False, smooth_idf=False, dtype=np.float32)\n",
    "corpus = list(train_df.doc)\n",
    "bow_list = vectorizer.fit_transform(corpus)\n",
    "bow_features = vectorizer.get_feature_names()\n",
    "print(len(bow_features))\n",
    "assert len(train_df) == len(bow_list.toarray())\n",
    "assert all([word in word_to_idx for word in bow_features])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8752268126134284e-18"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_doc_idx = 1\n",
    "sample_doc = corpus[0]\n",
    "sample_tokens = word_tokenize(sample_doc)\n",
    "filtered_tokens = [w for w in sample_tokens if (w not in vectorizer.stop_words_) and (w not in stop_words)]\n",
    "assert all([word in bow_features for word in filtered_tokens])\n",
    "\n",
    "filtered_word_cnt_dict = Counter(filtered_tokens)\n",
    "pseudo_bow = np.array([float(filtered_word_cnt_dict[w]) if w in filtered_word_cnt_dict else 0. for w in bow_features])\n",
    "pseudo_bow = pseudo_bow / np.sum(pseudo_bow)\n",
    "sample_bow = bow_list.toarray()[0]\n",
    "np.max((sample_bow - pseudo_bow)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_instances(data_df, word_to_idx, bow_list=None):\n",
    "    instances = []\n",
    "    if bow_list is not None: bows = bow_list.toarray()\n",
    "    for idx_doc, doc in data_df.iterrows():\n",
    "        instance = Instance()\n",
    "        instance.idx = idx_doc\n",
    "        doc_token_idxs = []\n",
    "        for sent_tokens in doc.tokens:\n",
    "            sent_token_idxs = [word_to_idx[token] if token in word_to_idx else word_to_idx[UNK] for token in sent_tokens]\n",
    "            doc_token_idxs.append(sent_token_idxs)            \n",
    "        instance.token_idxs = doc_token_idxs\n",
    "        instance.doc_l = doc.doc_l\n",
    "        instance.max_sent_l = doc.max_sent_l\n",
    "        if bow_list is not None: instance.bow = bows[idx_doc]\n",
    "        instances.append(instance)\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train = prepare_instances(train_df, word_to_idx, bow_list=bow_list)\n",
    "instances_valid = prepare_instances(valid_df, word_to_idx)\n",
    "instances_test = prepare_instances(test_df, word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving preprocessed instances...\n"
     ]
    }
   ],
   "source": [
    "print('saving preprocessed instances...')\n",
    "cPickle.dump((instances_train, instances_valid, instances_test, word_to_idx, idx_to_word, bow_features),open(config.output_path,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
