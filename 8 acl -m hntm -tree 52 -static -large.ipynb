{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from data_structure import get_batches\n",
    "from hntm import HierarchicalNeuralTopicModel\n",
    "from tree import get_descendant_idxs\n",
    "from evaluation import validate, get_hierarchical_affinity, get_topic_specialization, print_topic_sample\n",
    "from configure import get_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(nb_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu\n",
    "np.random.seed(config.seed)\n",
    "random.seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.path_data,'rb'))\n",
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)\n",
    "config.dim_bow = len(bow_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0,
     10
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables, model):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, model, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    return _variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = []\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "ppl_min = np.inf\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','','','','','VALID:','','','','','TEST:','', 'SPEC:', '', '', 'HIER:', ''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL','REG','LOSS','PPL','NLL','KL','REG','LOSS','PPL', '1', '2', '3', 'CHILD', 'OTHER']]))))\n",
    "\n",
    "cmd_rm = 'rm -r %s' % config.dir_model\n",
    "res = subprocess.call(cmd_rm.split())\n",
    "cmd_mk = 'mkdir %s' % config.dir_model\n",
    "res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "def update_checkpoint(config, checkpoint, global_step):\n",
    "    checkpoint.append(config.path_model + '-%i' % global_step)\n",
    "    if len(checkpoint) > config.max_to_keep:\n",
    "        path_model = checkpoint.pop(0) + '.*'\n",
    "        for p in glob.glob(path_model):\n",
    "            os.remove(p)\n",
    "    cPickle.dump(checkpoint, open(config.path_checkpoint, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "model = HierarchicalNeuralTopicModel(config)\n",
    "sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(max_to_keep=config.max_to_keep)\n",
    "update_tree_flg = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train & validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th colspan=\"4\" halign=\"left\"></th>\n",
       "      <th>VALID:</th>\n",
       "      <th colspan=\"4\" halign=\"left\"></th>\n",
       "      <th>TEST:</th>\n",
       "      <th></th>\n",
       "      <th>SPEC:</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "      <th>HIER:</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>CHILD</th>\n",
       "      <th>OTHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>79</td>\n",
       "      <td>8</td>\n",
       "      <td>463</td>\n",
       "      <td>9171.62</td>\n",
       "      <td>1857</td>\n",
       "      <td>9150.76</td>\n",
       "      <td>20.58</td>\n",
       "      <td>0.28</td>\n",
       "      <td>8881.78</td>\n",
       "      <td>1728</td>\n",
       "      <td>8858.38</td>\n",
       "      <td>23.19</td>\n",
       "      <td>0.21</td>\n",
       "      <td>8881.62</td>\n",
       "      <td>1727</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>75</td>\n",
       "      <td>17</td>\n",
       "      <td>360</td>\n",
       "      <td>9128.46</td>\n",
       "      <td>1791</td>\n",
       "      <td>9106.47</td>\n",
       "      <td>21.75</td>\n",
       "      <td>0.23</td>\n",
       "      <td>8863.84</td>\n",
       "      <td>1703</td>\n",
       "      <td>8840.98</td>\n",
       "      <td>22.68</td>\n",
       "      <td>0.18</td>\n",
       "      <td>8863.35</td>\n",
       "      <td>1702</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>74</td>\n",
       "      <td>26</td>\n",
       "      <td>257</td>\n",
       "      <td>9109.16</td>\n",
       "      <td>1763</td>\n",
       "      <td>9086.96</td>\n",
       "      <td>21.99</td>\n",
       "      <td>0.21</td>\n",
       "      <td>8860.67</td>\n",
       "      <td>1696</td>\n",
       "      <td>8838.13</td>\n",
       "      <td>22.38</td>\n",
       "      <td>0.17</td>\n",
       "      <td>8860.50</td>\n",
       "      <td>1697</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>74</td>\n",
       "      <td>35</td>\n",
       "      <td>154</td>\n",
       "      <td>9098.09</td>\n",
       "      <td>1748</td>\n",
       "      <td>9075.77</td>\n",
       "      <td>22.04</td>\n",
       "      <td>0.20</td>\n",
       "      <td>8857.72</td>\n",
       "      <td>1693</td>\n",
       "      <td>8835.48</td>\n",
       "      <td>22.08</td>\n",
       "      <td>0.16</td>\n",
       "      <td>8857.56</td>\n",
       "      <td>1693</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>74</td>\n",
       "      <td>44</td>\n",
       "      <td>51</td>\n",
       "      <td>9090.61</td>\n",
       "      <td>1737</td>\n",
       "      <td>9068.32</td>\n",
       "      <td>22.03</td>\n",
       "      <td>0.19</td>\n",
       "      <td>8857.16</td>\n",
       "      <td>1693</td>\n",
       "      <td>8835.18</td>\n",
       "      <td>21.82</td>\n",
       "      <td>0.16</td>\n",
       "      <td>8856.29</td>\n",
       "      <td>1692</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425000</th>\n",
       "      <td>78</td>\n",
       "      <td>749</td>\n",
       "      <td>316</td>\n",
       "      <td>9050.99</td>\n",
       "      <td>1681</td>\n",
       "      <td>9029.04</td>\n",
       "      <td>21.01</td>\n",
       "      <td>0.13</td>\n",
       "      <td>8861.92</td>\n",
       "      <td>1700</td>\n",
       "      <td>8841.39</td>\n",
       "      <td>20.42</td>\n",
       "      <td>0.10</td>\n",
       "      <td>8856.98</td>\n",
       "      <td>1693</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430000</th>\n",
       "      <td>84</td>\n",
       "      <td>758</td>\n",
       "      <td>213</td>\n",
       "      <td>9050.93</td>\n",
       "      <td>1681</td>\n",
       "      <td>9028.96</td>\n",
       "      <td>21.01</td>\n",
       "      <td>0.13</td>\n",
       "      <td>8861.79</td>\n",
       "      <td>1700</td>\n",
       "      <td>8841.21</td>\n",
       "      <td>20.49</td>\n",
       "      <td>0.10</td>\n",
       "      <td>8856.98</td>\n",
       "      <td>1693</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435000</th>\n",
       "      <td>80</td>\n",
       "      <td>767</td>\n",
       "      <td>110</td>\n",
       "      <td>9050.86</td>\n",
       "      <td>1681</td>\n",
       "      <td>9028.89</td>\n",
       "      <td>21.01</td>\n",
       "      <td>0.13</td>\n",
       "      <td>8863.22</td>\n",
       "      <td>1702</td>\n",
       "      <td>8842.66</td>\n",
       "      <td>20.46</td>\n",
       "      <td>0.10</td>\n",
       "      <td>8856.98</td>\n",
       "      <td>1693</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440000</th>\n",
       "      <td>80</td>\n",
       "      <td>776</td>\n",
       "      <td>7</td>\n",
       "      <td>9050.79</td>\n",
       "      <td>1680</td>\n",
       "      <td>9028.81</td>\n",
       "      <td>21.00</td>\n",
       "      <td>0.13</td>\n",
       "      <td>8863.02</td>\n",
       "      <td>1702</td>\n",
       "      <td>8842.48</td>\n",
       "      <td>20.44</td>\n",
       "      <td>0.10</td>\n",
       "      <td>8856.98</td>\n",
       "      <td>1693</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445000</th>\n",
       "      <td>81</td>\n",
       "      <td>784</td>\n",
       "      <td>471</td>\n",
       "      <td>9050.72</td>\n",
       "      <td>1680</td>\n",
       "      <td>9028.74</td>\n",
       "      <td>21.00</td>\n",
       "      <td>0.13</td>\n",
       "      <td>8861.26</td>\n",
       "      <td>1699</td>\n",
       "      <td>8840.74</td>\n",
       "      <td>20.42</td>\n",
       "      <td>0.10</td>\n",
       "      <td>8856.98</td>\n",
       "      <td>1693</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        TRAIN:                               VALID:        \\\n",
       "       Time   Ep   Ct     LOSS   PPL      NLL     KL   REG     LOSS   PPL   \n",
       "5000     79    8  463  9171.62  1857  9150.76  20.58  0.28  8881.78  1728   \n",
       "10000    75   17  360  9128.46  1791  9106.47  21.75  0.23  8863.84  1703   \n",
       "15000    74   26  257  9109.16  1763  9086.96  21.99  0.21  8860.67  1696   \n",
       "20000    74   35  154  9098.09  1748  9075.77  22.04  0.20  8857.72  1693   \n",
       "25000    74   44   51  9090.61  1737  9068.32  22.03  0.19  8857.16  1693   \n",
       "...     ...  ...  ...      ...   ...      ...    ...   ...      ...   ...   \n",
       "425000   78  749  316  9050.99  1681  9029.04  21.01  0.13  8861.92  1700   \n",
       "430000   84  758  213  9050.93  1681  9028.96  21.01  0.13  8861.79  1700   \n",
       "435000   80  767  110  9050.86  1681  9028.89  21.01  0.13  8863.22  1702   \n",
       "440000   80  776    7  9050.79  1680  9028.81  21.00  0.13  8863.02  1702   \n",
       "445000   81  784  471  9050.72  1680  9028.74  21.00  0.13  8861.26  1699   \n",
       "\n",
       "                                TEST:       SPEC:             HIER:        \n",
       "            NLL     KL   REG     LOSS   PPL     1     2     3 CHILD OTHER  \n",
       "5000    8858.38  23.19  0.21  8881.62  1727  0.42  0.39  0.46  0.33  0.25  \n",
       "10000   8840.98  22.68  0.18  8863.35  1702  0.46  0.42  0.47  0.30  0.24  \n",
       "15000   8838.13  22.38  0.17  8860.50  1697  0.45  0.43  0.48  0.30  0.24  \n",
       "20000   8835.48  22.08  0.16  8857.56  1693  0.49  0.44  0.49  0.29  0.23  \n",
       "25000   8835.18  21.82  0.16  8856.29  1692  0.48  0.44  0.48  0.29  0.23  \n",
       "...         ...    ...   ...      ...   ...   ...   ...   ...   ...   ...  \n",
       "425000  8841.39  20.42  0.10  8856.98  1693  0.50  0.44  0.49  0.27  0.23  \n",
       "430000  8841.21  20.49  0.10  8856.98  1693  0.51  0.44  0.49  0.27  0.23  \n",
       "435000  8842.66  20.46  0.10  8856.98  1693  0.51  0.44  0.49  0.27  0.23  \n",
       "440000  8842.48  20.44  0.10  8856.98  1693  0.50  0.44  0.49  0.28  0.23  \n",
       "445000  8840.74  20.42  0.10  8856.98  1693  0.50  0.44  0.49  0.28  0.24  \n",
       "\n",
       "[89 rows x 20 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 R: 1.000 P: 0.098 tree grammar rules node algorithm nodes rule structure parsing trees\n",
      "   1 R: 0.210 P: 0.060 features sentiment classification tweets positive negative analysis feature polarity opinion\n",
      "     11 R: 0.091 P: 0.091 verb semantic syntactic structure verbs sentences phrase argument discourse noun\n",
      "     12 R: 0.059 P: 0.059 user dialogue utterance human utterances speech users systems participants dialog\n",
      "   2 R: 0.157 P: 0.047 models neural embeddings network vector vectors representations layer embedding training\n",
      "     21 R: 0.040 P: 0.040 relation event entity relations entities events features coreference mentions temporal\n",
      "     22 R: 0.070 P: 0.070 features training dependency performance parser parsing pos feature accuracy models\n",
      "   3 R: 0.198 P: 0.081 training features feature models learning performance algorithm method probability domain\n",
      "     31 R: 0.066 P: 0.066 question query terms answer questions patterns term extraction domain documents\n",
      "     32 R: 0.051 P: 0.051 speech error errors segmentation character recognition test characters chinese speakers\n",
      "   4 R: 0.202 P: 0.062 document topic sentences similarity documents evaluation human scores topics summary\n",
      "     41 R: 0.063 P: 0.063 semantic sense lexical wordnet similarity senses context relations concepts concept\n",
      "     42 R: 0.077 P: 0.077 annotation texts linguistic research project resources user tools tool annotations\n",
      "   5 R: 0.135 P: 0.058 translation phrase source training target english models systems bleu alignment\n",
      "     51 R: 0.043 P: 0.043 morphological lexicon forms form rules arabic languages noun verb dictionary\n",
      "     52 R: 0.034 P: 0.034 translation english languages alignment parallel source target chinese translations pairs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8ea66be3d146>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m# validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_recon_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_kl_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_reg_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mppl_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mppls_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mloss_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_recon_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_kl_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_reg_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppl_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs_topic_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   3116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3117\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 3118\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   3119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m     \"\"\"\n\u001b[0;32m--> 591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "while epoch < config.n_epochs:\n",
    "    # train\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = model.get_feed_dict(batch)\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, ppls_batch, global_step_log = \\\n",
    "        sess.run([model.opt, model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_reg, model.topic_ppls, tf.train.get_global_step()], feed_dict = feed_dict)\n",
    "\n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if global_step_log % config.log_period == 0:\n",
    "            # validate\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "\n",
    "            # test\n",
    "            if ppl_dev < ppl_min:\n",
    "                ppl_min = ppl_dev\n",
    "                loss_test, _, _, _, ppl_test, _ = validate(sess, test_batches, model)\n",
    "                saver.save(sess, config.path_model, global_step=global_step_log)\n",
    "                cPickle.dump(config, open(config.path_config % global_step_log, 'wb'))\n",
    "                update_checkpoint(config, checkpoint, global_step_log)\n",
    "            \n",
    "            # visualize topic\n",
    "            topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "            topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "            topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "            topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "            descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "            recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "            \n",
    "            depth_specs = get_topic_specialization(sess, model, instances_test)\n",
    "            hierarchical_affinities = get_hierarchical_affinity(sess, model)\n",
    "            \n",
    "            # log\n",
    "            clear_output()\n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_reg_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_reg_dev, \\\n",
    "                    '%.2f'%loss_test, '%.0f'%ppl_test, \\\n",
    "                    '%.2f'%depth_specs[1], '%.2f'%depth_specs[2], '%.2f'%depth_specs[3], \\\n",
    "                    '%.2f'%hierarchical_affinities[0], '%.2f'%hierarchical_affinities[1]],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "            cPickle.dump(log_df, open(os.path.join(config.path_log), 'wb'))\n",
    "            print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)\n",
    "\n",
    "            # update tree\n",
    "            if not config.static:\n",
    "                config.tree_idxs, update_tree_flg = model.update_tree(topic_prob_topic, recur_prob_topic)\n",
    "                if update_tree_flg:\n",
    "                    print(config.tree_idxs)\n",
    "                    name_variables = {tensor.name: variable for tensor, variable in zip(tf.global_variables(), sess.run(tf.global_variables()))} # store paremeters\n",
    "                    if 'sess' in globals(): sess.close()\n",
    "                    model = HierarchicalNeuralTopicModel(config)\n",
    "                    sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "                    name_tensors = {tensor.name: tensor for tensor in tf.global_variables()}\n",
    "                    sess.run([name_tensors[name].assign(variable) for name, variable in name_variables.items()]) # restore parameters\n",
    "                    saver = tf.train.Saver(max_to_keep=1)\n",
    "                \n",
    "            time_start = time.time()\n",
    "\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "    epoch += 1\n",
    "\n",
    "loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "display(log_df)\n",
    "print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
