{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import pdb\n",
    "import _pickle as cPickle\n",
    "import time\n",
    "import subprocess\n",
    "import glob\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from ncrp import Topic, Doc, init, sample, get_perplexity, get_topic_specialization, get_hierarchical_affinities, get_freq_tokens_ncrp, get_docs, get_sum_cnt_words\n",
    "from configure import get_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load config & data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(nb_name)\n",
    "np.random.seed(config.seed)\n",
    "random.seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.path_data,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31943, 1035)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.n_doc = len(instances_train)\n",
    "config.n_vocab = len(bow_idxs)\n",
    "config.n_doc, config.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568401"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_bow = [instance.bow for instance in instances_train]\n",
    "docs_raw = [[[bow_index]*int(doc_bow[bow_index]) for bow_index in np.where(doc_bow > 0)[0]] for doc_bow in docs_bow]\n",
    "docs_words = [[idx for idxs in doc for idx in idxs] for doc in docs_raw]\n",
    "np.sum([len(doc_words) for doc_words in docs_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = []\n",
    "ppl_min = np.inf\n",
    "epoch = 0\n",
    "\n",
    "cmd_rm = 'rm -r %s' % config.dir_model\n",
    "res = subprocess.call(cmd_rm.split())\n",
    "cmd_mk = 'mkdir %s' % config.dir_model\n",
    "res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','VALID:','TEST:','SPEC:', '', '', 'HIER:', ''],\n",
    "                            ['Time','Ep','Ct','PPL','PPL', 'PPL','1', '2', '3', 'CHILD', 'OTHER']]))))\n",
    "\n",
    "def update_checkpoint(config, checkpoint, epoch):\n",
    "    checkpoint.append(config.path_model + '-%i' % epoch)\n",
    "    if len(checkpoint) > config.max_to_keep:\n",
    "        path_model = checkpoint.pop(0)\n",
    "        for p in glob.glob(path_model):\n",
    "            os.remove(p)\n",
    "    cPickle.dump(checkpoint, open(config.path_checkpoint, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10000 20000 30000 0 0 "
     ]
    }
   ],
   "source": [
    "topic_root = Topic(idx='0', sibling_idx=0, parent=None, depth=0, config=config)\n",
    "train_docs = get_docs(instances_train, config)\n",
    "dev_docs = get_docs(instances_dev, config)\n",
    "test_docs = get_docs(instances_test, config)\n",
    "init(train_docs, dev_docs, test_docs, topic_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th>VALID:</th>\n",
       "      <th>TEST:</th>\n",
       "      <th>SPEC:</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "      <th>HIER:</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>PPL</th>\n",
       "      <th>PPL</th>\n",
       "      <th>PPL</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>CHILD</th>\n",
       "      <th>OTHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>367</td>\n",
       "      <td>349</td>\n",
       "      <td>348</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>269</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>328</td>\n",
       "      <td>320</td>\n",
       "      <td>322</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>302</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>317</td>\n",
       "      <td>315</td>\n",
       "      <td>312</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>324</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>306</td>\n",
       "      <td>305</td>\n",
       "      <td>304</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>342</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>298</td>\n",
       "      <td>294</td>\n",
       "      <td>296</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>356</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>291</td>\n",
       "      <td>293</td>\n",
       "      <td>288</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             TRAIN: VALID: TEST: SPEC:             HIER:      \n",
       "  Time Ep Ct    PPL    PPL   PPL     1     2     3 CHILD OTHER\n",
       "0  222  0  0    367    349   348  0.09  0.50  0.64  0.47  0.34\n",
       "1  269  1  0    328    320   322  0.10  0.53  0.66  0.44  0.29\n",
       "2  302  2  0    317    315   312  0.10  0.52  0.67  0.46  0.29\n",
       "3  324  3  0    306    305   304  0.10  0.54  0.65  0.44  0.29\n",
       "4  342  4  0    298    294   296  0.10  0.54  0.65  0.37  0.27\n",
       "5  356  5  0    291    293   288  0.11  0.56  0.66  0.40  0.28"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 31943 280996.0 ! nice bought price quality love made perfect recommend put\n",
      "   0-1 2021 14305.0 ! love color cover mac perfectly recommend pro apple easy\n",
      "     0-1-2 162 322.0 keyboard cover pink green purple blue lighter darker hot picture\n",
      "     0-1-4 65 53.0 dark gift negative blue black file complaint beautiful husband sizes\n",
      "     0-1-5 1794 1753.0 cover keyboard perfectly smell purple kuzy typing packaging texture keys\n",
      "   0-2 778 6616.0 carry work travel back airport pack handle seat security easy\n",
      "     0-2-1 17 17.0 falls comfortably update purple wheels binder rain returned cost highly\n",
      "     0-2-2 747 1044.0 camera roll equipment lenses trip compartments wheels travelling friendly compact\n",
      "     0-2-3 14 9.0 sit sits floor messenger higher overnight stay recently personal feeling\n",
      "   0-3 1763 13307.0 pocket sleeve power mouse netbook charger room small inch extra\n",
      "     0-3-1 66 67.0 travelling usb accessory items suggest personally drives safe nice part\n",
      "     0-3-2 178 225.0 inch toshiba battery inches laptops lenovo ample purchased accommodate store\n",
      "     0-3-3 1282 1526.0 cord drive power usb external pouch pro cable adapter protection\n",
      "     0-3-4 237 241.0 player bigger dvd bit reasonable travelling luggage sony purpose edge\n",
      "   0-4 919 8407.0 bottom cover top months plastic piece corners part cracked color\n",
      "     0-4-1 610 882.0 cover bottom keyboard snap covers top rest keys piece clips\n",
      "     0-4-2 14 13.0 paper camera uncomfortable rear pouch separate mind cloth worn sharp\n",
      "     0-4-3 295 317.0 speck cracks base started broken year cracked fan cracking corners\n",
      "   0-5 1919 14995.0 ! carry room & ; pockets plenty pocket love ipad\n",
      "     0-5-1 4 5.0 interior 've cushioned making removable glad finally flexible flaw flat\n",
      "     0-5-2 159 467.0 ; & roller happy constructed trips mbp front wheels zipper\n",
      "     0-5-3 111 288.0 camera lenses lens canon equipment batteries flash tripod body room\n",
      "     0-5-4 1599 1752.0 ; & dvd camera highly kindle player plenty external wallet\n",
      "     0-5-5 46 25.0 true wonderful friends bucks friendly hard plain securely shell organize\n",
      "   0-6 3075 21362.0 ! cover color keyboard mac pro item ordered apple buy\n",
      "     0-6-1 270 1083.0 keyboard cover protector screen keys type key mouse typing match\n",
      "     0-6-2 1182 1659.0 return description item refund customer received shipping wrong shown service\n",
      "     0-6-3 1429 2225.0 cover keyboard mcover pink apple kuzy speck ipearl covers keys\n",
      "     0-6-4 194 201.0 feel dollars plug investment backpacks honestly -- saved feeling clear\n",
      "   0-7 351 2706.0 cards sd memory card small slots camera plastic works place\n",
      "     0-7-1 79 317.0 usb drive ports card hard power external port install original\n",
      "     0-7-2 272 202.0 camera lens canon hole handy run video traveling number doesnt\n",
      "   0-8 900 7984.0 broke zipper months strap handle straps started shoulder years zippers\n",
      "     0-8-1 148 217.0 customer service binder contacted lunch photos separate clothes heavy base\n",
      "     0-8-2 50 60.0 camera length equipment unit realized picked careful carry mention series\n",
      "     0-8-3 699 767.0 completely working weeks manufacturer handle falling service loved ago properly\n",
      "     0-8-4 3 3.0 poor car notebook zips flap finger fingerprints finish fitting fix\n",
      "   0-9 3738 26916.0 cover color mac bottom pro air scratches hard protection apple\n",
      "     0-9-1 203 371.0 smell smells chemical strong cute weird terrible bad days odor\n",
      "     0-9-2 605 1795.0 cover keyboard keys protector screen typing type mbp dust clear\n",
      "     0-9-3 619 1890.0 ; & retina mbp pro kuzy surface display late texture\n",
      "     0-9-4 2015 3306.0 cover keyboard screen mbp dirt blue ports protector clean top\n",
      "     0-9-5 85 86.0 order dropped protecting complaint received rubber desk gave gray grab\n",
      "     0-9-6 211 266.0 weeks mbp layer 've brand exterior amazing opens incase cool\n",
      "   0-10 4017 33608.0 pockets carry strap shoulder pocket books straps comfortable compartment pack\n",
      "     0-10-1 685 779.0 bags loves swiss gear son compartments holds lug supplies gift\n",
      "     0-10-2 1386 2009.0 strap water notebook comfortable folders pens holder things rain adapter\n",
      "     0-10-3 1330 2115.0 camera ; & smaller zippers - accessories items lenses cons\n",
      "     0-10-4 459 568.0 college pain wheels plan larger spacious started bags years heavy\n",
      "     0-10-5 157 190.0 business change number bags live center spent design dell noticeable\n",
      "   0-11 7336 47011.0 sleeve pocket zipper protection ipad netbook air ; & neoprene\n",
      "     0-11-1 239 834.0 usb drive port cable external power ports plug adapter cord\n",
      "     0-11-2 1324 3607.0 & ; description tablet dimensions wide retina larger seller size\n",
      "     0-11-3 1104 2115.0 smell strong chemical inch odor smells days sleeve bad series\n",
      "     0-11-4 1782 2272.0 battery netbook barely zip memory pink foam neoprene mba pattern\n",
      "     0-11-5 2219 3169.0 retina mbp & ; 've inch protection logic usb sleeve\n",
      "     0-11-6 470 675.0 ipad finding wallet free kindle apple board smell velcro flat\n",
      "     0-11-7 198 338.0 iphone based excited side headphones mp areas line searched fabric\n",
      "   0-12 2375 16908.0 've zipper handle months strap years straps year pockets broke\n",
      "     0-12-1 520 1184.0 dvd player drive smell hard portable seat kids sony car\n",
      "     0-12-2 844 1173.0 gray return customer stay years amount today targus roller needed\n",
      "     0-12-3 860 1328.0 run zippers heavy stand ; canon camera quickly rolls save\n",
      "     0-12-4 151 236.0 takes kids fabric months straps feature section bags addition support\n",
      "   0-13 2124 16305.0 pockets compartment carry compartments room pack straps space plenty storage\n",
      "     0-13-1 549 2654.0 camera lenses lens canon tripod equipment flash body batteries gear\n",
      "     0-13-2 1050 1674.0 swiss compartments swissgear hold bags targus gear ` weekend -\n",
      "     0-13-3 525 982.0 capacity & section equipment space exterior materials waterproof appears weight\n",
      "   0-14 626 4334.0 cover ipad keyboard tablet screen air apple job water version\n",
      "     0-14-1 308 691.0 usb cover reading typing stay add port key feature cable\n",
      "     0-14-2 237 418.0 smell keyboard battery keys odor install mbp cover rest typing\n",
      "     0-14-3 81 131.0 leather pull delivery literally pen sound contacted holder save cut\n",
      "   0-15 1 42.0 purse compartment phone cell description option shows refund purchased manufacturer\n",
      "     0-15-1 1 3.0 ordering option return zips fine flimsy flexible flaw flat flash\n",
      "0 10000 20000 30000 0 0 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9ba99cf2604e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mppl_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_perplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mppl_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_perplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/topicsum/ncrp.py\u001b[0m in \u001b[0;36mget_perplexity\u001b[0;34m(docs, topic_root)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0mset_prob_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0mlogit_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;31m# Path Probability for each document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/topicsum/ncrp.py\u001b[0m in \u001b[0;36mget_probs_child\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_new_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_probs_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/topicsum/ncrp.py\u001b[0m in \u001b[0;36mget_s_child_likelihood\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_s_child_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mchildren_cnt_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnt_words\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_vocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (Children+1) x Vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mchildren_cnt_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_vocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (Children+1) x Vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "while epoch < config.n_epochs:\n",
    "    sample(train_docs, dev_docs, test_docs, topic_root)\n",
    "    ppl_train = get_perplexity(train_docs, topic_root)\n",
    "    ppl_dev = get_perplexity(dev_docs, topic_root)\n",
    "    \n",
    "    if ppl_dev < ppl_min:\n",
    "        ppl_min = ppl_dev\n",
    "        ppl_test = get_perplexity(test_docs, topic_root)\n",
    "        cPickle.dump([test_docs, topic_root], open(config.path_model + '-%i'%epoch, 'wb'))\n",
    "        update_checkpoint(config, checkpoint, epoch)\n",
    "        \n",
    "    depth_spec = get_topic_specialization(test_docs, topic_root)\n",
    "    hierarchical_affinities = get_hierarchical_affinities(topic_root)\n",
    "    \n",
    "    clear_output()\n",
    "    time_log = int(time.time() - time_start)\n",
    "    time_start = time.time()\n",
    "    log_series = pd.Series([time_log, epoch, 0, \\\n",
    "            '%.0f'%ppl_train, '%.0f'%ppl_dev, '%.0f'%ppl_test, \\\n",
    "            '%.2f'%depth_spec[1], '%.2f'%depth_spec[2], '%.2f'%depth_spec[3], \\\n",
    "            '%.2f'%hierarchical_affinities[0], '%.2f'%hierarchical_affinities[1]],\n",
    "            index=log_df.columns)\n",
    "    log_df.loc[epoch] = log_series    \n",
    "    display(log_df)\n",
    "    get_freq_tokens_ncrp(topic_root, idx_to_word, bow_idxs)\n",
    "    \n",
    "    cPickle.dump(log_df, open(config.path_log, 'wb'))\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568401.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sum_cnt_words(topic_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = topic_root\n",
    "doc = train_docs[0]\n",
    "from scipy.special import gammaln\n",
    "\n",
    "if len(topic.children) > 0:\n",
    "    children_cnt_words = np.concatenate([np.array([child.cnt_words for child in topic.children]), np.zeros([1, topic.config.n_vocab])], 0) # (Children+1) x Vocabulary\n",
    "else:\n",
    "    children_cnt_words = np.zeros([1, topic.config.n_vocab]) # (Children+1) x Vocabulary\n",
    "\n",
    "cnt_words_doc = doc.cnt_words[None, :] # 1 x Vocabulary\n",
    "\n",
    "logits_child_likelihood = gammaln(np.sum(children_cnt_words, -1) + topic.config.n_vocab*topic.config.eta) \\\n",
    "                    - np.sum(gammaln(children_cnt_words + topic.config.eta), -1) \\\n",
    "                    - gammaln(np.sum(children_cnt_words + cnt_words_doc, -1) + topic.config.n_vocab*topic.config.eta) \\\n",
    "                    + np.sum(gammaln(children_cnt_words + cnt_words_doc + topic.config.eta), -1)\n",
    "s_child_likelihood = np.exp(logits_child_likelihood)\n",
    "\n",
    "s_child_prior = [child.cnt_doc for child in topic.children]\n",
    "s_child_prior += [topic.config.gam]\n",
    "logits_child_prior = np.log(s_child_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.24841793e-22, 4.82464953e-14, 7.66076645e-11, 1.71026880e-12,\n",
       "       1.60369016e-18, 3.16964611e-16, 1.40075187e-13, 2.67633165e-14,\n",
       "       3.85705196e-11, 8.59807887e-07, 1.79931112e-03, 6.24280982e-06,\n",
       "       5.12144522e-11, 9.97551412e-01, 6.38559096e-04, 4.15368429e-08,\n",
       "       3.57330384e-06])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_child_likelihood * s_child_prior / np.sum(s_child_likelihood * s_child_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.24841793e-22, 4.82464953e-14, 7.66076645e-11, 1.71026880e-12,\n",
       "       1.60369016e-18, 3.16964611e-16, 1.40075187e-13, 2.67633165e-14,\n",
       "       3.85705196e-11, 8.59807887e-07, 1.79931112e-03, 6.24280982e-06,\n",
       "       5.12144522e-11, 9.97551412e-01, 6.38559096e-04, 4.15368429e-08,\n",
       "       3.57330384e-06])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_child = logits_child_likelihood + logits_child_prior\n",
    "s_child = np.exp(logits_child)\n",
    "s_child/np.sum(s_child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_child = np.zeros_like(logits_child)\n",
    "p_child[np.argmax(s_child)] = 1\n",
    "p_child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits_child_prior(topic):\n",
    "    s_child_prior = [child.cnt_doc for child in topic.children]\n",
    "    s_child_prior += [topic.config.gam]\n",
    "    logits_child_prior = np.log(s_child_prior, dtype=np.float128)\n",
    "    return logits_child_prior\n",
    "\n",
    "def get_logits_child_likelihood(topic, doc):\n",
    "    if len(topic.children) > 0:\n",
    "        children_cnt_words = np.concatenate([np.array([child.cnt_words for child in topic.children]), np.zeros([1, topic.config.n_vocab])], 0) # (Children+1) x Vocabulary\n",
    "    else:\n",
    "        children_cnt_words = np.zeros([1, topic.config.n_vocab]) # (Children+1) x Vocabulary\n",
    "\n",
    "    cnt_words_doc = doc.cnt_words[None, :] # 1 x Vocabulary\n",
    "\n",
    "    logits_child_likelihood = gammaln(np.sum(children_cnt_words, -1) + topic.config.n_vocab*topic.config.eta) \\\n",
    "                        - np.sum(gammaln(children_cnt_words + topic.config.eta), -1) \\\n",
    "                        - gammaln(np.sum(children_cnt_words + cnt_words_doc, -1) + topic.config.n_vocab*topic.config.eta) \\\n",
    "                        + np.sum(gammaln(children_cnt_words + cnt_words_doc + topic.config.eta), -1)\n",
    "    return logits_child_likelihood\n",
    "\n",
    "def get_probs_child(topic, doc):\n",
    "    logits_child_prior = get_logits_child_prior(topic)\n",
    "    logits_child_likelihood = get_logits_child_likelihood(topic, doc)\n",
    "    logits_child = logits_child_prior + logits_child_likelihood\n",
    "    \n",
    "    logits_child -= np.min(logits_child)\n",
    "    s_child = np.exp(logits_child)\n",
    "\n",
    "    p_child = s_child/np.sum(s_child)\n",
    "#     if np.sum(s_child) > 0:\n",
    "#         p_child = s_child/np.sum(s_child)\n",
    "#         p_child = p_child.astype(np.float64)\n",
    "#     else:\n",
    "#         p_child = np.zeros_like(logits_child, dtype=np.float64)\n",
    "#         p_child[np.argmax(logits_child)] = 1.\n",
    "        \n",
    "    return p_child"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
