{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from data_structure import get_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "from topic_model import TopicModel\n",
    "\n",
    "from topic_beam_search_decoder import BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '3', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/apnews/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/tglm_vae_tmp', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'apnews', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 50, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 500, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.1, 'lr')\n",
    "flags.DEFINE_float('reg', 1., 'regularization term')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_bool('warmup', True, 'flg of warming up')\n",
    "flags.DEFINE_integer('epochs_cycle', 10, 'number of epochs within a cycle')\n",
    "flags.DEFINE_float('r_cycle', 0.5, 'proportion used to increase beta within a cycle')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 10, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_bow', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_hidden_topic', 512, 'dim_hidden_topic')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "flags.DEFINE_bool('bidirectional', True, 'flg of bidirectional encoding')\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('PAD_IDX', word_to_idx[PAD], 'PAD_IDX')\n",
    "flags.DEFINE_integer('UNK_IDX', word_to_idx[UNK], 'UNK_IDX')\n",
    "flags.DEFINE_integer('BOS_IDX', word_to_idx[BOS], 'BOS_IDX')\n",
    "flags.DEFINE_integer('EOS_IDX', word_to_idx[EOS], 'EOS_IDX')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "maximum_iterations = max([max([instance.max_sent_l for instance in batch]) for ct, batch in dev_batches])\n",
    "flags.DEFINE_integer('maximum_iterations', maximum_iterations, 'maximum_iterations')\n",
    "\n",
    "flags.DEFINE_integer('cycle_steps', len(train_batches)*config.epochs_cycle, 'number of steps for each cycle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build language model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['bow'] = tf.placeholder(tf.float32, [None, config.dim_bow], name='bow')\n",
    "t_variables['input_token_idxs'] = tf.placeholder(tf.int32, [None, None], name='input_token_idxs')\n",
    "t_variables['dec_input_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_input_idxs')\n",
    "t_variables['dec_target_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_target_idxs')\n",
    "t_variables['batch_l'] = tf.placeholder(tf.int32, name='batch_l')\n",
    "t_variables['doc_l'] = tf.placeholder(tf.int32, [None], name='doc_l')\n",
    "t_variables['sent_l'] = tf.placeholder(tf.int32, [None], name='sent_l')\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train', assertion=False):\n",
    "    def token_dropout(sent_idxs):\n",
    "        sent_idxs_dropout = np.asarray(sent_idxs)\n",
    "        sent_idxs_dropout[np.random.rand(len(sent_idxs)) > config.word_keep_prob] = config.UNK_IDX\n",
    "        return list(sent_idxs_dropout)\n",
    "\n",
    "    bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "    \n",
    "    doc_l = np.array([len(instance.token_idxs) for instance in batch])\n",
    "    \n",
    "    feed_input_token_idxs_list = [sent_idxs for instance in batch for sent_idxs in instance.token_idxs]\n",
    "    feed_dec_input_idxs_list = [[config.BOS_IDX] + token_dropout(sent_idxs) for sent_idxs in feed_input_token_idxs_list]\n",
    "    feed_dec_target_idxs_list = [sent_idxs + [config.EOS_IDX]  for sent_idxs in feed_input_token_idxs_list]\n",
    "        \n",
    "    sent_l = np.array([len(sent_idxs) for sent_idxs in feed_input_token_idxs_list], np.int32)\n",
    "    batch_l = len(sent_l)\n",
    "    \n",
    "    feed_input_token_idxs = pad_sequences(feed_input_token_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_input_idxs = pad_sequences(feed_dec_input_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_target_idxs = pad_sequences(feed_dec_target_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    \n",
    "    if assertion:\n",
    "        index = 0\n",
    "        for instance in batch:\n",
    "            for line_idxs in instance.token_idxs:\n",
    "                assert feed_input_token_idxs_list[index] == line_idxs\n",
    "                index += 1\n",
    "        assert feed_input_token_idxs.shape[1] == np.max(sent_l)\n",
    "        assert feed_dec_input_idxs.shape[1] == np.max(sent_l) + 1\n",
    "        assert feed_dec_target_idxs.shape[1] == np.max(sent_l) + 1\n",
    "    \n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "\n",
    "    feed_dict = {\n",
    "                t_variables['bow']: bow, \n",
    "                t_variables['batch_l']: batch_l, t_variables['doc_l']: doc_l, t_variables['sent_l']: sent_l, \n",
    "                t_variables['input_token_idxs']: feed_input_token_idxs, t_variables['dec_input_idxs']: feed_dec_input_idxs, t_variables['dec_target_idxs']: feed_dec_target_idxs, \n",
    "                t_variables['keep_prob']: keep_prob\n",
    "    }\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     10,
     24,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "def check_shape(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()\n",
    "    \n",
    "def check_value(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()    \n",
    "    \n",
    "# sent_loss_kl_categ_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, tf_log(prob_topic_infer/prob_topic_sents)), 1))\n",
    "# debug_value([sent_loss_kl_categ, sent_loss_kl_categ_tmp])\n",
    "# sent_loss_kl_gauss_tmp = 0.5 * tf.reduce_sum(tf.exp(logvars_topic_infer-logvars_topic) + tf.square(means_topic - means_topic_infer) / tf.exp(logvars_topic) - 1 + (logvars_topic - logvars_topic_infer), -1)\n",
    "# sent_loss_kl_gmm_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss_tmp), -1))\n",
    "# debug_value([sent_loss_kl_gmm_tmp, sent_loss_kl_gmm])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encode bow\n",
    "with tf.variable_scope('topic/enc', reuse=False):\n",
    "    hidden_bow_ = tf.layers.Dense(units=config.dim_hidden_bow, activation=tf.nn.relu, name='hidden_bow')(t_variables['bow'])\n",
    "    hidden_bow = tf.layers.Dropout(t_variables['keep_prob'])(hidden_bow_)\n",
    "    means_bow = tf.layers.Dense(units=config.dim_latent_bow, name='mean_bow')(hidden_bow)\n",
    "    logvars_bow = tf.layers.Dense(units=config.dim_latent_bow, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_bow)\n",
    "    latents_bow = sample_latents(means_bow, logvars_bow) # sample latent vectors\n",
    "\n",
    "    prob_topic = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic')(latents_bow) # inference of topic probabilities\n",
    "\n",
    "# decode bow\n",
    "with tf.variable_scope('shared', reuse=False):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "bow_embeddings = tf.nn.embedding_lookup(embeddings, bow_idxs) # embeddings of each bow features\n",
    "\n",
    "with tf.variable_scope('topic/dec', reuse=False):\n",
    "    topic_embeddings = tf.get_variable('topic_emb', [config.n_topic, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of topics\n",
    "\n",
    "    topic_bow = tf.nn.softmax(tf.matmul(topic_embeddings, bow_embeddings, transpose_b=True), 1) # bow vectors for each topic\n",
    "    logits_bow = tf_log(tf.matmul(prob_topic, topic_bow)) # predicted bow distribution\n",
    "\n",
    "    # prior of each gaussian distribution (computed for each topic)\n",
    "    hidden_topic = tf.layers.Dense(units=config.dim_hidden_topic, activation=tf.nn.relu, name='hidden_topic')(topic_bow)\n",
    "    means_topic = tf.layers.Dense(units=config.dim_latent, name='mean_topic')(hidden_topic)\n",
    "    logvars_topic = tf.layers.Dense(units=config.dim_latent, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_topic)\n",
    "    sigma_topic = tf.exp(0.5 * logvars_topic)\n",
    "    gauss_topic = tfd.Normal(loc=means_topic, scale=sigma_topic)    \n",
    "    \n",
    "# define losses\n",
    "topic_losses_recon = -tf.reduce_sum(tf.multiply(t_variables['bow'], logits_bow), 1)\n",
    "topic_loss_recon = tf.reduce_mean(topic_losses_recon) # negative log likelihood of each words\n",
    "\n",
    "topic_loss_kl = compute_kl_loss(means_bow, logvars_bow) # KL divergence b/w latent dist & gaussian std\n",
    "\n",
    "topic_bow_norm = topic_bow / tf.norm(topic_bow, axis=1, keepdims=True)\n",
    "topic_dots = tf.clip_by_value(tf.matmul(topic_bow_norm, tf.transpose(topic_bow_norm)), -1., 1.)\n",
    "topic_loss_reg = tf.reduce_mean(tf.square(topic_dots - tf.eye(config.n_topic)))\n",
    "# topic_angles = tf.acos(topic_dots)\n",
    "# topic_angles_mean = tf.reduce_mean(topic_angles)\n",
    "# topic_angles_vars = tf.reduce_mean(tf.square(topic_angles - topic_angles_mean))\n",
    "# topic_loss_reg = tf.exp(topic_angles_vars - topic_angles_mean)\n",
    "\n",
    "# monitor\n",
    "n_bow = tf.reduce_sum(t_variables['bow'], 1)\n",
    "topic_ppls = tf.divide(topic_losses_recon, tf.maximum(1e-5, n_bow))\n",
    "topics_freq_bow_indices = tf.nn.top_k(topic_bow, 10, name='topic_freq_bow').indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "input_token_idxs = t_variables['input_token_idxs']\n",
    "batch_l = t_variables['batch_l']\n",
    "sent_l = t_variables['sent_l']\n",
    "max_sent_l = tf.reduce_max(sent_l)\n",
    "\n",
    "with tf.variable_scope('sent/enc', reuse=False):\n",
    "    # get word embedding\n",
    "    enc_input = tf.nn.embedding_lookup(embeddings, input_token_idxs)\n",
    "\n",
    "    # get sentence embedding\n",
    "    _, enc_state = dynamic_bi_rnn(enc_input, sent_l, config.dim_hidden, t_variables['keep_prob'])\n",
    "\n",
    "    # TODO House Holder flow\n",
    "    hidden_topic_infer =  tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='hidden_topic_infer')(enc_state)\n",
    "    prob_topic_infer = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic_infer')(hidden_topic_infer)\n",
    "\n",
    "    w_mean_topic_infer = tf.get_variable('mean_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32)\n",
    "    b_mean_topic_infer = tf.get_variable('mean_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32)\n",
    "    means_topic_infer = tf.tensordot(enc_state, w_mean_topic_infer, axes=[[1], [1]]) + b_mean_topic_infer\n",
    "    \n",
    "    w_logvar_topic_infer = tf.get_variable('logvar_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    b_logvar_topic_infer = tf.get_variable('logvar_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    logvars_topic_infer = tf.tensordot(enc_state, w_logvar_topic_infer, axes=[[1], [1]]) + b_logvar_topic_infer\n",
    "    sigma_topic_infer = tf.exp(0.5 * logvars_topic_infer)\n",
    "    gauss_topic_infer = tfd.Normal(loc=means_topic_infer, scale=sigma_topic_infer)\n",
    "    \n",
    "    # latent vectors from each gaussian dist.\n",
    "    latents_topic_infer = sample_latents(means_topic_infer, logvars_topic_infer) \n",
    "    # latent vector from gaussian mixture\n",
    "    latents_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), latents_topic_infer, transpose_a=True)\n",
    "    \n",
    "    # for beam search\n",
    "    means_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), means_topic_infer, transpose_a=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for decoding\n",
    "dec_sent_l = tf.add(sent_l, 1)\n",
    "dec_input_idxs = t_variables['dec_input_idxs']\n",
    "dec_input = tf.nn.embedding_lookup(embeddings, dec_input_idxs)\n",
    "\n",
    "dec_latents_input = tf.tile(latents_input, [1, tf.shape(dec_input)[1], 1])\n",
    "dec_concat_input = tf.concat([dec_input, dec_latents_input], -1)\n",
    "\n",
    "# decode for training\n",
    "with tf.variable_scope('sent/dec/rnn', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32, reuse=False):\n",
    "    dec_cell = tf.contrib.rnn.GRUCell(config.dim_hidden)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob = t_variables['keep_prob'])\n",
    "\n",
    "    dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(latents_input, 1))\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_concat_input, sequence_length=dec_sent_l)\n",
    "\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=dec_cell,\n",
    "        helper=helper,\n",
    "        initial_state=dec_initial_state)\n",
    "\n",
    "    dec_outputs, _, output_sent_l = tf.contrib.seq2seq.dynamic_decode(train_decoder)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(config.n_vocab, use_bias=False, name='out')\n",
    "    output_logits = output_layer(dec_outputs.rnn_output)\n",
    "    \n",
    "    output_token_idxs = tf.argmax(output_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tokens = tf.fill([batch_l], config.BOS_IDX)\n",
    "end_token = config.EOS_IDX\n",
    "\n",
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    infer_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(means_input, 1))\n",
    "    beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(infer_dec_initial_state, multiplier=config.beam_width)\n",
    "    beam_latents_input = tf.contrib.seq2seq.tile_batch(tf.squeeze(means_input, 1), multiplier=config.beam_width) # added\n",
    "    \n",
    "    beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=beam_latents_input)\n",
    "\n",
    "    beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    beam_output_token_idxs = beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    inter_means_input = tf.placeholder(tf.float32, [None, config.dim_latent])\n",
    "    \n",
    "    inter_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(inter_means_input)\n",
    "    inter_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(inter_dec_initial_state, multiplier=config.beam_width)\n",
    "    inter_beam_latents_input = tf.contrib.seq2seq.tile_batch(inter_means_input, multiplier=config.beam_width) # added\n",
    "    \n",
    "    inter_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=inter_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=inter_beam_latents_input)\n",
    "\n",
    "    inter_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inter_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    inter_beam_output_token_idxs = inter_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    topic_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic)\n",
    "    topic_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(topic_dec_initial_state, multiplier=config.beam_width)\n",
    "    topic_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic, multiplier=config.beam_width) # added\n",
    "    \n",
    "    topic_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=topic_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=topic_beam_latents_input)\n",
    "\n",
    "    topic_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        topic_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    topic_beam_output_token_idxs = topic_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language modeling cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target and mask\n",
    "dec_target_idxs = t_variables['dec_target_idxs']\n",
    "dec_mask_tokens = tf.sequence_mask(dec_sent_l, maxlen=max_sent_l+1, dtype=tf.float32)\n",
    "\n",
    "# nll for each token (averaged over batch & sentence)\n",
    "sent_loss_recon = tf.contrib.seq2seq.sequence_loss(output_logits, dec_target_idxs, dec_mask_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_l = t_variables['doc_l']\n",
    "mask_sents = tf.sequence_mask(doc_l)\n",
    "mask_sents_flatten = tf.reshape(mask_sents, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1]])\n",
    "\n",
    "prob_topic_tiled = tf.tile(tf.expand_dims(prob_topic, 1), [1, tf.shape(mask_sents)[1], 1])\n",
    "prob_topic_flatten = tf.reshape(prob_topic_tiled, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1], config.n_topic])\n",
    "prob_topic_sents = tf.boolean_mask(prob_topic_flatten, mask_sents_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferred mixture probabilities (computed for each sentence)\n",
    "categ_topic_infer = tfd.Categorical(probs=prob_topic_infer)\n",
    "\n",
    "# prior of mixture probabilities (computed for each document, tiled for each sentence)\n",
    "categ_topic = tfd.Categorical(probs=prob_topic_sents)\n",
    "\n",
    "sent_loss_kl_categ = tf.reduce_mean(tfd.kl_divergence(categ_topic_infer, categ_topic))\n",
    "\n",
    "# inference of each gaussian gaussribution (computed for each sentence)\n",
    "\n",
    "sent_loss_kl_gauss = tf.reduce_sum(tfd.kl_divergence(gauss_topic_infer, gauss_topic), -1)\n",
    "sent_loss_kl_gmm = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss), -1))\n",
    "\n",
    "sent_loss_kl = sent_loss_kl_categ + sent_loss_kl_gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "tau = tf.cast(tf.divide(tf.mod(global_step, tf.constant(config.cycle_steps)), tf.constant(config.cycle_steps)), dtype=tf.float32)\n",
    "beta = tf.minimum(1., tau/config.r_cycle)\n",
    "\n",
    "sent_loss = sent_loss_recon + beta * sent_loss_kl\n",
    "\n",
    "topic_loss = topic_loss_recon + topic_loss_kl + config.reg * topic_loss_reg\n",
    "loss = topic_loss + sent_loss\n",
    "\n",
    "# define optimizer\n",
    "if config.opt == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif config.opt == 'Adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "    \n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_sents(token_idxs, config, idx_to_word):\n",
    "    sents = []\n",
    "    for sent_idxs in token_idxs:\n",
    "        tokens = []\n",
    "        for idx in sent_idxs:\n",
    "            if idx == config.EOS_IDX: break\n",
    "            tokens.append(idx_to_word[idx])\n",
    "        sent = ' '.join(tokens)\n",
    "        sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_batch, sent_loss_batch, ppls_batch = sess.run([loss, topic_loss, sent_loss, topic_ppls], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_batch, sent_loss_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "    loss_mean, topic_loss_mean, sent_loss_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    return loss_mean, topic_loss_mean, sent_loss_mean, ppl_mean\n",
    "\n",
    "def get_all_losses(sess, batches):\n",
    "    losses = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch = \\\n",
    "        sess.run([loss, topic_loss_recon, topic_loss_kl, sent_loss_recon, sent_loss_kl], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "    print('LOSS %.2f | TM NLL: %.2f, KL: %.4f | LM NLL: %.2f, KL: %.4f' %  np.mean(losses, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_batch):\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    pred_token_idxs = sess.run(output_token_idxs, feed_dict = feed_dict)\n",
    "    true_token_idxs = [sent_idxs for instance in sample_batch for sent_idxs in instance.token_idxs]\n",
    "    \n",
    "    assert len(pred_token_idxs) == len(true_token_idxs)\n",
    "    \n",
    "    pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "    true_sents = idxs_to_sents(true_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    for i, (true_sent, pred_sent) in enumerate(zip(true_sents, pred_sents)):        \n",
    "        print(i, 'True: %s' % true_sent)\n",
    "        print(i, 'Pred: %s' % pred_sent)\n",
    "\n",
    "def print_topic_sample():\n",
    "    pred_topics_freq_bow_indices, pred_topic_token_idxs = sess.run([topics_freq_bow_indices, topic_beam_output_token_idxs], \n",
    "                                                                                                           feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "    pred_topic_sents = idxs_to_sents(pred_topic_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Topic Samples-----------')\n",
    "    for i, (topic_freq_bow_idxs, pred_topic_sent) in enumerate(zip(topics_freq_bow_idxs, pred_topic_sents)):\n",
    "        print(i, ' bow:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' sent:', pred_topic_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "logs = []\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "saver = tf.train.Saver(max_to_keep=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "011[s], Ep: 00, Ct: 0000|TR LOSS: 351.72, PPL: 2659|TM NLL: 339.88, KL: 0.61, REG:0.90 | LM NLL: 10.34, KL: 1.45|DE LOSS: 347.83, PPL: 2656, TM: 337.49, LM: 10.34|BETA: 0.000069\n",
      "091[s], Ep: 00, Ct: 0500|TR LOSS: 326.70, PPL: 1598|TM NLL: 317.32, KL: 1.37, REG:0.48 | LM NLL: 7.49, KL: 2.60|DE LOSS: 318.49, PPL: 1406, TM: 311.54, LM: 6.95|BETA: 0.034743\n",
      "092[s], Ep: 00, Ct: 1000|TR LOSS: 324.13, PPL: 1466|TM NLL: 314.53, KL: 2.03, REG:0.32 | LM NLL: 7.18, KL: 2.27|DE LOSS: 315.01, PPL: 1305, TM: 308.16, LM: 6.86|BETA: 0.069417\n",
      "092[s], Ep: 00, Ct: 1500|TR LOSS: 322.44, PPL: 1397|TM NLL: 312.67, KL: 2.37, REG:0.25 | LM NLL: 7.06, KL: 1.98|DE LOSS: 313.42, PPL: 1249, TM: 306.56, LM: 6.86|BETA: 0.104092\n",
      "093[s], Ep: 00, Ct: 2000|TR LOSS: 320.82, PPL: 1349|TM NLL: 310.94, KL: 2.58, REG:0.21 | LM NLL: 6.99, KL: 1.76|DE LOSS: 311.36, PPL: 1184, TM: 304.54, LM: 6.83|BETA: 0.138766\n",
      "092[s], Ep: 00, Ct: 2500|TR LOSS: 319.50, PPL: 1310|TM NLL: 309.53, KL: 2.75, REG:0.18 | LM NLL: 6.94, KL: 1.59|DE LOSS: 310.30, PPL: 1143, TM: 303.50, LM: 6.80|BETA: 0.173440\n",
      "072[s], Ep: 01, Ct: 0000|TR LOSS: 318.74, PPL: 1286|TM NLL: 308.72, KL: 2.84, REG:0.16 | LM NLL: 6.91, KL: 1.49|DE LOSS: 309.79, PPL: 1137, TM: 303.02, LM: 6.77|BETA: 0.200069\n",
      "093[s], Ep: 01, Ct: 0500|TR LOSS: 318.09, PPL: 1261|TM NLL: 308.01, KL: 2.94, REG:0.14 | LM NLL: 6.87, KL: 1.39|DE LOSS: 309.08, PPL: 1114, TM: 302.36, LM: 6.72|BETA: 0.234743\n",
      "093[s], Ep: 01, Ct: 1000|TR LOSS: 317.14, PPL: 1239|TM NLL: 307.03, KL: 3.02, REG:0.13 | LM NLL: 6.83, KL: 1.30|DE LOSS: 308.92, PPL: 1107, TM: 302.23, LM: 6.69|BETA: 0.269417\n",
      "093[s], Ep: 01, Ct: 1500|TR LOSS: 316.61, PPL: 1223|TM NLL: 306.46, KL: 3.09, REG:0.12 | LM NLL: 6.80, KL: 1.23|DE LOSS: 308.25, PPL: 1092, TM: 301.61, LM: 6.64|BETA: 0.304092\n",
      "093[s], Ep: 01, Ct: 2000|TR LOSS: 316.22, PPL: 1208|TM NLL: 306.05, KL: 3.15, REG:0.11 | LM NLL: 6.76, KL: 1.17|DE LOSS: 307.99, PPL: 1092, TM: 301.42, LM: 6.58|BETA: 0.338766\n",
      "094[s], Ep: 01, Ct: 2500|TR LOSS: 315.85, PPL: 1196|TM NLL: 305.66, KL: 3.20, REG:0.11 | LM NLL: 6.73, KL: 1.12|DE LOSS: 307.64, PPL: 1082, TM: 301.10, LM: 6.54|BETA: 0.373440\n",
      "075[s], Ep: 02, Ct: 0000|TR LOSS: 315.49, PPL: 1187|TM NLL: 305.28, KL: 3.24, REG:0.10 | LM NLL: 6.70, KL: 1.09|DE LOSS: 307.68, PPL: 1075, TM: 301.17, LM: 6.51|BETA: 0.400069\n",
      "093[s], Ep: 02, Ct: 0500|TR LOSS: 315.09, PPL: 1176|TM NLL: 304.87, KL: 3.28, REG:0.10 | LM NLL: 6.67, KL: 1.06|DE LOSS: 307.28, PPL: 1068, TM: 300.80, LM: 6.48|BETA: 0.434743\n",
      "093[s], Ep: 02, Ct: 1000|TR LOSS: 314.88, PPL: 1166|TM NLL: 304.65, KL: 3.32, REG:0.10 | LM NLL: 6.64, KL: 1.03|DE LOSS: 306.80, PPL: 1050, TM: 300.35, LM: 6.44|BETA: 0.469417\n",
      "092[s], Ep: 02, Ct: 1500|TR LOSS: 314.55, PPL: 1158|TM NLL: 304.30, KL: 3.36, REG:0.09 | LM NLL: 6.61, KL: 1.00|DE LOSS: 306.63, PPL: 1047, TM: 300.20, LM: 6.43|BETA: 0.504092\n",
      "093[s], Ep: 02, Ct: 2000|TR LOSS: 314.27, PPL: 1151|TM NLL: 304.01, KL: 3.40, REG:0.09 | LM NLL: 6.58, KL: 0.97|DE LOSS: 306.37, PPL: 1041, TM: 299.98, LM: 6.39|BETA: 0.538766\n",
      "093[s], Ep: 02, Ct: 2500|TR LOSS: 313.96, PPL: 1143|TM NLL: 303.69, KL: 3.43, REG:0.09 | LM NLL: 6.55, KL: 0.95|DE LOSS: 306.03, PPL: 1033, TM: 299.64, LM: 6.39|BETA: 0.573440\n",
      "071[s], Ep: 03, Ct: 0000|TR LOSS: 313.78, PPL: 1138|TM NLL: 303.50, KL: 3.45, REG:0.09 | LM NLL: 6.53, KL: 0.94|DE LOSS: 306.17, PPL: 1036, TM: 299.80, LM: 6.37|BETA: 0.600069\n",
      "089[s], Ep: 03, Ct: 0500|TR LOSS: 313.62, PPL: 1132|TM NLL: 303.32, KL: 3.48, REG:0.08 | LM NLL: 6.51, KL: 0.92|DE LOSS: 305.94, PPL: 1029, TM: 299.57, LM: 6.38|BETA: 0.634743\n",
      "087[s], Ep: 03, Ct: 1000|TR LOSS: 313.33, PPL: 1125|TM NLL: 303.03, KL: 3.51, REG:0.08 | LM NLL: 6.48, KL: 0.91|DE LOSS: 305.78, PPL: 1025, TM: 299.42, LM: 6.36|BETA: 0.669418\n",
      "088[s], Ep: 03, Ct: 1500|TR LOSS: 313.11, PPL: 1120|TM NLL: 302.80, KL: 3.53, REG:0.08 | LM NLL: 6.46, KL: 0.89|DE LOSS: 305.59, PPL: 1026, TM: 299.24, LM: 6.35|BETA: 0.704092\n",
      "087[s], Ep: 03, Ct: 2000|TR LOSS: 312.91, PPL: 1115|TM NLL: 302.59, KL: 3.55, REG:0.08 | LM NLL: 6.43, KL: 0.88|DE LOSS: 305.68, PPL: 1025, TM: 299.31, LM: 6.36|BETA: 0.738766\n",
      "087[s], Ep: 03, Ct: 2500|TR LOSS: 312.78, PPL: 1111|TM NLL: 302.45, KL: 3.57, REG:0.08 | LM NLL: 6.41, KL: 0.87|DE LOSS: 305.42, PPL: 1014, TM: 299.09, LM: 6.33|BETA: 0.773440\n",
      "069[s], Ep: 04, Ct: 0000|TR LOSS: 312.64, PPL: 1107|TM NLL: 302.32, KL: 3.59, REG:0.08 | LM NLL: 6.40, KL: 0.86|DE LOSS: 305.42, PPL: 1018, TM: 299.09, LM: 6.33|BETA: 0.800069\n",
      "087[s], Ep: 04, Ct: 0500|TR LOSS: 312.45, PPL: 1102|TM NLL: 302.11, KL: 3.61, REG:0.08 | LM NLL: 6.38, KL: 0.85|DE LOSS: 305.45, PPL: 1011, TM: 299.11, LM: 6.34|BETA: 0.834743\n",
      "087[s], Ep: 04, Ct: 1000|TR LOSS: 312.30, PPL: 1098|TM NLL: 301.95, KL: 3.63, REG:0.08 | LM NLL: 6.36, KL: 0.84|DE LOSS: 305.23, PPL: 1011, TM: 298.92, LM: 6.31|BETA: 0.869417\n",
      "088[s], Ep: 04, Ct: 1500|TR LOSS: 312.21, PPL: 1095|TM NLL: 301.85, KL: 3.65, REG:0.07 | LM NLL: 6.34, KL: 0.83|DE LOSS: 305.09, PPL: 1009, TM: 298.79, LM: 6.30|BETA: 0.904092\n",
      "087[s], Ep: 04, Ct: 2000|TR LOSS: 312.15, PPL: 1091|TM NLL: 301.79, KL: 3.66, REG:0.07 | LM NLL: 6.32, KL: 0.82|DE LOSS: 305.30, PPL: 1013, TM: 298.97, LM: 6.33|BETA: 0.938766\n",
      "087[s], Ep: 04, Ct: 2500|TR LOSS: 312.00, PPL: 1088|TM NLL: 301.64, KL: 3.68, REG:0.07 | LM NLL: 6.30, KL: 0.81|DE LOSS: 305.37, PPL: 1017, TM: 299.04, LM: 6.32|BETA: 0.973440\n",
      "069[s], Ep: 05, Ct: 0000|TR LOSS: 311.86, PPL: 1086|TM NLL: 301.50, KL: 3.69, REG:0.07 | LM NLL: 6.29, KL: 0.81|DE LOSS: 304.96, PPL: 1004, TM: 298.67, LM: 6.29|BETA: 1.000000\n",
      "087[s], Ep: 05, Ct: 0500|TR LOSS: 311.72, PPL: 1082|TM NLL: 301.35, KL: 3.70, REG:0.07 | LM NLL: 6.27, KL: 0.80|DE LOSS: 305.05, PPL: 1003, TM: 298.78, LM: 6.27|BETA: 1.000000\n",
      "088[s], Ep: 05, Ct: 1000|TR LOSS: 311.66, PPL: 1079|TM NLL: 301.28, KL: 3.72, REG:0.07 | LM NLL: 6.25, KL: 0.79|DE LOSS: 304.98, PPL: 1007, TM: 298.70, LM: 6.27|BETA: 1.000000\n",
      "088[s], Ep: 05, Ct: 1500|TR LOSS: 311.51, PPL: 1077|TM NLL: 301.13, KL: 3.73, REG:0.07 | LM NLL: 6.24, KL: 0.78|DE LOSS: 304.99, PPL: 1006, TM: 298.75, LM: 6.24|BETA: 1.000000\n",
      "087[s], Ep: 05, Ct: 2000|TR LOSS: 311.42, PPL: 1074|TM NLL: 301.03, KL: 3.74, REG:0.07 | LM NLL: 6.22, KL: 0.78|DE LOSS: 305.01, PPL: 1005, TM: 298.79, LM: 6.22|BETA: 1.000000\n",
      "087[s], Ep: 05, Ct: 2500|TR LOSS: 311.35, PPL: 1072|TM NLL: 300.96, KL: 3.75, REG:0.07 | LM NLL: 6.21, KL: 0.77|DE LOSS: 304.91, PPL: 1003, TM: 298.71, LM: 6.20|BETA: 1.000000\n",
      "069[s], Ep: 06, Ct: 0000|TR LOSS: 311.28, PPL: 1070|TM NLL: 300.89, KL: 3.76, REG:0.07 | LM NLL: 6.20, KL: 0.77|DE LOSS: 304.71, PPL: 1000, TM: 298.53, LM: 6.18|BETA: 1.000000\n",
      "088[s], Ep: 06, Ct: 0500|TR LOSS: 311.17, PPL: 1068|TM NLL: 300.78, KL: 3.77, REG:0.07 | LM NLL: 6.18, KL: 0.76|DE LOSS: 304.80, PPL: 1003, TM: 298.62, LM: 6.18|BETA: 1.000000\n",
      "087[s], Ep: 06, Ct: 1000|TR LOSS: 311.12, PPL: 1066|TM NLL: 300.73, KL: 3.78, REG:0.07 | LM NLL: 6.17, KL: 0.75|DE LOSS: 304.73, PPL: 1002, TM: 298.58, LM: 6.15|BETA: 1.000000\n",
      "087[s], Ep: 06, Ct: 1500|TR LOSS: 311.02, PPL: 1064|TM NLL: 300.63, KL: 3.79, REG:0.07 | LM NLL: 6.16, KL: 0.75|DE LOSS: 304.74, PPL: 1001, TM: 298.60, LM: 6.14|BETA: 1.000000\n",
      "087[s], Ep: 06, Ct: 2000|TR LOSS: 310.94, PPL: 1062|TM NLL: 300.54, KL: 3.80, REG:0.07 | LM NLL: 6.14, KL: 0.74|DE LOSS: 304.58, PPL: 998, TM: 298.47, LM: 6.11|BETA: 1.000000\n",
      "087[s], Ep: 06, Ct: 2500|TR LOSS: 310.86, PPL: 1060|TM NLL: 300.47, KL: 3.81, REG:0.07 | LM NLL: 6.13, KL: 0.74|DE LOSS: 304.38, PPL: 991, TM: 298.26, LM: 6.12|BETA: 1.000000\n",
      "069[s], Ep: 07, Ct: 0000|TR LOSS: 310.82, PPL: 1058|TM NLL: 300.43, KL: 3.82, REG:0.07 | LM NLL: 6.12, KL: 0.74|DE LOSS: 304.74, PPL: 1003, TM: 298.62, LM: 6.12|BETA: 1.000000\n",
      "087[s], Ep: 07, Ct: 0500|TR LOSS: 310.71, PPL: 1056|TM NLL: 300.32, KL: 3.83, REG:0.06 | LM NLL: 6.11, KL: 0.73|DE LOSS: 304.61, PPL: 998, TM: 298.51, LM: 6.10|BETA: 1.000000\n",
      "088[s], Ep: 07, Ct: 1000|TR LOSS: 310.67, PPL: 1055|TM NLL: 300.27, KL: 3.84, REG:0.06 | LM NLL: 6.10, KL: 0.73|DE LOSS: 304.58, PPL: 1002, TM: 298.49, LM: 6.09|BETA: 1.000000\n",
      "087[s], Ep: 07, Ct: 1500|TR LOSS: 310.59, PPL: 1053|TM NLL: 300.20, KL: 3.84, REG:0.06 | LM NLL: 6.09, KL: 0.72|DE LOSS: 304.42, PPL: 995, TM: 298.35, LM: 6.07|BETA: 1.000000\n",
      "087[s], Ep: 07, Ct: 2000|TR LOSS: 310.52, PPL: 1052|TM NLL: 300.13, KL: 3.85, REG:0.06 | LM NLL: 6.07, KL: 0.72|DE LOSS: 304.45, PPL: 1000, TM: 298.39, LM: 6.06|BETA: 1.000000\n",
      "087[s], Ep: 07, Ct: 2500|TR LOSS: 310.48, PPL: 1050|TM NLL: 300.10, KL: 3.86, REG:0.06 | LM NLL: 6.06, KL: 0.71|DE LOSS: 304.51, PPL: 998, TM: 298.46, LM: 6.06|BETA: 1.000000\n",
      "069[s], Ep: 08, Ct: 0000|TR LOSS: 310.44, PPL: 1049|TM NLL: 300.05, KL: 3.86, REG:0.06 | LM NLL: 6.05, KL: 0.71|DE LOSS: 304.37, PPL: 998, TM: 298.34, LM: 6.03|BETA: 1.000000\n",
      "088[s], Ep: 08, Ct: 0500|TR LOSS: 310.40, PPL: 1048|TM NLL: 300.02, KL: 3.87, REG:0.06 | LM NLL: 6.04, KL: 0.71|DE LOSS: 304.32, PPL: 993, TM: 298.28, LM: 6.04|BETA: 1.000000\n",
      "087[s], Ep: 08, Ct: 1000|TR LOSS: 310.37, PPL: 1046|TM NLL: 299.99, KL: 3.87, REG:0.06 | LM NLL: 6.03, KL: 0.70|DE LOSS: 304.33, PPL: 994, TM: 298.30, LM: 6.03|BETA: 1.000000\n",
      "088[s], Ep: 08, Ct: 1500|TR LOSS: 310.32, PPL: 1045|TM NLL: 299.94, KL: 3.88, REG:0.06 | LM NLL: 6.02, KL: 0.70|DE LOSS: 304.29, PPL: 993, TM: 298.28, LM: 6.01|BETA: 1.000000\n",
      "088[s], Ep: 08, Ct: 2000|TR LOSS: 310.25, PPL: 1043|TM NLL: 299.87, KL: 3.89, REG:0.06 | LM NLL: 6.01, KL: 0.70|DE LOSS: 304.26, PPL: 994, TM: 298.24, LM: 6.02|BETA: 1.000000\n",
      "088[s], Ep: 08, Ct: 2500|TR LOSS: 310.21, PPL: 1042|TM NLL: 299.83, KL: 3.89, REG:0.06 | LM NLL: 6.00, KL: 0.69|DE LOSS: 304.28, PPL: 993, TM: 298.28, LM: 6.00|BETA: 1.000000\n",
      "069[s], Ep: 09, Ct: 0000|TR LOSS: 310.12, PPL: 1041|TM NLL: 299.75, KL: 3.90, REG:0.06 | LM NLL: 6.00, KL: 0.69|DE LOSS: 304.28, PPL: 990, TM: 298.29, LM: 5.99|BETA: 1.000000\n",
      "087[s], Ep: 09, Ct: 0500|TR LOSS: 310.05, PPL: 1040|TM NLL: 299.68, KL: 3.90, REG:0.06 | LM NLL: 5.99, KL: 0.69|DE LOSS: 304.27, PPL: 992, TM: 298.29, LM: 5.98|BETA: 1.000000\n",
      "087[s], Ep: 09, Ct: 1000|TR LOSS: 309.98, PPL: 1039|TM NLL: 299.61, KL: 3.91, REG:0.06 | LM NLL: 5.98, KL: 0.68|DE LOSS: 304.19, PPL: 987, TM: 298.23, LM: 5.97|BETA: 1.000000\n",
      "088[s], Ep: 09, Ct: 1500|TR LOSS: 309.96, PPL: 1037|TM NLL: 299.59, KL: 3.91, REG:0.06 | LM NLL: 5.97, KL: 0.68|DE LOSS: 304.24, PPL: 990, TM: 298.27, LM: 5.97|BETA: 1.000000\n",
      "087[s], Ep: 09, Ct: 2000|TR LOSS: 309.91, PPL: 1036|TM NLL: 299.54, KL: 3.92, REG:0.06 | LM NLL: 5.96, KL: 0.68|DE LOSS: 304.12, PPL: 988, TM: 298.19, LM: 5.93|BETA: 1.000000\n",
      "088[s], Ep: 09, Ct: 2500|TR LOSS: 309.88, PPL: 1036|TM NLL: 299.52, KL: 3.92, REG:0.06 | LM NLL: 5.95, KL: 0.67|DE LOSS: 304.11, PPL: 993, TM: 298.17, LM: 5.94|BETA: 1.000000\n",
      "069[s], Ep: 10, Ct: 0000|TR LOSS: 309.84, PPL: 1035|TM NLL: 299.48, KL: 3.93, REG:0.06 | LM NLL: 5.94, KL: 0.67|DE LOSS: 303.62, PPL: 995, TM: 298.23, LM: 5.39|BETA: 0.000069\n",
      "088[s], Ep: 10, Ct: 0500|TR LOSS: 309.81, PPL: 1034|TM NLL: 299.46, KL: 3.93, REG:0.06 | LM NLL: 5.94, KL: 0.67|DE LOSS: 303.39, PPL: 987, TM: 298.01, LM: 5.38|BETA: 0.034743\n",
      "087[s], Ep: 10, Ct: 1000|TR LOSS: 309.72, PPL: 1033|TM NLL: 299.38, KL: 3.93, REG:0.06 | LM NLL: 5.93, KL: 0.67|DE LOSS: 303.39, PPL: 988, TM: 298.00, LM: 5.40|BETA: 0.069417\n",
      "087[s], Ep: 10, Ct: 1500|TR LOSS: 309.67, PPL: 1032|TM NLL: 299.34, KL: 3.94, REG:0.06 | LM NLL: 5.92, KL: 0.67|DE LOSS: 303.55, PPL: 992, TM: 298.14, LM: 5.41|BETA: 0.104092\n",
      "033[s], Ep: 10, Ct: 2000|TR LOSS: 309.62, PPL: 1031|TM NLL: 299.31, KL: 3.94, REG:0.06 | LM NLL: 5.91, KL: 0.67|DE LOSS: 303.56, PPL: 987, TM: 298.14, LM: 5.42|BETA: 0.138766\n",
      "092[s], Ep: 10, Ct: 2500|TR LOSS: 309.59, PPL: 1030|TM NLL: 299.29, KL: 3.95, REG:0.06 | LM NLL: 5.90, KL: 0.67|DE LOSS: 303.43, PPL: 986, TM: 298.00, LM: 5.43|BETA: 0.173440\n",
      "073[s], Ep: 11, Ct: 0000|TR LOSS: 309.56, PPL: 1029|TM NLL: 299.26, KL: 3.95, REG:0.06 | LM NLL: 5.90, KL: 0.67|DE LOSS: 303.60, PPL: 988, TM: 298.15, LM: 5.44|BETA: 0.200069\n",
      "092[s], Ep: 11, Ct: 0500|TR LOSS: 309.51, PPL: 1028|TM NLL: 299.22, KL: 3.95, REG:0.06 | LM NLL: 5.89, KL: 0.67|DE LOSS: 303.55, PPL: 987, TM: 298.09, LM: 5.46|BETA: 0.234743\n",
      "092[s], Ep: 11, Ct: 1000|TR LOSS: 309.48, PPL: 1028|TM NLL: 299.19, KL: 3.96, REG:0.06 | LM NLL: 5.88, KL: 0.67|DE LOSS: 303.54, PPL: 984, TM: 298.08, LM: 5.46|BETA: 0.269417\n",
      "093[s], Ep: 11, Ct: 1500|TR LOSS: 309.42, PPL: 1027|TM NLL: 299.15, KL: 3.96, REG:0.06 | LM NLL: 5.87, KL: 0.66|DE LOSS: 303.58, PPL: 988, TM: 298.10, LM: 5.47|BETA: 0.304092\n",
      "093[s], Ep: 11, Ct: 2000|TR LOSS: 309.38, PPL: 1026|TM NLL: 299.11, KL: 3.96, REG:0.06 | LM NLL: 5.86, KL: 0.66|DE LOSS: 303.53, PPL: 986, TM: 298.04, LM: 5.48|BETA: 0.338766\n",
      "092[s], Ep: 11, Ct: 2500|TR LOSS: 309.35, PPL: 1025|TM NLL: 299.09, KL: 3.97, REG:0.06 | LM NLL: 5.86, KL: 0.66|DE LOSS: 303.59, PPL: 984, TM: 298.09, LM: 5.50|BETA: 0.373440\n",
      "073[s], Ep: 12, Ct: 0000|TR LOSS: 309.33, PPL: 1025|TM NLL: 299.07, KL: 3.97, REG:0.06 | LM NLL: 5.85, KL: 0.66|DE LOSS: 303.39, PPL: 983, TM: 297.89, LM: 5.50|BETA: 0.400069\n",
      "092[s], Ep: 12, Ct: 0500|TR LOSS: 309.27, PPL: 1024|TM NLL: 299.02, KL: 3.97, REG:0.06 | LM NLL: 5.84, KL: 0.66|DE LOSS: 303.71, PPL: 987, TM: 298.19, LM: 5.53|BETA: 0.434743\n",
      "092[s], Ep: 12, Ct: 1000|TR LOSS: 309.23, PPL: 1023|TM NLL: 298.99, KL: 3.97, REG:0.06 | LM NLL: 5.84, KL: 0.66|DE LOSS: 303.51, PPL: 987, TM: 297.98, LM: 5.53|BETA: 0.469417\n",
      "092[s], Ep: 12, Ct: 1500|TR LOSS: 309.21, PPL: 1022|TM NLL: 298.97, KL: 3.98, REG:0.06 | LM NLL: 5.83, KL: 0.65|DE LOSS: 303.60, PPL: 986, TM: 298.05, LM: 5.55|BETA: 0.504092\n",
      "094[s], Ep: 12, Ct: 2000|TR LOSS: 309.19, PPL: 1022|TM NLL: 298.96, KL: 3.98, REG:0.06 | LM NLL: 5.82, KL: 0.65|DE LOSS: 303.59, PPL: 983, TM: 298.03, LM: 5.56|BETA: 0.538766\n",
      "092[s], Ep: 12, Ct: 2500|TR LOSS: 309.17, PPL: 1021|TM NLL: 298.94, KL: 3.99, REG:0.06 | LM NLL: 5.81, KL: 0.65|DE LOSS: 303.59, PPL: 986, TM: 298.03, LM: 5.56|BETA: 0.573440\n",
      "069[s], Ep: 13, Ct: 0000|TR LOSS: 309.13, PPL: 1020|TM NLL: 298.90, KL: 3.99, REG:0.06 | LM NLL: 5.81, KL: 0.65|DE LOSS: 303.41, PPL: 981, TM: 297.83, LM: 5.57|BETA: 0.600069\n",
      "088[s], Ep: 13, Ct: 0500|TR LOSS: 309.09, PPL: 1020|TM NLL: 298.87, KL: 3.99, REG:0.06 | LM NLL: 5.80, KL: 0.65|DE LOSS: 303.66, PPL: 986, TM: 298.06, LM: 5.60|BETA: 0.634743\n",
      "088[s], Ep: 13, Ct: 1000|TR LOSS: 309.05, PPL: 1019|TM NLL: 298.83, KL: 3.99, REG:0.06 | LM NLL: 5.80, KL: 0.65|DE LOSS: 303.79, PPL: 987, TM: 298.17, LM: 5.62|BETA: 0.669418\n",
      "088[s], Ep: 13, Ct: 1500|TR LOSS: 309.01, PPL: 1018|TM NLL: 298.80, KL: 4.00, REG:0.06 | LM NLL: 5.79, KL: 0.64|DE LOSS: 303.64, PPL: 983, TM: 298.01, LM: 5.63|BETA: 0.704092\n",
      "087[s], Ep: 13, Ct: 2000|TR LOSS: 308.98, PPL: 1018|TM NLL: 298.77, KL: 4.00, REG:0.06 | LM NLL: 5.78, KL: 0.64|DE LOSS: 303.60, PPL: 982, TM: 297.92, LM: 5.68|BETA: 0.738766\n",
      "087[s], Ep: 13, Ct: 2500|TR LOSS: 308.97, PPL: 1017|TM NLL: 298.77, KL: 4.00, REG:0.06 | LM NLL: 5.78, KL: 0.64|DE LOSS: 303.49, PPL: 976, TM: 297.86, LM: 5.64|BETA: 0.773440\n",
      "069[s], Ep: 14, Ct: 0000|TR LOSS: 308.96, PPL: 1017|TM NLL: 298.76, KL: 4.01, REG:0.06 | LM NLL: 5.77, KL: 0.64|DE LOSS: 303.63, PPL: 984, TM: 297.98, LM: 5.65|BETA: 0.800069\n",
      "088[s], Ep: 14, Ct: 0500|TR LOSS: 308.94, PPL: 1016|TM NLL: 298.74, KL: 4.01, REG:0.06 | LM NLL: 5.76, KL: 0.64|DE LOSS: 303.68, PPL: 983, TM: 298.00, LM: 5.68|BETA: 0.834743\n",
      "088[s], Ep: 14, Ct: 1000|TR LOSS: 308.92, PPL: 1016|TM NLL: 298.72, KL: 4.01, REG:0.06 | LM NLL: 5.76, KL: 0.64|DE LOSS: 303.66, PPL: 983, TM: 297.97, LM: 5.69|BETA: 0.869417\n",
      "088[s], Ep: 14, Ct: 1500|TR LOSS: 308.89, PPL: 1015|TM NLL: 298.69, KL: 4.02, REG:0.06 | LM NLL: 5.75, KL: 0.64|DE LOSS: 303.76, PPL: 985, TM: 298.06, LM: 5.70|BETA: 0.904092\n",
      "087[s], Ep: 14, Ct: 2000|TR LOSS: 308.86, PPL: 1015|TM NLL: 298.67, KL: 4.02, REG:0.06 | LM NLL: 5.75, KL: 0.63|DE LOSS: 303.70, PPL: 982, TM: 297.98, LM: 5.72|BETA: 0.938766\n",
      "088[s], Ep: 14, Ct: 2500|TR LOSS: 308.83, PPL: 1014|TM NLL: 298.64, KL: 4.02, REG:0.06 | LM NLL: 5.74, KL: 0.63|DE LOSS: 303.59, PPL: 982, TM: 297.85, LM: 5.74|BETA: 0.973440\n",
      "069[s], Ep: 15, Ct: 0000|TR LOSS: 308.81, PPL: 1014|TM NLL: 298.62, KL: 4.02, REG:0.06 | LM NLL: 5.74, KL: 0.63|DE LOSS: 303.75, PPL: 983, TM: 298.01, LM: 5.73|BETA: 1.000000\n",
      "088[s], Ep: 15, Ct: 0500|TR LOSS: 308.79, PPL: 1013|TM NLL: 298.60, KL: 4.03, REG:0.06 | LM NLL: 5.73, KL: 0.63|DE LOSS: 303.80, PPL: 984, TM: 298.07, LM: 5.74|BETA: 1.000000\n",
      "087[s], Ep: 15, Ct: 1000|TR LOSS: 308.77, PPL: 1012|TM NLL: 298.58, KL: 4.03, REG:0.06 | LM NLL: 5.72, KL: 0.63|DE LOSS: 303.61, PPL: 979, TM: 297.88, LM: 5.73|BETA: 1.000000\n",
      "088[s], Ep: 15, Ct: 1500|TR LOSS: 308.74, PPL: 1012|TM NLL: 298.55, KL: 4.03, REG:0.06 | LM NLL: 5.72, KL: 0.63|DE LOSS: 303.66, PPL: 980, TM: 297.94, LM: 5.72|BETA: 1.000000\n",
      "088[s], Ep: 15, Ct: 2000|TR LOSS: 308.73, PPL: 1011|TM NLL: 298.55, KL: 4.04, REG:0.06 | LM NLL: 5.71, KL: 0.63|DE LOSS: 303.75, PPL: 982, TM: 298.01, LM: 5.74|BETA: 1.000000\n",
      "087[s], Ep: 15, Ct: 2500|TR LOSS: 308.69, PPL: 1011|TM NLL: 298.50, KL: 4.04, REG:0.05 | LM NLL: 5.71, KL: 0.62|DE LOSS: 303.61, PPL: 979, TM: 297.90, LM: 5.71|BETA: 1.000000\n",
      "069[s], Ep: 16, Ct: 0000|TR LOSS: 308.68, PPL: 1011|TM NLL: 298.50, KL: 4.04, REG:0.05 | LM NLL: 5.70, KL: 0.62|DE LOSS: 303.56, PPL: 977, TM: 297.87, LM: 5.69|BETA: 1.000000\n",
      "087[s], Ep: 16, Ct: 0500|TR LOSS: 308.65, PPL: 1010|TM NLL: 298.47, KL: 4.04, REG:0.05 | LM NLL: 5.70, KL: 0.62|DE LOSS: 303.59, PPL: 978, TM: 297.87, LM: 5.72|BETA: 1.000000\n",
      "088[s], Ep: 16, Ct: 1000|TR LOSS: 308.63, PPL: 1010|TM NLL: 298.45, KL: 4.05, REG:0.05 | LM NLL: 5.69, KL: 0.62|DE LOSS: 303.59, PPL: 978, TM: 297.91, LM: 5.68|BETA: 1.000000\n",
      "087[s], Ep: 16, Ct: 1500|TR LOSS: 308.61, PPL: 1009|TM NLL: 298.44, KL: 4.05, REG:0.05 | LM NLL: 5.69, KL: 0.62|DE LOSS: 303.57, PPL: 981, TM: 297.89, LM: 5.68|BETA: 1.000000\n",
      "088[s], Ep: 16, Ct: 2000|TR LOSS: 308.59, PPL: 1009|TM NLL: 298.42, KL: 4.05, REG:0.05 | LM NLL: 5.68, KL: 0.62|DE LOSS: 303.41, PPL: 976, TM: 297.75, LM: 5.67|BETA: 1.000000\n",
      "088[s], Ep: 16, Ct: 2500|TR LOSS: 308.57, PPL: 1008|TM NLL: 298.40, KL: 4.05, REG:0.05 | LM NLL: 5.68, KL: 0.62|DE LOSS: 303.47, PPL: 977, TM: 297.82, LM: 5.65|BETA: 1.000000\n",
      "069[s], Ep: 17, Ct: 0000|TR LOSS: 308.56, PPL: 1008|TM NLL: 298.39, KL: 4.06, REG:0.05 | LM NLL: 5.67, KL: 0.61|DE LOSS: 303.52, PPL: 977, TM: 297.84, LM: 5.67|BETA: 1.000000\n",
      "087[s], Ep: 17, Ct: 0500|TR LOSS: 308.54, PPL: 1007|TM NLL: 298.37, KL: 4.06, REG:0.05 | LM NLL: 5.67, KL: 0.61|DE LOSS: 303.60, PPL: 978, TM: 297.93, LM: 5.67|BETA: 1.000000\n",
      "088[s], Ep: 17, Ct: 1000|TR LOSS: 308.53, PPL: 1007|TM NLL: 298.36, KL: 4.06, REG:0.05 | LM NLL: 5.66, KL: 0.61|DE LOSS: 303.57, PPL: 978, TM: 297.90, LM: 5.67|BETA: 1.000000\n",
      "087[s], Ep: 17, Ct: 1500|TR LOSS: 308.50, PPL: 1006|TM NLL: 298.34, KL: 4.07, REG:0.05 | LM NLL: 5.66, KL: 0.61|DE LOSS: 303.56, PPL: 980, TM: 297.90, LM: 5.66|BETA: 1.000000\n",
      "087[s], Ep: 17, Ct: 2000|TR LOSS: 308.49, PPL: 1006|TM NLL: 298.33, KL: 4.07, REG:0.05 | LM NLL: 5.65, KL: 0.61|DE LOSS: 303.59, PPL: 982, TM: 297.93, LM: 5.66|BETA: 1.000000\n",
      "087[s], Ep: 17, Ct: 2500|TR LOSS: 308.47, PPL: 1006|TM NLL: 298.31, KL: 4.07, REG:0.05 | LM NLL: 5.65, KL: 0.61|DE LOSS: 303.51, PPL: 976, TM: 297.86, LM: 5.65|BETA: 1.000000\n",
      "069[s], Ep: 18, Ct: 0000|TR LOSS: 308.45, PPL: 1005|TM NLL: 298.29, KL: 4.07, REG:0.05 | LM NLL: 5.64, KL: 0.61|DE LOSS: 303.37, PPL: 973, TM: 297.73, LM: 5.64|BETA: 1.000000\n",
      "087[s], Ep: 18, Ct: 0500|TR LOSS: 308.43, PPL: 1005|TM NLL: 298.27, KL: 4.07, REG:0.05 | LM NLL: 5.64, KL: 0.60|DE LOSS: 303.60, PPL: 976, TM: 297.94, LM: 5.66|BETA: 1.000000\n",
      "088[s], Ep: 18, Ct: 1000|TR LOSS: 308.43, PPL: 1005|TM NLL: 298.27, KL: 4.08, REG:0.05 | LM NLL: 5.63, KL: 0.60|DE LOSS: 303.53, PPL: 977, TM: 297.89, LM: 5.64|BETA: 1.000000\n",
      "087[s], Ep: 18, Ct: 1500|TR LOSS: 308.42, PPL: 1004|TM NLL: 298.26, KL: 4.08, REG:0.05 | LM NLL: 5.63, KL: 0.60|DE LOSS: 303.58, PPL: 980, TM: 297.95, LM: 5.64|BETA: 1.000000\n",
      "087[s], Ep: 18, Ct: 2000|TR LOSS: 308.38, PPL: 1004|TM NLL: 298.22, KL: 4.08, REG:0.05 | LM NLL: 5.62, KL: 0.60|DE LOSS: 303.51, PPL: 977, TM: 297.87, LM: 5.64|BETA: 1.000000\n",
      "087[s], Ep: 18, Ct: 2500|TR LOSS: 308.36, PPL: 1003|TM NLL: 298.20, KL: 4.08, REG:0.05 | LM NLL: 5.62, KL: 0.60|DE LOSS: 303.49, PPL: 979, TM: 297.85, LM: 5.63|BETA: 1.000000\n",
      "069[s], Ep: 19, Ct: 0000|TR LOSS: 308.35, PPL: 1003|TM NLL: 298.19, KL: 4.09, REG:0.05 | LM NLL: 5.62, KL: 0.60|DE LOSS: 303.47, PPL: 975, TM: 297.85, LM: 5.61|BETA: 1.000000\n",
      "087[s], Ep: 19, Ct: 0500|TR LOSS: 308.32, PPL: 1003|TM NLL: 298.17, KL: 4.09, REG:0.05 | LM NLL: 5.61, KL: 0.60|DE LOSS: 303.37, PPL: 976, TM: 297.74, LM: 5.63|BETA: 1.000000\n",
      "087[s], Ep: 19, Ct: 1000|TR LOSS: 308.29, PPL: 1002|TM NLL: 298.14, KL: 4.09, REG:0.05 | LM NLL: 5.61, KL: 0.60|DE LOSS: 303.39, PPL: 974, TM: 297.74, LM: 5.65|BETA: 1.000000\n",
      "088[s], Ep: 19, Ct: 1500|TR LOSS: 308.29, PPL: 1002|TM NLL: 298.14, KL: 4.09, REG:0.05 | LM NLL: 5.60, KL: 0.59|DE LOSS: 303.46, PPL: 976, TM: 297.85, LM: 5.61|BETA: 1.000000\n",
      "087[s], Ep: 19, Ct: 2000|TR LOSS: 308.27, PPL: 1002|TM NLL: 298.12, KL: 4.10, REG:0.05 | LM NLL: 5.60, KL: 0.59|DE LOSS: 303.58, PPL: 977, TM: 297.95, LM: 5.64|BETA: 1.000000\n",
      "088[s], Ep: 19, Ct: 2500|TR LOSS: 308.26, PPL: 1001|TM NLL: 298.12, KL: 4.10, REG:0.05 | LM NLL: 5.59, KL: 0.59|DE LOSS: 303.46, PPL: 978, TM: 297.85, LM: 5.61|BETA: 1.000000\n",
      "069[s], Ep: 20, Ct: 0000|TR LOSS: 308.25, PPL: 1001|TM NLL: 298.10, KL: 4.10, REG:0.05 | LM NLL: 5.59, KL: 0.59|DE LOSS: 302.83, PPL: 976, TM: 297.77, LM: 5.06|BETA: 0.000069\n",
      "087[s], Ep: 20, Ct: 0500|TR LOSS: 308.21, PPL: 1001|TM NLL: 298.07, KL: 4.10, REG:0.05 | LM NLL: 5.59, KL: 0.59|DE LOSS: 302.86, PPL: 978, TM: 297.80, LM: 5.06|BETA: 0.034743\n",
      "087[s], Ep: 20, Ct: 1000|TR LOSS: 308.19, PPL: 1000|TM NLL: 298.06, KL: 4.10, REG:0.05 | LM NLL: 5.58, KL: 0.59|DE LOSS: 302.99, PPL: 980, TM: 297.91, LM: 5.08|BETA: 0.069417\n",
      "087[s], Ep: 20, Ct: 1500|TR LOSS: 308.18, PPL: 1000|TM NLL: 298.06, KL: 4.11, REG:0.05 | LM NLL: 5.58, KL: 0.59|DE LOSS: 302.87, PPL: 975, TM: 297.77, LM: 5.10|BETA: 0.104092\n",
      "087[s], Ep: 20, Ct: 2000|TR LOSS: 308.16, PPL: 1000|TM NLL: 298.04, KL: 4.11, REG:0.05 | LM NLL: 5.57, KL: 0.59|DE LOSS: 302.88, PPL: 975, TM: 297.76, LM: 5.11|BETA: 0.138766\n",
      "088[s], Ep: 20, Ct: 2500|TR LOSS: 308.14, PPL: 999|TM NLL: 298.03, KL: 4.11, REG:0.05 | LM NLL: 5.57, KL: 0.59|DE LOSS: 302.81, PPL: 973, TM: 297.69, LM: 5.12|BETA: 0.173440\n",
      "069[s], Ep: 21, Ct: 0000|TR LOSS: 308.14, PPL: 999|TM NLL: 298.03, KL: 4.11, REG:0.05 | LM NLL: 5.57, KL: 0.59|DE LOSS: 302.88, PPL: 973, TM: 297.74, LM: 5.14|BETA: 0.200069\n",
      "087[s], Ep: 21, Ct: 0500|TR LOSS: 308.11, PPL: 999|TM NLL: 298.00, KL: 4.11, REG:0.05 | LM NLL: 5.56, KL: 0.58|DE LOSS: 302.95, PPL: 978, TM: 297.79, LM: 5.16|BETA: 0.234743\n",
      "087[s], Ep: 21, Ct: 1000|TR LOSS: 308.08, PPL: 998|TM NLL: 297.98, KL: 4.11, REG:0.05 | LM NLL: 5.56, KL: 0.58|DE LOSS: 303.05, PPL: 976, TM: 297.86, LM: 5.18|BETA: 0.269417\n",
      "087[s], Ep: 21, Ct: 1500|TR LOSS: 308.07, PPL: 998|TM NLL: 297.97, KL: 4.11, REG:0.05 | LM NLL: 5.55, KL: 0.58|DE LOSS: 302.98, PPL: 975, TM: 297.78, LM: 5.19|BETA: 0.304092\n",
      "087[s], Ep: 21, Ct: 2000|TR LOSS: 308.06, PPL: 998|TM NLL: 297.97, KL: 4.12, REG:0.05 | LM NLL: 5.55, KL: 0.58|DE LOSS: 302.94, PPL: 973, TM: 297.73, LM: 5.21|BETA: 0.338766\n",
      "088[s], Ep: 21, Ct: 2500|TR LOSS: 308.05, PPL: 997|TM NLL: 297.97, KL: 4.12, REG:0.05 | LM NLL: 5.54, KL: 0.58|DE LOSS: 303.07, PPL: 976, TM: 297.84, LM: 5.23|BETA: 0.373440\n",
      "069[s], Ep: 22, Ct: 0000|TR LOSS: 308.04, PPL: 997|TM NLL: 297.95, KL: 4.12, REG:0.05 | LM NLL: 5.54, KL: 0.58|DE LOSS: 302.94, PPL: 973, TM: 297.70, LM: 5.24|BETA: 0.400069\n",
      "087[s], Ep: 22, Ct: 0500|TR LOSS: 308.02, PPL: 997|TM NLL: 297.94, KL: 4.12, REG:0.05 | LM NLL: 5.54, KL: 0.58|DE LOSS: 303.02, PPL: 973, TM: 297.77, LM: 5.25|BETA: 0.434743\n",
      "088[s], Ep: 22, Ct: 1000|TR LOSS: 308.01, PPL: 997|TM NLL: 297.94, KL: 4.12, REG:0.05 | LM NLL: 5.53, KL: 0.58|DE LOSS: 303.21, PPL: 978, TM: 297.93, LM: 5.28|BETA: 0.469417\n",
      "087[s], Ep: 22, Ct: 1500|TR LOSS: 307.99, PPL: 996|TM NLL: 297.92, KL: 4.13, REG:0.05 | LM NLL: 5.53, KL: 0.58|DE LOSS: 303.11, PPL: 975, TM: 297.82, LM: 5.29|BETA: 0.504092\n",
      "088[s], Ep: 22, Ct: 2000|TR LOSS: 307.98, PPL: 996|TM NLL: 297.91, KL: 4.13, REG:0.05 | LM NLL: 5.53, KL: 0.58|DE LOSS: 303.09, PPL: 973, TM: 297.78, LM: 5.31|BETA: 0.538766\n",
      "088[s], Ep: 22, Ct: 2500|TR LOSS: 307.97, PPL: 996|TM NLL: 297.90, KL: 4.13, REG:0.05 | LM NLL: 5.52, KL: 0.58|DE LOSS: 303.13, PPL: 974, TM: 297.79, LM: 5.34|BETA: 0.573440\n",
      "069[s], Ep: 23, Ct: 0000|TR LOSS: 307.95, PPL: 996|TM NLL: 297.88, KL: 4.13, REG:0.05 | LM NLL: 5.52, KL: 0.57|DE LOSS: 303.17, PPL: 976, TM: 297.82, LM: 5.34|BETA: 0.600069\n",
      "088[s], Ep: 23, Ct: 0500|TR LOSS: 307.94, PPL: 995|TM NLL: 297.88, KL: 4.13, REG:0.05 | LM NLL: 5.52, KL: 0.57|DE LOSS: 303.27, PPL: 979, TM: 297.91, LM: 5.36|BETA: 0.634743\n",
      "087[s], Ep: 23, Ct: 1000|TR LOSS: 307.94, PPL: 995|TM NLL: 297.88, KL: 4.13, REG:0.05 | LM NLL: 5.51, KL: 0.57|DE LOSS: 303.12, PPL: 975, TM: 297.75, LM: 5.37|BETA: 0.669418\n",
      "087[s], Ep: 23, Ct: 1500|TR LOSS: 307.92, PPL: 995|TM NLL: 297.86, KL: 4.13, REG:0.05 | LM NLL: 5.51, KL: 0.57|DE LOSS: 303.25, PPL: 977, TM: 297.86, LM: 5.39|BETA: 0.704092\n",
      "087[s], Ep: 23, Ct: 2000|TR LOSS: 307.90, PPL: 994|TM NLL: 297.85, KL: 4.14, REG:0.05 | LM NLL: 5.50, KL: 0.57|DE LOSS: 303.19, PPL: 974, TM: 297.78, LM: 5.41|BETA: 0.738766\n",
      "087[s], Ep: 23, Ct: 2500|TR LOSS: 307.88, PPL: 994|TM NLL: 297.82, KL: 4.14, REG:0.05 | LM NLL: 5.50, KL: 0.57|DE LOSS: 303.13, PPL: 972, TM: 297.71, LM: 5.42|BETA: 0.773440\n",
      "069[s], Ep: 24, Ct: 0000|TR LOSS: 307.87, PPL: 994|TM NLL: 297.82, KL: 4.14, REG:0.05 | LM NLL: 5.50, KL: 0.57|DE LOSS: 303.16, PPL: 971, TM: 297.59, LM: 5.57|BETA: 0.800069\n",
      "087[s], Ep: 24, Ct: 0500|TR LOSS: 307.87, PPL: 994|TM NLL: 297.82, KL: 4.14, REG:0.05 | LM NLL: 5.49, KL: 0.57|DE LOSS: 303.50, PPL: 982, TM: 298.03, LM: 5.47|BETA: 0.834743\n",
      "087[s], Ep: 24, Ct: 1000|TR LOSS: 307.85, PPL: 993|TM NLL: 297.80, KL: 4.14, REG:0.05 | LM NLL: 5.49, KL: 0.57|DE LOSS: 303.37, PPL: 976, TM: 297.89, LM: 5.48|BETA: 0.869417\n",
      "087[s], Ep: 24, Ct: 1500|TR LOSS: 307.84, PPL: 993|TM NLL: 297.79, KL: 4.14, REG:0.05 | LM NLL: 5.49, KL: 0.57|DE LOSS: 303.19, PPL: 974, TM: 297.71, LM: 5.48|BETA: 0.904092\n",
      "087[s], Ep: 24, Ct: 2000|TR LOSS: 307.82, PPL: 993|TM NLL: 297.78, KL: 4.15, REG:0.05 | LM NLL: 5.48, KL: 0.57|DE LOSS: 303.22, PPL: 972, TM: 297.66, LM: 5.56|BETA: 0.938766\n",
      "088[s], Ep: 24, Ct: 2500|TR LOSS: 307.81, PPL: 993|TM NLL: 297.77, KL: 4.15, REG:0.05 | LM NLL: 5.48, KL: 0.56|DE LOSS: 303.42, PPL: 977, TM: 297.88, LM: 5.54|BETA: 0.973440\n",
      "069[s], Ep: 25, Ct: 0000|TR LOSS: 307.80, PPL: 993|TM NLL: 297.76, KL: 4.15, REG:0.05 | LM NLL: 5.48, KL: 0.56|DE LOSS: 303.23, PPL: 976, TM: 297.71, LM: 5.52|BETA: 1.000000\n",
      "088[s], Ep: 25, Ct: 0500|TR LOSS: 307.79, PPL: 992|TM NLL: 297.75, KL: 4.15, REG:0.05 | LM NLL: 5.47, KL: 0.56|DE LOSS: 303.18, PPL: 970, TM: 297.66, LM: 5.51|BETA: 1.000000\n",
      "087[s], Ep: 25, Ct: 1000|TR LOSS: 307.77, PPL: 992|TM NLL: 297.74, KL: 4.15, REG:0.05 | LM NLL: 5.47, KL: 0.56|DE LOSS: 303.21, PPL: 971, TM: 297.68, LM: 5.53|BETA: 1.000000\n",
      "087[s], Ep: 25, Ct: 1500|TR LOSS: 307.75, PPL: 992|TM NLL: 297.72, KL: 4.15, REG:0.05 | LM NLL: 5.47, KL: 0.56|DE LOSS: 303.36, PPL: 976, TM: 297.81, LM: 5.55|BETA: 1.000000\n",
      "087[s], Ep: 25, Ct: 2000|TR LOSS: 307.74, PPL: 992|TM NLL: 297.71, KL: 4.16, REG:0.05 | LM NLL: 5.46, KL: 0.56|DE LOSS: 303.40, PPL: 978, TM: 297.88, LM: 5.52|BETA: 1.000000\n",
      "087[s], Ep: 25, Ct: 2500|TR LOSS: 307.74, PPL: 991|TM NLL: 297.70, KL: 4.16, REG:0.05 | LM NLL: 5.46, KL: 0.56|DE LOSS: 303.28, PPL: 973, TM: 297.76, LM: 5.52|BETA: 1.000000\n",
      "069[s], Ep: 26, Ct: 0000|TR LOSS: 307.74, PPL: 991|TM NLL: 297.70, KL: 4.16, REG:0.05 | LM NLL: 5.46, KL: 0.56|DE LOSS: 303.10, PPL: 972, TM: 297.58, LM: 5.52|BETA: 1.000000\n",
      "087[s], Ep: 26, Ct: 0500|TR LOSS: 307.73, PPL: 991|TM NLL: 297.70, KL: 4.16, REG:0.05 | LM NLL: 5.46, KL: 0.56|DE LOSS: 303.18, PPL: 971, TM: 297.66, LM: 5.52|BETA: 1.000000\n",
      "088[s], Ep: 26, Ct: 1000|TR LOSS: 307.72, PPL: 991|TM NLL: 297.69, KL: 4.16, REG:0.05 | LM NLL: 5.45, KL: 0.56|DE LOSS: 303.29, PPL: 971, TM: 297.78, LM: 5.51|BETA: 1.000000\n",
      "088[s], Ep: 26, Ct: 1500|TR LOSS: 307.71, PPL: 990|TM NLL: 297.68, KL: 4.16, REG:0.05 | LM NLL: 5.45, KL: 0.56|DE LOSS: 303.31, PPL: 973, TM: 297.81, LM: 5.50|BETA: 1.000000\n",
      "088[s], Ep: 26, Ct: 2000|TR LOSS: 307.69, PPL: 990|TM NLL: 297.66, KL: 4.16, REG:0.05 | LM NLL: 5.45, KL: 0.55|DE LOSS: 303.10, PPL: 970, TM: 297.62, LM: 5.48|BETA: 1.000000\n",
      "088[s], Ep: 26, Ct: 2500|TR LOSS: 307.68, PPL: 990|TM NLL: 297.66, KL: 4.17, REG:0.05 | LM NLL: 5.44, KL: 0.55|DE LOSS: 303.17, PPL: 970, TM: 297.66, LM: 5.50|BETA: 1.000000\n",
      "069[s], Ep: 27, Ct: 0000|TR LOSS: 307.67, PPL: 990|TM NLL: 297.65, KL: 4.17, REG:0.05 | LM NLL: 5.44, KL: 0.55|DE LOSS: 303.27, PPL: 973, TM: 297.77, LM: 5.50|BETA: 1.000000\n",
      "088[s], Ep: 27, Ct: 0500|TR LOSS: 307.66, PPL: 989|TM NLL: 297.64, KL: 4.17, REG:0.05 | LM NLL: 5.44, KL: 0.55|DE LOSS: 303.12, PPL: 970, TM: 297.62, LM: 5.50|BETA: 1.000000\n",
      "087[s], Ep: 27, Ct: 1000|TR LOSS: 307.65, PPL: 989|TM NLL: 297.63, KL: 4.17, REG:0.05 | LM NLL: 5.43, KL: 0.55|DE LOSS: 303.18, PPL: 973, TM: 297.68, LM: 5.50|BETA: 1.000000\n",
      "087[s], Ep: 27, Ct: 1500|TR LOSS: 307.65, PPL: 989|TM NLL: 297.62, KL: 4.17, REG:0.05 | LM NLL: 5.43, KL: 0.55|DE LOSS: 303.10, PPL: 967, TM: 297.59, LM: 5.50|BETA: 1.000000\n",
      "087[s], Ep: 27, Ct: 2000|TR LOSS: 307.63, PPL: 989|TM NLL: 297.61, KL: 4.17, REG:0.05 | LM NLL: 5.43, KL: 0.55|DE LOSS: 303.24, PPL: 971, TM: 297.74, LM: 5.50|BETA: 1.000000\n",
      "087[s], Ep: 27, Ct: 2500|TR LOSS: 307.62, PPL: 989|TM NLL: 297.60, KL: 4.18, REG:0.05 | LM NLL: 5.42, KL: 0.55|DE LOSS: 303.09, PPL: 970, TM: 297.59, LM: 5.50|BETA: 1.000000\n",
      "069[s], Ep: 28, Ct: 0000|TR LOSS: 307.61, PPL: 989|TM NLL: 297.60, KL: 4.18, REG:0.05 | LM NLL: 5.42, KL: 0.55|DE LOSS: 303.21, PPL: 970, TM: 297.72, LM: 5.49|BETA: 1.000000\n",
      "057[s], Ep: 28, Ct: 0500|TR LOSS: 307.61, PPL: 988|TM NLL: 297.59, KL: 4.18, REG:0.05 | LM NLL: 5.42, KL: 0.55|DE LOSS: 303.32, PPL: 974, TM: 297.81, LM: 5.51|BETA: 1.000000\n",
      "087[s], Ep: 28, Ct: 1000|TR LOSS: 307.59, PPL: 988|TM NLL: 297.57, KL: 4.18, REG:0.05 | LM NLL: 5.42, KL: 0.55|DE LOSS: 303.30, PPL: 974, TM: 297.79, LM: 5.52|BETA: 1.000000\n",
      "087[s], Ep: 28, Ct: 1500|TR LOSS: 307.58, PPL: 988|TM NLL: 297.57, KL: 4.18, REG:0.05 | LM NLL: 5.41, KL: 0.55|DE LOSS: 303.28, PPL: 974, TM: 297.77, LM: 5.50|BETA: 1.000000\n",
      "022[s], Ep: 28, Ct: 2000|TR LOSS: 307.57, PPL: 988|TM NLL: 297.56, KL: 4.18, REG:0.05 | LM NLL: 5.41, KL: 0.54|DE LOSS: 303.16, PPL: 971, TM: 297.67, LM: 5.49|BETA: 1.000000\n",
      "063[s], Ep: 28, Ct: 2500|TR LOSS: 307.57, PPL: 988|TM NLL: 297.56, KL: 4.18, REG:0.05 | LM NLL: 5.41, KL: 0.54|DE LOSS: 303.10, PPL: 970, TM: 297.61, LM: 5.49|BETA: 1.000000\n",
      "072[s], Ep: 29, Ct: 0000|TR LOSS: 307.56, PPL: 987|TM NLL: 297.55, KL: 4.18, REG:0.05 | LM NLL: 5.41, KL: 0.54|DE LOSS: 303.15, PPL: 969, TM: 297.67, LM: 5.48|BETA: 1.000000\n",
      "093[s], Ep: 29, Ct: 0500|TR LOSS: 307.54, PPL: 987|TM NLL: 297.53, KL: 4.19, REG:0.05 | LM NLL: 5.40, KL: 0.54|DE LOSS: 303.11, PPL: 969, TM: 297.62, LM: 5.49|BETA: 1.000000\n",
      "092[s], Ep: 29, Ct: 1000|TR LOSS: 307.53, PPL: 987|TM NLL: 297.52, KL: 4.19, REG:0.05 | LM NLL: 5.40, KL: 0.54|DE LOSS: 303.15, PPL: 970, TM: 297.67, LM: 5.48|BETA: 1.000000\n",
      "091[s], Ep: 29, Ct: 1500|TR LOSS: 307.53, PPL: 987|TM NLL: 297.52, KL: 4.19, REG:0.05 | LM NLL: 5.40, KL: 0.54|DE LOSS: 303.18, PPL: 972, TM: 297.70, LM: 5.48|BETA: 1.000000\n",
      "091[s], Ep: 29, Ct: 2000|TR LOSS: 307.53, PPL: 987|TM NLL: 297.52, KL: 4.19, REG:0.05 | LM NLL: 5.39, KL: 0.54|DE LOSS: 303.29, PPL: 977, TM: 297.81, LM: 5.48|BETA: 1.000000\n",
      "088[s], Ep: 29, Ct: 2500|TR LOSS: 307.51, PPL: 986|TM NLL: 297.51, KL: 4.19, REG:0.05 | LM NLL: 5.39, KL: 0.54|DE LOSS: 303.11, PPL: 970, TM: 297.63, LM: 5.47|BETA: 1.000000\n",
      "068[s], Ep: 30, Ct: 0000|TR LOSS: 307.50, PPL: 986|TM NLL: 297.50, KL: 4.19, REG:0.05 | LM NLL: 5.39, KL: 0.54|DE LOSS: 302.68, PPL: 971, TM: 297.77, LM: 4.91|BETA: 0.000069\n",
      "087[s], Ep: 30, Ct: 0500|TR LOSS: 307.49, PPL: 986|TM NLL: 297.50, KL: 4.19, REG:0.05 | LM NLL: 5.39, KL: 0.54|DE LOSS: 302.59, PPL: 970, TM: 297.67, LM: 4.92|BETA: 0.034743\n",
      "087[s], Ep: 30, Ct: 1000|TR LOSS: 307.47, PPL: 986|TM NLL: 297.48, KL: 4.20, REG:0.05 | LM NLL: 5.38, KL: 0.54|DE LOSS: 302.74, PPL: 974, TM: 297.80, LM: 4.94|BETA: 0.069417\n",
      "087[s], Ep: 30, Ct: 1500|TR LOSS: 307.47, PPL: 986|TM NLL: 297.48, KL: 4.20, REG:0.05 | LM NLL: 5.38, KL: 0.54|DE LOSS: 302.70, PPL: 975, TM: 297.75, LM: 4.95|BETA: 0.104092\n",
      "087[s], Ep: 30, Ct: 2000|TR LOSS: 307.45, PPL: 985|TM NLL: 297.47, KL: 4.20, REG:0.05 | LM NLL: 5.38, KL: 0.54|DE LOSS: 302.62, PPL: 973, TM: 297.65, LM: 4.97|BETA: 0.138766\n",
      "087[s], Ep: 30, Ct: 2500|TR LOSS: 307.44, PPL: 985|TM NLL: 297.46, KL: 4.20, REG:0.05 | LM NLL: 5.37, KL: 0.54|DE LOSS: 302.61, PPL: 971, TM: 297.62, LM: 4.99|BETA: 0.173440\n",
      "069[s], Ep: 31, Ct: 0000|TR LOSS: 307.44, PPL: 985|TM NLL: 297.46, KL: 4.20, REG:0.05 | LM NLL: 5.37, KL: 0.53|DE LOSS: 302.66, PPL: 970, TM: 297.66, LM: 5.00|BETA: 0.200069\n",
      "087[s], Ep: 31, Ct: 0500|TR LOSS: 307.42, PPL: 985|TM NLL: 297.45, KL: 4.20, REG:0.05 | LM NLL: 5.37, KL: 0.53|DE LOSS: 302.81, PPL: 973, TM: 297.79, LM: 5.02|BETA: 0.234743\n",
      "087[s], Ep: 31, Ct: 1000|TR LOSS: 307.41, PPL: 985|TM NLL: 297.44, KL: 4.20, REG:0.05 | LM NLL: 5.37, KL: 0.53|DE LOSS: 302.69, PPL: 971, TM: 297.65, LM: 5.04|BETA: 0.269417\n",
      "087[s], Ep: 31, Ct: 1500|TR LOSS: 307.40, PPL: 985|TM NLL: 297.43, KL: 4.20, REG:0.05 | LM NLL: 5.36, KL: 0.53|DE LOSS: 302.79, PPL: 971, TM: 297.73, LM: 5.06|BETA: 0.304092\n",
      "087[s], Ep: 31, Ct: 2000|TR LOSS: 307.39, PPL: 984|TM NLL: 297.43, KL: 4.20, REG:0.05 | LM NLL: 5.36, KL: 0.53|DE LOSS: 302.78, PPL: 971, TM: 297.70, LM: 5.08|BETA: 0.338766\n",
      "087[s], Ep: 31, Ct: 2500|TR LOSS: 307.39, PPL: 984|TM NLL: 297.42, KL: 4.20, REG:0.05 | LM NLL: 5.36, KL: 0.53|DE LOSS: 302.55, PPL: 967, TM: 297.45, LM: 5.09|BETA: 0.373440\n",
      "069[s], Ep: 32, Ct: 0000|TR LOSS: 307.38, PPL: 984|TM NLL: 297.41, KL: 4.21, REG:0.05 | LM NLL: 5.36, KL: 0.53|DE LOSS: 302.75, PPL: 971, TM: 297.64, LM: 5.11|BETA: 0.400069\n",
      "087[s], Ep: 32, Ct: 0500|TR LOSS: 307.36, PPL: 984|TM NLL: 297.40, KL: 4.21, REG:0.05 | LM NLL: 5.35, KL: 0.53|DE LOSS: 302.83, PPL: 969, TM: 297.70, LM: 5.13|BETA: 0.434743\n",
      "087[s], Ep: 32, Ct: 1000|TR LOSS: 307.36, PPL: 984|TM NLL: 297.40, KL: 4.21, REG:0.05 | LM NLL: 5.35, KL: 0.53|DE LOSS: 302.88, PPL: 972, TM: 297.72, LM: 5.16|BETA: 0.469417\n",
      "087[s], Ep: 32, Ct: 1500|TR LOSS: 307.34, PPL: 984|TM NLL: 297.39, KL: 4.21, REG:0.05 | LM NLL: 5.35, KL: 0.53|DE LOSS: 303.05, PPL: 980, TM: 297.88, LM: 5.17|BETA: 0.504092\n",
      "087[s], Ep: 32, Ct: 2000|TR LOSS: 307.33, PPL: 984|TM NLL: 297.38, KL: 4.21, REG:0.05 | LM NLL: 5.35, KL: 0.53|DE LOSS: 302.96, PPL: 972, TM: 297.78, LM: 5.18|BETA: 0.538766\n",
      "087[s], Ep: 32, Ct: 2500|TR LOSS: 307.33, PPL: 983|TM NLL: 297.37, KL: 4.21, REG:0.05 | LM NLL: 5.34, KL: 0.53|DE LOSS: 302.83, PPL: 972, TM: 297.64, LM: 5.20|BETA: 0.573440\n",
      "069[s], Ep: 33, Ct: 0000|TR LOSS: 307.32, PPL: 983|TM NLL: 297.37, KL: 4.21, REG:0.05 | LM NLL: 5.34, KL: 0.53|DE LOSS: 303.01, PPL: 974, TM: 297.76, LM: 5.25|BETA: 0.600069\n",
      "087[s], Ep: 33, Ct: 0500|TR LOSS: 307.31, PPL: 983|TM NLL: 297.36, KL: 4.21, REG:0.05 | LM NLL: 5.34, KL: 0.53|DE LOSS: 302.80, PPL: 969, TM: 297.56, LM: 5.24|BETA: 0.634743\n",
      "087[s], Ep: 33, Ct: 1000|TR LOSS: 307.30, PPL: 983|TM NLL: 297.35, KL: 4.21, REG:0.05 | LM NLL: 5.34, KL: 0.53|DE LOSS: 302.82, PPL: 967, TM: 297.56, LM: 5.26|BETA: 0.669418\n",
      "087[s], Ep: 33, Ct: 1500|TR LOSS: 307.29, PPL: 983|TM NLL: 297.35, KL: 4.21, REG:0.05 | LM NLL: 5.33, KL: 0.52|DE LOSS: 303.00, PPL: 972, TM: 297.72, LM: 5.28|BETA: 0.704092\n",
      "087[s], Ep: 33, Ct: 2000|TR LOSS: 307.29, PPL: 983|TM NLL: 297.35, KL: 4.22, REG:0.05 | LM NLL: 5.33, KL: 0.52|DE LOSS: 302.98, PPL: 971, TM: 297.69, LM: 5.30|BETA: 0.738766\n",
      "087[s], Ep: 33, Ct: 2500|TR LOSS: 307.29, PPL: 983|TM NLL: 297.35, KL: 4.22, REG:0.05 | LM NLL: 5.33, KL: 0.52|DE LOSS: 302.99, PPL: 970, TM: 297.69, LM: 5.31|BETA: 0.773440\n",
      "069[s], Ep: 34, Ct: 0000|TR LOSS: 307.28, PPL: 982|TM NLL: 297.34, KL: 4.22, REG:0.05 | LM NLL: 5.33, KL: 0.52|DE LOSS: 303.02, PPL: 972, TM: 297.69, LM: 5.33|BETA: 0.800069\n",
      "088[s], Ep: 34, Ct: 0500|TR LOSS: 307.28, PPL: 982|TM NLL: 297.34, KL: 4.22, REG:0.05 | LM NLL: 5.33, KL: 0.52|DE LOSS: 302.93, PPL: 964, TM: 297.47, LM: 5.46|BETA: 0.834743\n",
      "087[s], Ep: 34, Ct: 1000|TR LOSS: 307.27, PPL: 982|TM NLL: 297.34, KL: 4.22, REG:0.05 | LM NLL: 5.32, KL: 0.52|DE LOSS: 303.19, PPL: 971, TM: 297.82, LM: 5.37|BETA: 0.869417\n",
      "087[s], Ep: 34, Ct: 1500|TR LOSS: 307.26, PPL: 982|TM NLL: 297.32, KL: 4.22, REG:0.05 | LM NLL: 5.32, KL: 0.52|DE LOSS: 302.94, PPL: 966, TM: 297.57, LM: 5.37|BETA: 0.904092\n",
      "087[s], Ep: 34, Ct: 2000|TR LOSS: 307.24, PPL: 982|TM NLL: 297.31, KL: 4.22, REG:0.05 | LM NLL: 5.32, KL: 0.52|DE LOSS: 303.05, PPL: 969, TM: 297.64, LM: 5.41|BETA: 0.938766\n",
      "087[s], Ep: 34, Ct: 2500|TR LOSS: 307.23, PPL: 982|TM NLL: 297.30, KL: 4.22, REG:0.05 | LM NLL: 5.32, KL: 0.52|DE LOSS: 303.11, PPL: 969, TM: 297.70, LM: 5.42|BETA: 0.973440\n",
      "069[s], Ep: 35, Ct: 0000|TR LOSS: 307.23, PPL: 981|TM NLL: 297.30, KL: 4.22, REG:0.05 | LM NLL: 5.31, KL: 0.52|DE LOSS: 303.08, PPL: 971, TM: 297.64, LM: 5.44|BETA: 1.000000\n",
      "087[s], Ep: 35, Ct: 0500|TR LOSS: 307.22, PPL: 981|TM NLL: 297.29, KL: 4.23, REG:0.05 | LM NLL: 5.31, KL: 0.52|DE LOSS: 303.09, PPL: 969, TM: 297.66, LM: 5.44|BETA: 1.000000\n",
      "087[s], Ep: 35, Ct: 1000|TR LOSS: 307.21, PPL: 981|TM NLL: 297.28, KL: 4.23, REG:0.05 | LM NLL: 5.31, KL: 0.52|DE LOSS: 303.07, PPL: 968, TM: 297.62, LM: 5.45|BETA: 1.000000\n",
      "087[s], Ep: 35, Ct: 1500|TR LOSS: 307.20, PPL: 981|TM NLL: 297.27, KL: 4.23, REG:0.05 | LM NLL: 5.31, KL: 0.52|DE LOSS: 303.07, PPL: 968, TM: 297.63, LM: 5.44|BETA: 1.000000\n",
      "087[s], Ep: 35, Ct: 2000|TR LOSS: 307.19, PPL: 981|TM NLL: 297.27, KL: 4.23, REG:0.05 | LM NLL: 5.30, KL: 0.52|DE LOSS: 303.22, PPL: 973, TM: 297.77, LM: 5.45|BETA: 1.000000\n",
      "087[s], Ep: 35, Ct: 2500|TR LOSS: 307.19, PPL: 981|TM NLL: 297.26, KL: 4.23, REG:0.05 | LM NLL: 5.30, KL: 0.52|DE LOSS: 303.32, PPL: 968, TM: 297.63, LM: 5.69|BETA: 1.000000\n",
      "069[s], Ep: 36, Ct: 0000|TR LOSS: 307.19, PPL: 981|TM NLL: 297.26, KL: 4.23, REG:0.05 | LM NLL: 5.30, KL: 0.51|DE LOSS: 303.08, PPL: 970, TM: 297.66, LM: 5.42|BETA: 1.000000\n",
      "087[s], Ep: 36, Ct: 0500|TR LOSS: 307.17, PPL: 980|TM NLL: 297.25, KL: 4.23, REG:0.05 | LM NLL: 5.30, KL: 0.51|DE LOSS: 303.02, PPL: 968, TM: 297.59, LM: 5.44|BETA: 1.000000\n",
      "087[s], Ep: 36, Ct: 1000|TR LOSS: 307.17, PPL: 980|TM NLL: 297.24, KL: 4.23, REG:0.05 | LM NLL: 5.30, KL: 0.51|DE LOSS: 303.00, PPL: 970, TM: 297.55, LM: 5.45|BETA: 1.000000\n",
      "087[s], Ep: 36, Ct: 1500|TR LOSS: 307.15, PPL: 980|TM NLL: 297.23, KL: 4.23, REG:0.05 | LM NLL: 5.29, KL: 0.51|DE LOSS: 303.09, PPL: 971, TM: 297.66, LM: 5.43|BETA: 1.000000\n",
      "087[s], Ep: 36, Ct: 2000|TR LOSS: 307.15, PPL: 980|TM NLL: 297.23, KL: 4.23, REG:0.05 | LM NLL: 5.29, KL: 0.51|DE LOSS: 302.94, PPL: 968, TM: 297.52, LM: 5.42|BETA: 1.000000\n",
      "0 True: at least two incumbent indiana legislators have lost their re election bids in the republican primary , while some others held off strong challengers\n",
      "0 Pred: the the one years of the have been the way election\n",
      "1 True: rep. rebecca kubacki of syracuse and rep. kathy heuer of columbia city both lost to challengers who argued they were n't conservative enough , including their votes against the proposed state constitutional gay marriage ban\n",
      "1 Pred: the john <unk> of the , his dan <unk> of the are the the the the the the the would not enough enough to to a members\n",
      "2 True: kubacki was defeated by curt <unk> of goshen\n",
      "2 Pred: the is the the sen. sen. sen. the , and\n",
      "3 True: he 's the husband of the elkhart county republican chairwoman\n",
      "3 Pred: the says the bill 's the # republican republican party\n",
      "4 True: heuer lost to tea party conservative christopher judy of fort wayne\n",
      "4 Pred: the is the # # # points , # the worth , , , , and\n",
      "5 True: meanwhile , house education chairman robert <unk> of indianapolis defeated a critic of state funded private school vouchers\n",
      "5 Pred: the , the speaker senate mike <unk> said the , the bill of the 's and law and\n",
      "6 True: gop rep. casey cox of fort wayne won over two social conservatives\n",
      "6 Pred: the gubernatorial . <unk> of the wayne , a the year media in\n",
      "7 True: veteran republican sen. john waterman of <unk> was in a tight race with washington city councilman eric <unk>\n",
      "7 Pred: the of the john <unk> of the , elected elected # vote to the 's the\n",
      "8 True: a maryland man who shared in a newspaper 's pulitzer prize for covering a race related riot in detroit in the # s has died\n",
      "8 Pred: the federal fire has pleaded a a <unk> in lawsuit scheme has has the new for to has the has # world s has has of\n",
      "9 True: saul friedman , # , died friday at his home in edgewater\n",
      "9 Pred: the police was who , was , , the wife in the , , ,\n",
      "10 True: he had a rare form of stomach cancer\n",
      "10 Pred: the says been # message of the and and\n",
      "11 True: his wife confirmed his death to the capital of annapolis\n",
      "11 Pred: the police was the family , the weekend\n",
      "12 True: a memorial service was held tuesday\n",
      "12 Pred: the new service is is the\n",
      "13 True: during his career as a newspaper reporter friedman worked for the houston chronicle , knight <unk> newspapers and the washington bureau of new york 's newsday\n",
      "13 Pred: the say wife , , , , the , for the agency based of where , the the\n",
      "14 True: he also worked for the detroit free press where in # the staff shared a pulitzer prize for their coverage of race related riots that resulted when police raided an unlicensed drinking establishment\n",
      "14 Pred: the says says as the department 's press conference the # # #\n",
      "15 True: more than # buildings were destroyed , and # people died\n",
      "15 Pred: the than # people were killed by but the and the in and\n",
      "16 True: the new department of talent and economic development is expected to launch with the aim of making michigan a national leader in talent development for skilled trades\n",
      "16 Pred: the state york of natural and says the will planning to be a # # of the #\n",
      "17 True: gov . rick snyder issued an executive order in december creating the department to house the state 's economic growth and job training efforts\n",
      "17 Pred: the . rick walker says the emergency order to the to the state of education for to budget federal development\n",
      "18 True: that order also created the michigan talent investment agency within the new department\n",
      "18 Pred: the 's would will the the 's to in\n",
      "19 True: snyder says the move will help achieve a goal of his second term : developing a talented workforce trained to do skilled jobs that employers are having trouble filling\n",
      "19 Pred: the 's the bill is be the the new of the # term\n",
      "20 True: an event is monday\n",
      "20 Pred: the audit is scheduled\n",
      "21 True: the department of talent and economic development will fold existing unemployment , workforce development and housing agencies , and the michigan strategic fund under one roof\n",
      "21 Pred: the company of environmental and urban development will be the # benefits which and and the development\n",
      "22 True: it will also coordinate job preparedness and worker training programs\n",
      "22 Pred: the says be be the as and other ,\n",
      "23 True: a north dakota house committee supports legislation that would require the university of north dakota to keep its fighting sioux nickname\n",
      "23 Pred: the new carolina house has has a that would require the state of the to to pay the budget on the\n",
      "24 True: house republican majority leader al carlson is the sponsor of the bill\n",
      "24 Pred: the speaker senate leader the <unk> of running republican of the state\n",
      "25 True: it says und must keep its nickname and indian head logo , and it says attorney general wayne stenehjem should sue the ncaa if there are any penalties for keeping them\n",
      "25 Pred: the 's the will will the budget in the the of and but the and he general 's <unk> will not the state of\n",
      "26 True: the north dakota house education committee reviewed three fighting sioux nickname bills wednesday\n",
      "26 Pred: the bill dakota house of committee has the republican of the the to\n",
      "27 True: it is recommending approval of carlson 's bill and the defeat of the other two\n",
      "27 Pred: the house a a to the bill nomination\n",
      "28 True: the other two measures would require und to keep the fighting sioux nickname and logo unless the standing rock sioux tribe says it ca n't be used any more\n",
      "28 Pred: the 's the the the be the to be the state of falls\n",
      "29 True: all three measures will go to the house floor for a vote\n",
      "29 Pred: the of candidates are be to the senate\n",
      "30 True: a former thurston high school basketball coach and campus monitor has pleaded guilty to providing a student athlete with a prescription opioid\n",
      "30 Pred: the judge man man school teacher coach has a man has has guilty to charges a $ of to a minor drug\n",
      "31 True: the register guard reports ( http : //bit.ly/ # <unk> # ) that # year old jeffrey braswell , who now lives in california , was sentenced thursday to three years of supervised probation\n",
      "31 Pred: the # of reports ( http : <unk> # <unk> ) <unk> that # year old <unk> <unk> was of was the guilty the county pleaded sentenced to in # years in supervised probation\n",
      "32 True: he will also either serve # days in jail or complete an alternative assignment such as work crew for six months\n",
      "32 Pred: the says the the of as years in prison\n",
      "33 True: braswell pleaded guilty as part of a plea deal to six counts relating to the delivery of hydrocodone\n",
      "33 Pred: the was guilty to year of a plea to\n",
      "34 True: the # year old student testified wednesday that the drugs allowed her to participate in sports events without experiencing pain\n",
      "34 Pred: the say the the <unk> <unk> <unk> of was , pleaded in husband # his in of the his\n",
      "35 True: she said braswell had told her that the pills were just `` stronger <unk> .\n",
      "35 Pred: the was he was been `` husband he was to not before with `` .\n",
      "36 True: braswell will be sentenced to prison if he violates the term of his probation\n",
      "36 Pred: the 's be sentenced to # in convicted was the plea of his probation\n",
      "37 True: federal agents have arrested a former supervisor at a south texas immigrant detention center on charges of excessive force , obstruction of justice and lying to a federal agent about the # beating of a detainee\n",
      "37 Pred: the prosecutors say been a a u.s. who a university carolina prison accused center in charges of sexual a and a a a\n",
      "38 True: the u.s. attorney 's office says raul leal , then a lieutenant at the port isabel detention center in los <unk> , kicked detainee richard <unk> <unk> in the face , breaking his <unk> bone\n",
      "38 Pred: the was attorney 's office says the year was who <unk> , , the time of correctional center , # , , was up and <unk> , a the # # of into home\n",
      "39 True: he told investigators the detainee injured himself when his face inadvertently hit his knee at the immigration and customs enforcement run facility\n",
      "39 Pred: the was the that boy was the in the home\n",
      "40 True: department of homeland security special agents arrested the # year old leal on friday in georgia where he currently resides\n",
      "40 Pred: the of corrections spokeswoman <unk> agent say a # year old <unk> in the\n",
      "41 True: a u.s. magistrate judge set his bond at $ # and ordered him to appear at the federal court in brownsville\n",
      "41 Pred: the man man has has a # hearing $ # hearing in to to to # the # prison in the\n",
      "42 True: leal did not have an attorney\n",
      "42 Pred: the was the to been been to and and , , , , ,\n",
      "43 True: husted 's office says roughly # million people or # percent of ohio 's registered voters cast ballots in the election\n",
      "43 Pred: the is # says the will will in have # to of the\n",
      "44 True: turnout topped the # percent reported in last year 's contests , when gov . john kasich ( kay ' sik ) was re elected\n",
      "44 Pred: the of $ # # # by # year 's primary\n",
      "45 True: last november 's election included a marijuana legalization initiative , which was rejected on a vote of nearly # million to about # million\n",
      "45 Pred: the year , election , the # # of that which # a by the #\n",
      "46 True: voters easily approved a new system for drawing state legislative districts that 's intended to reduce partisan gerrymandering\n",
      "46 Pred: the in have the bill bill that the to law districts\n",
      "47 True: over # million voted in favor , while nearly # opposed it\n",
      "47 Pred: the the years , the the of the the # percent by the , , ,\n",
      "48 True: ohioans also passed an anti monopoly amendment , with about # million voters in favor , compared to the # million against it\n",
      "48 Pred: the will the the amendment of , that the the the\n",
      "49 True: a plan to create los angeles photo identification cards to help identify illegal immigrants has passed through a city council committee and is moving closer to approval\n",
      "49 Pred: the new of study the # of of is and be the the immigrants\n",
      "50 True: city news service reports the plan backed by mayor antonio villaraigosa was approved # tuesday by the arts , parks , health and aging committee\n",
      "50 Pred: the officials radio reports ( http : the the # and and a by\n",
      "51 True: the plan will create a photo id that would act as a library card and a debit card , and it would be up to law enforcement to decide whether to recognize the cards as identification\n",
      "51 Pred: the bill 's be the $ of from would be for a state and to a child\n",
      "52 True: councilman richard alarcon says the id cards are necessary because federal government has failed to make significant reform in immigration policy and it 's <unk> that businesses and individuals are hiring illegal immigrants\n",
      "52 Pred: the john <unk> says the 's will will `` to the regulators will been to make the funding\n",
      "53 True: the plan now goes before the full council in about three weeks\n",
      "53 Pred: the bill would goes to the the house\n",
      "54 True: the newspaper association of america spent $ # in the second quarter lobbying the federal government on issues including the future of media , privacy and legislation aimed at making government information public\n",
      "54 Pred: the 's says of the and in # million the , quarter of\n",
      "55 True: that 's up from $ # in the first quarter and $ # in the same quarter a year ago , according to a congressional disclosure form\n",
      "55 Pred: the 's a to $ # million # first quarter of year # million # last quarter\n",
      "56 True: the group , which represents nearly # newspapers in the u.s. , lobbied the u.s. senate and house of representatives on a proposed measure called the free flow of information act\n",
      "56 Pred: the bill says the is the # percent of the state , of the state department\n",
      "57 True: the bill would shield reporters in some cases from having to obey court orders to reveal confidential sources , a protection that media organizations argue would encourage more whistle blowers to come forward\n",
      "57 Pred: the bill would be the\n",
      "58 True: the group also lobbied on the federal communications commission 's future of media project , a rethinking of federal policy on media ownership\n",
      "58 Pred: the says of will the the # government commission\n",
      "59 True: a # year old colorado springs man has been indicted on charges of identity theft and fraud after he was accused of collecting more than $ # in fraudulent tax refunds\n",
      "59 Pred: the former man old man has man has been sentenced in charges of a a than a to a a of of stealing a\n",
      "60 True: the u.s. attorney 's office says randall heath is being held in new mexico on state fraud charges and is expected to return to colorado to face federal charges\n",
      "60 Pred: the state attorney 's office says the <unk> of charged held to the york\n",
      "61 True: the indictment alleges that heath got personal information of clients from college consultant group , inc. after the business had ceased operations\n",
      "61 Pred: the # says that the was a checks from the , the and and and which and she was of been\n",
      "62 True: authorities say he used the records to produce counterfeit driver 's licenses and counterfeit government photo identifications\n",
      "62 Pred: the say the was the money to a the and and and to the money and and\n",
      "-----------Topic Samples-----------\n",
      "0  bow: bill house republican senate gov election committee lawmakers democratic campaign\n",
      "0  sent: the bill would require the state to pay a $ # million budget shortfall\n",
      "1  bow: percent million company cents shares share revenue billion average quarter\n",
      "1  sent: that 's up # percent in the last # months\n",
      "2  bow: national u.s. day president group event military people world show\n",
      "2  sent: he says he 's not sure how many of the money is being used\n",
      "3  bow: court federal law attorney u.s. judge health filed lawsuit pay\n",
      "3  sent: he says he will be able to make sure the state 's health care law is unconstitutional\n",
      "4  bow: school university students schools board education district high college student\n",
      "4  sent: he says he 's not sure how many of the money will be able to get the money\n",
      "5  bow: fire service area water national weather miles tuesday monday river\n",
      "5  sent: he says he 's not sure how many of the victims are being made\n",
      "6  bow: department http people reports health //bit.ly/ news security told found\n",
      "6  sent: he says he 's not sure how many people were injured\n",
      "7  bow: million company federal project department program plans energy development plant\n",
      "7  sent: he says the bill would be a `` <unk> `` `` and that it will be able to make sure it would be used\n",
      "8  bow: found authorities home died car woman sheriff fire hospital shot\n",
      "8  sent: the sheriff 's office says the # year old man was found dead in the head of a # year old man who was found dead in the parking lot\n",
      "9  bow: charges charged court guilty prosecutors prison years arrested death attorney\n",
      "9  sent: the sheriff 's office says the # year old <unk> was arrested thursday in the u.s. district court of appeals\n"
     ]
    }
   ],
   "source": [
    "if len(logs) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "while epoch < config.epochs:\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = get_feed_dict(batch)\n",
    "\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, sent_loss_kl_categ_batch, sent_loss_kl_gmm_batch, ppls_batch = \\\n",
    "        sess.run([opt, loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, sent_loss_kl_categ, sent_loss_kl_gmm, topic_ppls], feed_dict = feed_dict)\n",
    "   \n",
    "        if sent_loss_kl_batch == np.inf:\n",
    "            print('Nan occured')\n",
    "            ckpt = tf.train.get_checkpoint_state(config.modeldir)\n",
    "            model_checkpoint_path = ckpt.all_model_checkpoint_paths[-1]\n",
    "            saver.restore(sess, model_checkpoint_path)            \n",
    "            break\n",
    "            \n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if ct%config.log_period==0:\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train, sent_loss_recon_train, sent_loss_kl_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_dev, sent_loss_dev, ppl_dev = get_loss(sess, dev_batches)\n",
    "\n",
    "            if config.warmup: beta_eval = beta.eval(session=sess)\n",
    "            global_step_log = sess.run(tf.train.get_global_step())            \n",
    "            \n",
    "#             if loss_dev < loss_min:\n",
    "#                 loss_min = loss_dev\n",
    "#                 saver.save(sess, config.modelpath, global_step=global_step_log)\n",
    "\n",
    "            clear_output()\n",
    "\n",
    "            time_finish = time.time()\n",
    "            time_log = int(time_finish - time_start)\n",
    "            logs += [(time_log, epoch, ct, loss_train, ppl_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train, sent_loss_recon_train, sent_loss_kl_train, loss_dev, ppl_dev, topic_loss_dev, sent_loss_dev, beta_eval)]\n",
    "            for log in logs:\n",
    "                print('%03d[s], Ep: %02d, Ct: %04d|TR LOSS: %.2f, PPL: %.0f|TM NLL: %.2f, KL: %.2f, REG:%.2f | LM NLL: %.2f, KL: %.2f|DE LOSS: %.2f, PPL: %.0f, TM: %.2f, LM: %.2f|BETA: %.6f' %  log)\n",
    "\n",
    "            print_sample(batch)\n",
    "\n",
    "            time_start = time.time()\n",
    "            \n",
    "            print_topic_sample()\n",
    "                \n",
    "    epoch += 1\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_prob_topic, _prob_topic_sents, _prob_topic_infer, _means_topic_infer = debug_value([prob_topic, prob_topic_sents, prob_topic_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_i = 4\n",
    "_prob_topic_sents[batch_i], _prob_topic_infer[batch_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_means_topic_infer[0][:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_means_topic, b_means_topic = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"topic/dec/mean_topic\")\n",
    "\n",
    "pred_topic_embeddings, pred_topic_bow, pred_means_topic, pred_logvars_topic, pred_token_idxs, _w_means_topic, _b_means_topic, _w_mean_topic_infer = \\\n",
    "                                sess.run([topic_embeddings, topic_bow, means_topic, logvars_topic, topic_beam_output_token_idxs, w_means_topic, b_means_topic, w_mean_topic_infer], \n",
    "                                         feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "\n",
    "pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "\n",
    "pred_topics_freq_bow_indices = np.argsort(pred_topic_bow, 1)[:, ::-1][:, :10]\n",
    "pred_topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idxs in pred_topics_freq_bow_idxs:\n",
    "    print([idx_to_word[idx] for idx in idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_topic_embeddings[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_topic_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_w_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_w_mean_topic_infer[:, :10, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_enc_state_infer, _means_topic_infer = debug_value([enc_state_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_enc_state_infer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_means_topic_infer[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
