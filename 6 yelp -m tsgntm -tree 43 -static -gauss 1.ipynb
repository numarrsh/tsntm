{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from data_structure import get_batches\n",
    "from hntm import HierarchicalNeuralTopicModel\n",
    "from nhdp import nestedHierarchicalNeuralTopicModel\n",
    "from tsgntm import TreeStructuredGaussianNeuralTopicModel\n",
    "from tree import get_descendant_idxs\n",
    "from evaluation import get_hierarchical_affinity, get_topic_specialization, print_topic_sample, print_topic_year\n",
    "from configure import get_config\n",
    "\n",
    "pd.set_option('display.max_columns', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(nb_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu\n",
    "np.random.seed(config.seed)\n",
    "random.seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.path_data,'rb'))\n",
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)\n",
    "config.dim_bow = len(bow_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     6
    ]
   },
   "outputs": [],
   "source": [
    "def debug(variable, sample_batch=None):\n",
    "    if sample_batch is None: sample_batch = test_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch, mode='test')\n",
    "    _variable = sess.run(variable, feed_dict=feed_dict)\n",
    "    return _variable\n",
    "\n",
    "def check(variable):\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sample_batch = test_batches[0]\n",
    "    feed_dict = model.get_feed_dict(sample_batch, mode='test', assertion=True)\n",
    "    _variable = sess.run(variable, feed_dict=feed_dict)\n",
    "    return _variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "TopicModels = {'hntm': HierarchicalNeuralTopicModel, 'nhdp': nestedHierarchicalNeuralTopicModel, 'tsgntm': TreeStructuredGaussianNeuralTopicModel}\n",
    "TopicModel = TopicModels[config.model]\n",
    "model = TopicModel(config)\n",
    "sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(max_to_keep=config.max_to_keep)\n",
    "update_tree_flg = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     24
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = []\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "ppl_min = np.inf\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','','','','','','VALID:','','','','','','TEST:','', 'SPEC:', '', '', 'HIER:', ''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL', 'GAUSS', 'REG','LOSS','PPL','NLL','KL', 'GAUSS','REG','LOSS','PPL', '1', '2', '3', 'CHILD', 'OTHER']]))))\n",
    "\n",
    "cmd_rm = 'rm -r %s' % config.dir_model\n",
    "res = subprocess.call(cmd_rm.split())\n",
    "cmd_mk = 'mkdir %s' % config.dir_model\n",
    "res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "def update_checkpoint(config, checkpoint, global_step):\n",
    "    checkpoint.append(config.path_model + '-%i' % global_step)\n",
    "    if len(checkpoint) > config.max_to_keep:\n",
    "        path_model = checkpoint.pop(0) + '.*'\n",
    "        for p in glob.glob(path_model):\n",
    "            os.remove(p)\n",
    "    cPickle.dump(checkpoint, open(config.path_checkpoint, 'wb'))\n",
    "    \n",
    "def validate(sess, batches, model):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    prob_topic_list = []\n",
    "    n_bow_list = []\n",
    "    n_topics_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = model.get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch, ppls_batch, prob_topic_batch, n_bow_batch, n_topics_batch \\\n",
    "            = sess.run([model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_gauss, model.topic_loss_reg, model.topic_ppls, model.prob_topic, model.n_bow, model.n_topics], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "        prob_topic_list.append(prob_topic_batch)\n",
    "        n_bow_list.append(n_bow_batch)\n",
    "        n_topics_list.append(n_topics_batch)\n",
    "    loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_gauss_mean, topic_loss_reg_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    \n",
    "    probs_topic = np.concatenate(prob_topic_list, 0)\n",
    "    \n",
    "    n_bow = np.concatenate(n_bow_list, 0)\n",
    "    n_topics = np.concatenate(n_topics_list, 0)\n",
    "    probs_topic_mean = np.sum(n_topics, 0) / np.sum(n_bow)\n",
    "    \n",
    "    return loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_gauss_mean, topic_loss_reg_mean, ppl_mean, probs_topic_mean    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train & validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th colspan=\"5\" halign=\"left\"></th>\n",
       "      <th>VALID:</th>\n",
       "      <th colspan=\"5\" halign=\"left\"></th>\n",
       "      <th>TEST:</th>\n",
       "      <th></th>\n",
       "      <th>SPEC:</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "      <th>HIER:</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>GAUSS</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>GAUSS</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>CHILD</th>\n",
       "      <th>OTHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>999</td>\n",
       "      <td>929.74</td>\n",
       "      <td>2714</td>\n",
       "      <td>925.12</td>\n",
       "      <td>1.06</td>\n",
       "      <td>3.31</td>\n",
       "      <td>0.24</td>\n",
       "      <td>892.02</td>\n",
       "      <td>2647</td>\n",
       "      <td>888.27</td>\n",
       "      <td>0.85</td>\n",
       "      <td>2.67</td>\n",
       "      <td>0.22</td>\n",
       "      <td>890.74</td>\n",
       "      <td>2605</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1999</td>\n",
       "      <td>923.93</td>\n",
       "      <td>2579</td>\n",
       "      <td>918.36</td>\n",
       "      <td>1.81</td>\n",
       "      <td>3.53</td>\n",
       "      <td>0.23</td>\n",
       "      <td>881.42</td>\n",
       "      <td>2363</td>\n",
       "      <td>874.87</td>\n",
       "      <td>2.77</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.19</td>\n",
       "      <td>881.28</td>\n",
       "      <td>2383</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>295</td>\n",
       "      <td>921.08</td>\n",
       "      <td>2498</td>\n",
       "      <td>915.06</td>\n",
       "      <td>2.18</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0.23</td>\n",
       "      <td>879.42</td>\n",
       "      <td>2317</td>\n",
       "      <td>872.31</td>\n",
       "      <td>3.05</td>\n",
       "      <td>3.93</td>\n",
       "      <td>0.13</td>\n",
       "      <td>879.06</td>\n",
       "      <td>2330</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>1295</td>\n",
       "      <td>918.36</td>\n",
       "      <td>2446</td>\n",
       "      <td>912.01</td>\n",
       "      <td>2.41</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.22</td>\n",
       "      <td>874.82</td>\n",
       "      <td>2215</td>\n",
       "      <td>866.97</td>\n",
       "      <td>3.51</td>\n",
       "      <td>4.13</td>\n",
       "      <td>0.20</td>\n",
       "      <td>878.40</td>\n",
       "      <td>2317</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>2295</td>\n",
       "      <td>916.05</td>\n",
       "      <td>2406</td>\n",
       "      <td>909.46</td>\n",
       "      <td>2.58</td>\n",
       "      <td>3.79</td>\n",
       "      <td>0.21</td>\n",
       "      <td>873.10</td>\n",
       "      <td>2192</td>\n",
       "      <td>865.46</td>\n",
       "      <td>3.49</td>\n",
       "      <td>4.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>874.01</td>\n",
       "      <td>2236</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234000</th>\n",
       "      <td>56</td>\n",
       "      <td>86</td>\n",
       "      <td>1455</td>\n",
       "      <td>896.93</td>\n",
       "      <td>2032</td>\n",
       "      <td>887.84</td>\n",
       "      <td>4.16</td>\n",
       "      <td>4.83</td>\n",
       "      <td>0.14</td>\n",
       "      <td>862.34</td>\n",
       "      <td>1985</td>\n",
       "      <td>852.79</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.95</td>\n",
       "      <td>0.14</td>\n",
       "      <td>861.61</td>\n",
       "      <td>2006</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235000</th>\n",
       "      <td>56</td>\n",
       "      <td>86</td>\n",
       "      <td>2455</td>\n",
       "      <td>896.91</td>\n",
       "      <td>2032</td>\n",
       "      <td>887.82</td>\n",
       "      <td>4.16</td>\n",
       "      <td>4.83</td>\n",
       "      <td>0.14</td>\n",
       "      <td>861.69</td>\n",
       "      <td>1973</td>\n",
       "      <td>852.17</td>\n",
       "      <td>4.48</td>\n",
       "      <td>4.92</td>\n",
       "      <td>0.13</td>\n",
       "      <td>861.61</td>\n",
       "      <td>2006</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236000</th>\n",
       "      <td>57</td>\n",
       "      <td>87</td>\n",
       "      <td>751</td>\n",
       "      <td>896.90</td>\n",
       "      <td>2032</td>\n",
       "      <td>887.82</td>\n",
       "      <td>4.16</td>\n",
       "      <td>4.83</td>\n",
       "      <td>0.14</td>\n",
       "      <td>863.03</td>\n",
       "      <td>1996</td>\n",
       "      <td>853.64</td>\n",
       "      <td>4.46</td>\n",
       "      <td>4.83</td>\n",
       "      <td>0.10</td>\n",
       "      <td>861.61</td>\n",
       "      <td>2006</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237000</th>\n",
       "      <td>57</td>\n",
       "      <td>87</td>\n",
       "      <td>1751</td>\n",
       "      <td>896.89</td>\n",
       "      <td>2031</td>\n",
       "      <td>887.81</td>\n",
       "      <td>4.16</td>\n",
       "      <td>4.83</td>\n",
       "      <td>0.14</td>\n",
       "      <td>861.93</td>\n",
       "      <td>1979</td>\n",
       "      <td>852.51</td>\n",
       "      <td>4.41</td>\n",
       "      <td>4.94</td>\n",
       "      <td>0.08</td>\n",
       "      <td>861.61</td>\n",
       "      <td>2006</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238000</th>\n",
       "      <td>57</td>\n",
       "      <td>88</td>\n",
       "      <td>47</td>\n",
       "      <td>896.89</td>\n",
       "      <td>2031</td>\n",
       "      <td>887.80</td>\n",
       "      <td>4.16</td>\n",
       "      <td>4.83</td>\n",
       "      <td>0.14</td>\n",
       "      <td>861.96</td>\n",
       "      <td>1978</td>\n",
       "      <td>852.45</td>\n",
       "      <td>4.43</td>\n",
       "      <td>4.95</td>\n",
       "      <td>0.13</td>\n",
       "      <td>861.61</td>\n",
       "      <td>2006</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>238 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       TRAIN:                                  VALID:        \\\n",
       "       Time  Ep    Ct    LOSS   PPL     NLL    KL GAUSS   REG    LOSS   PPL   \n",
       "1000     47   0   999  929.74  2714  925.12  1.06  3.31  0.24  892.02  2647   \n",
       "2000     41   0  1999  923.93  2579  918.36  1.81  3.53  0.23  881.42  2363   \n",
       "3000     42   1   295  921.08  2498  915.06  2.18  3.62  0.23  879.42  2317   \n",
       "4000     42   1  1295  918.36  2446  912.01  2.41  3.72  0.22  874.82  2215   \n",
       "5000     43   1  2295  916.05  2406  909.46  2.58  3.79  0.21  873.10  2192   \n",
       "...     ...  ..   ...     ...   ...     ...   ...   ...   ...     ...   ...   \n",
       "234000   56  86  1455  896.93  2032  887.84  4.16  4.83  0.14  862.34  1985   \n",
       "235000   56  86  2455  896.91  2032  887.82  4.16  4.83  0.14  861.69  1973   \n",
       "236000   57  87   751  896.90  2032  887.82  4.16  4.83  0.14  863.03  1996   \n",
       "237000   57  87  1751  896.89  2031  887.81  4.16  4.83  0.14  861.93  1979   \n",
       "238000   57  88    47  896.89  2031  887.80  4.16  4.83  0.14  861.96  1978   \n",
       "\n",
       "                                   TEST:       SPEC:             HIER:        \n",
       "           NLL    KL GAUSS   REG    LOSS   PPL     1     2     3 CHILD OTHER  \n",
       "1000    888.27  0.85  2.67  0.22  890.74  2605  0.12  0.11  0.12  0.97  0.97  \n",
       "2000    874.87  2.77  3.60  0.19  881.28  2383  0.17  0.16  0.19  0.96  0.88  \n",
       "3000    872.31  3.05  3.93  0.13  879.06  2330  0.20  0.22  0.22  0.96  0.76  \n",
       "4000    866.97  3.51  4.13  0.20  878.40  2317  0.21  0.18  0.21  0.94  0.78  \n",
       "5000    865.46  3.49  4.08  0.07  874.01  2236  0.19  0.20  0.25  0.94  0.78  \n",
       "...        ...   ...   ...   ...     ...   ...   ...   ...   ...   ...   ...  \n",
       "234000  852.79  4.45  4.95  0.14  861.61  2006  0.19  0.22  0.32  0.81  0.63  \n",
       "235000  852.17  4.48  4.92  0.13  861.61  2006  0.24  0.27  0.33  0.83  0.65  \n",
       "236000  853.64  4.46  4.83  0.10  861.61  2006  0.20  0.24  0.33  0.84  0.68  \n",
       "237000  852.51  4.41  4.94  0.08  861.61  2006  0.20  0.23  0.30  0.78  0.58  \n",
       "238000  852.45  4.43  4.95  0.13  861.61  2006  0.19  0.27  0.29  0.82  0.63  \n",
       "\n",
       "[238 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 R: 1.000 P: 0.044 years check free found helpful asked wonderful wanted full told\n",
      "   1 R: 0.490 P: 0.033 rice soup flavor beef fried spicy tacos dishes thai meat\n",
      "     11 R: 0.164 P: 0.164 rice mexican thai chinese tacos soup spicy rolls beef noodles\n",
      "     12 R: 0.064 P: 0.064 soup meat tacos thai beef rice fried fish items found\n",
      "     13 R: 0.230 P: 0.230 sandwich bread wings steak wine cooked burgers waitress meat italian\n",
      "   2 R: 0.136 P: 0.012 cream ice tea chocolate sandwich flavor cake flavors coffee tasted\n",
      "     21 R: 0.005 P: 0.005 sandwich cream ice coffee chocolate cake flavor free tea options\n",
      "     22 R: 0.087 P: 0.087 cream ice tea cake chocolate flavors flavor donuts boba milk\n",
      "     23 R: 0.033 P: 0.033 coffee shop starbucks drive store sandwich latte morning sandwiches line\n",
      "   3 R: 0.029 P: 0.015 years found free today asked drive house helpful check wanted\n",
      "     31 R: 0.002 P: 0.002 helpful found years wanted told check asked free today care\n",
      "     32 R: 0.003 P: 0.003 years care job told found wanted helpful asked left check\n",
      "     33 R: 0.009 P: 0.009 free shop store helpful line coffee ice found stop inside\n",
      "   4 R: 0.300 P: 0.003 helpful store free check line buy shop kids found money\n",
      "     41 R: 0.120 P: 0.120 room hotel show music pool game rooms view cool beers\n",
      "     42 R: 0.059 P: 0.059 store helpful gym buy items check told money phone worst\n",
      "     43 R: 0.118 P: 0.118 car job hair nails professional care massage salon nail years\n",
      "[-11.49809  -31.621399 -16.913765 -11.852089 -19.961655 -45.84613\n",
      " -34.831726 -48.05314  -17.199253 -26.090546 -24.98602  -11.491016\n",
      " -11.492179 -11.804007 -30.875927 -28.258589 -33.74595 ]\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "while epoch < config.n_epochs:\n",
    "    # train\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = model.get_feed_dict(batch)\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch, ppls_batch, global_step_log = \\\n",
    "        sess.run([model.opt, model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_gauss, model.topic_loss_reg, model.topic_ppls, tf.train.get_global_step()], feed_dict = feed_dict)\n",
    "\n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "#         if global_step_log % config.log_period == 0:\n",
    "        if global_step_log % 1000 == 0:\n",
    "            # validate\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_gauss_train, topic_loss_reg_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_gauss_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "\n",
    "            # test\n",
    "            if ppl_dev < ppl_min:\n",
    "                ppl_min = ppl_dev\n",
    "                loss_test, _, _, _, _, ppl_test, _ = validate(sess, test_batches, model)\n",
    "                saver.save(sess, config.path_model, global_step=global_step_log)\n",
    "                cPickle.dump(config, open(config.path_config % global_step_log, 'wb'))\n",
    "                update_checkpoint(config, checkpoint, global_step_log)\n",
    "            \n",
    "            # visualize topic\n",
    "            topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "            topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "            topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "            topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "            descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "            recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "            \n",
    "            depth_specs = get_topic_specialization(sess, model, instances_test)\n",
    "            hierarchical_affinities = get_hierarchical_affinity(sess, model)\n",
    "            \n",
    "            # log\n",
    "            clear_output()\n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_gauss_train, '%.2f'%topic_loss_reg_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_gauss_dev, '%.2f'%topic_loss_reg_dev, \\\n",
    "                    '%.2f'%loss_test, '%.0f'%ppl_test, \\\n",
    "                    '%.2f'%depth_specs[1], '%.2f'%depth_specs[2], '%.2f'%depth_specs[3], \\\n",
    "                    '%.2f'%hierarchical_affinities[0], '%.2f'%hierarchical_affinities[1]],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "            cPickle.dump(log_df, open(os.path.join(config.path_log), 'wb'))\n",
    "            print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)\n",
    "            print(np.sum(debug(model.topic_logvars), 1))\n",
    "            \n",
    "            # update tree\n",
    "            if not config.static:\n",
    "                config.tree_idxs, update_tree_flg = model.update_tree(topic_prob_topic, recur_prob_topic)\n",
    "                if update_tree_flg:\n",
    "                    print(config.tree_idxs)\n",
    "                    name_variables = {tensor.name: variable for tensor, variable in zip(tf.global_variables(), sess.run(tf.global_variables()))} # store paremeters\n",
    "                    if 'sess' in globals(): sess.close()\n",
    "                    model = HierarchicalNeuralTopicModel(config)\n",
    "                    sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "                    name_tensors = {tensor.name: tensor for tensor in tf.global_variables()}\n",
    "                    sess.run([name_tensors[name].assign(variable) for name, variable in name_variables.items()]) # restore parameters\n",
    "                    saver = tf.train.Saver(max_to_keep=1)\n",
    "                \n",
    "            time_start = time.time()\n",
    "\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "    epoch += 1\n",
    "\n",
    "loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "display(log_df)\n",
    "print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_year(sample_batches):\n",
    "    probs_topics = []\n",
    "    years = []\n",
    "    for i, sample_batch in sample_batches:\n",
    "        probs_topics_batch = sess.run(model.prob_topic, feed_dict=model.get_feed_dict(sample_batch, mode='test'))\n",
    "        years_batch = [instance.year for instance in sample_batch]\n",
    "        probs_topics += [probs_topics_batch]\n",
    "        years += years_batch\n",
    "    probs_topics = np.concatenate(probs_topics)\n",
    "    years = np.array(years)\n",
    "\n",
    "    topic_years = years.dot(probs_topics) / np.sum(probs_topics, 0)\n",
    "    topic_year = {model.topic_idxs[i]: year for i, year in enumerate(topic_years)}\n",
    "    return topic_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Avg Year: 2005 level analysis english systems domain lexical linguistic test process lexicon\n",
      "   1 Avg Year: 2000 grammar structure rules semantic form representation rule lexical type structures\n",
      "     11 Avg Year: 2006 verb semantic syntactic verbs event relations discourse argument relation arguments\n",
      "     12 Avg Year: 2007 tree parsing dependency parser grammar trees node parse rules syntactic\n",
      "   2 Avg Year: 2008 english languages dictionary errors morphological chinese lexical rules corpora pos\n",
      "     21 Avg Year: 2009 entity sense relations relation semantic entities wordnet senses lexical patterns\n",
      "     22 Avg Year: 2010 translation english source alignment phrase target languages sentences systems parallel\n",
      "   3 Avg Year: 2008 models probability training algorithm probabilities search parameters sequence segmentation gram\n",
      "     31 Avg Year: 2011 features feature training performance learning classifier classification accuracy test class\n",
      "     32 Avg Year: 2015 models network vector embeddings neural training vectors representations embedding input\n",
      "   4 Avg Year: 2007 knowledge semantic terms term natural values context type representation concept\n",
      "     41 Avg Year: 2009 question query terms documents answer questions document domain term web\n",
      "     42 Avg Year: 2010 similarity topic document sentences method score scores measure clustering pairs\n",
      "   5 Avg Year: 2007 user annotation resources project tools tool database research interface linguistic\n",
      "     51 Avg Year: 2007 speech dialogue user speaker utterance utterances recognition spoken human speakers\n",
      "     52 Avg Year: 2013 sentiment tweets polarity twitter negative opinion positive social emotion annotators\n"
     ]
    }
   ],
   "source": [
    "sample_batches = get_batches(instances_train, config.batch_size)\n",
    "topic_year = get_topic_year(sample_batches)\n",
    "print_topic_year(sess, model, topic_freq_tokens=topic_freq_tokens, topic_year=topic_year)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
