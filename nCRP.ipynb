{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "import _pickle as cPickle\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import defaultdict\n",
    "from scipy.special import gammaln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/synthetic/instances_ncrp.pkl', 'path of data')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "config = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))\n",
    "docs_raw = [[[bow_idxs[bow_index]]*int(instance.bow[bow_index]) for bow_index in np.where(instance.bow > 0)[0]] for instance in instances_train]\n",
    "docs = [[idx for idxs in doc for idx in idxs] for doc in docs_raw][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_doc = len(docs)\n",
    "n_vocab = len(np.unique(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "899"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.\n",
    "beta = 1.\n",
    "gam = 0.5\n",
    "eta = 1.\n",
    "n_depth = 3\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assign docs to tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Topic:\n",
    "    def __init__(self, idx, parent, depth, n_doc, n_vocab):\n",
    "        self.idx = idx\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.depth = depth\n",
    "        self.cnt_doc = 0\n",
    "        self.n_doc = n_doc\n",
    "        self.n_vocab = n_vocab\n",
    "        self.cnt_words = np.zeros([n_doc, n_vocab])\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def sample_child(self, doc_idx, doc, gam, verbose=False):\n",
    "        p_child_prior = self.get_p_child_prior(gam)\n",
    "        p_child_likelihood = self.get_p_child_likelihood(doc_idx, doc, eta)\n",
    "        p_child = np.array(p_child_prior * p_child_likelihood) / np.sum(p_child_prior * p_child_likelihood)\n",
    "        \n",
    "        child_index = np.random.multinomial(1, p_child).argmax()\n",
    "        if verbose: print(self.depth, p_child, p_child_likelihood, child_index)\n",
    "        \n",
    "        if child_index < len(self.children):\n",
    "            child = self.children[child_index]\n",
    "        else:\n",
    "            child = self.add_child()\n",
    "        return child\n",
    "    \n",
    "    def get_p_child_prior(self, gam):\n",
    "        p_child_prior = [child.cnt_doc for child in self.children]\n",
    "        p_child_prior += [gam]\n",
    "        return p_child_prior\n",
    "    \n",
    "    def get_p_child_likelihood(self, doc_idx, doc, eta):\n",
    "        if len(self.children) > 0:\n",
    "            children_cnt_words = np.array([child.cnt_words for child in self.children]) # Children x Document x Vocabulary\n",
    "            children_cnt_words = np.concatenate([children_cnt_words, np.zeros([1, self.n_doc, self.n_vocab])], 0) # (Children+1) x Document x Vocabulary\n",
    "        else:\n",
    "            children_cnt_words = np.zeros([1, self.n_doc, self.n_vocab]) # 1 x Document x Vocabulary\n",
    "        \n",
    "        children_cnt_words_sum = np.sum(children_cnt_words, 1) # (Children + 1) x Vocabulary\n",
    "        children_cnt_words_doc = children_cnt_words_sum - children_cnt_words[:, doc_idx, :] # (Children + 1) x Children x Vocabulary\n",
    "\n",
    "        logits_prior = gammaln(np.sum(children_cnt_words_doc, -1) + n_vocab*eta) - np.sum(gammaln(children_cnt_words_doc[:, doc] + eta), -1)\n",
    "        logits_later = gammaln(np.sum(children_cnt_words_sum, -1) + n_vocab*eta) - np.sum(gammaln(children_cnt_words_sum[:, doc] + eta), -1)\n",
    "        logits_likelihood = logits_prior - logits_later\n",
    "        p_child_likelihood = np.exp(logits_likelihood)\n",
    "        return p_child_likelihood\n",
    "    \n",
    "    def add_child(self):\n",
    "        idx = self.idx * 10 + len(self.children)+1\n",
    "        depth = self.depth+1\n",
    "        child = Topic(idx=idx, parent=self, depth=depth, n_doc=self.n_doc, n_vocab=self.n_vocab)\n",
    "        self.children += [child]\n",
    "        \n",
    "        return child\n",
    "    \n",
    "    def delete_topic(self):\n",
    "        self.parent.children.remove(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample doc path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p({\\bf c}_{m}\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf w}, {\\bf c}_{-m}, {\\bf z})\\propto p({\\bf w}_{m}\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf c}, {\\bf w}_{-m}, {\\bf z})\\cdot p({\\bf c}_{m}\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf c}_{-m})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p({\\bf w}_{m}\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf c}, {\\bf w}_{-m}, {\\bf z})=\\prod_{\\ell=1}^{L}\\left(\\frac{\\Gamma(n_{c_{m,\\ell},-m}^{(\\cdot)}+W\\eta)}{\\prod_{w}\\Gamma(n_{c_{m,\\ell},-m}^{(w)}+\\eta)}\\frac{\\prod_{w}\\Gamma(n_{c_{m,\\ell},-m}^{(w)}+n_{c_{m,\\ell},m}^{(w)}+\\eta)}{\\Gamma(n_{c_{m,\\ell},-m}^{(\\cdot)}+n_{c_{m,\\ell},m}^{(\\cdot)}+W\\eta)}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_doc_topics(docs, topic_root):\n",
    "    for doc_idx, doc in enumerate(docs):\n",
    "        # reset count of docs\n",
    "        if doc_idx in doc_topics:\n",
    "            for topic in doc_topics[doc_idx]:\n",
    "                topic.cnt_doc -= 1\n",
    "                if topic.cnt_doc == 0: topic.delete_topic()\n",
    "\n",
    "        topic = topic_root\n",
    "        topic.cnt_doc += 1\n",
    "        doc_topics[doc_idx] += [topic]\n",
    "        for depth in range(n_depth):\n",
    "            topic = topic.sample_child(doc_idx, doc, gam, verbose)\n",
    "            topic.cnt_doc += 1\n",
    "            doc_topics[doc_idx] += [topic]\n",
    "            \n",
    "    return doc_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assign words to topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "p(z_{i}=j\\hspace{0.5ex}|\\hspace{0.5ex}{\\bf z}_{-i},{\\bf w})\\propto\\frac{n_{-i,j}^{(w_{i})}+\\beta}{n_{-i,j}^{(\\cdot)}+W\\beta}\\frac{n_{-i,j}^{(d_{i})}+\\alpha}{n_{-i,\\cdot}^{(d_{i})}+T\\alpha}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_word_topics(docs, doc_topics):\n",
    "    for doc_idx, doc in enumerate(docs):\n",
    "        topics = doc_topics[doc_idx]\n",
    "\n",
    "        for topic in topics:\n",
    "            for word_idx in doc:\n",
    "                topic.cnt_words[doc_idx, word_idx] = 0\n",
    "\n",
    "        s_docs = np.array([np.sum(topic.cnt_words[doc_idx, :])+alpha for topic in topics]) # L\n",
    "\n",
    "        for word_idx in doc:\n",
    "            s_words = np.array([np.sum(topic.cnt_words[:, word_idx])+beta for topic in topics]) # L\n",
    "            z_words = np.array([np.sum(topic.cnt_words)+n_vocab*beta for topic in topics]) # L\n",
    "\n",
    "            s_topics = s_docs*s_words/z_words\n",
    "            p_topics = s_topics/np.sum(s_topics) # L\n",
    "\n",
    "            word_topic = topics[np.argmax(np.random.multinomial(1, p_topics))]\n",
    "            word_topic.cnt_words[doc_idx, word_idx] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_sum_cnt_words(topic_root):\n",
    "    def recur_cnt_words(topic):\n",
    "        cnt_words = np.sum(topic.cnt_words)\n",
    "        for child in topic.children:\n",
    "            cnt_words += recur_cnt_words(child)\n",
    "        return cnt_words\n",
    "\n",
    "    sum_cnt_words = recur_cnt_words(topic_root)\n",
    "    assert sum_cnt_words == sum([len(doc) for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 899 is out of bounds for axis 1 with size 899",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-7ec488214210>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi_sample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# sample path of doc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdoc_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_doc_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# assign words to each topic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-f2f9bbcf2673>\u001b[0m in \u001b[0;36msample_doc_topics\u001b[0;34m(docs, topic_root)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mdoc_topics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mtopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mtopic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnt_doc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mdoc_topics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-962b9a120e58>\u001b[0m in \u001b[0;36msample_child\u001b[0;34m(self, doc_idx, doc, gam, verbose)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mp_child_prior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_p_child_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mp_child_likelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_p_child_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mp_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_child_prior\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp_child_likelihood\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_child_prior\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp_child_likelihood\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-962b9a120e58>\u001b[0m in \u001b[0;36mget_p_child_likelihood\u001b[0;34m(self, doc_idx, doc, eta)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mchildren_cnt_words_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchildren_cnt_words_sum\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mchildren_cnt_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# (Children + 1) x Children x Vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mlogits_prior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgammaln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchildren_cnt_words_doc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_vocab\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgammaln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchildren_cnt_words_doc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mlogits_later\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgammaln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchildren_cnt_words_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_vocab\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgammaln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchildren_cnt_words_sum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mlogits_likelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits_prior\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlogits_later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 899 is out of bounds for axis 1 with size 899"
     ]
    }
   ],
   "source": [
    "n_sample = 100\n",
    "topic_root = Topic(idx=0, parent=None, depth=0, n_doc=n_doc, n_vocab=n_vocab)\n",
    "doc_topics = defaultdict(list)\n",
    "\n",
    "for i_sample in range(n_sample):\n",
    "    # sample path of doc\n",
    "    doc_topics = sample_doc_topics(docs, topic_root)\n",
    "    \n",
    "    # assign words to each topic\n",
    "    sample_word_topics(docs, doc_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-29-962b9a120e58>\u001b[0m(42)\u001b[0;36mget_p_child_likelihood\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     40 \u001b[0;31m        \u001b[0mchildren_cnt_words_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchildren_cnt_words_sum\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mchildren_cnt_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# (Children + 1) x Children x Vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     41 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 42 \u001b[0;31m        \u001b[0mlogits_prior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgammaln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchildren_cnt_words_doc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_vocab\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgammaln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchildren_cnt_words_doc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     43 \u001b[0;31m        \u001b[0mlogits_later\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgammaln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchildren_cnt_words_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_vocab\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgammaln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchildren_cnt_words_sum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     44 \u001b[0;31m        \u001b[0mlogits_likelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits_prior\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlogits_later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> doc\n",
      "[37, 53, 62, 68, 69, 80, 104, 142, 157, 165, 166, 173, 203, 203, 216, 240, 240, 268, 273, 281, 287, 289, 297, 303, 316, 317, 328, 339, 339, 342, 346, 388, 390, 400, 402, 422, 430, 452, 462, 462, 469, 470, 477, 477, 481, 501, 507, 526, 532, 563, 572, 597, 604, 636, 652, 655, 658, 678, 689, 705, 711, 713, 713, 715, 724, 728, 729, 734, 734, 747, 750, 754, 759, 759, 761, 768, 786, 791, 791, 796, 804, 805, 818, 818, 820, 831, 834, 837, 839, 841, 842, 845, 861, 861, 866, 869, 873, 880, 894, 899]\n",
      "ipdb> children_cnt_words_doc.shape\n",
      "(1, 899)\n",
      "ipdb> self.n_vocab\n",
      "899\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 : [1, 2, 3] 1000 25295.0\n",
      "   1 : [11, 12, 13] 919 23059.0\n",
      "     11 : [111, 112, 113, 114, 115, 116, 117, 118] 889 22281.0\n",
      "       111 : [] 450 10917.0\n",
      "       112 : [] 352 8586.0\n",
      "       113 : [] 13 324.0\n",
      "       114 : [] 1 19.0\n",
      "       115 : [] 16 402.0\n",
      "       116 : [] 50 1262.0\n",
      "       117 : [] 5 129.0\n",
      "       118 : [] 2 58.0\n",
      "     12 : [121] 18 469.0\n",
      "       121 : [] 18 453.0\n",
      "     13 : [131] 12 316.0\n",
      "       131 : [] 12 329.0\n",
      "   2 : [21, 22, 23] 80 1959.0\n",
      "     21 : [211, 212, 213] 73 1819.0\n",
      "       211 : [] 40 1041.0\n",
      "       212 : [] 32 816.0\n",
      "       213 : [] 1 35.0\n",
      "     22 : [221] 6 150.0\n",
      "       221 : [] 6 160.0\n",
      "     23 : [231] 1 30.0\n",
      "       231 : [] 1 16.0\n",
      "   3 : [31] 1 25.0\n",
      "     31 : [311] 1 25.0\n",
      "       311 : [] 1 25.0\n"
     ]
    }
   ],
   "source": [
    "def print_child_idxs(topic):\n",
    "    print('  '*topic.depth, topic.idx, ':', [child.idx for child in topic.children], topic.cnt_doc, np.sum(topic.cnt_words))\n",
    "    for topic in topic.children:\n",
    "        print_child_idxs(topic)\n",
    "\n",
    "print_child_idxs(topic_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8586.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
