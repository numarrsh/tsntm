{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "# %matplotlib nbagg\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from data_structure import get_batches, get_test_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "\n",
    "from topic_beam_search_decoder import BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '0', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/synthetic/instances_ncrp_sparse.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/topic_vae', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'bags', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 1000, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 1000, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "# flags.DEFINE_string('opt', 'Adam', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.01, 'lr')\n",
    "flags.DEFINE_float('reg', 10., 'regularization term')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_bool('warmup', True, 'flg of warming up')\n",
    "flags.DEFINE_integer('epochs_cycle', 5, 'number of epochs within a cycle')\n",
    "flags.DEFINE_float('r_cycle', 0.5, 'proportion used to increase beta within a cycle')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 20, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_bow', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_hidden_topic', 512, 'dim_hidden_topic')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "flags.DEFINE_bool('bidirectional', True, 'flg of bidirectional encoding')\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(instances, batch_size, iterator=False):\n",
    "    iter_instances = iter(instances)\n",
    "    n_batch = len(instances)//batch_size\n",
    "    \n",
    "    batches = [(i_batch, [next(iter_instances) for i_doc in range(batch_size)]) for i_batch in range(n_batch)]\n",
    "    \n",
    "    if iterator: batches = iter(batches)\n",
    "    return batches\n",
    "\n",
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "flags.DEFINE_integer('cycle_steps', len(train_batches)*config.epochs_cycle, 'number of steps for each cycle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0,
     10,
     18,
     24,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables):\n",
    "    sample_batch = dev_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "def check_shape(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()\n",
    "    \n",
    "def check_value(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train'):\n",
    "    bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "    feed_dict = {\n",
    "                t_variables['bow']: bow, \n",
    "                t_variables['keep_prob']: keep_prob\n",
    "    }\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def get_depth(parent_idx=0, tree_depth=None, depth=1):\n",
    "    if tree_depth is None: tree_depth={0: depth}\n",
    "\n",
    "    child_idxs = tree_idxs[parent_idx]\n",
    "    depth +=1\n",
    "    for child_idx in child_idxs:\n",
    "        tree_depth[child_idx] = depth\n",
    "        if child_idx in tree_idxs: get_depth(child_idx, tree_depth, depth)\n",
    "    return tree_depth\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['bow'] = tf.placeholder(tf.float32, [None, config.dim_bow])\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32)\n",
    "\n",
    "# tree_idxs = {0:[1, 2, 3], \n",
    "#                       1:[10, 11], 2:[20, 21], 3:[30, 31]}\n",
    "\n",
    "tree_idxs = {0:[1, 2], \n",
    "              1:[10, 11, 12], 2:[20, 21, 22]}\n",
    "\n",
    "# tree_idxs = {0:[1, 2], \n",
    "#              1:[11, 12], 2:[21]}\n",
    "\n",
    "# tree_idxs = {0:[1, 2, 3], \n",
    "#                       1:[10, 11], 2:[20, 21], 3:[30, 31],\n",
    "#                       10: [100, 101], 11: [110, 111], 20: [200, 201], 21: [210, 211], 30:[300, 301], 31:[310, 311]}\n",
    "\n",
    "topic_idxs = [0] + [idx for child_idxs in tree_idxs.values() for idx in child_idxs]\n",
    "\n",
    "child_to_parent_idxs = {child_idx: parent_idx for parent_idx, child_idxs in tree_idxs.items() for child_idx in child_idxs}\n",
    "\n",
    "tree_depth = get_depth()\n",
    "max_depth = max(tree_depth.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doubly rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoublyRNNCell:\n",
    "    def __init__(self, dim_hidden, output_layer=None):\n",
    "        self.dim_hidden = dim_hidden\n",
    "        \n",
    "        self.ancestral_layer=tf.layers.Dense(units=dim_hidden, activation=tf.nn.tanh, name='ancestral')\n",
    "        self.fraternal_layer=tf.layers.Dense(units=dim_hidden, activation=tf.nn.tanh, name='fraternal')\n",
    "#         self.hidden_layer = tf.layers.Dense(units=dim_hidden, activation=tf.nn.tanh, name='hidden')\n",
    "        self.hidden_layer = tf.layers.Dense(units=dim_hidden, name='hidden')\n",
    "        \n",
    "        self.output_layer=output_layer\n",
    "        \n",
    "    def __call__(self, state_ancestral, state_fraternal, reuse=True):\n",
    "        with tf.variable_scope('input', reuse=reuse):\n",
    "            state_ancestral = self.ancestral_layer(state_ancestral)\n",
    "            state_fraternal = self.fraternal_layer(state_fraternal)\n",
    "\n",
    "        with tf.variable_scope('output', reuse=reuse):\n",
    "            state_hidden = self.hidden_layer(state_ancestral + state_fraternal)\n",
    "            if self.output_layer is not None: \n",
    "                output = self.output_layer(state_hidden)\n",
    "            else:\n",
    "                output = state_hidden\n",
    "            \n",
    "        return output, state_hidden\n",
    "    \n",
    "    def get_initial_state(self, name):\n",
    "        initial_state = tf.get_variable(name, [1, self.dim_hidden], dtype=tf.float32)\n",
    "        return initial_state\n",
    "    \n",
    "    def get_zero_state(self, name):\n",
    "        zero_state = tf.zeros([1, self.dim_hidden], dtype=tf.float32, name=name)\n",
    "        return zero_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubly_rnn(dim_hidden, tree_idxs, initial_state_parent=None, initial_state_sibling=None, output_layer=None, name=''):\n",
    "    outputs, states_parent = {}, {}\n",
    "    \n",
    "    with tf.variable_scope(name, reuse=False):\n",
    "        doubly_rnn_cell = DoublyRNNCell(dim_hidden, output_layer)\n",
    "\n",
    "        if initial_state_parent is None: \n",
    "            initial_state_parent = doubly_rnn_cell.get_initial_state('init_state_parent')\n",
    "#             initial_state_parent = doubly_rnn_cell.get_zero_state('init_state_parent')\n",
    "        if initial_state_sibling is None: \n",
    "#             initial_state_sibling = doubly_rnn_cell.get_initial_state('init_state_sibling')\n",
    "            initial_state_sibling = doubly_rnn_cell.get_zero_state('init_state_sibling')\n",
    "        output, state_sibling = doubly_rnn_cell(initial_state_parent, initial_state_sibling, reuse=False)\n",
    "        outputs[0], states_parent[0] = output, state_sibling\n",
    "\n",
    "        for parent_idx, child_idxs in tree_idxs.items():\n",
    "            state_parent = states_parent[parent_idx]\n",
    "            state_sibling = initial_state_sibling\n",
    "            for child_idx in child_idxs:\n",
    "                output, state_sibling = doubly_rnn_cell(state_parent, state_sibling)\n",
    "                outputs[child_idx], states_parent[child_idx] = output, state_sibling\n",
    "\n",
    "    return outputs, states_parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell:\n",
    "    def __init__(self, dim_hidden, output_layer=None):\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.hidden_layer = tf.layers.Dense(units=dim_hidden, activation=tf.nn.tanh, name='hidden')\n",
    "        self.output_layer=output_layer\n",
    "        \n",
    "    def __call__(self, state, reuse=True):\n",
    "        with tf.variable_scope('output', reuse=reuse):\n",
    "            state_hidden = self.hidden_layer(state)\n",
    "            if self.output_layer is not None: \n",
    "                output = self.output_layer(state_hidden)\n",
    "            else:\n",
    "                output = state_hidden\n",
    "            \n",
    "        return output, state_hidden\n",
    "    \n",
    "    def get_initial_state(self, name):\n",
    "        initial_state = tf.get_variable(name, [1, self.dim_hidden], dtype=tf.float32)\n",
    "        return initial_state\n",
    "    \n",
    "    def get_zero_state(self, name):\n",
    "        zero_state = tf.zeros([1, self.dim_hidden], dtype=tf.float32, name=name)\n",
    "        return zero_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(dim_hidden, max_depth, initial_state=None, output_layer=None, name=''):\n",
    "    outputs, states_hidden = [], []\n",
    "    \n",
    "    with tf.variable_scope(name, reuse=False):\n",
    "        rnn_cell = RNNCell(dim_hidden, output_layer)\n",
    "\n",
    "        if initial_state is not None: \n",
    "            state_hidden = initial_state\n",
    "        else:\n",
    "            state_hidden = rnn_cell.get_initial_state('init_state')\n",
    "#             state_hidden = rnn_cell.get_zero_state('init_state_parent')\n",
    "        \n",
    "        for depth in range(max_depth):\n",
    "            if depth == 0:                \n",
    "                output, state_hidden = rnn_cell(state_hidden, reuse=False)\n",
    "            else:\n",
    "                output, state_hidden = rnn_cell(state_hidden, reuse=True)\n",
    "            outputs.append(output)\n",
    "            states_hidden.append(state_hidden)\n",
    "\n",
    "    outputs = tf.concat(outputs, 1)\n",
    "    states_hidden = tf.concat(states_hidden, 0)\n",
    "    return outputs, states_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stick break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nCRP(tree_sticks_topic):\n",
    "    tree_prob_topic = {}\n",
    "    tree_prob_leaf = {}\n",
    "    # calculate topic probability and save\n",
    "    tree_prob_topic[0] = 1.\n",
    "    \n",
    "    for parent_idx, child_idxs in tree_idxs.items():\n",
    "        rest_prob_topic = tree_prob_topic[parent_idx]\n",
    "        for child_idx in child_idxs:\n",
    "            stick_topic = tree_sticks_topic[child_idx]\n",
    "            if child_idx == child_idxs[-1]:\n",
    "                prob_topic = rest_prob_topic * 1.\n",
    "            else:\n",
    "                prob_topic = rest_prob_topic * stick_topic\n",
    "            \n",
    "            if not child_idx in tree_idxs: # leaf childs\n",
    "                tree_prob_leaf[child_idx] = prob_topic\n",
    "            else:\n",
    "                tree_prob_topic[child_idx] = prob_topic\n",
    "                \n",
    "            rest_prob_topic -= prob_topic\n",
    "            \n",
    "    return tree_prob_leaf\n",
    "\n",
    "def sbp(sticks_depth, max_depth):\n",
    "    prob_depth_list = []\n",
    "    rest_prob_depth = 1.\n",
    "    for depth in range(max_depth):\n",
    "        stick_depth = tf.expand_dims(sticks_depth[:, depth], 1)\n",
    "        if depth == max_depth -1:\n",
    "            prob_depth = rest_prob_depth * 1.\n",
    "        else:\n",
    "            prob_depth = rest_prob_depth * stick_depth\n",
    "        prob_depth_list.append(prob_depth)\n",
    "        rest_prob_depth -= prob_depth\n",
    "    \n",
    "    prob_depth = tf.concat(prob_depth_list, 1)\n",
    "    return prob_depth\n",
    "\n",
    "def get_ancestor_idxs(leaf_idx, ancestor_idxs = None):\n",
    "    if ancestor_idxs is None: ancestor_idxs = [leaf_idx]\n",
    "    \n",
    "    parent_idx = child_to_parent_idxs[leaf_idx]\n",
    "    ancestor_idxs += [parent_idx]\n",
    "    if parent_idx in child_to_parent_idxs: get_ancestor_idxs(parent_idx, ancestor_idxs)\n",
    "    return ancestor_idxs[::-1]\n",
    "\n",
    "def get_prob_topic(tree_prob_leaf, prob_depth):\n",
    "    tree_prob_topic = defaultdict(float)\n",
    "    \n",
    "    leaf_ancestor_idxs = {leaf_idx: get_ancestor_idxs(leaf_idx) for leaf_idx in tree_prob_leaf}\n",
    "    for leaf_idx, ancestor_idxs in leaf_ancestor_idxs.items():\n",
    "        prob_leaf = tree_prob_leaf[leaf_idx]\n",
    "        for i, ancestor_idx in enumerate(ancestor_idxs):\n",
    "            prob_ancestor = prob_leaf * tf.expand_dims(prob_depth[:, i], -1)\n",
    "            tree_prob_topic[ancestor_idx] += prob_ancestor\n",
    "    prob_topic = tf.concat([tree_prob_topic[topic_idx] for topic_idx in topic_idxs], -1)\n",
    "    return prob_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree_topic_bow(tree_topic_embeddings):\n",
    "    def softmax_with_temperature(logits, axis=None, name=None, temperature=1.):\n",
    "        if axis is None:\n",
    "            axis = -1\n",
    "        return tf.exp(logits / temperature) / tf.reduce_sum(tf.exp(logits / temperature), axis=axis)\n",
    "\n",
    "    tree_topic_bow = {}\n",
    "    for topic_idx, depth in tree_depth.items():\n",
    "        topic_embedding = tree_topic_embeddings[topic_idx]\n",
    "        temperature = tf.constant(1 ** (1./depth), dtype=tf.float32)\n",
    "        logits = tf.matmul(topic_embedding, bow_embeddings, transpose_b=True)\n",
    "        tree_topic_bow[topic_idx] = softmax_with_temperature(logits, axis=-1, temperature=temperature)\n",
    "    \n",
    "    return tree_topic_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latents_bow(bow, name):\n",
    "    with tf.variable_scope(name, reuse=False):\n",
    "        hidden_bow_ = tf.layers.Dense(units=config.dim_hidden_bow, activation=tf.nn.tanh, name='hidden_bow')(bow)\n",
    "        hidden_bow = tf.layers.Dropout(t_variables['keep_prob'])(hidden_bow_)\n",
    "        means_bow = tf.layers.Dense(units=config.dim_latent_bow, name='mean_bow')(hidden_bow)\n",
    "        logvars_bow = tf.layers.Dense(units=config.dim_latent_bow, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_bow')(hidden_bow)        \n",
    "    return means_bow, logvars_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode bow\n",
    "with tf.variable_scope('topic/enc', reuse=False):\n",
    "    means_tree, logvars_tree = get_latents_bow(t_variables['bow'], 'tree') # sample latent vectors of tree path\n",
    "    means_depth, logvars_depth = get_latents_bow(t_variables['bow'], 'depth') # sample latent vectors  of depth\n",
    "    \n",
    "    latents_tree = sample_latents(means_tree, logvars_tree) # sample latent vectors\n",
    "    latents_depth = sample_latents(means_depth, logvars_depth) # sample latent vectors\n",
    "    \n",
    "    tree_prob_layer = lambda h: tf.nn.sigmoid(tf.matmul(latents_tree, h, transpose_b=True))\n",
    "    depth_prob_layer = lambda h: tf.nn.sigmoid(tf.matmul(latents_depth, h, transpose_b=True))\n",
    "    \n",
    "    tree_sticks_topic, tree_states_sticks_topic = doubly_rnn(config.dim_latent_bow, tree_idxs, output_layer=tree_prob_layer, name='sticks_topic')\n",
    "    tree_prob_leaf = nCRP(tree_sticks_topic)\n",
    "#     prob_depth = tf.layers.Dense(units=max_depth, activation=tf.nn.softmax, name='prob_depth')(latents_bow) # inference of topic probabilities\n",
    "    sticks_depth, _ = rnn(config.dim_latent_bow, max_depth, output_layer=depth_prob_layer, name='prob_depth')\n",
    "    prob_depth = sbp(sticks_depth, max_depth)\n",
    "    \n",
    "    prob_topic = get_prob_topic(tree_prob_leaf, prob_depth)\n",
    "\n",
    "# decode bow\n",
    "with tf.variable_scope('shared', reuse=False):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "bow_embeddings = tf.nn.embedding_lookup(embeddings, bow_idxs) # embeddings of each bow features\n",
    "\n",
    "with tf.variable_scope('topic/dec', reuse=False):\n",
    "#     tree_topic_embeddings, tree_states_topic_embeddings = doubly_rnn(config.dim_emb, tree_idxs, name='emb_topic')\n",
    "    emb_layer = lambda h: tf.layers.Dense(units=config.dim_emb, name='output')(tf.nn.tanh(h))\n",
    "    tree_topic_embeddings, tree_states_topic_embeddings = doubly_rnn(config.dim_emb, tree_idxs, output_layer=emb_layer, name='emb_topic')\n",
    "#     topic_embeddings = tf.get_variable('topic_emb', [len(topic_idxs), config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of topics\n",
    "#     tree_topic_embeddings = {topic_idx: tf.expand_dims(topic_embeddings[topic_idxs.index(topic_idx)], 0) for topic_idx in topic_idxs}\n",
    "\n",
    "    tree_topic_bow = get_tree_topic_bow(tree_topic_embeddings) # bow vectors for each topic\n",
    "    \n",
    "    topic_bow = tf.concat([tree_topic_bow[topic_idx] for topic_idx in topic_idxs], 0)\n",
    "    logits_bow = tf_log(tf.matmul(prob_topic, topic_bow)) # predicted bow distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree_mask_reg(tree_idxs):\n",
    "    tree_mask_reg = np.ones([len(topic_idxs), len(topic_idxs)], dtype=np.float32)\n",
    "    parent_to_descendant_idxs = {parent_idx: get_descendant_idxs(parent_idx) for parent_idx in tree_idxs}\n",
    "    \n",
    "    for parent_idx, descendant_idxs in parent_to_descendant_idxs.items():\n",
    "        for descendant_idx in descendant_idxs:\n",
    "            tree_mask_reg[topic_idxs.index(parent_idx), topic_idxs.index(descendant_idx)] = tree_mask_reg[topic_idxs.index(descendant_idx), topic_idxs.index(parent_idx)] = 0.\n",
    "            \n",
    "    return tree_mask_reg\n",
    "\n",
    "def get_depth_mask_reg(tree_idxs):\n",
    "    depth_mask_reg = np.zeros([len(topic_idxs), len(topic_idxs)], dtype=np.float32)\n",
    "\n",
    "    depth_topic_idxs = defaultdict(list)\n",
    "    for topic_idx, _depth in tree_depth.items():\n",
    "        depth_topic_idxs[_depth].append(topic_idx)\n",
    "    \n",
    "    for _depth, child_idxs in depth_topic_idxs.items():\n",
    "        for child_idx1 in child_idxs:\n",
    "            for child_idx2 in child_idxs:\n",
    "                depth_mask_reg[topic_idxs.index(child_idx1), topic_idxs.index(child_idx2)] = 1.\n",
    "                \n",
    "    return depth_mask_reg\n",
    "\n",
    "def get_descendant_idxs(parent_idx, descendant_idxs = None):\n",
    "    if descendant_idxs is None: descendant_idxs = []\n",
    "    \n",
    "    child_idxs = tree_idxs[parent_idx]\n",
    "    descendant_idxs += child_idxs\n",
    "    for child_idx in child_idxs:\n",
    "        if child_idx in tree_idxs: get_descendant_idxs(child_idx, descendant_idxs)\n",
    "    return descendant_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define losses\n",
    "topic_losses_recon = -tf.reduce_sum(tf.multiply(t_variables['bow'], logits_bow), 1)\n",
    "topic_loss_recon = tf.reduce_mean(topic_losses_recon) # negative log likelihood of each words\n",
    "\n",
    "topic_loss_kl_tree = compute_kl_loss(means_tree, logvars_tree) # KL divergence b/w latent dist & gaussian std\n",
    "topic_loss_kl_depth = compute_kl_loss(means_depth, logvars_depth) # KL divergence b/w latent dist & gaussian std\n",
    "topic_loss_kl = topic_loss_kl_tree + topic_loss_kl_depth\n",
    "\n",
    "topic_bow_norm = topic_bow / tf.norm(topic_bow, axis=1, keepdims=True)\n",
    "topic_dots = tf.clip_by_value(tf.matmul(topic_bow_norm, tf.transpose(topic_bow_norm)), -1., 1.)\n",
    "topic_loss_reg = tf.reduce_mean(tf.square(topic_dots - tf.eye(len(topic_idxs))))\n",
    "# tree_mask_reg = get_tree_mask_reg(tree_idxs)\n",
    "# # tree_mask_reg = get_depth_mask_reg(tree_idxs)\n",
    "# topic_losses_reg = tf.square(topic_dots - tf.eye(len(topic_idxs))) * tree_mask_reg\n",
    "# topic_loss_reg = tf.reduce_sum(topic_losses_reg) / tf.reduce_sum(tree_mask_reg)\n",
    "\n",
    "# topic_embeddings = tf.concat([tree_topic_embeddings[topic_idx] for topic_idx in topic_idxs], 0)\n",
    "# topic_embeddings_norm = topic_embeddings / tf.norm(topic_embeddings, axis=1, keepdims=True)\n",
    "# topic_dots = tf.clip_by_value(tf.matmul(topic_embeddings_norm, tf.transpose(topic_embeddings_norm)), -1., 1.)\n",
    "# tree_mask_reg = get_tree_mask_reg(tree_idxs)\n",
    "# topic_loss_reg = tf.reduce_sum(tf.square(topic_dots - tf.eye(len(topic_idxs))) * tree_mask_reg) / tf.reduce_sum(tree_mask_reg)\n",
    "\n",
    "# topic_embeddings = tf.concat([tree_topic_embeddings[topic_idx] for topic_idx in topic_idxs], 0)\n",
    "# topic_embeddings_norm = topic_embeddings / tf.norm(topic_embeddings, axis=1, keepdims=True)\n",
    "# topic_dots = tf.clip_by_value(tf.matmul(topic_embeddings_norm, tf.transpose(topic_embeddings_norm)), -1., 1.)\n",
    "# topic_angles = tf.acos(topic_dots)\n",
    "# tree_mask_reg = get_tree_mask_reg(tree_idxs)\n",
    "\n",
    "# mean_angles = tf.reduce_sum(topic_angles*tree_mask_reg) / tf.reduce_sum(tree_mask_reg)\n",
    "# var_angles = tf.reduce_sum(tf.square(topic_angles-mean_angles)*tree_mask_reg) / tf.reduce_sum(tree_mask_reg)\n",
    "# mean_angles = tf.reduce_mean(topic_angles)\n",
    "# var_angles = tf.reduce_mean(tf.square(topic_angles-mean_angles))\n",
    "# mean_angles = tf.asin(tf.sqrt(tf.linalg.det(topic_dots * tree_mask_reg)))\n",
    "# var_angles = tf.square(tf.constant(np.pi/2., dtype=tf.float32)-mean_angles)\n",
    "\n",
    "# topic_loss_reg = var_angles - mean_angles\n",
    "\n",
    "global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "\n",
    "loss = topic_loss_recon + topic_loss_kl + config.reg * topic_loss_reg\n",
    "\n",
    "# define optimizer\n",
    "if config.opt == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif config.opt == 'Adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "\n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars, global_step=global_step)\n",
    "\n",
    "# monitor\n",
    "n_bow = tf.reduce_sum(t_variables['bow'], 1)\n",
    "topic_ppls = tf.divide(topic_losses_recon, tf.maximum(1e-5, n_bow))\n",
    "topics_freq_bow_indices = tf.nn.top_k(topic_bow, 5, name='topic_freq_bow').indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, ppls_batch \\\n",
    "            = sess.run([loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, topic_ppls], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "    loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    return loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, ppl_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import Normalize # Normalizeã‚’import\n",
    "\n",
    "def print_topic_sample():\n",
    "    _topics_bow = sess.run(topic_bow)\n",
    "    plt.figure(figsize=(9, 15))\n",
    "    \n",
    "    for i in range(0, len(topic_idxs)):\n",
    "        _topic_bow = _topics_bow[i]\n",
    "        plt.subplot(3,3,i+1)\n",
    "        plt.axis('off')\n",
    "        plt.title(topic_idxs[i])\n",
    "        plt.imshow(_topic_bow.reshape(30,30), cmap='Wistia', norm=Normalize(vmin=0., vmax=0.01))\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "saver = tf.train.Saver(max_to_keep=10)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','TM','','','','VALID:','TM','','',''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL','REG','LOSS','PPL','NLL','KL','REG']]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>VALID:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>63</td>\n",
       "      <td>643.52</td>\n",
       "      <td>596</td>\n",
       "      <td>639.00</td>\n",
       "      <td>4.10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>640.57</td>\n",
       "      <td>584</td>\n",
       "      <td>636.96</td>\n",
       "      <td>3.53</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>127</td>\n",
       "      <td>641.13</td>\n",
       "      <td>585</td>\n",
       "      <td>637.12</td>\n",
       "      <td>3.78</td>\n",
       "      <td>0.02</td>\n",
       "      <td>639.40</td>\n",
       "      <td>578</td>\n",
       "      <td>635.95</td>\n",
       "      <td>3.40</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>35</td>\n",
       "      <td>640.09</td>\n",
       "      <td>580</td>\n",
       "      <td>636.28</td>\n",
       "      <td>3.64</td>\n",
       "      <td>0.02</td>\n",
       "      <td>638.93</td>\n",
       "      <td>576</td>\n",
       "      <td>635.60</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>99</td>\n",
       "      <td>639.50</td>\n",
       "      <td>577</td>\n",
       "      <td>635.80</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.01</td>\n",
       "      <td>638.67</td>\n",
       "      <td>574</td>\n",
       "      <td>635.31</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>639.13</td>\n",
       "      <td>575</td>\n",
       "      <td>635.49</td>\n",
       "      <td>3.53</td>\n",
       "      <td>0.01</td>\n",
       "      <td>638.55</td>\n",
       "      <td>574</td>\n",
       "      <td>635.19</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6000</th>\n",
       "      <td>11</td>\n",
       "      <td>38</td>\n",
       "      <td>71</td>\n",
       "      <td>638.86</td>\n",
       "      <td>574</td>\n",
       "      <td>635.26</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>638.52</td>\n",
       "      <td>573</td>\n",
       "      <td>635.13</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7000</th>\n",
       "      <td>10</td>\n",
       "      <td>44</td>\n",
       "      <td>135</td>\n",
       "      <td>638.66</td>\n",
       "      <td>573</td>\n",
       "      <td>635.08</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.01</td>\n",
       "      <td>638.36</td>\n",
       "      <td>572</td>\n",
       "      <td>635.00</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8000</th>\n",
       "      <td>11</td>\n",
       "      <td>51</td>\n",
       "      <td>43</td>\n",
       "      <td>638.51</td>\n",
       "      <td>572</td>\n",
       "      <td>634.95</td>\n",
       "      <td>3.48</td>\n",
       "      <td>0.01</td>\n",
       "      <td>638.33</td>\n",
       "      <td>572</td>\n",
       "      <td>634.95</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9000</th>\n",
       "      <td>11</td>\n",
       "      <td>57</td>\n",
       "      <td>107</td>\n",
       "      <td>638.38</td>\n",
       "      <td>572</td>\n",
       "      <td>634.83</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0.01</td>\n",
       "      <td>638.27</td>\n",
       "      <td>571</td>\n",
       "      <td>634.79</td>\n",
       "      <td>3.46</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>12</td>\n",
       "      <td>64</td>\n",
       "      <td>15</td>\n",
       "      <td>638.26</td>\n",
       "      <td>571</td>\n",
       "      <td>634.71</td>\n",
       "      <td>3.48</td>\n",
       "      <td>0.01</td>\n",
       "      <td>638.19</td>\n",
       "      <td>570</td>\n",
       "      <td>634.60</td>\n",
       "      <td>3.55</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11000</th>\n",
       "      <td>14</td>\n",
       "      <td>70</td>\n",
       "      <td>79</td>\n",
       "      <td>638.13</td>\n",
       "      <td>570</td>\n",
       "      <td>634.54</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.01</td>\n",
       "      <td>637.19</td>\n",
       "      <td>560</td>\n",
       "      <td>632.80</td>\n",
       "      <td>4.11</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12000</th>\n",
       "      <td>11</td>\n",
       "      <td>76</td>\n",
       "      <td>143</td>\n",
       "      <td>637.82</td>\n",
       "      <td>567</td>\n",
       "      <td>634.12</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.01</td>\n",
       "      <td>633.84</td>\n",
       "      <td>538</td>\n",
       "      <td>628.76</td>\n",
       "      <td>4.66</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13000</th>\n",
       "      <td>12</td>\n",
       "      <td>83</td>\n",
       "      <td>51</td>\n",
       "      <td>637.45</td>\n",
       "      <td>565</td>\n",
       "      <td>633.65</td>\n",
       "      <td>3.68</td>\n",
       "      <td>0.01</td>\n",
       "      <td>633.49</td>\n",
       "      <td>536</td>\n",
       "      <td>628.43</td>\n",
       "      <td>4.68</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14000</th>\n",
       "      <td>12</td>\n",
       "      <td>89</td>\n",
       "      <td>115</td>\n",
       "      <td>637.11</td>\n",
       "      <td>562</td>\n",
       "      <td>633.22</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.01</td>\n",
       "      <td>633.30</td>\n",
       "      <td>536</td>\n",
       "      <td>628.35</td>\n",
       "      <td>4.59</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>12</td>\n",
       "      <td>96</td>\n",
       "      <td>23</td>\n",
       "      <td>636.81</td>\n",
       "      <td>560</td>\n",
       "      <td>632.84</td>\n",
       "      <td>3.81</td>\n",
       "      <td>0.02</td>\n",
       "      <td>633.08</td>\n",
       "      <td>534</td>\n",
       "      <td>628.08</td>\n",
       "      <td>4.66</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16000</th>\n",
       "      <td>12</td>\n",
       "      <td>102</td>\n",
       "      <td>87</td>\n",
       "      <td>636.53</td>\n",
       "      <td>558</td>\n",
       "      <td>632.51</td>\n",
       "      <td>3.86</td>\n",
       "      <td>0.02</td>\n",
       "      <td>633.10</td>\n",
       "      <td>534</td>\n",
       "      <td>628.13</td>\n",
       "      <td>4.64</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17000</th>\n",
       "      <td>12</td>\n",
       "      <td>108</td>\n",
       "      <td>151</td>\n",
       "      <td>636.29</td>\n",
       "      <td>557</td>\n",
       "      <td>632.21</td>\n",
       "      <td>3.91</td>\n",
       "      <td>0.02</td>\n",
       "      <td>633.00</td>\n",
       "      <td>534</td>\n",
       "      <td>628.03</td>\n",
       "      <td>4.65</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18000</th>\n",
       "      <td>12</td>\n",
       "      <td>115</td>\n",
       "      <td>59</td>\n",
       "      <td>636.07</td>\n",
       "      <td>555</td>\n",
       "      <td>631.94</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.02</td>\n",
       "      <td>632.94</td>\n",
       "      <td>534</td>\n",
       "      <td>628.01</td>\n",
       "      <td>4.61</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19000</th>\n",
       "      <td>12</td>\n",
       "      <td>121</td>\n",
       "      <td>123</td>\n",
       "      <td>635.87</td>\n",
       "      <td>554</td>\n",
       "      <td>631.70</td>\n",
       "      <td>3.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>633.05</td>\n",
       "      <td>534</td>\n",
       "      <td>628.13</td>\n",
       "      <td>4.61</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>12</td>\n",
       "      <td>128</td>\n",
       "      <td>31</td>\n",
       "      <td>635.69</td>\n",
       "      <td>553</td>\n",
       "      <td>631.48</td>\n",
       "      <td>4.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>633.09</td>\n",
       "      <td>535</td>\n",
       "      <td>628.13</td>\n",
       "      <td>4.65</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21000</th>\n",
       "      <td>12</td>\n",
       "      <td>134</td>\n",
       "      <td>95</td>\n",
       "      <td>635.52</td>\n",
       "      <td>552</td>\n",
       "      <td>631.28</td>\n",
       "      <td>4.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>632.76</td>\n",
       "      <td>533</td>\n",
       "      <td>627.81</td>\n",
       "      <td>4.65</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22000</th>\n",
       "      <td>12</td>\n",
       "      <td>141</td>\n",
       "      <td>3</td>\n",
       "      <td>635.37</td>\n",
       "      <td>551</td>\n",
       "      <td>631.10</td>\n",
       "      <td>4.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>632.82</td>\n",
       "      <td>533</td>\n",
       "      <td>627.91</td>\n",
       "      <td>4.62</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23000</th>\n",
       "      <td>11</td>\n",
       "      <td>147</td>\n",
       "      <td>67</td>\n",
       "      <td>635.23</td>\n",
       "      <td>550</td>\n",
       "      <td>630.93</td>\n",
       "      <td>4.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>632.78</td>\n",
       "      <td>533</td>\n",
       "      <td>627.85</td>\n",
       "      <td>4.63</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24000</th>\n",
       "      <td>12</td>\n",
       "      <td>153</td>\n",
       "      <td>131</td>\n",
       "      <td>635.11</td>\n",
       "      <td>549</td>\n",
       "      <td>630.77</td>\n",
       "      <td>4.12</td>\n",
       "      <td>0.02</td>\n",
       "      <td>632.81</td>\n",
       "      <td>533</td>\n",
       "      <td>627.90</td>\n",
       "      <td>4.61</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>10</td>\n",
       "      <td>160</td>\n",
       "      <td>39</td>\n",
       "      <td>634.99</td>\n",
       "      <td>548</td>\n",
       "      <td>630.63</td>\n",
       "      <td>4.14</td>\n",
       "      <td>0.02</td>\n",
       "      <td>632.78</td>\n",
       "      <td>533</td>\n",
       "      <td>627.84</td>\n",
       "      <td>4.64</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26000</th>\n",
       "      <td>12</td>\n",
       "      <td>166</td>\n",
       "      <td>103</td>\n",
       "      <td>634.88</td>\n",
       "      <td>547</td>\n",
       "      <td>630.50</td>\n",
       "      <td>4.16</td>\n",
       "      <td>0.02</td>\n",
       "      <td>632.80</td>\n",
       "      <td>533</td>\n",
       "      <td>627.92</td>\n",
       "      <td>4.59</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27000</th>\n",
       "      <td>12</td>\n",
       "      <td>173</td>\n",
       "      <td>11</td>\n",
       "      <td>634.78</td>\n",
       "      <td>547</td>\n",
       "      <td>630.38</td>\n",
       "      <td>4.17</td>\n",
       "      <td>0.02</td>\n",
       "      <td>632.79</td>\n",
       "      <td>533</td>\n",
       "      <td>627.91</td>\n",
       "      <td>4.59</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28000</th>\n",
       "      <td>11</td>\n",
       "      <td>179</td>\n",
       "      <td>75</td>\n",
       "      <td>634.68</td>\n",
       "      <td>546</td>\n",
       "      <td>630.26</td>\n",
       "      <td>4.19</td>\n",
       "      <td>0.02</td>\n",
       "      <td>632.69</td>\n",
       "      <td>533</td>\n",
       "      <td>627.79</td>\n",
       "      <td>4.61</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29000</th>\n",
       "      <td>13</td>\n",
       "      <td>185</td>\n",
       "      <td>139</td>\n",
       "      <td>634.59</td>\n",
       "      <td>545</td>\n",
       "      <td>630.15</td>\n",
       "      <td>4.21</td>\n",
       "      <td>0.02</td>\n",
       "      <td>632.71</td>\n",
       "      <td>533</td>\n",
       "      <td>627.79</td>\n",
       "      <td>4.63</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>11</td>\n",
       "      <td>192</td>\n",
       "      <td>47</td>\n",
       "      <td>634.51</td>\n",
       "      <td>545</td>\n",
       "      <td>630.05</td>\n",
       "      <td>4.22</td>\n",
       "      <td>0.02</td>\n",
       "      <td>632.68</td>\n",
       "      <td>533</td>\n",
       "      <td>627.77</td>\n",
       "      <td>4.63</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31000</th>\n",
       "      <td>11</td>\n",
       "      <td>198</td>\n",
       "      <td>111</td>\n",
       "      <td>634.43</td>\n",
       "      <td>544</td>\n",
       "      <td>629.96</td>\n",
       "      <td>4.23</td>\n",
       "      <td>0.02</td>\n",
       "      <td>632.82</td>\n",
       "      <td>533</td>\n",
       "      <td>627.90</td>\n",
       "      <td>4.64</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32000</th>\n",
       "      <td>10</td>\n",
       "      <td>205</td>\n",
       "      <td>19</td>\n",
       "      <td>634.36</td>\n",
       "      <td>544</td>\n",
       "      <td>629.87</td>\n",
       "      <td>4.25</td>\n",
       "      <td>0.02</td>\n",
       "      <td>632.73</td>\n",
       "      <td>533</td>\n",
       "      <td>627.83</td>\n",
       "      <td>4.62</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33000</th>\n",
       "      <td>12</td>\n",
       "      <td>211</td>\n",
       "      <td>83</td>\n",
       "      <td>634.29</td>\n",
       "      <td>543</td>\n",
       "      <td>629.78</td>\n",
       "      <td>4.26</td>\n",
       "      <td>0.02</td>\n",
       "      <td>632.75</td>\n",
       "      <td>533</td>\n",
       "      <td>627.82</td>\n",
       "      <td>4.65</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      TRAIN:   TM                      VALID:   TM          \\\n",
       "      Time   Ep   Ct    LOSS  PPL     NLL    KL   REG    LOSS  PPL     NLL   \n",
       "1000    11    6   63  643.52  596  639.00  4.10  0.04  640.57  584  636.96   \n",
       "2000    10   12  127  641.13  585  637.12  3.78  0.02  639.40  578  635.95   \n",
       "3000     9   19   35  640.09  580  636.28  3.64  0.02  638.93  576  635.60   \n",
       "4000    10   25   99  639.50  577  635.80  3.57  0.01  638.67  574  635.31   \n",
       "5000     2   32    7  639.13  575  635.49  3.53  0.01  638.55  574  635.19   \n",
       "6000    11   38   71  638.86  574  635.26  3.50  0.01  638.52  573  635.13   \n",
       "7000    10   44  135  638.66  573  635.08  3.49  0.01  638.36  572  635.00   \n",
       "8000    11   51   43  638.51  572  634.95  3.48  0.01  638.33  572  634.95   \n",
       "9000    11   57  107  638.38  572  634.83  3.47  0.01  638.27  571  634.79   \n",
       "10000   12   64   15  638.26  571  634.71  3.48  0.01  638.19  570  634.60   \n",
       "11000   14   70   79  638.13  570  634.54  3.51  0.01  637.19  560  632.80   \n",
       "12000   11   76  143  637.82  567  634.12  3.60  0.01  633.84  538  628.76   \n",
       "13000   12   83   51  637.45  565  633.65  3.68  0.01  633.49  536  628.43   \n",
       "14000   12   89  115  637.11  562  633.22  3.75  0.01  633.30  536  628.35   \n",
       "15000   12   96   23  636.81  560  632.84  3.81  0.02  633.08  534  628.08   \n",
       "16000   12  102   87  636.53  558  632.51  3.86  0.02  633.10  534  628.13   \n",
       "17000   12  108  151  636.29  557  632.21  3.91  0.02  633.00  534  628.03   \n",
       "18000   12  115   59  636.07  555  631.94  3.95  0.02  632.94  534  628.01   \n",
       "19000   12  121  123  635.87  554  631.70  3.98  0.02  633.05  534  628.13   \n",
       "20000   12  128   31  635.69  553  631.48  4.02  0.02  633.09  535  628.13   \n",
       "21000   12  134   95  635.52  552  631.28  4.04  0.02  632.76  533  627.81   \n",
       "22000   12  141    3  635.37  551  631.10  4.07  0.02  632.82  533  627.91   \n",
       "23000   11  147   67  635.23  550  630.93  4.10  0.02  632.78  533  627.85   \n",
       "24000   12  153  131  635.11  549  630.77  4.12  0.02  632.81  533  627.90   \n",
       "25000   10  160   39  634.99  548  630.63  4.14  0.02  632.78  533  627.84   \n",
       "26000   12  166  103  634.88  547  630.50  4.16  0.02  632.80  533  627.92   \n",
       "27000   12  173   11  634.78  547  630.38  4.17  0.02  632.79  533  627.91   \n",
       "28000   11  179   75  634.68  546  630.26  4.19  0.02  632.69  533  627.79   \n",
       "29000   13  185  139  634.59  545  630.15  4.21  0.02  632.71  533  627.79   \n",
       "30000   11  192   47  634.51  545  630.05  4.22  0.02  632.68  533  627.77   \n",
       "31000   11  198  111  634.43  544  629.96  4.23  0.02  632.82  533  627.90   \n",
       "32000   10  205   19  634.36  544  629.87  4.25  0.02  632.73  533  627.83   \n",
       "33000   12  211   83  634.29  543  629.78  4.26  0.02  632.75  533  627.82   \n",
       "\n",
       "                   \n",
       "         KL   REG  \n",
       "1000   3.53  0.01  \n",
       "2000   3.40  0.01  \n",
       "3000   3.30  0.00  \n",
       "4000   3.34  0.00  \n",
       "5000   3.33  0.00  \n",
       "6000   3.36  0.00  \n",
       "7000   3.34  0.00  \n",
       "8000   3.36  0.00  \n",
       "9000   3.46  0.00  \n",
       "10000  3.55  0.00  \n",
       "11000  4.11  0.03  \n",
       "12000  4.66  0.04  \n",
       "13000  4.68  0.04  \n",
       "14000  4.59  0.04  \n",
       "15000  4.66  0.03  \n",
       "16000  4.64  0.03  \n",
       "17000  4.65  0.03  \n",
       "18000  4.61  0.03  \n",
       "19000  4.61  0.03  \n",
       "20000  4.65  0.03  \n",
       "21000  4.65  0.03  \n",
       "22000  4.62  0.03  \n",
       "23000  4.63  0.03  \n",
       "24000  4.61  0.03  \n",
       "25000  4.64  0.03  \n",
       "26000  4.59  0.03  \n",
       "27000  4.59  0.03  \n",
       "28000  4.61  0.03  \n",
       "29000  4.63  0.03  \n",
       "30000  4.63  0.03  \n",
       "31000  4.64  0.03  \n",
       "32000  4.62  0.03  \n",
       "33000  4.65  0.03  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhsAAAMCCAYAAADAiQRaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3X+srdlZH/Zn7X1+3Pk9Y49nbA/FuJCo1CSEKn9EbRWQkjSCCiGFNFXBlaiEhEhMHWFibGyPx7+oCzINiiMikEOQS5qEBBpVFWoSJY3IP5GsKqC4spJCHMLYM3PtmfH8uL/O2fvtH3Mtzd3ruXPXnHOevffZ5/ORLPmu++73Xfvda+/zzL7f86w2TVMAAFSZbXoCAMBuU2wAAKUUGwBAKcUGAFBKsQEAlFJsAAClFBsAQCnFxhZprb2htfYbrbWXW2v/vrX2A5ueE7werbV3tdY+11q73lr7W5ueD7werbXD1tpnbn7+vtha+1ette/e9Lx2wd6mJ8At/npE3IiIRyPij0XE/9la++1pmj6/2WnBsC9FxMcj4s9GxF0bngu8XnsR8R8i4jsj4vcj4nsi4u+11v7INE1f3OTEzrumg+h2aK3dExHPRcS3TdP0b26OfTYinpym6X0bnRy8Tq21j0fEN0zT9EObngucRmvtdyLiI9M0/YNNz+U8888o2+MPR8Tx1wuNm347It6xofkAXGittUfjlc9m3y6fkmJje9wbES+sjH0tIu7bwFwALrTW2n5E/GpE/Mo0TV/Y9HzOO8XG9ngpIu5fGbs/Il7cwFwALqzW2iwiPhuvZOjeteHp7ATFxvb4NxGx11r7Q68a+/bw9R3A2rTWWkR8Jl4J6n//NE1HG57STlBsbIlpml6OiF+PiI+21u5prf0XEfF98Up1DedCa22vtXYpIuYRMW+tXWqt+a03zpNfiIhvjYjvnabp6qYnsysUG9vlL8Yrvy74TET8bxHxo37tlXPmgxFxNSLeFxHvvPn/P7jRGcGg1trbIuJH4pXWA0+11l66+b8f3PDUzj2/+goAlPLNBgBQSrEBAJRSbAAApRQbAEApxQYAUGqtv//+5eV7hn71ZYqWjPZ1UYvF4GOrjdVsU+vn1tLfBloOXqM/rkV/vuXsUjc2X/SNSaeYJ9dYPX9/z5ft4I6Pu93cYurPF4Ov4Vvm/8v6X+wvvaV7Esu9w+6w2dG1sfMla2Kxf283Nr+RNJJd3DqV5WH/Os8WN7qxl+7rt9uZL17qxpYtOd+UPa9+bWZrYjb1c2nJ6z+1fh1m57vn2eS3wq8f92OHKx9z2aq5kazDWXLgc1f6sfv7+xR7yft1maz/P/Ts2tfwk/EBv4LImXksPjG0hn2zAQCUUmwAAKUUGwBAqbVmNpbJFgl5ZmHMaD4jzQqsyOY2m/p//83+7Tj/d+exuWX/Pt2yaMPocelF+ucxks945SK31qPTcH3aHzdlWZTRvEfy7/0bkbyus+PrQ8elGYDj/p7Mj/r8xOLgvv50y1vzE7Pr/TYOi7se6M+f5DOO5/1xs2X/vNL1n6y6ada/n5ZTcu+ya8ySDMwy2aLiSrImHrq7H1vNWWR5iuyNkx33cJ+niaMk73El2bvr3v55sWLKPiP8N/Eu8CoCAKUUGwBAKcUGAFBKsQEAlFprQHQ8DJo0qxps/pOG1QaahGVh0DxEmTXcysb2T/HY7PknhyXyIGnWOGvssauNmLJ7Ms2SBk5J8G9Kw7VZ8DN7vTbRrK13vNcHBPeOk0DnvG/01LLXdT9pkpY1Ycuaeq2EUKfDPoCYPW55+A3d2MhrHxFxtPeGbmxv8bV+7LgfO97rQ6hZ4HS+6BtnZfNLA7cvJk3HRpZOdsxecs2ryXq9K3mvL5L39SnC8BeGMOjO8soCAKUUGwBAKcUGAFBKsQEAlFprQDTXB6nSMFgq220y21n1zuHS0RBlFnIdnW/+2LEdbkd3jG1T0rkwMRokXQ2ELpMwaBr8Tc+fhOuye3eKnWCrZeHNxV4/3/mi73g5JeG3tkgeO73cX3igI2lbJvct2QV1dk8f3s06dGbrYe/4uf64bKfhJPi5f/Rsf91kTSxmdyVz6YZyWTDzYOVjLluvSSfXuJHsIJvN47mku2l2jSxICheEbzYAgFKKDQCglGIDACil2AAASm1BQLSvd7Junplsy/LhINnKY7Muoy2S7o7JVvSj8iDpaAfRsdBopF1Pk2BiEiQd6ciahQZHu7a25Pxpl9b0eW2HgytP94PZfOfZPc9e60S2hrOw4so1poM+gNju7oOameP5/d3YfhIGnSdB0qn1152S90kWLr6+99Zu7ODocjeWhUbTEOZqGDSiD41eS+7lfcn278vk/MvBzqCj54MLwjcbAEApxQYAUEqxAQCUUmwAAKXWHBDNAnJJ8HFwa/eWbQufBjjvXFPlHTXHgqq5sed6uvMlgcC0+2YS6kyPSzqSrlw3i25mr1cb3Cp6tONpnprcgCT4GTeSe5lsf3508GA3tn/Ub8W+nCWdJpPuk7Pu3vXXXMzv6cay136+6LuWZut/OeuDj4v5vf1jl8l7M1mvhze+1I0dJ+fLrhv7yedE0jG1u3eHyf3Ngp8vJx1vLyWfLw8lc7uWrGsB0d2UBb8HP/8uEncEACil2AAASik2AIBSig0AoNSaA6JZF8yxrc7TQONAx8uI2207v3pcFkDtQ14tDa/2xredT7Z1Tzoy5oHOfs6jXUrTTqtZh8/V49It4cc6Y+ZdRce6m26NpFvkdFcfEMy2e58vrnRjWRg0C1zuX/1KP5e9W1+L473kccd9APXawVv6cyUWs7uT8z2fHJn9N0u25sbWetZB+OD6H/SXyAKiWXfQ1dcs68aaBUvvSbqvZgnpo+S9mQWJkzXBDhAGHeIuAQClFBsAQCnFBgBQSrEBAJRaa0A027J9tDFktj316HbnWeBstetlFqzMOh7eZnbJ2Ng26afZdj7rIJrpw7C3mUsW4FwdS4OqY/KAYHLf29i26JuwOOg7cs6vvjj02Nmsfw0Xh/d1Y2kYNOlIuhpM3F/0W8Jn58/eD8vknk9J8O36YR8uzYKvmaO9h5K59GHg/aNnu7Frh4/1x73wdH+RrMPnaofXe5KOn6PBz+x1yN5e8+S4LNAKF4RvNgCAUooNAKCUYgMAKLXWzEa+m2uvTf1ui1NL/p01vUiWC8ie5q3/fj4bzDWMN9IabVaVNTrL/gE5O1/2XPvHZvObTdf647LdW1dyNtm/7ecNx3rDjc6SfwQfXTvV5jf63VGzf/A/uvvhbmz/Sp/FmL/Y5yzi7iSzkjZhu3Vwmo++nZOGW+nOwP16PWp97qIlO7LuJc2/5u1qN5a9rllW5ODocjcWx0mOaS/576fV98Rxsl6zc2Wvw+Ukn3P/XXe+JlxwvtkAAEopNgCAUooNAKCUYgMAKLXmXV8zWeCyn9ZIY66IyHcuHQkmpgHRsd1MR8Obo4abcA3tZhsxtewejzUEGwlmzpJA77JdOvFxkQVpB8Ol1aa95L4l76L9K0mgcT85MGsStUx2Lt7rQ5iza7cGLttxf3/bYb82s5BvtoazRnrZLrLZWr966e39FZZ9KHm2vN6NZe/XG/tv6sYOj3+vG0sDoqvviWxn2LuTAHr2PsyCn9eT9fpgv2NuLEZD6LB7fLMBAJRSbAAApRQbAEApxQYAUGrjAdHR7ptZaDDtlpk8pTxceutjs8DkLAm+LWdJoDGRdUFch2mwceFs0XdCbMv+HncdRJPnnwf/+m6ReXj1pX5soJNpRERsYHPYLz3yzn5wQ691Z0oCzcnc3nz57yQPHnsOWVfR1aBqRMT+cb9za7ar8N6if/2P9+7vxi5dfbIbe/I/fddt51nmm872dP1etudQtu4Sacfj2crnbvJZHckvDDz25Ge6saPDN3Zj+8d9h95s9+FsXbfs50bWjTp5j2VrPbP3v39h6LjVH09phj5rPJx8bKYNtZOxqc9uR0tyz/EXPpEM9rbkUxIA2FWKDQCglGIDACil2AAASq13i/lTdLzMu3Rmj81Coyfrgrkt25pXyMJaWQhz9R7MloP3KQtNZd0Xk3lkgd5lso35RqwjDDoY9DzRMZG/9rMbLw89dnF4Xze2TDLT86M++Hl08GB/vvk9yfySMPh841n2i2d0HQ6uu6HQfJpe7F27+z/qxrJutGkX3Gtf7eeWdPfNPtdmaZC0fz8d7T3QjWVh6Cz33vq3WCxX8tbtrv6Y9MdV1sg6aYI89Rn/9KWY+gbCg1FY32wAAMUUGwBAKcUGAFBKsQEAlNqC1FUfQmrTYLgwrZXGutl150tab+bX3BFJ+CvrDroaflrHJtl5V9kLtD33WYZQk9d5St43afAzSYjt33i+v8asf58c7/fny0LY86Mr/Vz27+2vwfptS2fcxDzpUDxPAphpePNS32l0vuwD0lkYPl7ok5RHb3hLN5YFU6/Pk864D/xBNzb1Ty1mK1Oekqzp1L+VInnLpY+dvTU57nI/1vo897DtXU0AwE5QbAAApRQbAEApxQYAUGqtAdG842eynXi2L+5gN8/RrqKrAaPFrG/JlnayHOxwt/WS+753/EI31t2XLDSWhBBbGugcDe8mLe6G+9RtsZN2Bj2N5PzzZR9yyzp5zhd94uzo4A3d2N6iXzdZN8fFvN+f+vhS32lx/6jv8NiW2WcCF9UsWZs39h/uxg5u9CnHLKicBkn3+7XeHuofu3/j2W4sjpP3ehKkjmwb96Qjbxzf+ZgsWJp2Gs0+wvu3cBoubW9KzjfINxsAQCnFBgBQSrEBAJRSbAAApTa+xXx+XBYQzMKFSXfENHDabwucBUK7s+9KGDSRBviSe9IFPZMgVdrxMz2u76rXkhBu9notB16vrbclHRmP9/pOhplp3t/zLAyahTdbS8ayDo/H/Z7VR3sPdWOLvR0ICG+LdQSVi6+xnB12Y5eufakbO97ru9Fmn32ZWdKldJklM5Pg53SYfNYt+s+67Cfw1Dcz7X/UDTbUbv1tyoOkDyZjfZ47InnsqO349AMAdpZiAwAopdgAAEopNgCAUmvuIDoWLsw6vE2DHUSnNhYa7fV1Vz7foWlsvSxglYU1p9UkUhry6m9Kdv4scJWFRjMt2+6ZE0mDwMlaHx3LQngt+uPmx0ngbt6//mlHxqz7IidzzsKgmWyL+cU8a73ZW+zd141lHUmPky3h95KQ8yIJUqfb02dB0heTCY78fkTSoDlrvJ2GQbNQatJBNM39Zx1P+6GUbzYAgFKKDQCglGIDACil2AAASq25g2iWfEmOS8KgWeCsCy9GxCzZYn6sE+jY9ue7LA/Ert67JA6UBQkHr7lM1kQW/hoL+XJSWWfQ7P2aheEy86Nke/rDN3ZjWRh0sd8HTne5m+/anXWgcwOdcWfHSVJx2X/2H93VbzufBdXTNGTyKZb9zNlLgs+R/BxK1//9X+4f2mdQoz2wckwW6Mw6iN6TTC07f/K2nvrmvhFjef6UbzYAgFKKDQCglGIDACil2AAASm1B6irr+JkFc7JkSvLY5KhsS+Gsw+XIPHZFS7q0ZiG8aXZrSDALkY7KuoVmHfnSeexqQHAT3RcX2R7WvSwMl70j9o6TxFnWLTHdA7sfml9L0m+X+m6OnNAOdBC9dtdj3dj+UR823jt+vhtbzJOOt9NRMpYE5mf9Z9j1w0e7scMbT/ePzdZ/f9m8O+i1lYHslvf504is8XLWQTTbOv7OPyJfF99sAAClFBsAQCnFBgBQSrEBAJTagi3mkxBa0n0t2zp+VBZMXL1GHgbd3VpstJtrWw1/ZWGw9Pz9vcuCiVln0JYmpI6HrnvubKD7Yma26JNk016yRrIOvck28cvkPZc53uuDn4uDu7uxw+tPDZ2PYhsIg2bmy9XEZMRy3q+bWJ485J91Gp0lwfqs9+hi1m93P0vmnG3ZPvXNd6P7uM46fvb52LTjZ/rRn92mbEwHUQBgWyk2AIBSig0AoJRiAwAotfG2jHlHytEaKDtuNMB4a/plPJS6K/VZlv7p78Fqp9H8PvXnmi36QOdoKDUNjSYd/jiZLOQWi36tz1sf6F0mr+HsqE+5TQdZQLjvNNqy92sSQjze10F0K2xJoDlbS1koeTnrk5TpGl5mMc/eLLnuYp5dY6xbcvaxNntDP9b9mBztPJqFTR9Ijuuzq9GSDqLp+ZKpZLZj5QAAO0uxAQCUUmwAAKUUGwBAqbUGRN/65K/0g1nnziSsFvOkLlomiZh5cr7jgfMlW2Jnj7t+71u7sa7L5m1kXTVzWVfVLEibhTX7oFM2dun3/t+xqSxXntthEvI8Htx2Pn1scu+y2zRLBt/xN8eue5Ela/PLj/xg+TW2JUjIbjrzNbwpf+mEj9ui99xjg8f5RAAASik2AIBSig0AoJRiAwAotfEOomlAcC8LgybHpQHRpBNc2ixz5bFZxjM7f3aqtONl30FznmxZvEzatGXnyyxnSYu3xGw51jEy9pPrzleWSPa47DYlnUbjSjKPLNCb5U3Tbq4XyEkDYesIjWXX2KIAG5TaxFo/h++l8zdjAOBcUWwAAKUUGwBAKcUGAFBq8wHRrHNnGswc3Mg2fejAY7NAYxJUzbqFZp1Bl7N+i+FYjj2HdAvwdGv3fmvj2bLf7ns5Szp3HiYvfXbfj1aCrvvJ49KSdbAzbPbaZN1CR7uU7qrzFgg7b/PlbG0iNLmpULK1PsRdAgBKKTYAgFKKDQCg1HozG1kDr6yRVJbjuN43yUofmzX/ynIcq9fI8gqtH1vMx3ISoxmT7HxZQ7D8fFmt2B83Wx4NzSV1sLJE0l16x5qfpY8d3c03a/TGnQ3uSHyqf3dO12tyvm35t20Nx+qd5f3M1ldLfnSd4pot+QyfBpsmMsY7DAAopdgAAEopNgCAUooNAKDUegOi6Q6fSVjrOAkIZmHQ0Z1Ls2DqSKYxCTTOF1f6UyW7tGah0Tb1jamysXQqSUhqan2zruy6x3v39SfM7l3aTG2lHk1zqqMNvJLHHiXPPwuNXj1FyPUiW0tTo833BnxdhEF72xyaPeP11ZLAvDBovS1ZTQDArlJsAAClFBsAQCnFBgBQar3Jrmw3zxtJd7isW2QWXszGst1Bs4DoamfMrAvmYGPMLCCa7Q67mN/bjc2W17qxbMfY2TLZCTZJXOYdSQdDmFnpuRrqHQ3bZt1C0x1ek7FsTdx3KbkIDFh9L25L8HGbXKB7MmW7YFPu4qwwAGAjFBsAQCnFBgBQSrEBAJTafOu/S0lYJwshZlvHZ+HC7LFZ4HQ1SJqFHPeylpdJV9Ek5LmY9YHG2fLq0Pn2Fi8m57u7f2TafTRNa/ZDWbg26/q5WHlsdkuysGl2rkx2WHa+rNPottjm7ou74jT3eOQ4ryHF8s6lFyes6t0EAJRSbAAApRQbAEApxQYAUGq9AdEsNDhLtoSfkvBi1n0yOy7bYj7LTK52xszOlTUtHezumYU3p2Sr5GxL+CwMOmo29SGkLKyahmbTE64ct0iCmlln2LQzaPbY5JpZeLUNtnOtJkhYbxP32Gt4trb5fTI6tzN+Dlmn6YtkS159AGBXKTYAgFKKDQCglGIDACi13oBoFkrMthPPwoVpyHOwVloNg0ZEHK2Efw6TW5HMLQt0tqQN5hR9GCgLCGWB02ysTf1cpiSs1KZ+LOtwGteT+76f3IOjleOy8GYk58pkIdysJWn2umbB303YlpBb5qxDecn50uDzabogZnPm/FtD4PLEzrLzbMT489rmz441uNjPHgAop9gAAEopNgCAUooNAKDUmjuIJgHBrPtkFujMHptuYz7YaXQ1hJoFJge7bC5bH+jMEq1ZuC4b2z9+fui4o70Hh45L5zeY1eyCmdnjsvt7nLw22f3MAqfZbd+WLeZPE3JLQr6RdJU9sbMOoCXny0LJZ30NdtQmXuvR99xp3pvW8BB3CQAopdgAAEopNgCAUooNAKBUm9KujgAAZ8M3GwBAKcUGAFBKsQEAlFJsAAClFBsAQCnFBgBQSrEBAJRSbAAApRQbAEApxQYAUEqxAQCUUmwAAKUUGwBAKcUGAFBKsQEAlFJsAAClFBsAQCnFBgBQSrEBAJRSbAAApRQbAEApxQYAUEqxAQCUUmwAAKUUGwBAKcUGAFBKsQEAlFJsAAClFBsAQCnFBgBQSrEBAJRSbAAApRQbAEApxQYAUEqxAQCUUmwAAKUUGwBAKcUGAFBKsQEAlFJsAAClFBsAQCnFBgBQSrEBAJRSbAAApRQbAEApxQYAUEqxAQCUUmwAAKUUGwBAKcUGAFBKsQEAlFJsAAClFBsAQCnFBgBQSrEBAJRSbAAApRQbAEApxQYAUEqxAQCUUmwAAKUUGwBAKcUGAFBKsQEAlFJsAAClFBsAQCnFBgBQSrEBAJRSbAAApRQbAEApxQYAUEqxAQCUUmwAAKUUGwBAKcUGAFBKsQEAlFJsAAClFBsAQCnFBgBQSrEBAJRSbAAApRQbAEApxQYAUEqxAQCUUmwAAKUUGwBAKcUGAFBKsQEAlFJsAAClFBsAQCnFBgBQSrEBAJRSbAAApRQbAEApxQYAUEqxAQCUUmwAAKUUGwBAKcUGAFBKsQEAlFJsAAClFBsAQCnFBgBQSrEBAJRSbAAApRQbAEApxQYAUEqxAQCUUmwAAKUUGwBAKcUGAFBKsQEAlFJsAAClFBsAQCnFBgBQSrEBAJRSbAAApRQbAEApxQYAUEqxAQCUUmwAAKUUGwBAKcUGAFBKsQEAlFJsAAClFBsAQCnFxga11t7VWvtca+16a+1vrfzdn2qtfaG1dqW19s9aa2/b0DThtm63hltrB621v99a+2JrbWqtfdfmZgm39xpr+E+01v5xa+3Z1trl1tqvtdbessGpnmuKjc36UkR8PCL+5qsHW2sPR8SvR8SHIuINEfG5iPi7a58d3Fm6hm/6FxHxzoh4aq0zgtfndmv4oYj4xYj4poh4W0S8GBG/vNaZ7ZC9TU/gIpum6dcjIlprfzwivuFVf/XnIuLz0zT92s2/fyIivtJa+0+mafrC2icKt3G7NTxN042I+Ks3/26xmdnBnb3GGv7NVx/XWvt0RPzz9c5ud/hmYzu9IyJ+++t/mKbp5Yj43ZvjAKzfn4yIz296EueVbza2070RcXll7GsRcd8G5gJwobXW/mhEPB4R37fpuZxXvtnYTi9FxP0rY/fHK/9mCMCatNa+JSJ+MyLePU3Tb216PueVYmM7fT4ivv3rf2it3RMR3xy+wgNYm5u/BfhPIuJj0zR9dtPzOc8UGxvUWttrrV2KiHlEzFtrl1prexHxGxHxba2177/5949HxO8Ih7JtXmMNR2vt8ObfRUQc3Py7trHJQuJ2a7i19lhE/NOI+PQ0TX9js7M8/9o0TZuew4V187dMPrwy/JFpmp5orf3piPh0vPIrV/8yIn5omqYvrneG8NrusIa/GK+s31d7u3XMNrndGo6IKSKeiIiXX/0X0zTdu5aJ7RjFBgBQyj+jAAClFBsAQCnFBgBQSrEBAJRaawfRJ+MD9WnUadmPtYGa6qSPi4jZ8trQcct2MHRcanAu1dryejc2zQ43MJOIx+ITa/81yrWsYS4Ma5jzbnQNb8dPMABgZyk2AIBSig0AoJRiAwAotfkt5keDmaPHpY89To5beeqjAczkXG15oxtb7K1u2ppry6P+Em0+NpdMNr+kS+w0208ee+d7PBoGnS2udGPL+d1Dj02lczv56YA1O0UIfxekn/XZ5/COujivNACwEYoNAKCUYgMAKKXYAABKrTcgOhLUvJ3hAGcWQkqukR13wmtmYdDR4Gebzjg0lDzX6SyDlIOv4XAY9IKHxmDtNvWe2+L39ZkH2hNbHQZdw5rY3lcfANgJig0AoJRiAwAopdgAAEqtNSA6SzpttrjajS1m9/QPPk1X0cxJwy+DgdYWi25sij4gmoWQNraN+0j31dGw7Zm/DgOBXuDOtjioeeYGP5vOOgx6Kmf4ywvD1rAmLtCqAwA2QbEBAJRSbAAApRQbAECptQZEzzyEc5rQ6OpxZxyQycKgo9doSZfOKZKAaPJcZ8tr/WGt71w33M1uIBA7W/Yh3+U8CflmRsJQg/MAOCsb2xJ+RwO8u/msAICtodgAAEopNgCAUooNAKDU+UndZVubp7VSFprsO5cuZ5cGrnnCsGnkQaIsvLlsB/1jk7HRLZCXs+SxaeD0hEGn5FzL2V0nO1fEzoahgC0w+ksEiVOFQbOfVxc85O6THgAopdgAAEopNgCAUooNAKDU5hMroyHM4XBNsn3wwGOHu8UNBlVHA52ZKXn+aaAz7SDab08/3M1zxBmHnPaOn+vGjvceOtNrAHcw+jm8C/NYx/O64GHQjG82AIBSig0AoJRiAwAopdgAAEqtN8WyLSGkxKm2XM+6amZh0NN0lUvu3f7xs93Y0f7DJz5fpk2LW/48m/ouqIv5fWPXTGRh0PnxC/019u4/8TWAO9iSz+GNzeM0P5vW8XNt9Rrb8nq9DudvxgDAuaLYAABKKTYAgFLrzWyct3+PG81YDOY4TiWZ73A+Y/B8mdUGY4tpPnb+U+yYu5jfO3YNgLMwnM/Y0G6u5zCjser8PwMAYKspNgCAUooNAKCUYgMAKLWdTb3OuEnKamOqiHxn1bM0yxp9zS4NPTbdgba1/rhp6o9LmpO1ZCfYaXbYje0ffbUbO9p/48rJxu5bm7Ln0F8zO99s2TcOW7axewecI6ufk2cdtjzrhlvnbTfXLWqk6ZsNAKCUYgMAKKXYAABKKTYAgFIb7yD65id++sSne+qJnxo6LgtNrl43O9ebP/IzJ77maBg0M7oD7dRnRm9zviSYmejCoKcwes3Mae5dtWy9Pv/+/74bu77/SPLofv0/+tGf7cae+vB7hx47Ig3bZjsSc2FkgfG7rv9+N7aY3dONXT/o1/UsOd8jH/v5obk8/74fuOXPD37yb/cH/aVv6YaeetNfGDr/LnTePJUtev7bMxMAYCcpNgCAUooNAKCUYgMAKLXxdmhPffh93dibP/LJoceeKlyaXBfuJA9v9mbLG8lo31X28od+rBubL692Y1kH1dVwaT63vsssF9ujH/3U0HHzP39fN/b8f/ydRpiSAAAXoklEQVRnurEHn/mtbuypD76rG5vavBtrkXS4XPGVh/prcv74ZgMAKKXYAABKKTYAgFKKDQCg1FoDolmg8+nH3zP02NEgadoJNLnuaAgVXm1v8WI39vDHf6Ebu5wE5N708U93Y1/54I92Y1P0Qbq95de6sWce//Fb/rx//Hx3zPH8gW6Mi+2F9/75bux4r18nhzee7MauHzzajT336Hd1Y9lav/Ke/6ofu+vW7qAv/8R3d8e0adGNcf74ZgMAKKXYAABKKTYAgFKKDQCgVJum9XUYfHJ6f3+x0S1wpzt3mos42+Bn2mV0i7bsvegei0+0dV9z8UQrf8NkIbmX7/7WOz7u8MaXurEHPvl3urEsRM1mbGINX77xw90azgKimTYddWNT8t+sb/k/+i3mL3/P/9A/tu3f8udLyVb3Ef0tevHe73iNWbJOo2vYT04AoJRiAwAopdgAAEopNgCAUuvdYj4JV55mm/jMaAfR1e24WxKUHe1QysWWdQE9vN6HNa/e9c3d2LIddGPZunvp8W/rxla3mM/k285zkWVh0Luu/bvkyD73d/XwG7uxg6PL3diXv/cvd2P3XP233di1w8duPf+lt3fHTG29P6ao4ZsNAKCUYgMAKKXYAABKKTYAgFJbmbx5+vG/0o1lYbh02/nRwOlK6GiKvkPp04+/Z+xcXGiL2d3d2H0/+w+7sasf6kNzLfrts7Pt6SNZn6uy900WfJ7W3rOSrZJ0Y86CmfPFS93Y/vGz3diDn/zVbuyFn/xvkmu8rRtbrnQQfeRjfefRjKD++eObDQCglGIDACil2AAASik2AIBSa91iPtue++jdf7w7bv/nP7eW+bxaFjadTTe6seXs0jqmw4BNbM/95PT+bg1n22LfOHhzN7ZMOiHuHz/fjS3m9w7NZb54eeVx93THzJbXurHjvQeHzk+9TazhZ45+pFvDb/zEL/bHPf7j3dj+UR8Q3Vu80I3Nlle6sZYEU+++9nu3Dvz1/687JiMguj1sMQ8AbAXFBgBQSrEBAJRSbAAApTbeQfQ0YdDR7eSHtL7uWm7+9rBlsq6K2Zbdq+HNiIj95dVu7PrBW7uxvcXXurG2POrGVgOhj3z057pjnvnQu7sxLrZsDb/43j/XjWVh0Huv/Ov+uOQzPOsg2qa+W+6zD3znLX9efvDPdMc8/PFf6MY4f3yzAQCUUmwAAKUUGwBAKcUGAFBq4wnIdJv4j3yyPy4Jg2bdEZ/68HuT8/3MHc83GizVuY5VWYAt6754tPdQN5Z2H91/pBu79/rvdmN3f+of3XrNLAza5v0YF9ps2XdGvnb4WDeWBURHA/33/8+/1o0tf+wd3djlh77nlj8//Nz/1R3z/Pt+cOiabDffbAAApRQbAEApxQYAUEqxAQCU2nhANAuDpsclAc7LH3xXf+D8oBtKQ6gn7TTKhbZ/3IfmXv6J/3rosdlW3A9+8m93Y9l6XQ2DRkR89ad++JY/Zx0asxD1cnbpNefJbmtT34320V/6a93Ylf/uO7ux4//xO7qxq4ff2I3d97P/sBv7ykN/thtbDUg/98B/2R1z17V/141di7d1Y2w332wAAKUUGwBAKcUGAFBKsQEAlGrTNK3tYk/GB7qLZVtnT7P9tcyH8+2x+ERb9zWfnH6yf8O0jeesOac2soaTz+H5cR9eXuzdv5b5XBjTsh9r5/+/90fX8Pl/pgDAVlNsAAClFBsAQCnFBgBQauPJNmHQQTsaLjp3hEHZQcNh0Om4Hztv74lNfZZe8M/ri/3sAYByig0AoJRiAwAopdgAAEqds2TPim0OTZ51kGpbntcFt3/jmW7s6OCRDcwkuvV/cHS5O+TGwaPrmg2vts2fTaeRfIadaRfos/7c3NXX4Rxy1wGAUooNAKCUYgMAKHW+Mxvb/G9v563Rze2s/pvnNt/zNTjrfMb+0Vf7a+y/cezBK6/Fjf03ncWUOAsX6H0ytfnggSP5iTO+bxfoddh2XgkAoJRiAwAopdgAAEopNgCAUjuSYryY2vJ6NzbNDs/4IsX16AVvupOFQQ9uPN2NDTXnyu7bpu7v6HUv+Ou/E87y9drl1z5b65kdvQe7+awAgK2h2AAASik2AIBSig0AoNTFDYhuojNmEhBq06I/LNsxMdkN8czDoJuwo2Go0xjeqXV1TWRdazd1f0evm4ZGz3jnz7Mk0HpyF+k+bfMa3pAL9OoDAJug2AAASik2AIBSig0AoNR2JlbOOlyzLaGu5JrTcJAuef7b8rxGnWa+6WNPN51S63htqgNnZ/w+nC2vdWPL2aUzvQZnaBdCjsOdbMeea1se9Q/NAv1n/d/xm/iFhpF5RAx/Dm/xTyYAYBcoNgCAUooNAKCUYgMAKLWdaZ+zDiENhGnS4E+bn+hca7OJuZwm+JgcN1+82I0t5ved/Brb4qznu4kw8Bm/D9Mw6HkLOZ9ibm15vR/c4qd65qH0kz52i8LWeRh0DYrfE6PB16zjtYAoALAVFBsAQCnFBgBQSrEBAJTazoDoOqx0jNtY8GcdzjJgdcYBuTQMepGMvjbbHJo8jV19XolpdrjpKZxe2n2zeA2fZo1san0l1x3vPloseb1G53Ga+V6cdzoAsBGKDQCglGIDACil2AAASl3cgOgJuyNmIcfh4NemuiVuSQjvVPcpztkW89tsF7YOZ3uct07GG7I1v4QweJ/2j77ajR3P7+0PHLzt2/3qAADnnmIDACil2AAASik2AIBSbZqmTc8BANhhvtkAAEopNgCAUooNAKCUYgMAKKXYAABKKTYAgFKKDQCglGIDACil2AAASik2AIBSig0AoJRiAwAopdgAAEopNgCAUooNAKCUYgMAKKXYAABKKTYAgFKKDQCglGIDACil2AAASik2AIBSig0AoJRiAwAopdgAAEopNgCAUooNAKCUYgMAKKXYAABKKTYAgFKKDQCglGIDACil2AAASik2AIBSig0AoJRiAwAopdgAAEopNgCAUooNAKCUYgMAKKXYAABKKTYAgFKKDQCglGIDACil2AAASik2AIBSig0AoJRiAwAopdgAAEopNgCAUooNAKCUYgMAKKXYAABKKTYAgFKKDQCglGIDACil2AAASik2AIBSig0AoJRiAwAopdgAAEopNgCAUooNAKCUYgMAKKXYAABKKTYAgFKKDQCglGIDACil2AAASik2AIBSig0AoJRiAwAopdgAAEopNgCAUooNAKCUYgMAKKXYAABKKTYAgFKKDQCglGIDACil2AAASik2AIBSig0AoJRiAwAopdgAAEopNgCAUooNAKCUYgMAKKXYAABKKTYAgFKKDQCglGIDACil2AAASik2AIBSig0AoJRiAwAopdgAAEopNgCAUooNAKCUYgMAKKXYAABKKTYAgFKKDQCglGIDACil2AAASik2AIBSig0AoJRiAwAopdgAAEopNgCAUooNAKCUYgMAKKXYAABKKTYAgFKKDQCglGIDACil2AAASik2AIBSig0AoJRiAwAopdgAAEopNgCAUooNAKCUYgMAKKXYAABKKTY2pLV22Fr7TGvt37fWXmyt/avW2ne/6u//VGvtC621K621f9Zae9sm5wurXmsNt9YOWmt/v7X2xdba1Fr7rg1PF1J3WMd/orX2j1trz7bWLrfWfq219pZNz/k8Umxszl5E/IeI+M6IeCAiPhgRf6+19k2ttYcj4tcj4kMR8YaI+FxE/N1NTRRu47Zr+Obf/4uIeGdEPLWJycGg11rHD0XEL0bEN0XE2yLixYj45U1M8rxr0zRteg7c1Fr7nYj4SES8MSJ+aJqm//zm+D0R8ZWI+I5pmr6wwSnCa/r6Gp6m6R+8auwPIuKd0zT93xubGLwO2Tq+Of6fRcQ/n6bpvs3M7PzyzcaWaK09GhF/OCI+HxHviIjf/vrfTdP0ckT87s1x2EoraxjOpTus4z95m3HuYG/TEyCitbYfEb8aEb8yTdMXWmv3RsTllcO+FhGqabbS6hre9HzgJF5rHbfW/mhEPB4R37eJuZ13vtnYsNbaLCI+GxE3IuJdN4dfioj7Vw69P17590LYKrdZw3CuvNY6bq19S0T8ZkS8e5qm39rA9M49xcYGtdZaRHwmIh6NiO+fpuno5l99PiK+/VXH3RMR3xy+vmPLvMYahnPjtdbxzd8E/CcR8bFpmj67oSmee4qNzfqFiPjWiPjeaZquvmr8NyLi21pr399auxSvfHX3O76eZgvdbg1//VcKL93840Fr7dLND3XYNuk6bq09FhH/NCI+PU3T39jU5HaB30bZkJvV8hcj4npEHL/qr35kmqZfba396Yj4dLzy61b/Ml757ZQvrnuecDsDa/iL8cr6fbW3W8dsk9daxxHxLRHxRES8/OrHTNN075qmtzMUGwBAKf+MAgCUUmwAAKUUGwBAKcUGAFBqrR1En4wPSKNyZh6LT6z91yitYc6SNcx5N7qGfbMBAJRSbAAApRQbAEApxQYAUEqxAQCUUmwAAKUUGwBAKcUGAFBKsQEAlFJsAAClFBsAQCnFBgBQSrEBAJRSbAAApRQbAEApxQYAUEqxAQCUUmwAAKUUGwBAKcUGAFBKsQEAlFJsAAClFBsAQCnFBgBQSrEBAJRSbAAApRQbAEApxQYAUEqxAQCUUmwAAKUUGwBAKcUGAFBKsQEAlFJsAAClFBsAQCnFBgBQSrEBAJRSbAAApRQbAEApxQYAUEqxAQCUUmwAAKUUGwBAKcUGAFBKsQEAlFJsAAClFBsAQCnFBgBQam+tV5uOu6E3f+Rnhh761BM/1Y3tH321G1vODvqxdqmfSpvf8uc2HXXHPPrRTw3NY2dMy5M9rl2cmvXNT/x0N/bUh9/XH5jck+yxz3zo3d3YcnbX4GxWXq+WvJ2T91x6HL3s/bADa322vNaNLWf9Z+SobF2fpdH313k0X7yYjL3UjR3PH7jlz5eu/0F3zLXDx7qxKXmvT7PDbuwwOd+N/Uf7uS37uY1WEbvxigEAW0uxAQCUUmwAAKUUGwBAqbUmxUbDoF/9qR/uxtryejc2JSGhxfy+/oRJ0OvNH/nk0Fx21mj4bfW4kWNuZwdCXU8//p6h47Lw8qjRtbk6lywM5r8nWLVsfYg+C40+8tGf68ZGA/LDQfqVAPNeEo7chc+N25ktb3RjU/L6rN6DFov+XFN/rnuu/Otu7MW7/0g3dv3wG/pLJj9zF7O7+7kN2t1XEQDYCooNAKCUYgMAKKXYAABKtWma1naxxRNtfRe7g6c+/N5b/pyFV3e5c11b9h1TV7uqRvTBseV8MCB0mtDo4GMfa/9TG7vI2Xlyen+3htvUh7WmNji1JNT5lqf/127sy4/8QDe2GiTN1msWNt2ZLrjb0uHzFF1aH4tPbMUaPpX0Paxz7Yi7r/7bbuzq4Td2Yy1ufcn2j57tjrl+8OZuLAuNHl5/shs72n9j/9jslzKiX66PHPzS0BrejZ+cAMDWUmwAAKUUGwBAKcUGAFBqrYmdyx/6sW5sMbunPzAJHGWBxkc/+rND1826Pq4GQlcDoxERs+XVbmw5T+Z7Dk2z/aHjhgKhpwnqJY9NA5eD86121p1nn378r3RjT73pvx267mrQM+sCudM2EgbN1vrYx2j2GbaJ/9xrUz+P1QBiRMQyeV5ZaHA5u2vswgOfE9l29c88/uPJNS+NXXPLXbn09rEDV36R42jvge6Qg6PL3dgi+fy+sf+m5Lh7u7H95VfG5jbINxsAQCnFBgBQSrEBAJRSbAAApdYaEF22PtQzGrjLApxZx8QsXDgSJJ0lHe+Ws8Ohue2Ms+zIOHquZGza4i6tlz/4rm4sC8hl3ViztZ4dt7d4oRvL1vpqILQl21Vzxk6xNrPXehMe/einurHRrrLLWf/8hz/Ds2sMdAvelTBoJttifr680o2trp3FrA90jv68SrenT8LlR/sP93NbvDR0jcz2fqoDADtBsQEAlFJsAAClFBsAQKm1BkSzgFQW/My2e8/qooOjp7uxo703DF1jNUiahaayzqPTjmyTnAWCsu2Dp3Zr6Cjdmj7r7jm6xXy2FfVoDbz2zbkj3vTxTw8dl625rFto1s1xkQROsxDeVz74o7f8+Xh+39DcqDf8PtmAZz707n4wfb/2Y/ln88mtruuvfPAvdse0bKvznQnv951bs/f/avjz8MYz/eOSbqHZue698oVu7DjrSHqj//n6wr1/rBsb5ZsNAKCUYgMAKKXYAABKKTYAgFJrTjuOBY6ybomZN/z0L3djWQgvM9LN7zSd9rZdtn30yFbZ6X0bDHmeKug1GjgtNro20y3h08f2KdfsPZGt673j524dmPf3fHS+59JJO96OPu4Ux21LGDTzyMd+vhs7zTbuo51Bs+3jVx0nW51nXaF3xV3Xf78by37JYbHyszP75Yir8367+v3j57uxa4ePJcc9141lYdCs42kMNsb1zQYAUEqxAQCUUmwAAKUUGwBAqTZNfQezKl9avnfoYiNbwkdEPPOhv9wPJgGuRz76c0PnG7ErAdGtloTL9o++0o09cvBLa+8h+qXlT3RruCVdAIcDuMlzzbqKpkHalWDuLAnqLttBMg//jbEJWVfRt85+Zu1rePFEG/ocHv2sGwl+vp7zdbIA+o50cs6e26UbX+7Gjua3dvjMOllnW8cvZvd0Y9k28Vn30YOjy0Pne/PeXxtawz51AIBSig0AoJRiAwAopdgAAEqtNWUzGvzMuh7e/9L/04098rG/euK5rHZkzOa2090XR60GmE4TzDpFF9Cjg0dOft0zlHdQHexwOPj8p9Z3n8zChatrNl+v2TX9N8YmbEtX0eEuuIPBz6z76DTYVnJ1XWf3KO0yvSNB/SzUvZj1Yc3VDp839h/uz7W42o0tW98FNusWmgVEZ0nH59GushmfOgBAKcUGAFBKsQEAlFJsAACltrINW7al8N2f+kdDj33u/T/Ujd3Yf2M3NhJWTbcJ35FgUt65sr/vq4GtLKg4HnzLtuzeyiX4Gvr6vE39tsuPfvRT3VgWzEu7hSYB0TTAvLoWs06LsCrrIJt8HowH5JPt5JNQ59OPv6e/bNYZd3Ueu/KZm0nu+/H8/m7saO+hW/68t3ihPyYLjS770OiN/Td1Y9ln//WDt3Rj8+R8o3yzAQCUUmwAAKUUGwBAKcUGAFBqrVvMPzn9ZH+xcxcQZFs8Fp9Y+/bcT07vT9awmp2T2cgajg+s70OfnTe6hn1KAgClFBsAQCnFBgBQSrEBAJRabzpTGHR7ZNudCzre2WnukXsOXFA+6QCAUooNAKCUYgMAKKXYAABKnevE5mxxpRtbzu/ewEzOIcHE9Su+5215vRsb2cIbdpZQ9tZw1wGAUooNAKCUYgMAKHWuMxtnms+YjvsxTciodobrbmr7p5wMbFZbHnVj0+wU61o+Y2t4JQCAUooNAKCUYgMAKKXYAABKSUB+nTAom5Ctu5M2IjqPYTjB7K0wW17rxpazS2ufx6nCoNvugq/1c/jpBACcJ4oNAKCUYgMAKKXYAABKXZx0yqrVEN46wnUXPCDEoPMY9Dwp638rbCIMeuFc8LV+gT7VAIBNUGwAAKUUGwBAKcUGAFBqOxMrJ+2g+HpsIoR3moDQ4D058y2aNxGkBTjP1vEz7Jy52M8eACin2AAASik2AIBSig0AoNR2BkTPOkizibDOWV9zHWHQweuyAQJn22v0tUmPO/vpbK1d+Bwe5b3ZcUcAgFKKDQCglGIDACil2AAASm1nQHTUaPhnI91C6685HAY9y5DUac41HSePPd9LcG0ueuBsmwOyo/PYlvluykhodhc6RZPySgAApRQbAEApxQYAUEqxAQCUOt/pPOGf9TtVF9STL7f58Qv94PlevZuzzWHL29n2+XEyXtdzJetaPfqVhVcaACil2AAASik2AIBSig0AoFSbpmnTcwAAdphvNgCAUooNAKCUYgMAKKXYAABKKTYAgFKKDQCglGIDACil2AAASik2AIBSig0AoJRiAwAopdgAAEopNgCAUooNAKCUYgMAKKXYAABKKTYAgFKKDQCglGIDACil2AAASik2AIBSig0AoJRiAwAo9f8DN9n/EqyXWdwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 648x1080 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x1080 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if len(log_df) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "while epoch < config.epochs:\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = get_feed_dict(batch)\n",
    "\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, ppls_batch, global_step_log = \\\n",
    "        sess.run([opt, loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, topic_ppls, tf.train.get_global_step()], feed_dict = feed_dict)\n",
    "\n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if global_step_log%config.log_period==0:\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev = get_loss(sess, dev_batches)\n",
    "            \n",
    "#             if loss_dev < loss_min:\n",
    "#                 loss_min = loss_dev\n",
    "#                 saver.save(sess, config.modelpath, global_step=global_step_log)\n",
    "\n",
    "            clear_output()\n",
    "    \n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_reg_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_reg_dev],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "            \n",
    "            # visualize topic\n",
    "            print_topic_sample()\n",
    "\n",
    "            time_start = time.time()\n",
    "            \n",
    "    epoch += 1\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "    \n",
    "display(log_df)\n",
    "print_topic_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_topic_embeddings = tf.concat([tree_states_topic_embeddings[topic_idx] for topic_idx in topic_idxs], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "debug_value([states_topic_embeddings[:, :6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_embeddings = tf.concat([tree_topic_embeddings[topic_idx] for topic_idx in topic_idxs], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_value([topic_embeddings[:, :6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_topics_bow, = debug_value([topic_bow], return_value=True)\n",
    "np.max(_topics_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_topics_bow, = debug_value([topic_bow], return_value=True)\n",
    "\n",
    "plt.figure(figsize=(12, 20))\n",
    "    \n",
    "_topic_bow = _topics_bow[0]\n",
    "plt.subplot(5,3,2)\n",
    "plt.ylim([0, np.max(_topics_bow)])\n",
    "plt.bar(bow_idxs, _topic_bow)\n",
    "\n",
    "for i in range(1, len(topic_idxs)):\n",
    "    _topic_bow = _topics_bow[i]\n",
    "    plt.subplot(5,3,i+3)\n",
    "    plt.ylim([0, np.max(_topics_bow)])\n",
    "#     plt.axis('off')\n",
    "    plt.bar(bow_idxs, _topic_bow)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.37399197 0.10826537 0.0361492  0.09407969 0.07117239 0.04244342\n",
      " 0.05811434 0.02009552 0.01129199 0.01132447 0.09071141 0.0239397\n",
      " 0.05842046]\n"
     ]
    }
   ],
   "source": [
    "_prob_topics = []\n",
    "for ct, batch in dev_batches:\n",
    "    feed_dict = get_feed_dict(batch)\n",
    "    _prob_topic, = sess.run([prob_topic], feed_dict = feed_dict)\n",
    "    _prob_topics.append(_prob_topic)\n",
    "    \n",
    "_prob_topics = np.concatenate(_prob_topics, 0)\n",
    "_prob_topic_mean = np.mean(_prob_topics, 0)\n",
    "\n",
    "print(_prob_topic_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip_by_value:0 : [[9.9999982e-01 6.2611893e-02 7.1781740e-02 2.8388936e-02 1.6210236e-02\n",
      "  6.6913649e-02 7.6597407e-02 1.8865137e-01 1.1582102e-01]\n",
      " [6.2611893e-02 9.9999952e-01 5.0422205e-03 8.3748370e-02 5.4274704e-02\n",
      "  1.1030901e-03 4.8344899e-03 1.1982031e-03 1.2873390e-03]\n",
      " [7.1781740e-02 5.0422205e-03 9.9999976e-01 1.0342912e-02 3.1487900e-03\n",
      "  8.6840772e-04 9.8963618e-01 1.2799094e-03 2.7837367e-03]\n",
      " [2.8388936e-02 8.3748370e-02 1.0342912e-02 1.0000000e+00 6.8251409e-02\n",
      "  4.2217717e-02 1.3381815e-02 6.3530982e-02 2.2616977e-02]\n",
      " [1.6210236e-02 5.4274704e-02 3.1487900e-03 6.8251409e-02 9.9999958e-01\n",
      "  1.5286453e-02 4.1216360e-03 1.7095231e-02 2.4907567e-02]\n",
      " [6.6913649e-02 1.1030901e-03 8.6840772e-04 4.2217717e-02 1.5286453e-02\n",
      "  9.9999988e-01 6.3493215e-03 2.8145978e-02 1.5217496e-02]\n",
      " [7.6597407e-02 4.8344899e-03 9.8963618e-01 1.3381815e-02 4.1216360e-03\n",
      "  6.3493215e-03 9.9999988e-01 1.2755987e-02 6.7557693e-03]\n",
      " [1.8865137e-01 1.1982031e-03 1.2799094e-03 6.3530982e-02 1.7095231e-02\n",
      "  2.8145978e-02 1.2755987e-02 9.9999994e-01 4.1217245e-02]\n",
      " [1.1582102e-01 1.2873390e-03 2.7837367e-03 2.2616977e-02 2.4907567e-02\n",
      "  1.5217496e-02 6.7557693e-03 4.1217245e-02 9.9999988e-01]]\n"
     ]
    }
   ],
   "source": [
    "debug_value([topic_dots])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topic_losses_reg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-cb624c895c36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdebug_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic_losses_reg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'topic_losses_reg' is not defined"
     ]
    }
   ],
   "source": [
    "debug_value([topic_losses_reg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_mask_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_topic_bow, = debug_value([topic_bow], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bow_idxs, _topic_bow[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bow_idxs, _topic_bow[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bow_idxs, _topic_bow[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bow_idxs, _topic_bow[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(_topic_bow, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bow_idxs, _topic_bow[-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bow_idxs, _topic_bow[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bow_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_value([prob_topic[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_value([tf.exp(-tf.divide(topic_losses_recon, n_bow))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_shape([bow, hidden_bow, latents_bow, prob_topic, bow_embeddings, topic_embeddings, topic_bow, prob_bow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_shape([topic_losses_recon, topic_loss_recon, n_bow, ppls, topic_embeddings_norm, tf.expand_dims(topic_angles_mean, -1), topic_angles_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_value([tf.reduce_sum(tf.square(topic_embeddings_norm), 1)], return_value=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_value([tf.reduce_sum(prob_topic, -1), tf.reduce_sum(topic_bow, -1), tf.reduce_sum(tf.exp(prob_bow), 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_bow = tf.exp(0.5 * logvars_bow)\n",
    "dist_bow = tfd.Normal(means_bow, sigma_bow)\n",
    "dist_std = tfd.Normal(0., 1.)\n",
    "topic_loss_kl_tmp = tf.reduce_mean(tf.reduce_sum(tfd.kl_divergence(dist_bow, dist_std), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_value([topic_loss_recon, topic_loss_kl, topic_loss_kl_tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_logvars, _means, _kl_losses, _latents, _output_logits = sess.run([logvars, means, kl_losses, latents, output_logits], feed_dict=feed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_logvars.shape, _means.shape, _kl_losses.shape, _latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_output_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_output_logits, _dec_target_idxs_do, _dec_mask_tokens_do, _recon_loss, _kl_losses, _ = sess.run([output_logits, dec_target_idxs_do, dec_mask_tokens_do, recon_loss, kl_losses, opt], feed_dict=feed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_max(output_logits, 2).eval(session=sess, feed_dict=feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_output_logits.shape, _dec_target_idxs_do.shape, _dec_mask_tokens_do.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_logits = np.exp(_output_logits) / np.sum(np.exp(_output_logits), 2)[:, :, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_idxs = _dec_target_idxs_do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_losses = np.array([[-np.log(_logits[i, j, _idxs[i, j]]) for j in range(_idxs.shape[1])] for i in range(_idxs.shape[0])]) * _dec_mask_tokens_do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(_losses)/np.sum(_dec_mask_tokens_do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_recon_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_kl_losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
