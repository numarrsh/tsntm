{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from data_structure import get_batches, get_test_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "from topic_model import TopicModel\n",
    "\n",
    "from topic_beam_search_decoder import BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '4', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/bags/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/tglm_vae', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'bags', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 50, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 500, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.1, 'lr')\n",
    "flags.DEFINE_float('reg', 1., 'regularization term')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_bool('warmup', True, 'flg of warming up')\n",
    "flags.DEFINE_integer('epochs_cycle', 5, 'number of epochs within a cycle')\n",
    "flags.DEFINE_float('r_cycle', 0.5, 'proportion used to increase beta within a cycle')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 30, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_bow', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_hidden_topic', 512, 'dim_hidden_topic')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "flags.DEFINE_bool('bidirectional', True, 'flg of bidirectional encoding')\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_test_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('PAD_IDX', word_to_idx[PAD], 'PAD_IDX')\n",
    "flags.DEFINE_integer('UNK_IDX', word_to_idx[UNK], 'UNK_IDX')\n",
    "flags.DEFINE_integer('BOS_IDX', word_to_idx[BOS], 'BOS_IDX')\n",
    "flags.DEFINE_integer('EOS_IDX', word_to_idx[EOS], 'EOS_IDX')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "maximum_iterations = max([max([instance.max_sent_l for instance in batch]) for ct, batch in dev_batches])\n",
    "flags.DEFINE_integer('maximum_iterations', maximum_iterations, 'maximum_iterations')\n",
    "\n",
    "flags.DEFINE_integer('cycle_steps', len(train_batches)*config.epochs_cycle, 'number of steps for each cycle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build language model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['bow'] = tf.placeholder(tf.float32, [None, config.dim_bow], name='bow')\n",
    "t_variables['input_token_idxs'] = tf.placeholder(tf.int32, [None, None], name='input_token_idxs')\n",
    "t_variables['dec_input_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_input_idxs')\n",
    "t_variables['dec_target_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_target_idxs')\n",
    "t_variables['batch_l'] = tf.placeholder(tf.int32, name='batch_l')\n",
    "t_variables['doc_l'] = tf.placeholder(tf.int32, [None], name='doc_l')\n",
    "t_variables['sent_l'] = tf.placeholder(tf.int32, [None], name='sent_l')\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train', assertion=False):\n",
    "    def token_dropout(sent_idxs):\n",
    "        sent_idxs_dropout = np.asarray(sent_idxs)\n",
    "        sent_idxs_dropout[np.random.rand(len(sent_idxs)) > config.word_keep_prob] = config.UNK_IDX\n",
    "        return list(sent_idxs_dropout)\n",
    "\n",
    "    bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "    \n",
    "    doc_l = np.array([len(instance.token_idxs) for instance in batch])\n",
    "    \n",
    "    feed_input_token_idxs_list = [sent_idxs for instance in batch for sent_idxs in instance.token_idxs]\n",
    "    feed_dec_input_idxs_list = [[config.BOS_IDX] + token_dropout(sent_idxs) for sent_idxs in feed_input_token_idxs_list]\n",
    "    feed_dec_target_idxs_list = [sent_idxs + [config.EOS_IDX]  for sent_idxs in feed_input_token_idxs_list]\n",
    "        \n",
    "    sent_l = np.array([len(sent_idxs) for sent_idxs in feed_input_token_idxs_list], np.int32)\n",
    "    batch_l = len(sent_l)\n",
    "    \n",
    "    feed_input_token_idxs = pad_sequences(feed_input_token_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_input_idxs = pad_sequences(feed_dec_input_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_target_idxs = pad_sequences(feed_dec_target_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    \n",
    "    if assertion:\n",
    "        index = 0\n",
    "        for instance in batch:\n",
    "            for line_idxs in instance.token_idxs:\n",
    "                assert feed_input_token_idxs_list[index] == line_idxs\n",
    "                index += 1\n",
    "        assert feed_input_token_idxs.shape[1] == np.max(sent_l)\n",
    "        assert feed_dec_input_idxs.shape[1] == np.max(sent_l) + 1\n",
    "        assert feed_dec_target_idxs.shape[1] == np.max(sent_l) + 1\n",
    "    \n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "\n",
    "    feed_dict = {\n",
    "                t_variables['bow']: bow, \n",
    "                t_variables['batch_l']: batch_l, t_variables['doc_l']: doc_l, t_variables['sent_l']: sent_l, \n",
    "                t_variables['input_token_idxs']: feed_input_token_idxs, t_variables['dec_input_idxs']: feed_dec_input_idxs, t_variables['dec_target_idxs']: feed_dec_target_idxs, \n",
    "                t_variables['keep_prob']: keep_prob\n",
    "    }\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     10,
     24,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables):\n",
    "    sample_batch = dev_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "def check_shape(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()\n",
    "    \n",
    "def check_value(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()    \n",
    "    \n",
    "# sent_loss_kl_categ_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, tf_log(prob_topic_infer/prob_topic_sents)), 1))\n",
    "# debug_value([sent_loss_kl_categ, sent_loss_kl_categ_tmp])\n",
    "# sent_loss_kl_gauss_tmp = 0.5 * tf.reduce_sum(tf.exp(logvars_topic_infer-logvars_topic) + tf.square(means_topic - means_topic_infer) / tf.exp(logvars_topic) - 1 + (logvars_topic - logvars_topic_infer), -1)\n",
    "# sent_loss_kl_gmm_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss_tmp), -1))\n",
    "# debug_value([sent_loss_kl_gmm_tmp, sent_loss_kl_gmm])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encode bow\n",
    "with tf.variable_scope('topic/enc', reuse=False):\n",
    "    hidden_bow_ = tf.layers.Dense(units=config.dim_hidden_bow, activation=tf.nn.relu, name='hidden_bow')(t_variables['bow'])\n",
    "    hidden_bow = tf.layers.Dropout(t_variables['keep_prob'])(hidden_bow_)\n",
    "    means_bow = tf.layers.Dense(units=config.dim_latent_bow, name='mean_bow')(hidden_bow)\n",
    "    logvars_bow = tf.layers.Dense(units=config.dim_latent_bow, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_bow)\n",
    "    latents_bow = sample_latents(means_bow, logvars_bow) # sample latent vectors\n",
    "\n",
    "    prob_topic = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic')(latents_bow) # inference of topic probabilities\n",
    "\n",
    "# decode bow\n",
    "with tf.variable_scope('shared', reuse=False):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "bow_embeddings = tf.nn.embedding_lookup(embeddings, bow_idxs) # embeddings of each bow features\n",
    "\n",
    "with tf.variable_scope('topic/dec', reuse=False):\n",
    "    topic_embeddings = tf.get_variable('topic_emb', [config.n_topic, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of topics\n",
    "\n",
    "    topic_bow = tf.nn.softmax(tf.matmul(topic_embeddings, bow_embeddings, transpose_b=True), 1) # bow vectors for each topic\n",
    "    logits_bow = tf_log(tf.matmul(prob_topic, topic_bow)) # predicted bow distribution\n",
    "\n",
    "    # prior of each gaussian distribution (computed for each topic)\n",
    "    hidden_topic = tf.layers.Dense(units=config.dim_hidden_topic, activation=tf.nn.relu, name='hidden_topic')(topic_bow)\n",
    "    means_topic = tf.layers.Dense(units=config.dim_latent, name='mean_topic')(hidden_topic)\n",
    "    logvars_topic = tf.layers.Dense(units=config.dim_latent, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_topic)\n",
    "    sigma_topic = tf.exp(0.5 * logvars_topic)\n",
    "    gauss_topic = tfd.Normal(loc=means_topic, scale=sigma_topic)    \n",
    "    \n",
    "# define losses\n",
    "topic_losses_recon = -tf.reduce_sum(tf.multiply(t_variables['bow'], logits_bow), 1)\n",
    "topic_loss_recon = tf.reduce_mean(topic_losses_recon) # negative log likelihood of each words\n",
    "\n",
    "topic_loss_kl = compute_kl_loss(means_bow, logvars_bow) # KL divergence b/w latent dist & gaussian std\n",
    "\n",
    "topic_bow_norm = topic_bow / tf.norm(topic_bow, axis=1, keepdims=True)\n",
    "topic_dots = tf.clip_by_value(tf.matmul(topic_bow_norm, tf.transpose(topic_bow_norm)), -1., 1.)\n",
    "topic_loss_reg = tf.reduce_mean(tf.square(topic_dots - tf.eye(config.n_topic)))\n",
    "# topic_angles = tf.acos(topic_dots)\n",
    "# topic_angles_mean = tf.reduce_mean(topic_angles)\n",
    "# topic_angles_vars = tf.reduce_mean(tf.square(topic_angles - topic_angles_mean))\n",
    "# topic_loss_reg = tf.exp(topic_angles_vars - topic_angles_mean)\n",
    "\n",
    "# monitor\n",
    "n_bow = tf.reduce_sum(t_variables['bow'], 1)\n",
    "topic_ppls = tf.divide(topic_losses_recon, tf.maximum(1e-5, n_bow))\n",
    "topics_freq_bow_indices = tf.nn.top_k(topic_bow, 10, name='topic_freq_bow').indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "input_token_idxs = t_variables['input_token_idxs']\n",
    "batch_l = t_variables['batch_l']\n",
    "sent_l = t_variables['sent_l']\n",
    "max_sent_l = tf.reduce_max(sent_l)\n",
    "\n",
    "with tf.variable_scope('sent/enc', reuse=False):\n",
    "    # get word embedding\n",
    "    enc_input = tf.nn.embedding_lookup(embeddings, input_token_idxs)\n",
    "\n",
    "    # get sentence embedding\n",
    "    _, enc_state = dynamic_bi_rnn(enc_input, sent_l, config.dim_hidden, t_variables['keep_prob'])\n",
    "\n",
    "    # TODO House Holder flow\n",
    "    hidden_topic_infer =  tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='hidden_topic_infer')(enc_state)\n",
    "    prob_topic_infer = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic_infer')(hidden_topic_infer)\n",
    "\n",
    "    w_mean_topic_infer = tf.get_variable('mean_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32)\n",
    "    b_mean_topic_infer = tf.get_variable('mean_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32)\n",
    "    means_topic_infer = tf.tensordot(enc_state, w_mean_topic_infer, axes=[[1], [1]]) + b_mean_topic_infer\n",
    "    \n",
    "    w_logvar_topic_infer = tf.get_variable('logvar_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    b_logvar_topic_infer = tf.get_variable('logvar_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    logvars_topic_infer = tf.tensordot(enc_state, w_logvar_topic_infer, axes=[[1], [1]]) + b_logvar_topic_infer\n",
    "    sigma_topic_infer = tf.exp(0.5 * logvars_topic_infer)\n",
    "    gauss_topic_infer = tfd.Normal(loc=means_topic_infer, scale=sigma_topic_infer)\n",
    "    \n",
    "    # latent vectors from each gaussian dist.\n",
    "    latents_topic_infer = sample_latents(means_topic_infer, logvars_topic_infer) \n",
    "    # latent vector from gaussian mixture\n",
    "    latents_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), latents_topic_infer, transpose_a=True)\n",
    "    \n",
    "    # for beam search\n",
    "    means_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), means_topic_infer, transpose_a=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for decoding\n",
    "dec_sent_l = tf.add(sent_l, 1)\n",
    "dec_input_idxs = t_variables['dec_input_idxs']\n",
    "dec_input = tf.nn.embedding_lookup(embeddings, dec_input_idxs)\n",
    "\n",
    "dec_latents_input = tf.tile(latents_input, [1, tf.shape(dec_input)[1], 1])\n",
    "dec_concat_input = tf.concat([dec_input, dec_latents_input], -1)\n",
    "\n",
    "# decode for training\n",
    "with tf.variable_scope('sent/dec/rnn', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32, reuse=False):\n",
    "    dec_cell = tf.contrib.rnn.GRUCell(config.dim_hidden)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob = t_variables['keep_prob'])\n",
    "\n",
    "    dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(latents_input, 1))\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_concat_input, sequence_length=dec_sent_l)\n",
    "\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=dec_cell,\n",
    "        helper=helper,\n",
    "        initial_state=dec_initial_state)\n",
    "\n",
    "    dec_outputs, _, output_sent_l = tf.contrib.seq2seq.dynamic_decode(train_decoder)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(config.n_vocab, use_bias=False, name='out')\n",
    "    output_logits = output_layer(dec_outputs.rnn_output)\n",
    "    \n",
    "    output_token_idxs = tf.argmax(output_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tokens = tf.fill([batch_l], config.BOS_IDX)\n",
    "end_token = config.EOS_IDX\n",
    "\n",
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    infer_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(means_input, 1))\n",
    "    beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(infer_dec_initial_state, multiplier=config.beam_width)\n",
    "    beam_latents_input = tf.contrib.seq2seq.tile_batch(tf.squeeze(means_input, 1), multiplier=config.beam_width) # added\n",
    "    \n",
    "    beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=beam_latents_input)\n",
    "\n",
    "    beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    beam_output_token_idxs = beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    inter_means_input = tf.placeholder(tf.float32, [None, config.dim_latent])\n",
    "    \n",
    "    inter_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(inter_means_input)\n",
    "    inter_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(inter_dec_initial_state, multiplier=config.beam_width)\n",
    "    inter_beam_latents_input = tf.contrib.seq2seq.tile_batch(inter_means_input, multiplier=config.beam_width) # added\n",
    "    \n",
    "    inter_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=inter_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=inter_beam_latents_input)\n",
    "\n",
    "    inter_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inter_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    inter_beam_output_token_idxs = inter_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    topic_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic)\n",
    "    topic_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(topic_dec_initial_state, multiplier=config.beam_width)\n",
    "    topic_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic, multiplier=config.beam_width) # added\n",
    "    \n",
    "    topic_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=topic_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=topic_beam_latents_input)\n",
    "\n",
    "    topic_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        topic_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    topic_beam_output_token_idxs = topic_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    means_topic_summary = tf.reduce_mean(means_topic_infer, 0)\n",
    "    \n",
    "    summary_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic_summary)\n",
    "    summary_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(summary_dec_initial_state, multiplier=config.beam_width)\n",
    "    summary_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic_summary, multiplier=config.beam_width) # added\n",
    "    \n",
    "    summary_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=summary_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width,\n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=summary_beam_latents_input)\n",
    "\n",
    "    summary_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        summary_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    summary_beam_output_token_idxs = summary_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language modeling cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target and mask\n",
    "dec_target_idxs = t_variables['dec_target_idxs']\n",
    "dec_mask_tokens = tf.sequence_mask(dec_sent_l, maxlen=max_sent_l+1, dtype=tf.float32)\n",
    "\n",
    "# nll for each token (averaged over batch & sentence)\n",
    "sent_loss_recon = tf.contrib.seq2seq.sequence_loss(output_logits, dec_target_idxs, dec_mask_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_l = t_variables['doc_l']\n",
    "mask_sents = tf.sequence_mask(doc_l)\n",
    "mask_sents_flatten = tf.reshape(mask_sents, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1]])\n",
    "\n",
    "prob_topic_tiled = tf.tile(tf.expand_dims(prob_topic, 1), [1, tf.shape(mask_sents)[1], 1])\n",
    "prob_topic_flatten = tf.reshape(prob_topic_tiled, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1], config.n_topic])\n",
    "prob_topic_sents = tf.boolean_mask(prob_topic_flatten, mask_sents_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferred mixture probabilities (computed for each sentence)\n",
    "categ_topic_infer = tfd.Categorical(probs=prob_topic_infer)\n",
    "\n",
    "# prior of mixture probabilities (computed for each document, tiled for each sentence)\n",
    "categ_topic = tfd.Categorical(probs=prob_topic_sents)\n",
    "\n",
    "sent_loss_kl_categ = tf.reduce_mean(tfd.kl_divergence(categ_topic_infer, categ_topic))\n",
    "\n",
    "# inference of each gaussian gaussribution (computed for each sentence)\n",
    "\n",
    "sent_loss_kl_gauss = tf.reduce_sum(tfd.kl_divergence(gauss_topic_infer, gauss_topic), -1)\n",
    "sent_loss_kl_gmm = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss), -1))\n",
    "\n",
    "sent_loss_kl = sent_loss_kl_categ + sent_loss_kl_gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "tau = tf.cast(tf.divide(tf.mod(global_step, tf.constant(config.cycle_steps)), tf.constant(config.cycle_steps)), dtype=tf.float32)\n",
    "beta = tf.minimum(1., tau/config.r_cycle)\n",
    "\n",
    "sent_loss = sent_loss_recon + beta * sent_loss_kl\n",
    "\n",
    "topic_loss = topic_loss_recon + topic_loss_kl + config.reg * topic_loss_reg\n",
    "loss = topic_loss + sent_loss\n",
    "\n",
    "# define optimizer\n",
    "if config.opt == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif config.opt == 'Adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "    \n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_sents(token_idxs, config, idx_to_word):\n",
    "    sents = []\n",
    "    for sent_idxs in token_idxs:\n",
    "        tokens = []\n",
    "        for idx in sent_idxs:\n",
    "            if idx == config.EOS_IDX: break\n",
    "            tokens.append(idx_to_word[idx])\n",
    "        sent = ' '.join(tokens)\n",
    "        sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, ppls_batch \\\n",
    "            = sess.run([loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, topic_ppls], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "    loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, sent_loss_recon_mean, sent_loss_kl_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    return loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, sent_loss_recon_mean, sent_loss_kl_mean, ppl_mean\n",
    "\n",
    "def get_all_losses(sess, batches):\n",
    "    losses = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch = \\\n",
    "        sess.run([loss, topic_loss_recon, topic_loss_kl, sent_loss_recon, sent_loss_kl], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "    print('LOSS %.2f | TM NLL: %.2f, KL: %.4f | LM NLL: %.2f, KL: %.4f' %  np.mean(losses, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_batch):\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    pred_token_idxs = sess.run(output_token_idxs, feed_dict = feed_dict)\n",
    "    true_token_idxs = [sent_idxs for instance in sample_batch for sent_idxs in instance.token_idxs]\n",
    "    \n",
    "    assert len(pred_token_idxs) == len(true_token_idxs)\n",
    "    \n",
    "    pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "    true_sents = idxs_to_sents(true_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    for i, (true_sent, pred_sent) in enumerate(zip(true_sents, pred_sents)):        \n",
    "        print(i, 'TRUE: %s' % true_sent)\n",
    "        print(i, 'PRED: %s' % pred_sent)\n",
    "\n",
    "def print_topic_sample():\n",
    "    pred_topics_freq_bow_indices, pred_topic_token_idxs = sess.run([topics_freq_bow_indices, topic_beam_output_token_idxs], \n",
    "                                                                                                           feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "    pred_topic_sents = idxs_to_sents(pred_topic_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Topic Samples-----------')\n",
    "    for i, (topic_freq_bow_idxs, pred_topic_sent) in enumerate(zip(topics_freq_bow_idxs, pred_topic_sents)):\n",
    "        print(i, ' BOW:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' SENTENCE:', pred_topic_sent)\n",
    "        \n",
    "def print_summary(test_batch):\n",
    "    feed_dict = get_feed_dict(test_batch)\n",
    "    feed_dict[t_variables['batch_l']] = config.n_topic\n",
    "    feed_dict[t_variables['keep_prob']] = 1.\n",
    "    pred_topics_freq_bow_indices, pred_summary_token_idxs = sess.run([topics_freq_bow_indices, summary_beam_output_token_idxs], feed_dict=feed_dict)\n",
    "    pred_summary_sents = idxs_to_sents(pred_summary_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Output sentences for each topic-----------')\n",
    "    print('Item idx:', test_batch[0].item_idx)\n",
    "    for i, (topic_freq_bow_idxs, pred_summary_sent) in enumerate(zip(topics_freq_bow_idxs, pred_summary_sents)):\n",
    "        print(i, ' BOW:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' SENTENCE:', pred_summary_sent)\n",
    "        \n",
    "    print('-----------Summaries-----------')\n",
    "    for i, summary in enumerate(test_batch[0].summaries):\n",
    "        print('SUMMARY %i :'%i, '\\n', summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "saver = tf.train.Saver(max_to_keep=10)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','TM','','','','LM','','VALID:','TM','','','','LM','', ''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL','REG','NLL','KL','LOSS','PPL','NLL','KL','REG','NLL','KL', 'Beta']]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>LM</th>\n",
       "      <th></th>\n",
       "      <th>VALID:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>LM</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>133.50</td>\n",
       "      <td>1036</td>\n",
       "      <td>122.98</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.97</td>\n",
       "      <td>9.12</td>\n",
       "      <td>1.15</td>\n",
       "      <td>126.60</td>\n",
       "      <td>1034</td>\n",
       "      <td>116.19</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.97</td>\n",
       "      <td>9.12</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>121.47</td>\n",
       "      <td>598</td>\n",
       "      <td>114.37</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.66</td>\n",
       "      <td>6.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>111.65</td>\n",
       "      <td>533</td>\n",
       "      <td>105.29</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.45</td>\n",
       "      <td>5.74</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>120.07</td>\n",
       "      <td>576</td>\n",
       "      <td>113.30</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.52</td>\n",
       "      <td>6.05</td>\n",
       "      <td>0.82</td>\n",
       "      <td>111.37</td>\n",
       "      <td>524</td>\n",
       "      <td>105.00</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.32</td>\n",
       "      <td>5.75</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>1500</td>\n",
       "      <td>119.50</td>\n",
       "      <td>561</td>\n",
       "      <td>112.81</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.44</td>\n",
       "      <td>5.94</td>\n",
       "      <td>0.72</td>\n",
       "      <td>110.80</td>\n",
       "      <td>504</td>\n",
       "      <td>104.32</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.22</td>\n",
       "      <td>5.62</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>119.19</td>\n",
       "      <td>549</td>\n",
       "      <td>112.51</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.38</td>\n",
       "      <td>5.86</td>\n",
       "      <td>0.65</td>\n",
       "      <td>110.46</td>\n",
       "      <td>498</td>\n",
       "      <td>104.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.18</td>\n",
       "      <td>5.48</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119.09</td>\n",
       "      <td>545</td>\n",
       "      <td>112.41</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.35</td>\n",
       "      <td>5.82</td>\n",
       "      <td>0.62</td>\n",
       "      <td>110.30</td>\n",
       "      <td>496</td>\n",
       "      <td>103.96</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.16</td>\n",
       "      <td>5.36</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2776</th>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>118.79</td>\n",
       "      <td>537</td>\n",
       "      <td>112.12</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.31</td>\n",
       "      <td>5.74</td>\n",
       "      <td>0.57</td>\n",
       "      <td>109.84</td>\n",
       "      <td>481</td>\n",
       "      <td>103.49</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.13</td>\n",
       "      <td>5.23</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3276</th>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>118.44</td>\n",
       "      <td>529</td>\n",
       "      <td>111.78</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.29</td>\n",
       "      <td>5.67</td>\n",
       "      <td>0.53</td>\n",
       "      <td>109.59</td>\n",
       "      <td>475</td>\n",
       "      <td>103.33</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.11</td>\n",
       "      <td>5.13</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3776</th>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>118.35</td>\n",
       "      <td>523</td>\n",
       "      <td>111.70</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.26</td>\n",
       "      <td>5.61</td>\n",
       "      <td>0.50</td>\n",
       "      <td>109.43</td>\n",
       "      <td>472</td>\n",
       "      <td>103.13</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.06</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4276</th>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>118.13</td>\n",
       "      <td>517</td>\n",
       "      <td>111.50</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.24</td>\n",
       "      <td>5.55</td>\n",
       "      <td>0.48</td>\n",
       "      <td>109.37</td>\n",
       "      <td>467</td>\n",
       "      <td>103.02</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4551</th>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>117.99</td>\n",
       "      <td>514</td>\n",
       "      <td>111.36</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.23</td>\n",
       "      <td>5.52</td>\n",
       "      <td>0.47</td>\n",
       "      <td>109.23</td>\n",
       "      <td>467</td>\n",
       "      <td>102.98</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4.95</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5051</th>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>117.82</td>\n",
       "      <td>510</td>\n",
       "      <td>111.19</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.21</td>\n",
       "      <td>5.47</td>\n",
       "      <td>0.45</td>\n",
       "      <td>109.14</td>\n",
       "      <td>464</td>\n",
       "      <td>102.84</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.90</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5551</th>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>117.67</td>\n",
       "      <td>505</td>\n",
       "      <td>111.05</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.20</td>\n",
       "      <td>5.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>108.98</td>\n",
       "      <td>455</td>\n",
       "      <td>102.60</td>\n",
       "      <td>1.21</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4.86</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6051</th>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>1500</td>\n",
       "      <td>117.51</td>\n",
       "      <td>502</td>\n",
       "      <td>110.89</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.19</td>\n",
       "      <td>5.39</td>\n",
       "      <td>0.42</td>\n",
       "      <td>108.98</td>\n",
       "      <td>457</td>\n",
       "      <td>102.62</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4.81</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6551</th>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>117.42</td>\n",
       "      <td>498</td>\n",
       "      <td>110.81</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.18</td>\n",
       "      <td>5.35</td>\n",
       "      <td>0.41</td>\n",
       "      <td>108.74</td>\n",
       "      <td>450</td>\n",
       "      <td>102.39</td>\n",
       "      <td>1.24</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4.77</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6826</th>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>117.38</td>\n",
       "      <td>497</td>\n",
       "      <td>110.76</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.33</td>\n",
       "      <td>0.41</td>\n",
       "      <td>108.88</td>\n",
       "      <td>455</td>\n",
       "      <td>102.53</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4.76</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7326</th>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>117.31</td>\n",
       "      <td>494</td>\n",
       "      <td>110.70</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.16</td>\n",
       "      <td>5.30</td>\n",
       "      <td>0.40</td>\n",
       "      <td>108.78</td>\n",
       "      <td>451</td>\n",
       "      <td>102.45</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4.72</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7826</th>\n",
       "      <td>72</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>117.22</td>\n",
       "      <td>491</td>\n",
       "      <td>110.61</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.15</td>\n",
       "      <td>5.27</td>\n",
       "      <td>0.39</td>\n",
       "      <td>108.66</td>\n",
       "      <td>448</td>\n",
       "      <td>102.28</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.69</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8326</th>\n",
       "      <td>61</td>\n",
       "      <td>3</td>\n",
       "      <td>1500</td>\n",
       "      <td>117.14</td>\n",
       "      <td>489</td>\n",
       "      <td>110.53</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.15</td>\n",
       "      <td>5.24</td>\n",
       "      <td>0.39</td>\n",
       "      <td>108.67</td>\n",
       "      <td>452</td>\n",
       "      <td>102.35</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.67</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8826</th>\n",
       "      <td>73</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>116.99</td>\n",
       "      <td>487</td>\n",
       "      <td>110.39</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.14</td>\n",
       "      <td>5.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>108.55</td>\n",
       "      <td>446</td>\n",
       "      <td>102.18</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.64</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9101</th>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>116.97</td>\n",
       "      <td>485</td>\n",
       "      <td>110.36</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.14</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.38</td>\n",
       "      <td>108.38</td>\n",
       "      <td>441</td>\n",
       "      <td>102.03</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.62</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9601</th>\n",
       "      <td>56</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "      <td>116.88</td>\n",
       "      <td>483</td>\n",
       "      <td>110.27</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.13</td>\n",
       "      <td>5.18</td>\n",
       "      <td>0.37</td>\n",
       "      <td>108.45</td>\n",
       "      <td>443</td>\n",
       "      <td>102.09</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10101</th>\n",
       "      <td>68</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>116.82</td>\n",
       "      <td>481</td>\n",
       "      <td>110.21</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.13</td>\n",
       "      <td>5.15</td>\n",
       "      <td>0.37</td>\n",
       "      <td>108.37</td>\n",
       "      <td>441</td>\n",
       "      <td>102.01</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.57</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10601</th>\n",
       "      <td>69</td>\n",
       "      <td>4</td>\n",
       "      <td>1500</td>\n",
       "      <td>116.74</td>\n",
       "      <td>479</td>\n",
       "      <td>110.12</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.12</td>\n",
       "      <td>5.13</td>\n",
       "      <td>0.37</td>\n",
       "      <td>108.34</td>\n",
       "      <td>438</td>\n",
       "      <td>102.00</td>\n",
       "      <td>1.42</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.55</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11101</th>\n",
       "      <td>57</td>\n",
       "      <td>4</td>\n",
       "      <td>2000</td>\n",
       "      <td>116.69</td>\n",
       "      <td>477</td>\n",
       "      <td>110.08</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.12</td>\n",
       "      <td>5.11</td>\n",
       "      <td>0.37</td>\n",
       "      <td>108.45</td>\n",
       "      <td>441</td>\n",
       "      <td>102.04</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.53</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11376</th>\n",
       "      <td>47</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>116.66</td>\n",
       "      <td>476</td>\n",
       "      <td>110.04</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.11</td>\n",
       "      <td>5.10</td>\n",
       "      <td>0.37</td>\n",
       "      <td>107.93</td>\n",
       "      <td>436</td>\n",
       "      <td>101.89</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.52</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11876</th>\n",
       "      <td>56</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>116.61</td>\n",
       "      <td>475</td>\n",
       "      <td>110.00</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.11</td>\n",
       "      <td>5.08</td>\n",
       "      <td>0.37</td>\n",
       "      <td>107.96</td>\n",
       "      <td>435</td>\n",
       "      <td>101.76</td>\n",
       "      <td>1.66</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.48</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12376</th>\n",
       "      <td>68</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>116.52</td>\n",
       "      <td>473</td>\n",
       "      <td>109.91</td>\n",
       "      <td>1.22</td>\n",
       "      <td>0.11</td>\n",
       "      <td>5.06</td>\n",
       "      <td>0.38</td>\n",
       "      <td>107.75</td>\n",
       "      <td>428</td>\n",
       "      <td>101.55</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.45</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12876</th>\n",
       "      <td>56</td>\n",
       "      <td>5</td>\n",
       "      <td>1500</td>\n",
       "      <td>116.45</td>\n",
       "      <td>471</td>\n",
       "      <td>109.85</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.04</td>\n",
       "      <td>0.38</td>\n",
       "      <td>107.77</td>\n",
       "      <td>425</td>\n",
       "      <td>101.42</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.42</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13376</th>\n",
       "      <td>56</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>116.38</td>\n",
       "      <td>469</td>\n",
       "      <td>109.77</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.02</td>\n",
       "      <td>0.38</td>\n",
       "      <td>107.88</td>\n",
       "      <td>429</td>\n",
       "      <td>101.57</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.41</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100101</th>\n",
       "      <td>30</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>113.73</td>\n",
       "      <td>396</td>\n",
       "      <td>106.88</td>\n",
       "      <td>2.47</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.52</td>\n",
       "      <td>106.02</td>\n",
       "      <td>372</td>\n",
       "      <td>99.45</td>\n",
       "      <td>2.72</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100601</th>\n",
       "      <td>53</td>\n",
       "      <td>44</td>\n",
       "      <td>500</td>\n",
       "      <td>113.73</td>\n",
       "      <td>396</td>\n",
       "      <td>106.87</td>\n",
       "      <td>2.47</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.52</td>\n",
       "      <td>106.12</td>\n",
       "      <td>378</td>\n",
       "      <td>99.59</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101101</th>\n",
       "      <td>52</td>\n",
       "      <td>44</td>\n",
       "      <td>1000</td>\n",
       "      <td>113.73</td>\n",
       "      <td>396</td>\n",
       "      <td>106.87</td>\n",
       "      <td>2.47</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.52</td>\n",
       "      <td>106.05</td>\n",
       "      <td>374</td>\n",
       "      <td>99.48</td>\n",
       "      <td>2.67</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101601</th>\n",
       "      <td>52</td>\n",
       "      <td>44</td>\n",
       "      <td>1500</td>\n",
       "      <td>113.72</td>\n",
       "      <td>396</td>\n",
       "      <td>106.86</td>\n",
       "      <td>2.47</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.52</td>\n",
       "      <td>106.09</td>\n",
       "      <td>375</td>\n",
       "      <td>99.52</td>\n",
       "      <td>2.70</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102101</th>\n",
       "      <td>52</td>\n",
       "      <td>44</td>\n",
       "      <td>2000</td>\n",
       "      <td>113.72</td>\n",
       "      <td>396</td>\n",
       "      <td>106.86</td>\n",
       "      <td>2.48</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.52</td>\n",
       "      <td>106.09</td>\n",
       "      <td>375</td>\n",
       "      <td>99.57</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102376</th>\n",
       "      <td>29</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>113.71</td>\n",
       "      <td>396</td>\n",
       "      <td>106.85</td>\n",
       "      <td>2.48</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.52</td>\n",
       "      <td>105.45</td>\n",
       "      <td>374</td>\n",
       "      <td>99.51</td>\n",
       "      <td>2.67</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102876</th>\n",
       "      <td>53</td>\n",
       "      <td>45</td>\n",
       "      <td>500</td>\n",
       "      <td>113.70</td>\n",
       "      <td>396</td>\n",
       "      <td>106.85</td>\n",
       "      <td>2.48</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.52</td>\n",
       "      <td>105.46</td>\n",
       "      <td>375</td>\n",
       "      <td>99.45</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103376</th>\n",
       "      <td>52</td>\n",
       "      <td>45</td>\n",
       "      <td>1000</td>\n",
       "      <td>113.70</td>\n",
       "      <td>396</td>\n",
       "      <td>106.84</td>\n",
       "      <td>2.48</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.98</td>\n",
       "      <td>0.52</td>\n",
       "      <td>105.68</td>\n",
       "      <td>376</td>\n",
       "      <td>99.55</td>\n",
       "      <td>2.77</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103876</th>\n",
       "      <td>52</td>\n",
       "      <td>45</td>\n",
       "      <td>1500</td>\n",
       "      <td>113.69</td>\n",
       "      <td>395</td>\n",
       "      <td>106.84</td>\n",
       "      <td>2.48</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.98</td>\n",
       "      <td>0.52</td>\n",
       "      <td>105.68</td>\n",
       "      <td>376</td>\n",
       "      <td>99.51</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104376</th>\n",
       "      <td>63</td>\n",
       "      <td>45</td>\n",
       "      <td>2000</td>\n",
       "      <td>113.68</td>\n",
       "      <td>395</td>\n",
       "      <td>106.83</td>\n",
       "      <td>2.49</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.98</td>\n",
       "      <td>0.52</td>\n",
       "      <td>105.41</td>\n",
       "      <td>368</td>\n",
       "      <td>99.19</td>\n",
       "      <td>2.74</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104651</th>\n",
       "      <td>29</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>113.68</td>\n",
       "      <td>395</td>\n",
       "      <td>106.83</td>\n",
       "      <td>2.49</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.98</td>\n",
       "      <td>0.52</td>\n",
       "      <td>105.70</td>\n",
       "      <td>374</td>\n",
       "      <td>99.53</td>\n",
       "      <td>2.67</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105151</th>\n",
       "      <td>53</td>\n",
       "      <td>46</td>\n",
       "      <td>500</td>\n",
       "      <td>113.68</td>\n",
       "      <td>395</td>\n",
       "      <td>106.83</td>\n",
       "      <td>2.49</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.97</td>\n",
       "      <td>0.52</td>\n",
       "      <td>105.49</td>\n",
       "      <td>367</td>\n",
       "      <td>99.24</td>\n",
       "      <td>2.70</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105651</th>\n",
       "      <td>52</td>\n",
       "      <td>46</td>\n",
       "      <td>1000</td>\n",
       "      <td>113.67</td>\n",
       "      <td>395</td>\n",
       "      <td>106.82</td>\n",
       "      <td>2.49</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.97</td>\n",
       "      <td>0.52</td>\n",
       "      <td>105.72</td>\n",
       "      <td>371</td>\n",
       "      <td>99.35</td>\n",
       "      <td>2.77</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106151</th>\n",
       "      <td>52</td>\n",
       "      <td>46</td>\n",
       "      <td>1500</td>\n",
       "      <td>113.67</td>\n",
       "      <td>395</td>\n",
       "      <td>106.82</td>\n",
       "      <td>2.49</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.97</td>\n",
       "      <td>0.52</td>\n",
       "      <td>105.78</td>\n",
       "      <td>371</td>\n",
       "      <td>99.39</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106651</th>\n",
       "      <td>53</td>\n",
       "      <td>46</td>\n",
       "      <td>2000</td>\n",
       "      <td>113.66</td>\n",
       "      <td>395</td>\n",
       "      <td>106.81</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.97</td>\n",
       "      <td>0.52</td>\n",
       "      <td>105.78</td>\n",
       "      <td>372</td>\n",
       "      <td>99.39</td>\n",
       "      <td>2.66</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106926</th>\n",
       "      <td>30</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>113.65</td>\n",
       "      <td>395</td>\n",
       "      <td>106.81</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.96</td>\n",
       "      <td>0.52</td>\n",
       "      <td>105.83</td>\n",
       "      <td>369</td>\n",
       "      <td>99.33</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107426</th>\n",
       "      <td>54</td>\n",
       "      <td>47</td>\n",
       "      <td>500</td>\n",
       "      <td>113.65</td>\n",
       "      <td>395</td>\n",
       "      <td>106.80</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.96</td>\n",
       "      <td>0.52</td>\n",
       "      <td>105.98</td>\n",
       "      <td>375</td>\n",
       "      <td>99.49</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107926</th>\n",
       "      <td>54</td>\n",
       "      <td>47</td>\n",
       "      <td>1000</td>\n",
       "      <td>113.65</td>\n",
       "      <td>394</td>\n",
       "      <td>106.80</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.96</td>\n",
       "      <td>0.52</td>\n",
       "      <td>106.15</td>\n",
       "      <td>378</td>\n",
       "      <td>99.54</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108426</th>\n",
       "      <td>53</td>\n",
       "      <td>47</td>\n",
       "      <td>1500</td>\n",
       "      <td>113.64</td>\n",
       "      <td>394</td>\n",
       "      <td>106.79</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.96</td>\n",
       "      <td>0.52</td>\n",
       "      <td>106.00</td>\n",
       "      <td>372</td>\n",
       "      <td>99.39</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108926</th>\n",
       "      <td>52</td>\n",
       "      <td>47</td>\n",
       "      <td>2000</td>\n",
       "      <td>113.63</td>\n",
       "      <td>394</td>\n",
       "      <td>106.79</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.52</td>\n",
       "      <td>105.97</td>\n",
       "      <td>374</td>\n",
       "      <td>99.45</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109201</th>\n",
       "      <td>30</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>113.63</td>\n",
       "      <td>394</td>\n",
       "      <td>106.79</td>\n",
       "      <td>2.51</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.52</td>\n",
       "      <td>106.07</td>\n",
       "      <td>374</td>\n",
       "      <td>99.48</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109701</th>\n",
       "      <td>52</td>\n",
       "      <td>48</td>\n",
       "      <td>500</td>\n",
       "      <td>113.63</td>\n",
       "      <td>394</td>\n",
       "      <td>106.78</td>\n",
       "      <td>2.51</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.52</td>\n",
       "      <td>106.00</td>\n",
       "      <td>372</td>\n",
       "      <td>99.36</td>\n",
       "      <td>2.77</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110201</th>\n",
       "      <td>52</td>\n",
       "      <td>48</td>\n",
       "      <td>1000</td>\n",
       "      <td>113.63</td>\n",
       "      <td>394</td>\n",
       "      <td>106.78</td>\n",
       "      <td>2.51</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.52</td>\n",
       "      <td>105.89</td>\n",
       "      <td>372</td>\n",
       "      <td>99.33</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110701</th>\n",
       "      <td>53</td>\n",
       "      <td>48</td>\n",
       "      <td>1500</td>\n",
       "      <td>113.62</td>\n",
       "      <td>394</td>\n",
       "      <td>106.78</td>\n",
       "      <td>2.51</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0.52</td>\n",
       "      <td>106.06</td>\n",
       "      <td>374</td>\n",
       "      <td>99.52</td>\n",
       "      <td>2.72</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111201</th>\n",
       "      <td>52</td>\n",
       "      <td>48</td>\n",
       "      <td>2000</td>\n",
       "      <td>113.62</td>\n",
       "      <td>394</td>\n",
       "      <td>106.77</td>\n",
       "      <td>2.51</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0.52</td>\n",
       "      <td>106.02</td>\n",
       "      <td>371</td>\n",
       "      <td>99.43</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111476</th>\n",
       "      <td>30</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>113.61</td>\n",
       "      <td>394</td>\n",
       "      <td>106.77</td>\n",
       "      <td>2.51</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0.52</td>\n",
       "      <td>105.86</td>\n",
       "      <td>371</td>\n",
       "      <td>99.29</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111976</th>\n",
       "      <td>53</td>\n",
       "      <td>49</td>\n",
       "      <td>500</td>\n",
       "      <td>113.61</td>\n",
       "      <td>394</td>\n",
       "      <td>106.76</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0.52</td>\n",
       "      <td>106.00</td>\n",
       "      <td>374</td>\n",
       "      <td>99.45</td>\n",
       "      <td>2.77</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112476</th>\n",
       "      <td>52</td>\n",
       "      <td>49</td>\n",
       "      <td>1000</td>\n",
       "      <td>113.60</td>\n",
       "      <td>394</td>\n",
       "      <td>106.75</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.93</td>\n",
       "      <td>0.52</td>\n",
       "      <td>105.99</td>\n",
       "      <td>374</td>\n",
       "      <td>99.49</td>\n",
       "      <td>2.71</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112976</th>\n",
       "      <td>53</td>\n",
       "      <td>49</td>\n",
       "      <td>1500</td>\n",
       "      <td>113.60</td>\n",
       "      <td>393</td>\n",
       "      <td>106.75</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.93</td>\n",
       "      <td>0.52</td>\n",
       "      <td>106.02</td>\n",
       "      <td>375</td>\n",
       "      <td>99.45</td>\n",
       "      <td>2.74</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113476</th>\n",
       "      <td>52</td>\n",
       "      <td>49</td>\n",
       "      <td>2000</td>\n",
       "      <td>113.60</td>\n",
       "      <td>393</td>\n",
       "      <td>106.75</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.93</td>\n",
       "      <td>0.52</td>\n",
       "      <td>105.91</td>\n",
       "      <td>373</td>\n",
       "      <td>99.45</td>\n",
       "      <td>2.67</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.22</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows  18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       TRAIN:    TM                        LM        VALID:  \\\n",
       "       Time  Ep    Ct    LOSS   PPL     NLL    KL   REG   NLL    KL    LOSS   \n",
       "1        15   0     0  133.50  1036  122.98  0.43  0.97  9.12  1.15  126.60   \n",
       "501      74   0   500  121.47   598  114.37  0.08  0.66  6.32  1.04  111.65   \n",
       "1001     72   0  1000  120.07   576  113.30  0.13  0.52  6.05  0.82  111.37   \n",
       "1501     73   0  1500  119.50   561  112.81  0.24  0.44  5.94  0.72  110.80   \n",
       "2001     75   0  2000  119.19   549  112.51  0.36  0.38  5.86  0.65  110.46   \n",
       "2276     56   1     0  119.09   545  112.41  0.41  0.35  5.82  0.62  110.30   \n",
       "2776     74   1   500  118.79   537  112.12  0.50  0.31  5.74  0.57  109.84   \n",
       "3276     72   1  1000  118.44   529  111.78  0.58  0.29  5.67  0.53  109.59   \n",
       "3776     72   1  1500  118.35   523  111.70  0.65  0.26  5.61  0.50  109.43   \n",
       "4276     72   1  2000  118.13   517  111.50  0.71  0.24  5.55  0.48  109.37   \n",
       "4551     46   2     0  117.99   514  111.36  0.74  0.23  5.52  0.47  109.23   \n",
       "5051     72   2   500  117.82   510  111.19  0.78  0.21  5.47  0.45  109.14   \n",
       "5551     71   2  1000  117.67   505  111.05  0.83  0.20  5.43  0.43  108.98   \n",
       "6051     61   2  1500  117.51   502  110.89  0.87  0.19  5.39  0.42  108.98   \n",
       "6551     72   2  2000  117.42   498  110.81  0.90  0.18  5.35  0.41  108.74   \n",
       "6826     33   3     0  117.38   497  110.76  0.92  0.17  5.33  0.41  108.88   \n",
       "7326     62   3   500  117.31   494  110.70  0.96  0.16  5.30  0.40  108.78   \n",
       "7826     72   3  1000  117.22   491  110.61  0.99  0.15  5.27  0.39  108.66   \n",
       "8326     61   3  1500  117.14   489  110.53  1.02  0.15  5.24  0.39  108.67   \n",
       "8826     73   3  2000  116.99   487  110.39  1.04  0.14  5.21  0.38  108.55   \n",
       "9101     42   4     0  116.97   485  110.36  1.06  0.14  5.20  0.38  108.38   \n",
       "9601     56   4   500  116.88   483  110.27  1.09  0.13  5.18  0.37  108.45   \n",
       "10101    68   4  1000  116.82   481  110.21  1.11  0.13  5.15  0.37  108.37   \n",
       "10601    69   4  1500  116.74   479  110.12  1.14  0.12  5.13  0.37  108.34   \n",
       "11101    57   4  2000  116.69   477  110.08  1.16  0.12  5.11  0.37  108.45   \n",
       "11376    47   5     0  116.66   476  110.04  1.17  0.11  5.10  0.37  107.93   \n",
       "11876    56   5   500  116.61   475  110.00  1.20  0.11  5.08  0.37  107.96   \n",
       "12376    68   5  1000  116.52   473  109.91  1.22  0.11  5.06  0.38  107.75   \n",
       "12876    56   5  1500  116.45   471  109.85  1.25  0.10  5.04  0.38  107.77   \n",
       "13376    56   5  2000  116.38   469  109.77  1.28  0.10  5.02  0.38  107.88   \n",
       "...     ...  ..   ...     ...   ...     ...   ...   ...   ...   ...     ...   \n",
       "100101   30  44     0  113.73   396  106.88  2.47  0.02  4.00  0.52  106.02   \n",
       "100601   53  44   500  113.73   396  106.87  2.47  0.02  4.00  0.52  106.12   \n",
       "101101   52  44  1000  113.73   396  106.87  2.47  0.02  4.00  0.52  106.05   \n",
       "101601   52  44  1500  113.72   396  106.86  2.47  0.02  4.00  0.52  106.09   \n",
       "102101   52  44  2000  113.72   396  106.86  2.48  0.02  3.99  0.52  106.09   \n",
       "102376   29  45     0  113.71   396  106.85  2.48  0.02  3.99  0.52  105.45   \n",
       "102876   53  45   500  113.70   396  106.85  2.48  0.02  3.99  0.52  105.46   \n",
       "103376   52  45  1000  113.70   396  106.84  2.48  0.02  3.98  0.52  105.68   \n",
       "103876   52  45  1500  113.69   395  106.84  2.48  0.02  3.98  0.52  105.68   \n",
       "104376   63  45  2000  113.68   395  106.83  2.49  0.02  3.98  0.52  105.41   \n",
       "104651   29  46     0  113.68   395  106.83  2.49  0.02  3.98  0.52  105.70   \n",
       "105151   53  46   500  113.68   395  106.83  2.49  0.02  3.97  0.52  105.49   \n",
       "105651   52  46  1000  113.67   395  106.82  2.49  0.02  3.97  0.52  105.72   \n",
       "106151   52  46  1500  113.67   395  106.82  2.49  0.02  3.97  0.52  105.78   \n",
       "106651   53  46  2000  113.66   395  106.81  2.50  0.02  3.97  0.52  105.78   \n",
       "106926   30  47     0  113.65   395  106.81  2.50  0.02  3.96  0.52  105.83   \n",
       "107426   54  47   500  113.65   395  106.80  2.50  0.02  3.96  0.52  105.98   \n",
       "107926   54  47  1000  113.65   394  106.80  2.50  0.02  3.96  0.52  106.15   \n",
       "108426   53  47  1500  113.64   394  106.79  2.50  0.02  3.96  0.52  106.00   \n",
       "108926   52  47  2000  113.63   394  106.79  2.50  0.02  3.95  0.52  105.97   \n",
       "109201   30  48     0  113.63   394  106.79  2.51  0.02  3.95  0.52  106.07   \n",
       "109701   52  48   500  113.63   394  106.78  2.51  0.02  3.95  0.52  106.00   \n",
       "110201   52  48  1000  113.63   394  106.78  2.51  0.02  3.95  0.52  105.89   \n",
       "110701   53  48  1500  113.62   394  106.78  2.51  0.02  3.94  0.52  106.06   \n",
       "111201   52  48  2000  113.62   394  106.77  2.51  0.02  3.94  0.52  106.02   \n",
       "111476   30  49     0  113.61   394  106.77  2.51  0.02  3.94  0.52  105.86   \n",
       "111976   53  49   500  113.61   394  106.76  2.52  0.02  3.94  0.52  106.00   \n",
       "112476   52  49  1000  113.60   394  106.75  2.52  0.02  3.93  0.52  105.99   \n",
       "112976   53  49  1500  113.60   393  106.75  2.52  0.02  3.93  0.52  106.02   \n",
       "113476   52  49  2000  113.60   393  106.75  2.52  0.02  3.93  0.52  105.91   \n",
       "\n",
       "          TM                        LM               \n",
       "         PPL     NLL    KL   REG   NLL    KL   Beta  \n",
       "1       1034  116.19  0.32  0.97  9.12  1.01  0.000  \n",
       "501      533  105.29  0.11  0.45  5.74  0.75  0.088  \n",
       "1001     524  105.00  0.21  0.32  5.75  0.51  0.176  \n",
       "1501     504  104.32  0.51  0.22  5.62  0.47  0.264  \n",
       "2001     498  104.00  0.65  0.18  5.48  0.44  0.352  \n",
       "2276     496  103.96  0.67  0.16  5.36  0.36  0.400  \n",
       "2776     481  103.49  0.84  0.13  5.23  0.31  0.488  \n",
       "3276     475  103.33  0.85  0.11  5.13  0.28  0.576  \n",
       "3776     472  103.13  0.95  0.10  5.06  0.29  0.664  \n",
       "4276     467  103.02  1.04  0.09  5.00  0.30  0.752  \n",
       "4551     467  102.98  1.01  0.07  4.95  0.28  0.800  \n",
       "5051     464  102.84  1.11  0.06  4.90  0.27  0.888  \n",
       "5551     455  102.60  1.21  0.05  4.86  0.27  0.976  \n",
       "6051     457  102.62  1.23  0.05  4.81  0.27  1.000  \n",
       "6551     450  102.39  1.24  0.04  4.77  0.30  1.000  \n",
       "6826     455  102.53  1.26  0.04  4.76  0.28  1.000  \n",
       "7326     451  102.45  1.29  0.04  4.72  0.29  1.000  \n",
       "7826     448  102.28  1.36  0.03  4.69  0.30  1.000  \n",
       "8326     452  102.35  1.33  0.03  4.67  0.29  1.000  \n",
       "8826     446  102.18  1.40  0.03  4.64  0.30  1.000  \n",
       "9101     441  102.03  1.41  0.03  4.62  0.29  1.000  \n",
       "9601     443  102.09  1.41  0.02  4.60  0.32  1.000  \n",
       "10101    441  102.01  1.47  0.02  4.57  0.29  1.000  \n",
       "10601    438  102.00  1.42  0.02  4.55  0.35  1.000  \n",
       "11101    441  102.04  1.52  0.02  4.53  0.33  1.000  \n",
       "11376    436  101.89  1.50  0.02  4.52  0.34  0.000  \n",
       "11876    435  101.76  1.66  0.02  4.48  0.51  0.088  \n",
       "12376    428  101.55  1.65  0.02  4.45  0.51  0.176  \n",
       "12876    425  101.42  1.77  0.02  4.42  0.52  0.264  \n",
       "13376    429  101.57  1.71  0.02  4.41  0.49  0.352  \n",
       "...      ...     ...   ...   ...   ...   ...    ...  \n",
       "100101   372   99.45  2.72  0.01  3.30  0.54  1.000  \n",
       "100601   378   99.59  2.69  0.01  3.28  0.54  1.000  \n",
       "101101   374   99.48  2.67  0.01  3.30  0.59  1.000  \n",
       "101601   375   99.52  2.70  0.01  3.30  0.57  1.000  \n",
       "102101   375   99.57  2.65  0.01  3.30  0.56  1.000  \n",
       "102376   374   99.51  2.67  0.01  3.26  0.62  0.000  \n",
       "102876   375   99.45  2.69  0.01  3.26  0.64  0.088  \n",
       "103376   376   99.55  2.77  0.01  3.25  0.61  0.176  \n",
       "103876   376   99.51  2.75  0.01  3.25  0.58  0.264  \n",
       "104376   368   99.19  2.74  0.01  3.27  0.60  0.352  \n",
       "104651   374   99.53  2.67  0.01  3.26  0.60  0.400  \n",
       "105151   367   99.24  2.70  0.01  3.25  0.62  0.488  \n",
       "105651   371   99.35  2.77  0.01  3.24  0.61  0.576  \n",
       "106151   371   99.39  2.75  0.01  3.23  0.60  0.664  \n",
       "106651   372   99.39  2.66  0.01  3.24  0.64  0.752  \n",
       "106926   369   99.33  2.75  0.01  3.26  0.59  0.800  \n",
       "107426   375   99.49  2.69  0.01  3.25  0.61  0.888  \n",
       "107926   378   99.54  2.76  0.01  3.25  0.60  0.976  \n",
       "108426   372   99.39  2.75  0.01  3.28  0.57  1.000  \n",
       "108926   374   99.45  2.69  0.01  3.25  0.58  1.000  \n",
       "109201   374   99.48  2.75  0.01  3.28  0.56  1.000  \n",
       "109701   372   99.36  2.77  0.01  3.26  0.60  1.000  \n",
       "110201   372   99.33  2.75  0.01  3.24  0.56  1.000  \n",
       "110701   374   99.52  2.72  0.01  3.25  0.56  1.000  \n",
       "111201   371   99.43  2.76  0.01  3.25  0.57  1.000  \n",
       "111476   371   99.29  2.78  0.01  3.23  0.55  1.000  \n",
       "111976   374   99.45  2.77  0.01  3.23  0.54  1.000  \n",
       "112476   374   99.49  2.71  0.01  3.24  0.54  1.000  \n",
       "112976   375   99.45  2.74  0.01  3.25  0.57  1.000  \n",
       "113476   373   99.45  2.67  0.01  3.22  0.56  1.000  \n",
       "\n",
       "[250 rows x 18 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Output sentences for each topic-----------\n",
      "Item idx: B000VB7EFW\n",
      "0  BOW: zipper open strap zippers flap top front side handle main\n",
      "0  SENTENCE: the main flap on the front flap clips on the front flap clips on the bottom flap clips on the bottom flap\n",
      "1  BOW: cover keyboard color apple easy protector love hard screen put\n",
      "1  SENTENCE: the keyboard keyboard cover fits perfectly and the keyboard cover is a little difficult to type on\n",
      "2  BOW: months broke started year years bought weeks month ago week\n",
      "2  SENTENCE: the first one lasted about # months ago and the top piece broke within the first week\n",
      "3  BOW: inch price perfect size made small bought carry hp big\n",
      "3  SENTENCE: my perfect fit for my ipad perfectly ! !\n",
      "4  BOW: listed test sewn fair refund returning late stated website manufacturer\n",
      "4  SENTENCE: it is a little bit bigger than i thought it would be , but it 's not a problem\n",
      "5  BOW: smell strong chemical smells days odor bad opened package packaging\n",
      "5  SENTENCE: one of the other reviewers have mentioned , the case is a little flimsy , but after a few months it is still usable\n",
      "6  BOW: ; & pro smell air description laptops ordered hp big\n",
      "6  SENTENCE: i i bought bought & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & &\n",
      "7  BOW: : air bit cheap - pretty fine thin close material\n",
      "7  SENTENCE: the bottom piece of the case has a zipper on the bottom of the case to hold the laptop in place\n",
      "8  BOW: ! daughter loves absolutely awesome amazing loved christmas compliments wonderful\n",
      "8  SENTENCE: i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i\n",
      "9  BOW: handle back straps shoulder strap pack heavy bags 've zippers\n",
      "9  SENTENCE: the wheels roll the straps on the back of the back pack straps on the back of the pack\n",
      "10  BOW: scratches retina shell mbp hard weight protection protect scratch pro\n",
      "10  SENTENCE: the bottom of the sleeve is a little loose on the bottom of the case , and the outer shell is a tight fit\n",
      "11  BOW: power netbook mouse cord drive usb pocket adapter cable external\n",
      "11  SENTENCE: side pocket on the front pocket has a zippered pocket on the front to hold the power adapter , a usb cable , and a usb cable , and the power adapter in the front pocket on the front of\n",
      "12  BOW: inside nice padding bit zipper handles material side padded velcro\n",
      "12  SENTENCE: the usb port on the front of the zipper flap\n",
      "13  BOW: cover nice easily logo clear plastic 'm covers screen pretty\n",
      "13  SENTENCE: the keyboard cover snapped on easily and the keyboard cover does n't stay on the bottom of the keyboard cover , but the top cover does n't snap on\n",
      "14  BOW: 'm quality ... made - nice thing leather thought time\n",
      "14  SENTENCE: the customer service sent me a replacement and the replacement within a couple of weeks of use\n",
      "15  BOW: room ipad pocket charger accessories mouse cords extra perfect tablet\n",
      "15  SENTENCE: plenty of room for my power cord , mouse , power cord , mouse , power cord , mouse , power cord , mouse , power cords , pens , mouse , mouse , power cords , pens , pens\n",
      "16  BOW: pro perfectly mac recommend air bought protect protects book perfect\n",
      "16  SENTENCE: the perfect cover for my # & # # ; mac book pro fits perfectly perfectly fits perfectly perfectly\n",
      "17  BOW: price buy $ ... time money bought reviews quality worth\n",
      "17  SENTENCE: my customer service service service ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "18  BOW: school bought recommend college highly work gift purchased husband student\n",
      "18  SENTENCE: she loves he ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "19  BOW: arrived item fast shipping received ordered order days shipped seller\n",
      "19  SENTENCE: customer service service ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "20  BOW: sleeve protection inch neoprene snug zipper inside chromebook protect soft\n",
      "20  SENTENCE: the usb ports fits my samsung chromebook # . # -inch acer chromebook with the power cord inside\n",
      "21  BOW: pockets strap compartment pocket small side hold shoulder front large\n",
      "21  SENTENCE: the main compartment open open open open open open access access the main compartment for the main compartment , and the main compartment has a velcro pocket on the front flap\n",
      "22  BOW: returning returned cheap opened guess return pay disappointed terrible cheaply\n",
      "22  SENTENCE: the only issue i have is that the bottom piece of the case broke within a month of use\n",
      "23  BOW: return received company service seller customer disappointed item contacted amazon\n",
      "23  SENTENCE: the customer service was sent me a replacement and they sent me a replacement and they sent me a replacement\n",
      "24  BOW: color picture pink blue black green red bright purple colors\n",
      "24  SENTENCE: the keyboard cover ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "25  BOW: camera lenses lens gear canon equipment batteries flash pack accessories\n",
      "25  SENTENCE: with the canon # d , # lenses , # lenses , a # mm lens , and a couple of lenses , and a few other items\n",
      "26  BOW: carry pockets lots space room stuff books plenty comfortable compartments\n",
      "26  SENTENCE: lots of room for lenses , lenses , lenses , lenses , lenses , flash , flash , etc .\n",
      "27  BOW: test fair returned trust hours sewn returning floor stated return\n",
      "27  SENTENCE: however , the case is a little heavy , but it is not a problem with the bag\n",
      "28  BOW: bottom top piece part back plastic feet corners rubber stay\n",
      "28  SENTENCE: the clips on the bottom piece of the bottom piece of the top piece of the top piece of the bottom piece of the zipper\n",
      "29  BOW: love perfect cute beautiful super compliments ! color happy recommend\n",
      "29  SENTENCE: adore her\n",
      "-----------Summaries-----------\n",
      "SUMMARY 0 : \n",
      " This is a very well made bag, nice construction, lots of pockets.\n",
      "the straps are very comfortable.\n",
      "and protects everything inside.\n",
      "It says\n",
      "it fits a 17inch notebook,\n",
      "however it did not.\n",
      "after using the pack for less than a month,\n",
      "it is ripping out already.\n",
      "SUMMARY 1 : \n",
      " This is a very well made bag, nice construction, lots of pockets.\n",
      "The quality is excellent\n",
      "and it is very durable.\n",
      "the straps are very comfortable.\n",
      "and protects everything inside.\n",
      "The color is a true red\n",
      "and it fits nicely.\n",
      "it is ripping out already.\n",
      "and it doesnt fit.\n",
      "It's just not the lightest backpack\n",
      "because some zipper teeth were not aligned.\n",
      "SUMMARY 2 : \n",
      " The quality is excellent\n",
      "and I love all the pockets and compartments.\n",
      "and it is very durable.\n",
      "and can't beleive the price\n",
      "and protects everything inside.\n",
      "The laptop\n",
      "doesn't fit in it.\n",
      "it is ripping out already.\n",
      "It's just not the lightest backpack\n",
      "0 TRUE: hello all , this is my second patriot bag\n",
      "0 PRED: for , over this is a second backpack bag\n",
      "1 TRUE: the first lasted me for about # years , i used it daily and i extended the handle and <unk> it at least # times a day\n",
      "1 PRED: the first one i i a # years , i started to bag use the have use first and the it it the # years\n",
      "2 TRUE: the first bags handle finally broke after # years of daily use -lrb- i was happy with it lasting # years -rrb-\n",
      "2 PRED: the first day lasted broke broke after i years # use use , i have using to the bag and years\n",
      "3 TRUE: i decided to get another just like it\n",
      "3 PRED: i will to get it one right it\n",
      "4 TRUE: when i received the new bag i noticed the handle was redesigned -lrb- this is what broken on my last patriot -rrb- and i was happy to see that\n",
      "4 PRED: this i first this bag bag , was it bag and the the i one the one -rrb- the -rrb- -rrb- -rrb- -rrb- i have n't it return it\n",
      "5 TRUE: however , this bags handel will not stay fully extended\n",
      "5 PRED: also the the bag are the not stay on on\n",
      "6 TRUE: when i extend it fully and it seems as thought it is locked in place , it will drop down to the next <unk> # % of the time -lrb- if you know what i mean -rrb-\n",
      "6 PRED: when i first it up , , it i well it would be in , , but is n't my , my point time , but of the quality , i -rrb- have -rrb- -rrb- -rrb-\n",
      "7 TRUE: not super irritating but\n",
      "7 PRED: not than cheap\n",
      "8 TRUE: i think its worth the money i spent even though the handles slides down sometimes . http\n",
      "8 PRED: i do it a the money , you a with the zipper on around and\n",
      "9 TRUE: / / www.amazon.com/gp/product/b # e #\n",
      "9 PRED: pros is and # <unk> #\n",
      "10 TRUE: very nice bag works well for carrying my kindle fire and i pad and all charging cards , with spare back up battery\n",
      "10 PRED: great nice case , my padded my my laptop fire , mouse , , a , cords , etc a\n",
      "11 TRUE: as well as some purse items and it still holds a few files\n",
      "11 PRED: for well as a other , and , is has a lot books\n",
      "12 TRUE: one down fall with the bag is no shoulder strap to use it as a cross body bag\n",
      "12 PRED: the of the off the bag is the longer strap , it is is a carry-on backpack bag\n",
      "13 TRUE: if they come up with the same bag with a strap that can make it a cross body bag i 'll buy it again\n",
      "13 PRED: this i are the the the bag bag , the few bag is n't the a little bag bag , will be\n",
      "14 TRUE: keeps things dry in a lite rain shower\n",
      "14 PRED: also everything in and the backpack bag\n",
      "15 TRUE: would highly suggest the bag\n",
      "15 PRED: this recommend recommend this bag\n",
      "16 TRUE: the four stars is because their is no shoulder strap to use as a cross body bag\n",
      "16 PRED: the shoulder strap is that it shoulder a longer strap to hold it a shoulder bag bag\n",
      "17 TRUE: i like the overall size of the bag and individual <unk> , but the construction and quality is subpar\n",
      "17 PRED: i like the design design of the bag , the pockets , but it bag is is is very\n",
      "18 TRUE: after using it for one day on the washington metro , one wheel guard broke off\n",
      "18 PRED: after a year , a week , the , handle , the of started broke\n",
      "19 TRUE: this is cheaply made and i should have <unk> this based on the low price\n",
      "19 PRED: this is a made and i have n't to it thing on the reviews price\n",
      "20 TRUE: i will have to increase my budget for a replacement bag\n",
      "20 PRED: i will it to buy the money and a replacement\n",
      "21 TRUE: this one is going back\n",
      "21 PRED: this bag is falling back\n",
      "22 TRUE: another <unk> in horrible chinese made products\n",
      "22 PRED: update review i the # #\n",
      "23 TRUE: the sleeve is indeed bigger than the mbp with retina display\n",
      "23 PRED: the sleeve is soft soft to to laptop is a\n",
      "24 TRUE: however if you keep the white packing foam in the sleeve , it becomes pretty tight\n",
      "24 PRED: however it it have the sleeve macbook on inside the sleeve , the will tight tight\n",
      "25 TRUE: yes , that makes the whole thing bulky , but also gives additional padding\n",
      "25 PRED: also , the 's it case case for bulky and it fits it protection\n",
      "26 TRUE: they specifically say mbp with retina display , not only on the <unk> but also in their package\n",
      "26 PRED: this fit fit the , and display , but a a it case of it is the\n",
      "27 TRUE: i would expect something like my previous tucano second skin , but it is not\n",
      "27 PRED: i would have to that this previous case , case , it it is n't\n",
      "28 TRUE: i do n't wan na say they <unk> a universal sleeve as mbp specific simply after adding a foam pad , but that 's what seems to be\n",
      "28 PRED: i like like know na the the are a little sleeve , a , case , a a lot of , the it is not\n",
      "29 TRUE: a bit darker then i thought it was going to be ! but i still like it a lot\n",
      "29 PRED: the good color color i would n't would a to be but but but think n't the is little\n",
      "30 TRUE: my macbook pro fits nice and snug in this guy\n",
      "30 PRED: my macbook air # perfectly and snug and this case\n",
      "31 TRUE: will definitely keep it from getting scratched but wont do much from getting squished . soft case not a hard one\n",
      "31 PRED: if you does the from getting scratched , it be n't like to scratched from it and that a very\n",
      "32 TRUE: also make sure to keep inside clean\n",
      "32 PRED: but , the the the the\n",
      "33 TRUE: anything hard -lrb- like sand -rrb- will still be able to get between your screen and keyboard if its inside the soft case already\n",
      "33 PRED: hard : the <unk> the -rrb- , protect scratches a to fit the the macbook protector the cover you are the case\n",
      "34 TRUE: the bag is the perfect size for my pavilion dv # laptop\n",
      "34 PRED: the case is great perfect size for my # `` # laptop\n",
      "35 TRUE: it 's a gorgeous print\n",
      "35 PRED: it looks a great color\n",
      "36 TRUE: the only downside is that it doesnt really have a place for the cord to go but i just put it in with the computer\n",
      "36 PRED: the only thing is that it 's fit fit a little for the laptop , but to the sleeve it it in the a\n",
      "37 TRUE: also , there is a bit of a smell but nothing unbearable and nothing that wo n't probably go away once it 's been out a bit\n",
      "37 PRED: however , it is a little of <unk> smell to i 's , i to i it to will away after the 's not a\n",
      "38 TRUE: overall very satisfied with the product\n",
      "38 PRED: overall , good with this product\n",
      "39 TRUE: this does n't scratch my macbook at all and i love how it clips on\n",
      "39 PRED: this case n't fit the macbook and all , the do the it is on\n",
      "40 TRUE: the clear case is very chic and i have received several compliments\n",
      "40 PRED: the color color was a cheap to i was not it compliments\n",
      "41 TRUE: i do n't like the product label on the top but for $ # feel much better about how well protected my new investment is\n",
      "41 PRED: i was n't know it case would but my macbook of it i # , of better the this it it case macbook macbook\n",
      "42 TRUE: i would n't expect this case to hold up if i dropped the computer or dropped something on it , but it keeps from getting scratched\n",
      "42 PRED: i would have use this to to be up a a of , but , but , i a , but it n't\n",
      "43 TRUE: i love the color of the case , and it is not only very protective of my computer , but also has a nice soft feeling to it\n",
      "43 PRED: i love the color of this case , the it is a a to soft , , macbook is but it is n't good cover color\n",
      "44 TRUE: the one thing that i was n't very happy about was the fact that you ca n't see the apple logo through the case\n",
      "44 PRED: the only is i i was disappointed like disappointed with the the keyboard that the can n't see the apple logo on the case\n",
      "45 TRUE: also , the keyboard cover does not perfectly match up\n",
      "45 PRED: also the the keyboard cover is not fit to the\n",
      "46 TRUE: overall , i would purchase this product again\n",
      "46 PRED: overall , i am definitely this product again\n",
      "47 TRUE: i had read other reviews of this product before purchasing and was aware that it might be a tight fit -- and it was\n",
      "47 PRED: i have a the reviews before this product , i it it n't that it would fit a little fit\n",
      "48 TRUE: i have a # . # `` screen and this was advertised to fit up to # `` computers\n",
      "48 PRED: i have a sleeve . # `` laptop and it case a to fit the # fit ``\n",
      "49 TRUE: but i realize it is a ` skin ' and not a case , so i would expect a tight fit\n",
      "49 PRED: does if do it would n't little <unk> ' `` it a sleeve of , it can n't it good more\n",
      "50 TRUE: just be aware -- the zipper is a little hard to zip completely , but it is a great sleeve for carrying your laptop around\n",
      "50 PRED: also be the of it is is a little bit to get it , and it it a little case for the around laptop around\n",
      "51 TRUE: got this for my son for his # `` mac book pro\n",
      "51 PRED: bought this for my wife for his macbook `` macbook book pro\n",
      "52 TRUE: he loves it\n",
      "52 PRED: she loves it\n",
      "53 TRUE: fits the laptop and charger and a few papers for trips to the library at school\n",
      "53 PRED: fits my laptop , my , a few other , school and a is and home\n",
      "54 TRUE: bought it for my daughter who was going off to college for some added protection\n",
      "54 PRED: overall this for my daughter and is a to her her and and research\n",
      "55 TRUE: she loves it and even put a decal on it\n",
      "55 PRED: i loves the , it though her on on it\n",
      "56 TRUE: no drops yet to test its protection but it definitely makes it less slippery to hold on to\n",
      "56 PRED: only : , , protect the protection , the macbook does the a slippery to open the the the\n",
      "57 TRUE: i have n't had the chance yet to take the bag through the airport , but even as an everyday backpack it 's great\n",
      "57 PRED: i have been to a bag for to carry my bag out to bag security i i i a <unk> basis i 's still\n",
      "58 TRUE: my favorite part is that the excess nylon that you use to adjust the shoulder straps is rolled up and tucked away with velcro\n",
      "58 PRED: the only part is that the shoulder strap strap is can to the the bag strap are not down and the off from the\n",
      "59 TRUE: so you do n't have these ridiculous straps bouncing around everywhere\n",
      "59 PRED: if i can n't have to are straps\n",
      "60 TRUE: i loved this sleeve : it fit my # -inch mid- # macbook pro perfectly and had a pocket for the power cable , and even a little pocket for a usb drive\n",
      "60 PRED: i have a case for # fits my # . macbook # . pro with , the a little for the charger cord , and the cord pocket extra on the mouse cable\n",
      "61 TRUE: but the zipper broke within # month of use , so i can not recommend it to anyone\n",
      "61 PRED: however , bag broke broke a months , use it it i have not use it it\n",
      "62 TRUE: i love this produce very much , it made in good quality , i love the skin , color and every thing , i advice who love black skin to buy it its\n",
      "62 PRED: i love the case it much , i it it color color and great love the color and it and the , i it love it love the color and protect it\n"
     ]
    }
   ],
   "source": [
    "if len(log_df) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "while epoch < config.epochs:\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = get_feed_dict(batch)\n",
    "\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, sent_loss_kl_categ_batch, sent_loss_kl_gmm_batch, ppls_batch = \\\n",
    "        sess.run([opt, loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, sent_loss_kl_categ, sent_loss_kl_gmm, topic_ppls], feed_dict = feed_dict)\n",
    "   \n",
    "        if sent_loss_kl_batch == np.inf:\n",
    "            print('Nan occured')\n",
    "            ckpt = tf.train.get_checkpoint_state(config.modeldir)\n",
    "            model_checkpoint_path = ckpt.all_model_checkpoint_paths[-1]\n",
    "            saver.restore(sess, model_checkpoint_path)            \n",
    "            break\n",
    "            \n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if ct%config.log_period==0:\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train, sent_loss_recon_train, sent_loss_kl_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, sent_loss_recon_dev, sent_loss_kl_dev, ppl_dev = get_loss(sess, dev_batches)\n",
    "            global_step_log, beta_eval = sess.run([tf.train.get_global_step(), beta])\n",
    "            \n",
    "            if loss_dev < loss_min:\n",
    "                loss_min = loss_dev\n",
    "                saver.save(sess, config.modelpath, global_step=global_step_log)\n",
    "\n",
    "            clear_output()\n",
    "    \n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_reg_train, '%.2f'%sent_loss_recon_train, '%.2f'%sent_loss_kl_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_reg_dev, '%.2f'%sent_loss_recon_dev, '%.2f'%sent_loss_kl_dev,  '%.3f'%beta_eval],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "\n",
    "            print_summary(test_batches[1][1])\n",
    "            print_sample(batch)\n",
    "            \n",
    "            time_start = time.time()\n",
    "            \n",
    "    epoch += 1\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "_prob_topic, _prob_topic_sents, _prob_topic_infer, _means_topic_infer = debug_value([prob_topic, prob_topic_sents, prob_topic_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.01681865, 0.00085679, 0.05174972, 0.09216808, 0.00016615,\n",
       "        0.00087459, 0.00118037, 0.02513456, 0.01622178, 0.14665115,\n",
       "        0.0011753 , 0.02991384, 0.03108656, 0.00118961, 0.11844516,\n",
       "        0.01804251, 0.01136509, 0.12562811, 0.05762221, 0.01317067,\n",
       "        0.00810287, 0.06980555, 0.00149514, 0.00750153, 0.00170939,\n",
       "        0.01077554, 0.13081829, 0.00062634, 0.00238444, 0.00731998],\n",
       "       dtype=float32),\n",
       " array([9.0836927e-02, 1.0987123e-03, 1.5486477e-01, 3.6601584e-02,\n",
       "        8.1031001e-05, 3.8817880e-04, 1.8645950e-02, 6.4254373e-02,\n",
       "        1.1557489e-02, 1.8391356e-01, 1.7670565e-03, 9.3876962e-03,\n",
       "        3.3649437e-02, 2.5801428e-03, 1.2207399e-01, 2.7256794e-03,\n",
       "        5.1550437e-03, 5.9668675e-02, 1.9978454e-02, 3.9733057e-03,\n",
       "        7.7147051e-03, 6.4633414e-02, 3.8139557e-03, 1.5572711e-02,\n",
       "        3.4457527e-03, 1.8684692e-03, 5.4347310e-02, 7.9502229e-04,\n",
       "        2.2245880e-02, 2.3606885e-03], dtype=float32))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_i = 4\n",
    "_prob_topic_sents[batch_i], _prob_topic_infer[batch_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.56992847e-01,  6.93834186e-01, -8.92492712e-01,\n",
       "         2.42564678e-01],\n",
       "       [-4.57342863e-01, -4.06658985e-02,  1.23339975e+00,\n",
       "        -5.94517171e-01],\n",
       "       [-4.11060035e-01,  5.96444011e-02, -9.75041866e-01,\n",
       "         1.38034391e+00],\n",
       "       [ 5.46178877e-01, -2.98609585e-01,  8.14010501e-01,\n",
       "         9.63758230e-01],\n",
       "       [-4.58527356e-02,  7.22829029e-02, -1.19237393e-01,\n",
       "        -3.92585009e-01],\n",
       "       [-2.96975613e-01,  1.55666143e-01, -2.59647340e-01,\n",
       "         3.35099936e-01],\n",
       "       [-3.07893324e+00,  1.74073529e+00,  3.32384348e-01,\n",
       "        -1.66081464e+00],\n",
       "       [-3.01134288e-01,  4.63835001e-01, -1.16023615e-01,\n",
       "        -1.26744211e-02],\n",
       "       [ 2.22409916e+00, -2.20579481e+00, -4.48575258e+00,\n",
       "        -2.86763134e+01],\n",
       "       [-8.55066776e-02,  2.23345548e-01, -1.50715339e+00,\n",
       "         2.03623366e+00],\n",
       "       [-1.57129437e-01,  3.82197917e-01,  6.79618537e-01,\n",
       "        -7.08601534e-01],\n",
       "       [ 1.61208048e-01,  6.48200572e-01, -8.06558877e-03,\n",
       "        -2.73757488e-01],\n",
       "       [ 1.53458059e-01,  5.02333879e-01,  2.88346082e-01,\n",
       "        -2.76391447e-01],\n",
       "       [-5.54081559e-01,  2.53504694e-01,  7.54443288e-01,\n",
       "        -7.02945590e-01],\n",
       "       [-4.24191475e-01, -1.70171261e-05, -7.72340536e-01,\n",
       "         1.42114615e+00],\n",
       "       [ 6.63078725e-01,  3.45673382e-01,  5.61509013e-01,\n",
       "         2.37084746e-01],\n",
       "       [ 3.27046394e-01, -4.33003098e-01,  2.59693766e+00,\n",
       "        -9.64356303e-01],\n",
       "       [-4.07979697e-01, -4.30327207e-01, -4.52172369e-01,\n",
       "         1.80658662e+00],\n",
       "       [ 2.20051423e-01, -8.47631454e-01, -1.75820887e-01,\n",
       "        -9.41836238e-02],\n",
       "       [-4.53201592e-01, -3.50880176e-01, -1.93357795e-01,\n",
       "         9.81837451e-01],\n",
       "       [ 5.39368331e-01,  4.51801240e-01,  2.49500799e+00,\n",
       "        -2.20261312e+00],\n",
       "       [ 1.94506332e-01,  6.96098208e-01, -9.36168373e-01,\n",
       "         8.11360538e-01],\n",
       "       [-3.43362808e-01,  1.62257046e-01, -4.11428332e-01,\n",
       "         4.65924263e-01],\n",
       "       [-4.43569511e-01, -8.87472928e-02, -5.82772911e-01,\n",
       "         1.07559109e+00],\n",
       "       [-4.87735301e-01, -4.02365416e-01,  1.08538008e+00,\n",
       "         5.42437673e-01],\n",
       "       [ 1.38357431e-01,  2.15829059e-01, -7.93718159e-01,\n",
       "         1.17980063e+00],\n",
       "       [ 6.76810682e-01, -8.98454934e-02, -4.86663014e-01,\n",
       "         2.18945050e+00],\n",
       "       [-1.28618836e-01,  1.49767190e-01, -1.40007019e-01,\n",
       "         6.78563416e-02],\n",
       "       [-4.47370350e-01,  5.74193835e-01, -6.23793483e-01,\n",
       "         1.27127260e-01],\n",
       "       [ 1.81682062e+00, -2.12252259e+00,  1.95912433e+00,\n",
       "        -2.88068175e+00]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_means_topic_infer[0][:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_means_topic, b_means_topic = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"topic/dec/mean_topic\")\n",
    "\n",
    "pred_topic_embeddings, pred_topic_bow, pred_means_topic, pred_logvars_topic, pred_token_idxs, _w_means_topic, _b_means_topic, _w_mean_topic_infer = \\\n",
    "                                sess.run([topic_embeddings, topic_bow, means_topic, logvars_topic, topic_beam_output_token_idxs, w_means_topic, b_means_topic, w_mean_topic_infer], \n",
    "                                         feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "\n",
    "pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "\n",
    "pred_topics_freq_bow_indices = np.argsort(pred_topic_bow, 1)[:, ::-1][:, :10]\n",
    "pred_topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zipper', 'strap', 'flap', 'open', 'zippers', 'velcro', 'main', 'metal', 'side', 'front']\n",
      "['cover', 'keyboard', 'apple', 'color', 'easy', 'protector', 'love', 'screen', 'hard', 'put']\n",
      "['months', 'broke', 'year', 'years', 'started', 'bought', 'month', 'weeks', 'week', 'ago']\n",
      "['perfect', 'inch', 'price', 'size', 'carry', 'small', 'bought', 'carrying', 'made', 'big']\n",
      "['listed', 'test', 'sewn', 'fair', 'refund', 'late', 'stated', 'returning', 'website', 'manufacturer']\n",
      "['smell', 'days', 'strong', 'bad', 'smells', 'chemical', 'odor', 'package', 'packaging', 'opened']\n",
      "[';', '&', 'pro', 'air', 'smell', 'description', 'laptops', 'ordered', 'big', 'tablet']\n",
      "[':', 'air', 'bit', 'cheap', '-', 'pretty', 'fine', 'close', 'thin', 'protect']\n",
      "['!', 'awesome', 'loves', 'daughter', 'absolutely', 'amazing', 'loved', 'christmas', 'gift', 'wonderful']\n",
      "['straps', 'back', 'handle', 'shoulder', 'pack', 'strap', 'heavy', 'bags', \"'ve\", 'zippers']\n",
      "['scratches', 'retina', 'shell', 'hard', 'mbp', 'weight', 'protection', 'display', 'protect', 'pro']\n",
      "['power', 'netbook', 'mouse', 'cord', 'drive', 'pocket', 'usb', 'adapter', 'hard', 'small']\n",
      "['inside', 'nice', 'padding', 'bit', 'handles', 'zipper', 'padded', 'zip', 'material', 'velcro']\n",
      "['cover', 'nice', 'easily', 'logo', 'plastic', \"'m\", 'screen', 'pretty', 'covers', 'feel']\n",
      "[\"'m\", 'quality', '...', 'nice', '-', 'made', 'leather', 'thing', 'thought', 'pretty']\n",
      "['room', 'charger', 'pocket', 'ipad', 'cords', 'mouse', 'accessories', 'perfect', 'extra', 'tablet']\n",
      "['pro', 'perfectly', 'mac', 'air', 'recommend', 'bought', 'protect', 'protects', 'book', 'perfect']\n",
      "['price', 'buy', '$', 'time', '...', 'bought', 'worth', 'quality', 'money', 'reviews']\n",
      "['school', 'bought', 'recommend', 'highly', 'college', 'son', 'gift', 'husband', 'year', 'work']\n",
      "['shipping', 'arrived', 'item', 'received', 'ordered', 'order', 'fast', 'days', 'amazon', 'delivery']\n",
      "['sleeve', 'protection', 'inch', 'snug', 'protect', 'inside', 'soft', 'neoprene', 'zipper', 'extra']\n",
      "['pockets', 'compartment', 'strap', 'pocket', 'small', 'shoulder', 'side', 'hold', 'front', 'main']\n",
      "['returned', 'returning', 'cheap', 'opened', 'pay', 'return', 'disappointed', 'guess', 'terrible', 'cheaply']\n",
      "['return', 'received', 'service', 'customer', 'company', 'send', 'seller', 'amazon', 'disappointed', 'back']\n",
      "['color', 'picture', 'pink', 'blue', 'black', 'green', 'bright', 'red', 'purple', 'colors']\n",
      "['camera', 'lenses', 'lens', 'canon', 'equipment', 'flash', 'gear', 'accessories', 'pack', 'tripod']\n",
      "['carry', 'pockets', 'plenty', 'lots', 'space', 'room', 'comfortable', 'stuff', 'books', 'compartments']\n",
      "['test', 'fair', 'returned', 'trust', 'hours', 'sewn', 'returning', 'floor', 'stated', 'return']\n",
      "['bottom', 'top', 'part', 'back', 'plastic', 'piece', 'feet', 'rubber', 'corners', 'stay']\n",
      "['love', 'perfect', 'cute', 'beautiful', 'compliments', 'color', '!', 'recommend', 'super', 'happy']\n"
     ]
    }
   ],
   "source": [
    "for idxs in pred_topics_freq_bow_idxs:\n",
    "    print([idx_to_word[idx] for idx in idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04221381,  0.07032954,  0.02605449, -0.14514676, -0.10837159,\n",
       "        -0.13675061,  0.04316118,  0.21489286,  0.02900301, -0.11756933],\n",
       "       [-0.00191224,  0.09261359,  0.01232868, -0.0661851 ,  0.07643441,\n",
       "         0.12696671, -0.20366614,  0.1984897 ,  0.17354025,  0.02899385],\n",
       "       [ 0.02067169, -0.20038156, -0.09364118,  0.21172738, -0.02731904,\n",
       "        -0.04265526,  0.05918429,  0.1413685 , -0.12565222,  0.03097908],\n",
       "       [ 0.06493654,  0.0132221 ,  0.01086067,  0.04034178,  0.4537829 ,\n",
       "         0.11259412,  0.00850064,  0.14192611, -0.03143219, -0.27809662],\n",
       "       [-0.29869643,  0.26431245, -0.24967694, -0.13403602, -0.17315197,\n",
       "         0.24033956,  0.28124785,  0.10158894,  0.15352075, -0.12799466],\n",
       "       [-0.5816298 , -0.43490657,  0.17611949,  0.18747905,  0.13666977,\n",
       "        -0.30238554, -0.34233955,  0.47408575,  0.08233652, -0.03812001],\n",
       "       [-0.01370848,  0.33514827,  0.0198018 , -0.27161032,  0.32002565,\n",
       "        -0.52758247,  0.13847867,  0.06868012, -0.00483406,  0.03204579],\n",
       "       [-0.09599586, -0.02154429,  0.12527123,  0.01452046,  0.07959457,\n",
       "        -0.13458543,  0.19588438,  0.06547991, -0.07313657, -0.0109082 ],\n",
       "       [ 0.06967789,  0.49404705, -0.28609815,  0.32278985,  0.40337625,\n",
       "         0.25242886, -0.23485763,  0.06544693, -0.10294746,  0.05736642],\n",
       "       [-0.01701886, -0.08480541,  0.15281047,  0.6724302 , -0.01259776,\n",
       "         0.03165396,  0.03910705, -0.18956725, -0.00154719, -0.00479472],\n",
       "       [ 0.22453545, -0.29168802, -0.13092327,  0.00232083, -0.16716649,\n",
       "        -0.02409744,  0.06523509, -0.09875831,  0.16521567, -0.05491022],\n",
       "       [-0.06180208, -0.11752784, -0.17485783, -0.23905095,  0.12875971,\n",
       "         0.06910677, -0.08105803,  0.0072509 ,  0.03532727, -0.18066171],\n",
       "       [ 0.07781179,  0.07906892,  0.08003455,  0.05637592,  0.3426231 ,\n",
       "         0.0018317 ,  0.01129622, -0.18902628, -0.17290215,  0.0247026 ],\n",
       "       [-0.09195589, -0.01054317,  0.06166134,  0.01003112,  0.13506752,\n",
       "        -0.05647143,  0.09183009, -0.06100028,  0.11209927,  0.1642698 ],\n",
       "       [-0.16744362,  0.09064247,  0.14948869,  0.512609  ,  0.18247712,\n",
       "        -0.00711581, -0.00242178,  0.03543727, -0.01350794,  0.02799927],\n",
       "       [ 0.570048  , -0.23858926, -0.11629555, -0.4557064 ,  0.15530618,\n",
       "        -0.08475548, -0.10722307, -0.11913169, -0.20590708, -0.22634163],\n",
       "       [ 0.11967376, -0.08421919, -0.061414  , -0.24033915,  0.36114448,\n",
       "         0.08723626, -0.18445982, -0.04841575, -0.03436645, -0.04157114],\n",
       "       [ 0.10118676,  0.0071608 ,  0.22107947,  0.56091464,  0.02547326,\n",
       "        -0.0477402 , -0.41994584,  0.10262967,  0.08013427,  0.03091147],\n",
       "       [-0.02430055, -0.00301753,  0.17131078,  0.03749689,  0.12962432,\n",
       "        -0.11138848, -0.42386788, -0.09112586, -0.04432308, -0.20165683],\n",
       "       [ 0.04739588, -0.06331264, -0.05999106, -0.09428453,  0.0458748 ,\n",
       "         0.02031917, -0.42660815,  0.05409623,  0.03730098,  0.03244893],\n",
       "       [ 0.01980995,  0.00787331, -0.00546622, -0.12851284, -0.02723288,\n",
       "        -0.00923575, -0.13978691,  0.01614582, -0.07919737, -0.02275529],\n",
       "       [ 0.20089826,  0.10086202,  0.17296737,  0.11019783,  0.10079711,\n",
       "        -0.5508731 ,  0.11871882, -0.2548031 ,  0.07037319, -0.07621628],\n",
       "       [-0.4150683 ,  0.2070984 ,  0.0836641 ,  0.11529514, -0.13215053,\n",
       "         0.2149991 , -0.08547268,  0.4379532 , -0.00895514,  0.19786526],\n",
       "       [ 0.13622974, -0.00347805, -0.1327136 , -0.10897714,  0.02426509,\n",
       "        -0.09143984,  0.08887107, -0.15683764, -0.09054308,  0.46551514],\n",
       "       [-0.1208333 , -0.03490312,  0.01100595, -0.01161164,  0.51623154,\n",
       "        -0.00347004, -0.2633446 ,  0.07249236, -0.08588783,  0.05694823],\n",
       "       [ 0.29712307,  0.09136383,  0.03370313, -0.03661321,  0.29532   ,\n",
       "        -0.19510387,  0.14051439, -0.7360993 ,  0.16850366,  0.1420342 ],\n",
       "       [-0.05228012, -0.06674593,  0.13762522, -0.0414685 ,  0.223313  ,\n",
       "        -0.225095  , -0.02189118, -0.8226766 , -0.04114849,  0.01544718],\n",
       "       [-0.4442785 ,  0.1552419 ,  0.13449007,  0.33017704, -0.03361571,\n",
       "         0.23819815,  0.07797812, -0.09542368,  0.01914808, -0.3030565 ],\n",
       "       [ 0.0512217 ,  0.09433537,  0.00544517, -0.20728318, -0.01994779,\n",
       "        -0.14318827,  0.05671668,  0.11808565, -0.18636642,  0.01270655],\n",
       "       [-0.4423701 ,  0.25408822, -0.2716177 , -0.00274243,  0.45536885,\n",
       "         0.20060448, -0.2558809 ,  0.03094727, -0.30437803, -0.4638326 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_topic_embeddings[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.9581764e-06, 6.6142122e-05, 1.2562939e-04, ..., 1.0677581e-04,\n",
       "        2.9342832e-02, 9.0854871e-04],\n",
       "       [2.6542670e-03, 7.8307443e-05, 1.5476791e-04, ..., 1.0871761e-05,\n",
       "        4.9786659e-06, 2.2817678e-06],\n",
       "       [3.2476793e-05, 1.3679019e-03, 2.5892191e-04, ..., 3.5754842e-06,\n",
       "        1.2249883e-03, 1.1139793e-05],\n",
       "       ...,\n",
       "       [2.4093279e-07, 2.0777383e-04, 1.8917325e-04, ..., 1.6858297e-05,\n",
       "        3.9735093e-04, 3.7389019e-04],\n",
       "       [5.4072992e-05, 3.4708713e-04, 9.8067081e-05, ..., 1.0476207e-05,\n",
       "        1.7639939e-05, 6.9869757e-06],\n",
       "       [2.8947432e-02, 1.1348367e-05, 8.0334677e-05, ..., 1.4319714e-06,\n",
       "        6.2947379e-07, 1.1913493e-06]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_topic_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09132125,  0.06384595,  0.05307246, ...,  0.11143875,\n",
       "         0.0583626 , -0.00763746],\n",
       "       [-0.08056675,  0.05794175,  0.1096086 , ...,  0.02227432,\n",
       "        -0.11894352, -0.00020836],\n",
       "       [-0.01657696, -0.03343681,  0.13245685, ..., -0.05597756,\n",
       "        -0.07617768, -0.19336337],\n",
       "       ...,\n",
       "       [ 0.07739904, -0.05467948, -0.03028192, ...,  0.10324294,\n",
       "         0.16739792,  0.08804034],\n",
       "       [-0.06471275,  0.05133058, -0.05663415, ..., -0.03235153,\n",
       "         0.05406563, -0.08967461],\n",
       "       [ 0.03386614, -0.02483214, -0.02778944, ...,  0.0683267 ,\n",
       "         0.02738107, -0.07660544]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_w_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00559006,  0.05444232, -0.04536091,  0.18858178,  0.05161314,\n",
       "        0.09153585, -0.08774164,  0.2157619 ,  0.00775018, -0.11418495,\n",
       "        0.02302523,  0.05408015, -0.01412298,  0.25445768, -0.07876244,\n",
       "        0.09265354, -0.03667201,  0.13205457,  0.04882353,  0.1650994 ,\n",
       "       -0.11794744,  0.02307192, -0.12431027,  0.14413537, -0.14899743,\n",
       "       -0.10817293,  0.14402367, -0.02406884,  0.17764609, -0.04606541,\n",
       "        0.07183758,  0.00588563], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.57760793e-01,  6.92940652e-01, -8.86387587e-01,\n",
       "         2.32742757e-01,  9.04477775e-01, -3.78198564e-01,\n",
       "         6.45827174e-01, -4.35379326e-01,  1.85461867e+00,\n",
       "         2.75790304e-01,  1.85874355e+00, -1.98776901e+00,\n",
       "         1.86697483e+00,  2.70285726e+00,  7.95651555e-01,\n",
       "         1.16843891e+00, -2.92420149e-01,  4.81955886e-01,\n",
       "         2.64107585e+00, -1.38735437e+00,  3.86276960e-01,\n",
       "         2.49031812e-01, -2.64650798e+00,  5.41839659e-01,\n",
       "        -7.88616300e-01, -1.70691192e+00,  1.99204385e+00,\n",
       "         2.27924943e+00,  2.09087896e+00, -1.38838899e+00,\n",
       "         9.71355259e-01,  8.49284589e-01],\n",
       "       [-4.51019406e-01, -4.28144448e-02,  1.23017800e+00,\n",
       "        -5.81427395e-01, -1.11459160e+00, -8.72108400e-01,\n",
       "         9.18848634e-01,  3.49255800e-01,  3.26688600e+00,\n",
       "         2.31632733e+00,  2.68863630e+00,  1.14830577e+00,\n",
       "        -1.39267373e+00, -1.25154883e-01, -2.60801911e-01,\n",
       "         8.40898752e-01,  9.82761979e-01, -1.31821489e+00,\n",
       "        -6.96286619e-01,  2.29798532e+00, -8.11177135e-01,\n",
       "        -1.64898193e+00, -1.08448458e+00,  7.71293759e-01,\n",
       "         7.45800614e-01, -1.05909574e+00, -2.01475024e+00,\n",
       "        -1.15417123e+00, -1.64407611e+00, -9.28267062e-01,\n",
       "        -2.85443664e+00,  1.84412801e+00],\n",
       "       [-4.11032706e-01,  5.23106642e-02, -9.63664651e-01,\n",
       "         1.37577927e+00, -3.67689490e-01, -3.32272112e-01,\n",
       "         1.05511701e+00,  2.55790383e-01,  2.70997262e+00,\n",
       "        -7.53377199e-01, -7.46043026e-01,  2.27039009e-01,\n",
       "        -7.81528533e-01,  1.58539283e+00,  5.13564587e-01,\n",
       "         2.80245066e-01, -5.84948719e-01,  4.34450954e-02,\n",
       "         9.86856639e-01,  3.49760354e-01, -4.16215301e-01,\n",
       "        -5.74107692e-02, -1.97211123e+00,  1.80462062e+00,\n",
       "         3.44549835e-01,  1.43884230e+00,  2.00906205e+00,\n",
       "         7.62286067e-01,  2.97594666e-01,  1.75197795e-01,\n",
       "         6.29472792e-01,  2.32361937e+00],\n",
       "       [ 5.40623903e-01, -2.99204350e-01,  8.13646436e-01,\n",
       "         9.66286242e-01, -1.02164708e-02,  1.74680567e+00,\n",
       "        -1.36543465e+00,  6.80320203e-01, -2.81609559e+00,\n",
       "         1.51824176e-01, -2.42768839e-01,  1.37441528e+00,\n",
       "        -1.10611606e+00, -5.47208667e-01, -1.11248052e+00,\n",
       "        -7.30299532e-01, -2.95665294e-01,  8.54659736e-01,\n",
       "        -1.87525439e+00,  1.56026852e+00, -2.49921635e-01,\n",
       "        -1.01229344e-02,  2.54422998e+00, -9.16968226e-01,\n",
       "        -9.95169044e-01, -6.18969023e-01, -1.58860135e+00,\n",
       "        -1.94433534e+00,  1.44841075e-01, -8.64139140e-01,\n",
       "        -1.81704402e-01, -3.25662041e+00],\n",
       "       [-3.35653089e-02,  6.05760552e-02, -9.34092402e-02,\n",
       "        -3.89805973e-01,  1.23928837e-01, -7.75488466e-02,\n",
       "        -1.47666752e-01, -1.15529299e-02, -1.09079823e-01,\n",
       "        -3.19541276e-01,  2.18830600e-01, -2.92397171e-01,\n",
       "         3.69716614e-01, -4.91245300e-01, -4.13063690e-02,\n",
       "         2.91536897e-01,  2.12117478e-01,  1.41438589e-01,\n",
       "         2.14197636e-01, -2.87254393e-01, -5.93985692e-02,\n",
       "         1.95033491e-01, -1.31494090e-01, -2.00400695e-01,\n",
       "        -1.11025915e-01, -4.91962612e-01,  5.52823246e-02,\n",
       "         1.38732255e-01,  3.48945111e-02,  4.38818783e-01,\n",
       "        -3.81198600e-02,  1.08292758e-01],\n",
       "       [-3.03672105e-01,  1.46165922e-01, -2.70672351e-01,\n",
       "         3.20918024e-01, -1.69583291e-01, -4.03964281e-01,\n",
       "         4.31851119e-01,  7.01216161e-02,  1.60263312e+00,\n",
       "        -2.51917183e-01,  4.25564170e-01,  7.46656209e-03,\n",
       "        -2.40830090e-02,  4.76947308e-01,  2.50950426e-01,\n",
       "         9.00137424e-01,  8.88745338e-02, -5.02028465e-02,\n",
       "         5.10616839e-01,  2.23341316e-01, -2.54831612e-01,\n",
       "        -1.72272965e-01, -1.25812650e+00,  7.95979917e-01,\n",
       "         1.03043765e-01, -8.37258548e-02,  6.73689306e-01,\n",
       "         3.14238846e-01,  8.57291371e-02,  1.34176239e-01,\n",
       "        -1.30442053e-01,  1.42630243e+00],\n",
       "       [-3.04731655e+00,  1.71889246e+00,  3.32461715e-01,\n",
       "        -1.62863028e+00,  1.27386272e+00, -3.83477664e+00,\n",
       "        -5.73778343e+00, -3.67519236e+00, -4.01973677e+00,\n",
       "        -1.66967545e+01,  3.03267837e+00, -5.83901167e-01,\n",
       "         1.03508091e+01, -2.09624004e+01,  2.59123516e+00,\n",
       "         3.35171013e+01,  5.55173445e+00,  1.85645461e+00,\n",
       "        -2.18218255e+00, -5.38358450e+00, -6.24627590e-01,\n",
       "         2.23446012e+00, -3.31605363e+00,  6.21482432e-02,\n",
       "        -5.37933230e-01, -9.45471668e+00,  4.00959551e-02,\n",
       "        -3.78804088e+00,  1.22982514e+00,  1.82207565e+01,\n",
       "        -9.13462758e-01,  9.82451534e+00],\n",
       "       [-3.01159680e-01,  4.58158642e-01, -1.05513990e-01,\n",
       "        -1.07125044e-02,  2.63603628e-01, -4.13945496e-01,\n",
       "         5.58038116e-01, -1.50738299e-01,  2.05047774e+00,\n",
       "         9.27973151e-01,  2.20589280e+00, -8.54587317e-01,\n",
       "         8.41092050e-01,  1.65269613e+00,  3.81970048e-01,\n",
       "         1.21495235e+00,  1.77194864e-01, -4.80412543e-02,\n",
       "         1.34204495e+00, -8.00464898e-02, -1.43901631e-02,\n",
       "        -3.84904265e-01, -1.93301046e+00,  5.14456034e-01,\n",
       "        -3.83341074e-01, -1.65988779e+00,  5.02877593e-01,\n",
       "         9.37269688e-01,  8.93543184e-01, -1.27073264e+00,\n",
       "        -3.29514593e-01,  9.25537646e-01],\n",
       "       [ 2.20282054e+00, -2.19676304e+00, -4.44193077e+00,\n",
       "        -2.84252605e+01,  1.83375263e+00, -1.02507219e+01,\n",
       "         3.02903962e+00, -2.87342668e+00, -6.28237963e+00,\n",
       "        -4.09776783e+00, -1.03639812e+01, -1.05191355e+01,\n",
       "         8.40080929e+00, -2.30005836e+01,  2.23577261e+00,\n",
       "        -1.73989811e+01,  3.94388986e+00, -4.19326878e+00,\n",
       "         9.13354397e+00, -1.90552597e+01,  4.30414057e+00,\n",
       "         6.02747250e+00,  9.90070820e-01, -8.45333672e+00,\n",
       "         3.68604898e+00,  3.45372581e+00,  5.49608803e+00,\n",
       "         1.32406254e+01, -1.08005581e+01,  2.16440678e+01,\n",
       "         3.85246778e+00,  5.21795416e+00],\n",
       "       [-8.82331952e-02,  2.19874755e-01, -1.49049985e+00,\n",
       "         2.03077817e+00,  4.32753414e-01,  6.01201594e-01,\n",
       "         2.19791919e-01,  1.48397565e-01,  6.15558624e-01,\n",
       "        -1.88912487e+00, -1.57170010e+00, -3.72902870e-01,\n",
       "         1.36481032e-01,  2.05552936e+00,  5.20110011e-01,\n",
       "         2.19874263e-01, -1.18954885e+00,  1.14503896e+00,\n",
       "         1.30455244e+00, -6.84042275e-01, -9.65062529e-03,\n",
       "         8.33108783e-01, -1.30308008e+00,  1.24020624e+00,\n",
       "        -5.58756530e-01,  1.14117634e+00,  2.89724422e+00,\n",
       "         1.19333661e+00,  1.76983190e+00,  6.40368089e-03,\n",
       "         2.19974136e+00,  6.05985940e-01],\n",
       "       [-1.60334453e-01,  3.78117293e-01,  6.85027361e-01,\n",
       "        -7.07152605e-01,  2.22675502e-01, -2.52841651e-01,\n",
       "         6.25713170e-03, -1.50619209e-01,  9.96828794e-01,\n",
       "         1.53969061e+00,  3.08087730e+00, -5.56444228e-01,\n",
       "         8.79469275e-01,  5.71199834e-01, -3.50208133e-02,\n",
       "         1.35215735e+00,  6.74780250e-01, -1.95870310e-01,\n",
       "         4.80120987e-01,  3.55463922e-01, -4.87049371e-02,\n",
       "        -6.21517897e-01, -9.19217944e-01, -3.07832241e-01,\n",
       "        -5.48327982e-01, -2.70349884e+00, -9.97227669e-01,\n",
       "         8.34274217e-02,  4.21238005e-01, -1.40375316e+00,\n",
       "        -1.19434726e+00, -1.30423367e-01],\n",
       "       [ 1.57597512e-01,  6.44550383e-01, -2.92151421e-03,\n",
       "        -2.65204608e-01,  1.30308616e+00,  6.27051413e-01,\n",
       "        -1.10083234e+00, -4.92986917e-01, -1.57532692e+00,\n",
       "        -1.00194499e-01,  2.29858756e+00, -1.61245024e+00,\n",
       "         2.57596827e+00,  7.88679838e-01,  1.00349724e-01,\n",
       "         1.85853219e+00,  1.56636938e-01,  1.13628328e+00,\n",
       "         1.08530271e+00, -1.25081968e+00,  5.31205833e-01,\n",
       "         5.60665071e-01, -3.26458514e-01, -1.05252326e+00,\n",
       "        -1.67882943e+00, -3.40797162e+00,  2.16223836e-01,\n",
       "         7.62056053e-01,  2.25552535e+00, -1.18877208e+00,\n",
       "         6.98165417e-01, -1.94459283e+00],\n",
       "       [ 1.50830492e-01,  4.96469140e-01,  3.01555455e-01,\n",
       "        -2.79265046e-01,  8.85855198e-01,  4.17512357e-01,\n",
       "        -2.18689278e-01, -1.26843125e-01, -1.13705307e-01,\n",
       "         1.62040353e+00,  2.74554658e+00, -1.32586658e+00,\n",
       "         1.38066518e+00,  1.83914661e+00, -6.73329905e-02,\n",
       "         4.97350127e-01, -1.10493749e-02,  4.30080771e-01,\n",
       "         1.17665291e+00, -4.16310132e-01,  3.85107040e-01,\n",
       "        -1.48624629e-01, -6.34521067e-01, -7.43578136e-01,\n",
       "        -1.20982361e+00, -2.85781741e+00, -2.78010368e-01,\n",
       "         8.84987473e-01,  1.58663642e+00, -2.30098295e+00,\n",
       "        -8.24550465e-02, -1.57662416e+00],\n",
       "       [-5.50828695e-01,  2.50880718e-01,  7.50859976e-01,\n",
       "        -6.93531871e-01, -6.43950760e-01, -1.14084554e+00,\n",
       "         1.00459814e+00,  2.57699192e-02,  3.51355481e+00,\n",
       "         1.93418872e+00,  3.07177830e+00,  1.74038529e-01,\n",
       "        -2.31798187e-01,  4.65905249e-01,  1.55907512e-01,\n",
       "         1.64024949e+00,  9.36564445e-01, -1.09681392e+00,\n",
       "         3.48984420e-01,  1.27040672e+00, -5.20306110e-01,\n",
       "        -1.34424937e+00, -2.01918936e+00,  8.19682002e-01,\n",
       "         5.02702534e-01, -1.62520981e+00, -1.04533601e+00,\n",
       "        -1.61469966e-01, -7.44807839e-01, -9.24053073e-01,\n",
       "        -2.20104480e+00,  2.27263522e+00],\n",
       "       [-4.21905369e-01, -4.71765921e-03, -7.62243629e-01,\n",
       "         1.42470872e+00, -5.87148428e-01, -3.21450174e-01,\n",
       "         1.07526124e+00,  3.98521304e-01,  2.87959838e+00,\n",
       "        -5.16267240e-01, -6.99398220e-01,  6.14205837e-01,\n",
       "        -1.18725288e+00,  1.39706397e+00,  3.85492384e-01,\n",
       "         2.81253576e-01, -5.05964100e-01, -1.21161282e-01,\n",
       "         6.07298911e-01,  8.06770563e-01, -5.47465026e-01,\n",
       "        -2.81393886e-01, -1.79573822e+00,  1.89449918e+00,\n",
       "         4.70402479e-01,  1.53324986e+00,  1.60082102e+00,\n",
       "         3.58015865e-01, -5.30557185e-02,  1.44867584e-01,\n",
       "         2.41396159e-01,  2.38864923e+00],\n",
       "       [ 6.62054300e-01,  3.37500781e-01,  5.57735801e-01,\n",
       "         2.45503679e-01,  1.36794913e+00,  1.98645997e+00,\n",
       "        -2.48610759e+00, -1.37267470e-01, -4.80660963e+00,\n",
       "        -5.77719152e-01,  1.37641644e+00, -6.38415575e-01,\n",
       "         1.99749994e+00, -5.40878892e-01, -7.48807907e-01,\n",
       "         1.40641379e+00, -5.97302131e-02,  1.84030986e+00,\n",
       "        -6.39520109e-01, -5.79537749e-01,  5.34762979e-01,\n",
       "         8.50541055e-01,  2.22722244e+00, -2.17734694e+00,\n",
       "        -2.31085205e+00, -3.52186871e+00, -1.07792962e+00,\n",
       "        -8.73553872e-01,  2.24096966e+00, -1.11103606e+00,\n",
       "         8.62435997e-01, -4.71932936e+00],\n",
       "       [ 3.26473564e-01, -4.35699999e-01,  2.59925103e+00,\n",
       "        -9.51997697e-01, -9.01390791e-01,  7.58070648e-01,\n",
       "        -8.62050176e-01,  7.08915830e-01, -1.14007711e+00,\n",
       "         3.08080482e+00,  2.87707257e+00,  2.13852167e+00,\n",
       "        -1.98230219e+00, -1.94951463e+00, -1.59402859e+00,\n",
       "        -3.95940453e-01,  1.15143776e+00, -7.62673557e-01,\n",
       "        -2.96910667e+00,  3.36339164e+00, -7.12908924e-01,\n",
       "        -1.60796893e+00,  2.60579681e+00, -1.38654923e+00,\n",
       "        -2.22303987e-01, -2.37508965e+00, -4.84003592e+00,\n",
       "        -3.24756312e+00, -2.05902243e+00, -1.60443032e+00,\n",
       "        -3.49886155e+00, -2.63093090e+00],\n",
       "       [-4.05444413e-01, -4.34672713e-01, -4.42042470e-01,\n",
       "         1.79654539e+00, -1.30099487e+00, -8.59607011e-02,\n",
       "         8.87401342e-01,  8.03346694e-01,  2.47048283e+00,\n",
       "        -1.00339210e+00, -2.02405190e+00,  2.06073236e+00,\n",
       "        -2.68403244e+00,  9.86908972e-02, -6.35944530e-02,\n",
       "        -2.97095150e-01, -4.56316531e-01, -3.16159248e-01,\n",
       "        -8.69315386e-01,  1.97627831e+00, -9.97931719e-01,\n",
       "        -4.31791395e-01, -6.21117830e-01,  2.09663033e+00,\n",
       "         9.45929885e-01,  2.92801952e+00,  8.99133742e-01,\n",
       "        -9.77129340e-01, -1.28964722e+00,  1.04292405e+00,\n",
       "        -2.57151216e-01,  2.41552162e+00],\n",
       "       [ 2.19263732e-01, -8.44123423e-01, -1.69795036e-01,\n",
       "        -6.07150942e-02, -9.27681446e-01,  1.77278876e-01,\n",
       "         6.24294281e-02,  7.96724379e-01, -7.90519655e-01,\n",
       "        -1.24669862e+00, -3.19766784e+00,  1.71756577e+00,\n",
       "        -2.35108852e+00, -2.37084651e+00, -5.73633313e-01,\n",
       "        -2.41041183e+00, -2.05562532e-01, -1.53777927e-01,\n",
       "        -1.38759172e+00,  1.03330743e+00, -5.53933084e-01,\n",
       "         2.60704219e-01,  1.61319041e+00,  2.98897922e-01,\n",
       "         6.88829541e-01,  2.84676623e+00, -1.63123608e-02,\n",
       "        -1.06292605e+00, -2.05615139e+00,  2.24377418e+00,\n",
       "         5.51874638e-02,  2.25411117e-01],\n",
       "       [-4.49936837e-01, -3.50512266e-01, -1.87506720e-01,\n",
       "         9.87973094e-01, -1.24066496e+00, -4.82334673e-01,\n",
       "         8.11725140e-01,  5.90457857e-01,  2.42391658e+00,\n",
       "        -8.03736806e-01, -1.30351472e+00,  1.71811974e+00,\n",
       "        -2.12192273e+00, -6.37154102e-01,  1.30190402e-02,\n",
       "         2.62588948e-01,  2.17343755e-02, -5.31931520e-01,\n",
       "        -8.02236497e-01,  1.66479588e+00, -9.31076705e-01,\n",
       "        -5.02962649e-01, -7.13596344e-01,  1.71839631e+00,\n",
       "         1.00447977e+00,  2.15545726e+00,  3.76727223e-01,\n",
       "        -9.22742486e-01, -1.51990688e+00,  1.33404315e+00,\n",
       "        -7.29661524e-01,  2.63036251e+00],\n",
       "       [ 5.38357496e-01,  4.47055310e-01,  2.49706841e+00,\n",
       "        -2.19772935e+00,  9.70963299e-01,  9.18165207e-01,\n",
       "        -1.54003441e+00, -2.21556067e-01, -2.44445872e+00,\n",
       "         4.08210850e+00,  5.97850990e+00, -9.28261638e-01,\n",
       "         1.69696975e+00, -2.16909051e-02, -1.20356452e+00,\n",
       "         7.05855012e-01,  1.24037039e+00,  8.13109875e-02,\n",
       "        -6.73041523e-01,  6.96472645e-01,  3.92915487e-01,\n",
       "        -9.27847683e-01,  1.66890037e+00, -3.07063198e+00,\n",
       "        -1.89178646e+00, -6.25881767e+00, -4.29127789e+00,\n",
       "        -9.86345887e-01,  7.13421583e-01, -3.62062645e+00,\n",
       "        -2.42727685e+00, -4.71716595e+00],\n",
       "       [ 1.95773557e-01,  6.90488636e-01, -9.29347277e-01,\n",
       "         8.11000764e-01,  1.53866720e+00,  9.06663001e-01,\n",
       "        -7.48949409e-01, -3.94496977e-01, -1.24951589e+00,\n",
       "        -8.44779491e-01,  9.92447674e-01, -1.94891727e+00,\n",
       "         2.46177626e+00,  2.24408674e+00,  4.32715863e-01,\n",
       "         1.19010723e+00, -6.92049921e-01,  1.60971498e+00,\n",
       "         1.91835153e+00, -1.82222271e+00,  6.44751549e-01,\n",
       "         1.02358222e+00, -9.16709721e-01, -4.20439303e-01,\n",
       "        -1.78298891e+00, -2.20394921e+00,  1.89529431e+00,\n",
       "         1.66103256e+00,  3.15372801e+00, -1.37992394e+00,\n",
       "         2.06460357e+00, -1.70527124e+00],\n",
       "       [-3.41465801e-01,  1.61501601e-01, -4.09835905e-01,\n",
       "         4.68851924e-01, -1.38577670e-01, -4.34845150e-01,\n",
       "         6.36801481e-01,  6.17439747e-02,  2.00952458e+00,\n",
       "        -9.61217284e-02,  4.92821932e-01, -1.34842068e-01,\n",
       "        -4.07369807e-02,  9.52754259e-01,  3.52362663e-01,\n",
       "         7.33794630e-01, -2.72103436e-02, -6.67859018e-02,\n",
       "         8.22836161e-01,  1.75434142e-01, -2.51749516e-01,\n",
       "        -1.96668684e-01, -1.56993413e+00,  9.97976780e-01,\n",
       "         1.01204753e-01,  9.28666443e-03,  9.39024150e-01,\n",
       "         5.76106489e-01,  2.33892575e-01, -1.15060329e-01,\n",
       "        -9.81246680e-03,  1.58816123e+00],\n",
       "       [-4.37992811e-01, -9.54713225e-02, -5.78212500e-01,\n",
       "         1.06854820e+00, -6.98276341e-01, -4.55558360e-01,\n",
       "         9.21079874e-01,  3.52387607e-01,  2.56498814e+00,\n",
       "        -6.77751124e-01, -7.72232234e-01,  7.64304042e-01,\n",
       "        -1.19218278e+00,  6.21038556e-01,  3.07355255e-01,\n",
       "         3.38550180e-01, -2.27123961e-01, -2.34908938e-01,\n",
       "         2.58741319e-01,  8.54187250e-01, -6.09266520e-01,\n",
       "        -2.58100986e-01, -1.46390605e+00,  1.67275417e+00,\n",
       "         6.04182243e-01,  1.53600574e+00,  1.21458578e+00,\n",
       "         6.19600490e-02, -4.66661453e-01,  6.05521977e-01,\n",
       "        -3.37721482e-02,  2.43156910e+00],\n",
       "       [-4.90293533e-01, -4.04583186e-01,  1.09862173e+00,\n",
       "         5.50663710e-01, -1.75718212e+00, -5.76063454e-01,\n",
       "         1.02060115e+00,  8.32708001e-01,  3.47491980e+00,\n",
       "         1.52389956e+00,  8.43663275e-01,  2.46110702e+00,\n",
       "        -3.08102560e+00, -5.13758421e-01, -4.83408213e-01,\n",
       "         3.33922505e-01,  5.85881472e-01, -1.33858252e+00,\n",
       "        -1.69117069e+00,  3.30068421e+00, -1.24458647e+00,\n",
       "        -1.69035554e+00, -4.38167095e-01,  1.52533472e+00,\n",
       "         1.26750243e+00,  1.01725328e+00, -1.87631774e+00,\n",
       "        -2.03654718e+00, -2.49136734e+00, -1.69598654e-01,\n",
       "        -2.81342244e+00,  2.35053682e+00],\n",
       "       [ 1.35177910e-01,  1.95696518e-01, -7.84384370e-01,\n",
       "         1.16357338e+00,  6.88815892e-01,  8.72777164e-01,\n",
       "        -7.76067138e-01,  2.93131769e-02, -1.45356441e+00,\n",
       "        -1.87039578e+00, -8.67311716e-01, -4.22591150e-01,\n",
       "         8.95088971e-01,  3.97567987e-01,  1.03926197e-01,\n",
       "         9.35720861e-01, -6.29705906e-01,  1.29659796e+00,\n",
       "         4.51065004e-01, -8.16753566e-01,  1.48506209e-01,\n",
       "         9.20293629e-01,  6.22772425e-02,  2.50710100e-02,\n",
       "        -9.87707555e-01, -3.20802450e-01,  1.44291306e+00,\n",
       "         3.86405587e-01,  1.65685761e+00,  3.45494300e-01,\n",
       "         1.68376946e+00, -9.20098722e-01],\n",
       "       [ 6.72416329e-01, -9.20858979e-02, -4.77854490e-01,\n",
       "         2.18673682e+00,  8.75282347e-01,  2.48593831e+00,\n",
       "        -1.96781588e+00,  4.97479945e-01, -4.35452986e+00,\n",
       "        -2.17339921e+00, -2.05494404e+00,  4.89954084e-01,\n",
       "         8.00857469e-02,  2.49257445e-01, -6.44628644e-01,\n",
       "        -4.39663559e-01, -1.23625553e+00,  2.18205571e+00,\n",
       "        -9.23243582e-01, -1.21853456e-01,  1.65858820e-01,\n",
       "         1.29792607e+00,  2.41820168e+00, -7.91492879e-01,\n",
       "        -1.77671134e+00, -3.10502201e-02,  7.49828458e-01,\n",
       "        -8.92341971e-01,  2.08277845e+00, -2.86687016e-01,\n",
       "         2.30642724e+00, -3.86456966e+00],\n",
       "       [-1.14594132e-01,  1.22070476e-01, -1.44379869e-01,\n",
       "         7.67351985e-02,  1.31788880e-01, -9.82725471e-02,\n",
       "        -5.50989062e-03, -1.86838359e-02,  4.09364372e-01,\n",
       "        -2.00180948e-01,  5.11164963e-01, -2.17120141e-01,\n",
       "         2.89756298e-01,  1.74070209e-01,  5.26779890e-02,\n",
       "         5.75869620e-01,  1.40676618e-01,  1.67829365e-01,\n",
       "         3.61241996e-01, -5.80786318e-02, -7.09659234e-02,\n",
       "         6.98931962e-02, -5.73756695e-01,  1.30624786e-01,\n",
       "        -2.36642137e-01, -5.60099900e-01,  2.68201500e-01,\n",
       "         2.23120809e-01,  3.01047176e-01,  1.00929961e-02,\n",
       "         2.48952955e-03,  3.31200153e-01],\n",
       "       [-4.46792811e-01,  5.69132626e-01, -6.13311529e-01,\n",
       "         1.27417520e-01,  3.52516174e-01, -7.63285220e-01,\n",
       "         1.06736708e+00, -2.80032068e-01,  3.03618693e+00,\n",
       "         6.64448678e-01,  2.02448559e+00, -1.35166633e+00,\n",
       "         1.10776925e+00,  2.34479594e+00,  7.69012928e-01,\n",
       "         1.24778545e+00, -3.39653268e-02, -7.11765885e-02,\n",
       "         2.22635198e+00, -6.05080962e-01,  5.06058335e-02,\n",
       "        -2.50641882e-01, -2.93960643e+00,  1.00681984e+00,\n",
       "        -2.29492560e-01, -1.27102470e+00,  1.53992772e+00,\n",
       "         1.79172301e+00,  1.22489214e+00, -1.16787016e+00,\n",
       "         1.43604398e-01,  1.88288403e+00],\n",
       "       [ 1.79255021e+00, -2.11369371e+00,  1.95211303e+00,\n",
       "        -2.83681726e+00, -1.31356597e+00,  1.30908930e+00,\n",
       "         1.12798655e+00,  1.85704648e+00, -2.66194320e+00,\n",
       "         6.14001513e+00, -2.83383417e+00,  2.04995370e+00,\n",
       "        -6.79041195e+00, -3.74038547e-01, -2.56821489e+00,\n",
       "        -1.66670742e+01, -6.60938859e-01, -1.95296693e+00,\n",
       "        -2.13709450e+00,  3.01757026e+00, -1.58393562e-01,\n",
       "        -9.46927428e-01,  6.26847219e+00, -2.79391146e+00,\n",
       "         1.07912433e+00,  3.80929852e+00, -4.70272875e+00,\n",
       "        -1.27822804e+00, -5.42266560e+00, -2.96069956e+00,\n",
       "        -2.02901387e+00, -5.95090723e+00]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.46229323e-02,  1.00070061e-02, -1.43917352e-02,\n",
       "         2.56802281e-03, -3.66200729e-05,  3.88381211e-03,\n",
       "        -9.58103314e-03, -3.79253388e-03, -3.20388447e-03,\n",
       "        -9.68570262e-03],\n",
       "       [-5.87334018e-03, -7.74201099e-03, -1.63234342e-02,\n",
       "        -1.80622004e-03, -1.69612712e-03, -3.15694674e-03,\n",
       "        -2.67671468e-03, -9.32745636e-03,  4.14251350e-03,\n",
       "         5.37557947e-03],\n",
       "       [ 7.78287929e-03, -8.10541306e-03, -9.05863661e-03,\n",
       "        -3.13145341e-03,  6.40133582e-03, -9.14698525e-04,\n",
       "        -3.00810719e-03,  2.94189272e-03, -3.06034321e-03,\n",
       "         1.01561295e-02],\n",
       "       [-8.63092020e-03, -9.98362713e-03,  2.42341636e-03,\n",
       "         6.84057176e-03,  9.59877204e-03,  5.46535384e-03,\n",
       "         1.05908290e-02,  1.03722513e-02,  2.74874712e-03,\n",
       "         5.13785228e-04],\n",
       "       [-6.19821600e-04,  4.41455981e-03,  4.71293414e-03,\n",
       "        -9.49300826e-03, -4.08250466e-03,  1.10118845e-02,\n",
       "        -3.50344693e-03, -2.48622731e-03,  1.30777154e-02,\n",
       "         7.52505672e-04],\n",
       "       [-2.31470983e-03,  1.24498429e-02, -5.30273281e-03,\n",
       "         2.46462622e-03,  4.44593513e-03, -9.27733537e-03,\n",
       "        -9.66867805e-03,  6.52574934e-03,  7.08387047e-03,\n",
       "         3.23292078e-03],\n",
       "       [ 4.68048267e-02, -1.00470688e-02, -5.23759797e-02,\n",
       "         2.03951355e-02, -3.32640571e-04, -3.15693021e-02,\n",
       "        -3.85454521e-02, -2.09554490e-02, -3.33304890e-02,\n",
       "         7.06642726e-03],\n",
       "       [ 1.08114360e-02, -3.88590363e-03, -2.01990060e-03,\n",
       "         8.98970105e-03, -1.00764576e-02,  8.19369406e-03,\n",
       "        -1.77625928e-03,  6.53846841e-03, -5.51224221e-03,\n",
       "        -7.19089899e-03],\n",
       "       [-5.14488444e-02, -1.25520304e-02,  4.22644168e-02,\n",
       "        -2.03237012e-02,  2.78962543e-03,  3.53626907e-02,\n",
       "         3.17620561e-02,  2.91340891e-02,  3.45825404e-02,\n",
       "        -6.62558759e-03],\n",
       "       [-1.02281421e-02,  8.35016929e-03,  1.98661094e-03,\n",
       "         8.75501055e-03, -8.83500092e-03,  3.08847986e-03,\n",
       "        -6.16822951e-03,  4.37277276e-03,  6.70232577e-03,\n",
       "         1.51995267e-03],\n",
       "       [ 2.31454466e-02, -1.63935008e-03,  4.03902587e-03,\n",
       "        -3.35570658e-03, -2.14774068e-03, -1.98587915e-03,\n",
       "        -3.45319095e-05, -3.92554235e-03,  3.74543481e-03,\n",
       "         1.35664269e-03],\n",
       "       [ 7.43595371e-03,  8.46979115e-03,  5.74850291e-03,\n",
       "         1.14516867e-02,  7.30088446e-04, -3.88509384e-03,\n",
       "         5.51427482e-03, -8.98696389e-03, -5.62011218e-03,\n",
       "         7.62474840e-04],\n",
       "       [ 5.11275325e-03,  2.09741949e-04, -8.86526890e-03,\n",
       "         5.53667871e-03, -1.07896989e-02,  6.62478997e-05,\n",
       "         3.28556239e-03,  3.04981973e-03,  6.59973174e-03,\n",
       "        -3.28465574e-03],\n",
       "       [ 1.60780605e-02, -1.27367943e-03, -5.86967450e-03,\n",
       "         7.83233624e-03,  1.49838754e-03, -1.89339798e-02,\n",
       "        -7.52872042e-03, -9.70411231e-04, -9.54746455e-03,\n",
       "         2.65071727e-03],\n",
       "       [ 1.41635183e-02,  3.20987520e-03, -1.44601548e-02,\n",
       "        -6.81841886e-03, -4.79633594e-03, -1.19972192e-02,\n",
       "        -3.04321991e-03, -6.03641709e-03, -7.44559616e-03,\n",
       "        -3.32704303e-03],\n",
       "       [-2.20115501e-02,  2.06985697e-03,  1.23657547e-02,\n",
       "        -7.62467622e-04, -5.22251474e-03,  1.05855335e-02,\n",
       "         1.19203934e-02,  4.14712448e-03,  6.62296498e-03,\n",
       "         2.36347667e-03],\n",
       "       [-2.98714382e-03, -1.59433333e-03,  5.90875698e-03,\n",
       "        -4.75750910e-03,  6.22806698e-03, -8.62152781e-03,\n",
       "        -1.34010799e-03, -7.58266868e-03,  4.67959978e-03,\n",
       "        -5.11211576e-03],\n",
       "       [ 6.03787694e-03, -7.63774617e-03, -7.32623879e-03,\n",
       "         8.25403444e-03, -1.49911514e-03,  3.71220964e-03,\n",
       "        -1.09783066e-02, -5.07359626e-03, -1.20091913e-02,\n",
       "         1.46870082e-03],\n",
       "       [ 1.81039376e-03, -1.30956145e-02,  8.35777260e-03,\n",
       "         7.11766770e-04,  6.04064204e-03,  1.94727704e-02,\n",
       "         6.80934638e-03,  1.49956753e-03, -4.31544194e-03,\n",
       "        -1.47643324e-03],\n",
       "       [-5.44132246e-03,  6.45782379e-03, -9.85332765e-03,\n",
       "         5.48669090e-03, -1.61655748e-03, -3.44748679e-03,\n",
       "         7.96557311e-03, -2.46108649e-03, -1.44366464e-02,\n",
       "         4.85087512e-03],\n",
       "       [-8.82615335e-03,  9.54582077e-03,  1.10027066e-03,\n",
       "        -1.16283773e-02, -9.38636344e-03,  5.87139884e-03,\n",
       "         6.67420030e-03,  1.27697717e-02,  1.00016715e-02,\n",
       "        -1.27793534e-03],\n",
       "       [-1.51868572e-03,  1.34728607e-02,  1.57713294e-02,\n",
       "        -2.73691141e-03,  7.38640595e-03,  5.46094403e-03,\n",
       "        -4.36822977e-03, -5.67811355e-03,  3.81732965e-03,\n",
       "        -7.97819253e-03],\n",
       "       [ 3.01115727e-03, -2.62183812e-03, -1.39109287e-02,\n",
       "        -3.83109157e-03, -8.09945690e-04, -8.61140713e-03,\n",
       "         4.98610782e-03,  7.21140020e-03, -8.50693136e-03,\n",
       "         6.65591890e-03],\n",
       "       [-7.28014740e-04, -1.14127342e-03,  6.00485178e-03,\n",
       "         5.15386276e-03, -3.59033654e-03,  1.18331425e-03,\n",
       "        -1.22631835e-02, -1.01875700e-02, -1.28622893e-02,\n",
       "         9.70872492e-03],\n",
       "       [ 1.52248777e-02, -6.67819614e-03, -6.89320778e-03,\n",
       "         4.92652971e-03,  1.30936988e-02, -6.85528805e-03,\n",
       "        -2.62865121e-03, -3.24695650e-03, -1.63790188e-03,\n",
       "         3.58302915e-03],\n",
       "       [ 5.68372617e-03,  1.23013221e-02,  2.43106065e-03,\n",
       "         4.18402115e-03,  1.00077584e-03, -3.74273444e-03,\n",
       "         1.20569225e-02, -7.24039180e-03, -1.10534392e-02,\n",
       "        -1.18865240e-02],\n",
       "       [ 2.28108349e-03, -1.27712535e-02,  1.40135428e-02,\n",
       "        -1.26029542e-02,  8.16473924e-03,  3.62813938e-03,\n",
       "        -4.34132386e-03,  1.12163639e-02,  9.19160713e-03,\n",
       "         1.93542149e-03],\n",
       "       [-3.09186080e-03,  1.04014780e-02, -1.23636723e-02,\n",
       "        -7.42464745e-03, -7.30471080e-03, -1.32962754e-02,\n",
       "        -1.56925935e-02, -8.41937400e-03, -4.63367999e-03,\n",
       "        -1.15145510e-02],\n",
       "       [ 1.04351426e-02, -7.98565522e-03, -1.34267081e-02,\n",
       "         1.71474479e-02,  5.48876356e-04,  2.63456535e-03,\n",
       "        -4.48995735e-03, -5.52697433e-03, -3.29413591e-03,\n",
       "         6.50040573e-03],\n",
       "       [-4.17392515e-02,  2.24123243e-03,  1.39891310e-02,\n",
       "        -1.13424771e-02,  4.63787885e-03,  1.91243161e-02,\n",
       "         3.51724401e-02,  7.91706517e-03,  2.14308128e-02,\n",
       "         1.16460789e-02]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_w_mean_topic_infer[:, :10, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00559006,  0.05444232, -0.04536091,  0.18858178,  0.05161314,\n",
       "        0.09153585, -0.08774164,  0.2157619 ,  0.00775018, -0.11418495,\n",
       "        0.02302523,  0.05408015, -0.01412298,  0.25445768, -0.07876244,\n",
       "        0.09265354, -0.03667201,  0.13205457,  0.04882353,  0.1650994 ,\n",
       "       -0.11794744,  0.02307192, -0.12431027,  0.14413537, -0.14899743,\n",
       "       -0.10817293,  0.14402367, -0.02406884,  0.17764609, -0.04606541,\n",
       "        0.07183758,  0.00588563], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enc_state_infer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-ee7c3cd147b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_enc_state_infer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_means_topic_infer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdebug_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menc_state_infer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans_topic_infer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'enc_state_infer' is not defined"
     ]
    }
   ],
   "source": [
    "_enc_state_infer, _means_topic_infer = debug_value([enc_state_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_enc_state_infer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_means_topic_infer[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
