{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from data_structure import get_batches\n",
    "from hntm import HierarchicalNeuralTopicModel\n",
    "from tree import get_descendant_idxs\n",
    "from evaluation import validate, get_hierarchical_affinity, get_topic_specialization, print_topic_sample\n",
    "from configure import get_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(nb_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu\n",
    "np.random.seed(config.seed)\n",
    "random.seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.path_data,'rb'))\n",
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)\n",
    "config.dim_bow = len(bow_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     10
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables, model):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, model, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    return _variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = []\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "ppl_min = np.inf\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','','','','','VALID:','','','','','TEST:','', 'SPEC:', '', '', 'HIER:', ''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL','REG','LOSS','PPL','NLL','KL','REG','LOSS','PPL', '1', '2', '3', 'CHILD', 'OTHER']]))))\n",
    "\n",
    "cmd_rm = 'rm -r %s' % config.dir_model\n",
    "res = subprocess.call(cmd_rm.split())\n",
    "cmd_mk = 'mkdir %s' % config.dir_model\n",
    "res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "def update_checkpoint(config, checkpoint, global_step):\n",
    "    checkpoint.append(config.path_model + '-%i' % global_step)\n",
    "    if len(checkpoint) > config.max_to_keep:\n",
    "        path_model = checkpoint.pop(0) + '.*'\n",
    "        for p in glob.glob(path_model):\n",
    "            os.remove(p)\n",
    "    cPickle.dump(checkpoint, open(config.path_checkpoint, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "model = HierarchicalNeuralTopicModel(config)\n",
    "sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(max_to_keep=config.max_to_keep)\n",
    "update_tree_flg = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train & validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th colspan=\"4\" halign=\"left\"></th>\n",
       "      <th>VALID:</th>\n",
       "      <th colspan=\"4\" halign=\"left\"></th>\n",
       "      <th>TEST:</th>\n",
       "      <th></th>\n",
       "      <th>SPEC:</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "      <th>HIER:</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>CHILD</th>\n",
       "      <th>OTHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>79</td>\n",
       "      <td>8</td>\n",
       "      <td>463</td>\n",
       "      <td>9179.00</td>\n",
       "      <td>1881</td>\n",
       "      <td>9160.66</td>\n",
       "      <td>18.09</td>\n",
       "      <td>0.25</td>\n",
       "      <td>9157.78</td>\n",
       "      <td>1795</td>\n",
       "      <td>9137.22</td>\n",
       "      <td>20.39</td>\n",
       "      <td>0.17</td>\n",
       "      <td>9158.85</td>\n",
       "      <td>1797</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>76</td>\n",
       "      <td>17</td>\n",
       "      <td>360</td>\n",
       "      <td>9138.35</td>\n",
       "      <td>1819</td>\n",
       "      <td>9118.60</td>\n",
       "      <td>19.56</td>\n",
       "      <td>0.20</td>\n",
       "      <td>9141.21</td>\n",
       "      <td>1772</td>\n",
       "      <td>9120.32</td>\n",
       "      <td>20.75</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9141.49</td>\n",
       "      <td>1772</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>77</td>\n",
       "      <td>26</td>\n",
       "      <td>257</td>\n",
       "      <td>9119.68</td>\n",
       "      <td>1791</td>\n",
       "      <td>9099.47</td>\n",
       "      <td>20.03</td>\n",
       "      <td>0.18</td>\n",
       "      <td>9136.41</td>\n",
       "      <td>1764</td>\n",
       "      <td>9115.47</td>\n",
       "      <td>20.80</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9136.27</td>\n",
       "      <td>1764</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>82</td>\n",
       "      <td>35</td>\n",
       "      <td>154</td>\n",
       "      <td>9108.40</td>\n",
       "      <td>1775</td>\n",
       "      <td>9087.96</td>\n",
       "      <td>20.24</td>\n",
       "      <td>0.17</td>\n",
       "      <td>9131.93</td>\n",
       "      <td>1758</td>\n",
       "      <td>9111.07</td>\n",
       "      <td>20.73</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9132.58</td>\n",
       "      <td>1759</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>77</td>\n",
       "      <td>44</td>\n",
       "      <td>51</td>\n",
       "      <td>9100.47</td>\n",
       "      <td>1764</td>\n",
       "      <td>9079.91</td>\n",
       "      <td>20.35</td>\n",
       "      <td>0.16</td>\n",
       "      <td>9128.96</td>\n",
       "      <td>1753</td>\n",
       "      <td>9108.36</td>\n",
       "      <td>20.48</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9129.66</td>\n",
       "      <td>1755</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>76</td>\n",
       "      <td>52</td>\n",
       "      <td>515</td>\n",
       "      <td>9094.68</td>\n",
       "      <td>1755</td>\n",
       "      <td>9074.06</td>\n",
       "      <td>20.42</td>\n",
       "      <td>0.15</td>\n",
       "      <td>9130.43</td>\n",
       "      <td>1755</td>\n",
       "      <td>9109.65</td>\n",
       "      <td>20.66</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9129.66</td>\n",
       "      <td>1755</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35000</th>\n",
       "      <td>76</td>\n",
       "      <td>61</td>\n",
       "      <td>412</td>\n",
       "      <td>9090.26</td>\n",
       "      <td>1749</td>\n",
       "      <td>9069.71</td>\n",
       "      <td>20.47</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9127.13</td>\n",
       "      <td>1750</td>\n",
       "      <td>9106.54</td>\n",
       "      <td>20.48</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9126.85</td>\n",
       "      <td>1749</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40000</th>\n",
       "      <td>76</td>\n",
       "      <td>70</td>\n",
       "      <td>309</td>\n",
       "      <td>9086.61</td>\n",
       "      <td>1744</td>\n",
       "      <td>9066.14</td>\n",
       "      <td>20.50</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9126.26</td>\n",
       "      <td>1749</td>\n",
       "      <td>9105.65</td>\n",
       "      <td>20.50</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9125.06</td>\n",
       "      <td>1748</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45000</th>\n",
       "      <td>76</td>\n",
       "      <td>79</td>\n",
       "      <td>206</td>\n",
       "      <td>9083.46</td>\n",
       "      <td>1739</td>\n",
       "      <td>9063.04</td>\n",
       "      <td>20.52</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9124.95</td>\n",
       "      <td>1747</td>\n",
       "      <td>9104.35</td>\n",
       "      <td>20.50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9124.50</td>\n",
       "      <td>1746</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000</th>\n",
       "      <td>77</td>\n",
       "      <td>88</td>\n",
       "      <td>103</td>\n",
       "      <td>9080.78</td>\n",
       "      <td>1735</td>\n",
       "      <td>9060.37</td>\n",
       "      <td>20.54</td>\n",
       "      <td>0.13</td>\n",
       "      <td>9123.37</td>\n",
       "      <td>1745</td>\n",
       "      <td>9102.73</td>\n",
       "      <td>20.54</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9123.78</td>\n",
       "      <td>1746</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55000</th>\n",
       "      <td>76</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>9078.40</td>\n",
       "      <td>1732</td>\n",
       "      <td>9057.97</td>\n",
       "      <td>20.55</td>\n",
       "      <td>0.13</td>\n",
       "      <td>9124.27</td>\n",
       "      <td>1746</td>\n",
       "      <td>9103.76</td>\n",
       "      <td>20.40</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9123.78</td>\n",
       "      <td>1746</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60000</th>\n",
       "      <td>77</td>\n",
       "      <td>105</td>\n",
       "      <td>464</td>\n",
       "      <td>9076.45</td>\n",
       "      <td>1729</td>\n",
       "      <td>9056.00</td>\n",
       "      <td>20.55</td>\n",
       "      <td>0.13</td>\n",
       "      <td>9123.53</td>\n",
       "      <td>1744</td>\n",
       "      <td>9103.13</td>\n",
       "      <td>20.30</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9123.91</td>\n",
       "      <td>1745</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65000</th>\n",
       "      <td>77</td>\n",
       "      <td>114</td>\n",
       "      <td>361</td>\n",
       "      <td>9074.92</td>\n",
       "      <td>1727</td>\n",
       "      <td>9054.33</td>\n",
       "      <td>20.56</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9122.34</td>\n",
       "      <td>1743</td>\n",
       "      <td>9101.83</td>\n",
       "      <td>20.41</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9122.44</td>\n",
       "      <td>1744</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70000</th>\n",
       "      <td>75</td>\n",
       "      <td>123</td>\n",
       "      <td>258</td>\n",
       "      <td>9073.48</td>\n",
       "      <td>1724</td>\n",
       "      <td>9052.76</td>\n",
       "      <td>20.57</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9122.49</td>\n",
       "      <td>1743</td>\n",
       "      <td>9101.94</td>\n",
       "      <td>20.46</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9122.24</td>\n",
       "      <td>1743</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75000</th>\n",
       "      <td>98</td>\n",
       "      <td>132</td>\n",
       "      <td>155</td>\n",
       "      <td>9072.12</td>\n",
       "      <td>1722</td>\n",
       "      <td>9051.33</td>\n",
       "      <td>20.57</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9121.43</td>\n",
       "      <td>1741</td>\n",
       "      <td>9100.91</td>\n",
       "      <td>20.43</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9121.05</td>\n",
       "      <td>1740</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80000</th>\n",
       "      <td>77</td>\n",
       "      <td>141</td>\n",
       "      <td>52</td>\n",
       "      <td>9070.83</td>\n",
       "      <td>1721</td>\n",
       "      <td>9049.98</td>\n",
       "      <td>20.57</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9121.63</td>\n",
       "      <td>1742</td>\n",
       "      <td>9101.06</td>\n",
       "      <td>20.47</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9121.05</td>\n",
       "      <td>1740</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85000</th>\n",
       "      <td>77</td>\n",
       "      <td>149</td>\n",
       "      <td>516</td>\n",
       "      <td>9069.71</td>\n",
       "      <td>1719</td>\n",
       "      <td>9048.80</td>\n",
       "      <td>20.57</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9122.34</td>\n",
       "      <td>1742</td>\n",
       "      <td>9101.75</td>\n",
       "      <td>20.49</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9121.05</td>\n",
       "      <td>1740</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90000</th>\n",
       "      <td>77</td>\n",
       "      <td>158</td>\n",
       "      <td>413</td>\n",
       "      <td>9068.72</td>\n",
       "      <td>1717</td>\n",
       "      <td>9047.79</td>\n",
       "      <td>20.58</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9120.39</td>\n",
       "      <td>1740</td>\n",
       "      <td>9099.81</td>\n",
       "      <td>20.50</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9119.91</td>\n",
       "      <td>1739</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95000</th>\n",
       "      <td>77</td>\n",
       "      <td>167</td>\n",
       "      <td>310</td>\n",
       "      <td>9067.82</td>\n",
       "      <td>1716</td>\n",
       "      <td>9046.85</td>\n",
       "      <td>20.58</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9120.42</td>\n",
       "      <td>1739</td>\n",
       "      <td>9099.83</td>\n",
       "      <td>20.49</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9120.34</td>\n",
       "      <td>1739</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>77</td>\n",
       "      <td>176</td>\n",
       "      <td>207</td>\n",
       "      <td>9066.93</td>\n",
       "      <td>1715</td>\n",
       "      <td>9045.93</td>\n",
       "      <td>20.58</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9119.93</td>\n",
       "      <td>1739</td>\n",
       "      <td>9099.41</td>\n",
       "      <td>20.43</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9120.34</td>\n",
       "      <td>1739</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105000</th>\n",
       "      <td>78</td>\n",
       "      <td>185</td>\n",
       "      <td>104</td>\n",
       "      <td>9066.11</td>\n",
       "      <td>1713</td>\n",
       "      <td>9045.08</td>\n",
       "      <td>20.58</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9118.20</td>\n",
       "      <td>1737</td>\n",
       "      <td>9097.57</td>\n",
       "      <td>20.54</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9119.16</td>\n",
       "      <td>1738</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110000</th>\n",
       "      <td>78</td>\n",
       "      <td>194</td>\n",
       "      <td>1</td>\n",
       "      <td>9065.28</td>\n",
       "      <td>1712</td>\n",
       "      <td>9044.22</td>\n",
       "      <td>20.59</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9120.85</td>\n",
       "      <td>1740</td>\n",
       "      <td>9100.30</td>\n",
       "      <td>20.47</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9119.16</td>\n",
       "      <td>1738</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115000</th>\n",
       "      <td>78</td>\n",
       "      <td>202</td>\n",
       "      <td>465</td>\n",
       "      <td>9064.58</td>\n",
       "      <td>1711</td>\n",
       "      <td>9043.48</td>\n",
       "      <td>20.59</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9119.99</td>\n",
       "      <td>1739</td>\n",
       "      <td>9099.35</td>\n",
       "      <td>20.55</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9119.16</td>\n",
       "      <td>1738</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120000</th>\n",
       "      <td>77</td>\n",
       "      <td>211</td>\n",
       "      <td>362</td>\n",
       "      <td>9063.93</td>\n",
       "      <td>1710</td>\n",
       "      <td>9042.85</td>\n",
       "      <td>20.59</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9119.73</td>\n",
       "      <td>1739</td>\n",
       "      <td>9099.12</td>\n",
       "      <td>20.53</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9119.16</td>\n",
       "      <td>1738</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125000</th>\n",
       "      <td>79</td>\n",
       "      <td>220</td>\n",
       "      <td>259</td>\n",
       "      <td>9063.23</td>\n",
       "      <td>1709</td>\n",
       "      <td>9042.27</td>\n",
       "      <td>20.59</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9120.27</td>\n",
       "      <td>1739</td>\n",
       "      <td>9099.78</td>\n",
       "      <td>20.40</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9119.16</td>\n",
       "      <td>1738</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130000</th>\n",
       "      <td>79</td>\n",
       "      <td>229</td>\n",
       "      <td>156</td>\n",
       "      <td>9062.55</td>\n",
       "      <td>1708</td>\n",
       "      <td>9041.70</td>\n",
       "      <td>20.60</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9118.46</td>\n",
       "      <td>1737</td>\n",
       "      <td>9097.79</td>\n",
       "      <td>20.59</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9118.73</td>\n",
       "      <td>1738</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135000</th>\n",
       "      <td>77</td>\n",
       "      <td>238</td>\n",
       "      <td>53</td>\n",
       "      <td>9061.87</td>\n",
       "      <td>1708</td>\n",
       "      <td>9041.12</td>\n",
       "      <td>20.60</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9119.31</td>\n",
       "      <td>1738</td>\n",
       "      <td>9098.78</td>\n",
       "      <td>20.44</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9118.73</td>\n",
       "      <td>1738</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140000</th>\n",
       "      <td>77</td>\n",
       "      <td>246</td>\n",
       "      <td>517</td>\n",
       "      <td>9061.26</td>\n",
       "      <td>1707</td>\n",
       "      <td>9040.60</td>\n",
       "      <td>20.60</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9120.30</td>\n",
       "      <td>1739</td>\n",
       "      <td>9099.71</td>\n",
       "      <td>20.50</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9118.73</td>\n",
       "      <td>1738</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145000</th>\n",
       "      <td>78</td>\n",
       "      <td>255</td>\n",
       "      <td>414</td>\n",
       "      <td>9060.73</td>\n",
       "      <td>1706</td>\n",
       "      <td>9040.14</td>\n",
       "      <td>20.60</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9119.67</td>\n",
       "      <td>1739</td>\n",
       "      <td>9099.06</td>\n",
       "      <td>20.53</td>\n",
       "      <td>0.08</td>\n",
       "      <td>9118.73</td>\n",
       "      <td>1738</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150000</th>\n",
       "      <td>77</td>\n",
       "      <td>264</td>\n",
       "      <td>311</td>\n",
       "      <td>9060.24</td>\n",
       "      <td>1705</td>\n",
       "      <td>9039.71</td>\n",
       "      <td>20.60</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9119.81</td>\n",
       "      <td>1739</td>\n",
       "      <td>9099.25</td>\n",
       "      <td>20.47</td>\n",
       "      <td>0.08</td>\n",
       "      <td>9118.73</td>\n",
       "      <td>1738</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155000</th>\n",
       "      <td>78</td>\n",
       "      <td>273</td>\n",
       "      <td>208</td>\n",
       "      <td>9059.74</td>\n",
       "      <td>1705</td>\n",
       "      <td>9039.27</td>\n",
       "      <td>20.61</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9119.59</td>\n",
       "      <td>1738</td>\n",
       "      <td>9099.01</td>\n",
       "      <td>20.49</td>\n",
       "      <td>0.08</td>\n",
       "      <td>9118.73</td>\n",
       "      <td>1738</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160000</th>\n",
       "      <td>78</td>\n",
       "      <td>282</td>\n",
       "      <td>105</td>\n",
       "      <td>9059.26</td>\n",
       "      <td>1704</td>\n",
       "      <td>9038.85</td>\n",
       "      <td>20.61</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9118.99</td>\n",
       "      <td>1738</td>\n",
       "      <td>9098.49</td>\n",
       "      <td>20.42</td>\n",
       "      <td>0.08</td>\n",
       "      <td>9118.73</td>\n",
       "      <td>1738</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165000</th>\n",
       "      <td>77</td>\n",
       "      <td>291</td>\n",
       "      <td>2</td>\n",
       "      <td>9058.77</td>\n",
       "      <td>1703</td>\n",
       "      <td>9038.40</td>\n",
       "      <td>20.61</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9120.76</td>\n",
       "      <td>1740</td>\n",
       "      <td>9100.16</td>\n",
       "      <td>20.51</td>\n",
       "      <td>0.08</td>\n",
       "      <td>9118.73</td>\n",
       "      <td>1738</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        TRAIN:                               VALID:        \\\n",
       "       Time   Ep   Ct     LOSS   PPL      NLL     KL   REG     LOSS   PPL   \n",
       "5000     79    8  463  9179.00  1881  9160.66  18.09  0.25  9157.78  1795   \n",
       "10000    76   17  360  9138.35  1819  9118.60  19.56  0.20  9141.21  1772   \n",
       "15000    77   26  257  9119.68  1791  9099.47  20.03  0.18  9136.41  1764   \n",
       "20000    82   35  154  9108.40  1775  9087.96  20.24  0.17  9131.93  1758   \n",
       "25000    77   44   51  9100.47  1764  9079.91  20.35  0.16  9128.96  1753   \n",
       "30000    76   52  515  9094.68  1755  9074.06  20.42  0.15  9130.43  1755   \n",
       "35000    76   61  412  9090.26  1749  9069.71  20.47  0.14  9127.13  1750   \n",
       "40000    76   70  309  9086.61  1744  9066.14  20.50  0.14  9126.26  1749   \n",
       "45000    76   79  206  9083.46  1739  9063.04  20.52  0.14  9124.95  1747   \n",
       "50000    77   88  103  9080.78  1735  9060.37  20.54  0.13  9123.37  1745   \n",
       "55000    76   97    0  9078.40  1732  9057.97  20.55  0.13  9124.27  1746   \n",
       "60000    77  105  464  9076.45  1729  9056.00  20.55  0.13  9123.53  1744   \n",
       "65000    77  114  361  9074.92  1727  9054.33  20.56  0.12  9122.34  1743   \n",
       "70000    75  123  258  9073.48  1724  9052.76  20.57  0.12  9122.49  1743   \n",
       "75000    98  132  155  9072.12  1722  9051.33  20.57  0.12  9121.43  1741   \n",
       "80000    77  141   52  9070.83  1721  9049.98  20.57  0.12  9121.63  1742   \n",
       "85000    77  149  516  9069.71  1719  9048.80  20.57  0.12  9122.34  1742   \n",
       "90000    77  158  413  9068.72  1717  9047.79  20.58  0.12  9120.39  1740   \n",
       "95000    77  167  310  9067.82  1716  9046.85  20.58  0.11  9120.42  1739   \n",
       "100000   77  176  207  9066.93  1715  9045.93  20.58  0.11  9119.93  1739   \n",
       "105000   78  185  104  9066.11  1713  9045.08  20.58  0.11  9118.20  1737   \n",
       "110000   78  194    1  9065.28  1712  9044.22  20.59  0.11  9120.85  1740   \n",
       "115000   78  202  465  9064.58  1711  9043.48  20.59  0.11  9119.99  1739   \n",
       "120000   77  211  362  9063.93  1710  9042.85  20.59  0.11  9119.73  1739   \n",
       "125000   79  220  259  9063.23  1709  9042.27  20.59  0.11  9120.27  1739   \n",
       "130000   79  229  156  9062.55  1708  9041.70  20.60  0.11  9118.46  1737   \n",
       "135000   77  238   53  9061.87  1708  9041.12  20.60  0.11  9119.31  1738   \n",
       "140000   77  246  517  9061.26  1707  9040.60  20.60  0.11  9120.30  1739   \n",
       "145000   78  255  414  9060.73  1706  9040.14  20.60  0.11  9119.67  1739   \n",
       "150000   77  264  311  9060.24  1705  9039.71  20.60  0.10  9119.81  1739   \n",
       "155000   78  273  208  9059.74  1705  9039.27  20.61  0.10  9119.59  1738   \n",
       "160000   78  282  105  9059.26  1704  9038.85  20.61  0.10  9118.99  1738   \n",
       "165000   77  291    2  9058.77  1703  9038.40  20.61  0.10  9120.76  1740   \n",
       "\n",
       "                                TEST:       SPEC:             HIER:        \n",
       "            NLL     KL   REG     LOSS   PPL     1     2     3 CHILD OTHER  \n",
       "5000    9137.22  20.39  0.17  9158.85  1797  0.39  0.37  0.44  0.40  0.28  \n",
       "10000   9120.32  20.75  0.14  9141.49  1772  0.41  0.40  0.47  0.37  0.25  \n",
       "15000   9115.47  20.80  0.14  9136.27  1764  0.43  0.41  0.48  0.36  0.25  \n",
       "20000   9111.07  20.73  0.12  9132.58  1759  0.44  0.42  0.48  0.34  0.24  \n",
       "25000   9108.36  20.48  0.12  9129.66  1755  0.45  0.42  0.47  0.35  0.25  \n",
       "30000   9109.65  20.66  0.12  9129.66  1755  0.46  0.42  0.47  0.36  0.25  \n",
       "35000   9106.54  20.48  0.11  9126.85  1749  0.46  0.42  0.47  0.35  0.25  \n",
       "40000   9105.65  20.50  0.11  9125.06  1748  0.46  0.43  0.47  0.35  0.25  \n",
       "45000   9104.35  20.50  0.10  9124.50  1746  0.46  0.43  0.48  0.34  0.24  \n",
       "50000   9102.73  20.54  0.10  9123.78  1746  0.46  0.44  0.48  0.33  0.24  \n",
       "55000   9103.76  20.40  0.10  9123.78  1746  0.47  0.43  0.48  0.34  0.24  \n",
       "60000   9103.13  20.30  0.10  9123.91  1745  0.46  0.43  0.47  0.35  0.25  \n",
       "65000   9101.83  20.41  0.09  9122.44  1744  0.46  0.43  0.48  0.34  0.24  \n",
       "70000   9101.94  20.46  0.10  9122.24  1743  0.46  0.44  0.47  0.34  0.24  \n",
       "75000   9100.91  20.43  0.09  9121.05  1740  0.46  0.44  0.48  0.34  0.24  \n",
       "80000   9101.06  20.47  0.09  9121.05  1740  0.47  0.44  0.47  0.33  0.24  \n",
       "85000   9101.75  20.49  0.09  9121.05  1740  0.47  0.44  0.47  0.34  0.25  \n",
       "90000   9099.81  20.50  0.09  9119.91  1739  0.47  0.44  0.48  0.33  0.24  \n",
       "95000   9099.83  20.49  0.09  9120.34  1739  0.47  0.44  0.47  0.33  0.24  \n",
       "100000  9099.41  20.43  0.09  9120.34  1739  0.46  0.44  0.48  0.33  0.24  \n",
       "105000  9097.57  20.54  0.09  9119.16  1738  0.47  0.44  0.47  0.33  0.24  \n",
       "110000  9100.30  20.47  0.09  9119.16  1738  0.47  0.44  0.48  0.32  0.24  \n",
       "115000  9099.35  20.55  0.09  9119.16  1738  0.46  0.44  0.47  0.33  0.24  \n",
       "120000  9099.12  20.53  0.09  9119.16  1738  0.46  0.44  0.48  0.32  0.24  \n",
       "125000  9099.78  20.40  0.09  9119.16  1738  0.46  0.44  0.48  0.32  0.24  \n",
       "130000  9097.79  20.59  0.09  9118.73  1738  0.46  0.44  0.48  0.32  0.24  \n",
       "135000  9098.78  20.44  0.09  9118.73  1738  0.47  0.45  0.48  0.32  0.24  \n",
       "140000  9099.71  20.50  0.09  9118.73  1738  0.48  0.44  0.48  0.32  0.24  \n",
       "145000  9099.06  20.53  0.08  9118.73  1738  0.47  0.45  0.48  0.31  0.24  \n",
       "150000  9099.25  20.47  0.08  9118.73  1738  0.47  0.45  0.48  0.32  0.24  \n",
       "155000  9099.01  20.49  0.08  9118.73  1738  0.46  0.45  0.48  0.31  0.24  \n",
       "160000  9098.49  20.42  0.08  9118.73  1738  0.47  0.45  0.48  0.31  0.24  \n",
       "165000  9100.16  20.51  0.08  9118.73  1738  0.47  0.45  0.48  0.31  0.24  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 R: 1.000 P: 0.119 user dialogue speech users utterance systems utterances research knowledge domain\n",
      "   1 R: 0.238 P: 0.061 morphological pos languages rules analysis tags forms form annotation english\n",
      "     11 R: 0.097 P: 0.097 verb structure semantic verbs syntactic noun object type subject clause\n",
      "     12 R: 0.079 P: 0.079 tree parsing grammar parser dependency node trees rules parse structure\n",
      "   2 R: 0.142 P: 0.043 extraction entity patterns names domain entities named precision terms pattern\n",
      "     21 R: 0.049 P: 0.049 semantic sense lexical wordnet senses relations concepts knowledge concept frame\n",
      "     22 R: 0.049 P: 0.049 event relation relations entity discourse annotation events entities argument features\n",
      "   3 R: 0.137 P: 0.045 document documents topic question query sentences questions answer search web\n",
      "     31 R: 0.026 P: 0.026 english alignment languages translation parallel pairs chinese corpora bilingual source\n",
      "     32 R: 0.066 P: 0.066 similarity graph pairs vector context clustering semantic method cluster vectors\n",
      "   4 R: 0.194 P: 0.055 speech segmentation error training errors character performance recognition test chinese\n",
      "     41 R: 0.050 P: 0.050 translation phrase source target systems mt english training bleu models\n",
      "     42 R: 0.088 P: 0.088 features training feature models learning performance algorithm probability accuracy test\n",
      "   5 R: 0.171 P: 0.061 features sentiment classification feature tweets classifier negative positive analysis opinion\n",
      "     51 R: 0.056 P: 0.056 human evaluation sentences test annotators agreement annotation scores level texts\n",
      "     52 R: 0.055 P: 0.055 models neural embeddings training network vector layer input representations learning\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "while epoch < config.n_epochs:\n",
    "    # train\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = model.get_feed_dict(batch)\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, ppls_batch, global_step_log = \\\n",
    "        sess.run([model.opt, model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_reg, model.topic_ppls, tf.train.get_global_step()], feed_dict = feed_dict)\n",
    "\n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if global_step_log % config.log_period == 0:\n",
    "            # validate\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "\n",
    "            # test\n",
    "            if ppl_dev < ppl_min:\n",
    "                ppl_min = ppl_dev\n",
    "                loss_test, _, _, _, ppl_test, _ = validate(sess, test_batches, model)\n",
    "                saver.save(sess, config.path_model, global_step=global_step_log)\n",
    "                cPickle.dump(config, open(config.path_config % global_step_log, 'wb'))\n",
    "                update_checkpoint(config, checkpoint, global_step_log)\n",
    "            \n",
    "            # visualize topic\n",
    "            topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "            topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "            topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "            topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "            descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "            recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "            \n",
    "            depth_specs = get_topic_specialization(sess, model, instances_test)\n",
    "            hierarchical_affinities = get_hierarchical_affinity(sess, model)\n",
    "            \n",
    "            # log\n",
    "            clear_output()\n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_reg_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_reg_dev, \\\n",
    "                    '%.2f'%loss_test, '%.0f'%ppl_test, \\\n",
    "                    '%.2f'%depth_specs[1], '%.2f'%depth_specs[2], '%.2f'%depth_specs[3], \\\n",
    "                    '%.2f'%hierarchical_affinities[0], '%.2f'%hierarchical_affinities[1]],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "            cPickle.dump(log_df, open(os.path.join(config.path_log), 'wb'))\n",
    "            print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)\n",
    "\n",
    "            # update tree\n",
    "            if not config.static:\n",
    "                config.tree_idxs, update_tree_flg = model.update_tree(topic_prob_topic, recur_prob_topic)\n",
    "                if update_tree_flg:\n",
    "                    print(config.tree_idxs)\n",
    "                    name_variables = {tensor.name: variable for tensor, variable in zip(tf.global_variables(), sess.run(tf.global_variables()))} # store paremeters\n",
    "                    if 'sess' in globals(): sess.close()\n",
    "                    model = HierarchicalNeuralTopicModel(config)\n",
    "                    sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "                    name_tensors = {tensor.name: tensor for tensor in tf.global_variables()}\n",
    "                    sess.run([name_tensors[name].assign(variable) for name, variable in name_variables.items()]) # restore parameters\n",
    "                    saver = tf.train.Saver(max_to_keep=1)\n",
    "                \n",
    "            time_start = time.time()\n",
    "\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "    epoch += 1\n",
    "\n",
    "loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "display(log_df)\n",
    "print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5424"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.dim_bow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
