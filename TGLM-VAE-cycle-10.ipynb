{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from data_structure import get_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "from topic_model import TopicModel\n",
    "\n",
    "from topic_beam_search_decoder import BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '3', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/apnews/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/tglm_vae_tmp', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'apnews', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 50, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 500, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.1, 'lr')\n",
    "flags.DEFINE_float('reg', 1., 'regularization term')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_bool('warmup', True, 'flg of warming up')\n",
    "flags.DEFINE_integer('epochs_cycle', 10, 'number of epochs within a cycle')\n",
    "flags.DEFINE_float('r_cycle', 0.5, 'proportion used to increase beta within a cycle')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 10, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_bow', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_hidden_topic', 512, 'dim_hidden_topic')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "flags.DEFINE_bool('bidirectional', True, 'flg of bidirectional encoding')\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('PAD_IDX', word_to_idx[PAD], 'PAD_IDX')\n",
    "flags.DEFINE_integer('UNK_IDX', word_to_idx[UNK], 'UNK_IDX')\n",
    "flags.DEFINE_integer('BOS_IDX', word_to_idx[BOS], 'BOS_IDX')\n",
    "flags.DEFINE_integer('EOS_IDX', word_to_idx[EOS], 'EOS_IDX')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "maximum_iterations = max([max([instance.max_sent_l for instance in batch]) for ct, batch in dev_batches])\n",
    "flags.DEFINE_integer('maximum_iterations', maximum_iterations, 'maximum_iterations')\n",
    "\n",
    "flags.DEFINE_integer('cycle_steps', len(train_batches)*config.epochs_cycle, 'number of steps for each cycle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build language model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['bow'] = tf.placeholder(tf.float32, [None, config.dim_bow], name='bow')\n",
    "t_variables['input_token_idxs'] = tf.placeholder(tf.int32, [None, None], name='input_token_idxs')\n",
    "t_variables['dec_input_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_input_idxs')\n",
    "t_variables['dec_target_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_target_idxs')\n",
    "t_variables['batch_l'] = tf.placeholder(tf.int32, name='batch_l')\n",
    "t_variables['doc_l'] = tf.placeholder(tf.int32, [None], name='doc_l')\n",
    "t_variables['sent_l'] = tf.placeholder(tf.int32, [None], name='sent_l')\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train', assertion=False):\n",
    "    def token_dropout(sent_idxs):\n",
    "        sent_idxs_dropout = np.asarray(sent_idxs)\n",
    "        sent_idxs_dropout[np.random.rand(len(sent_idxs)) > config.word_keep_prob] = config.UNK_IDX\n",
    "        return list(sent_idxs_dropout)\n",
    "\n",
    "    bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "    \n",
    "    doc_l = np.array([len(instance.token_idxs) for instance in batch])\n",
    "    \n",
    "    feed_input_token_idxs_list = [sent_idxs for instance in batch for sent_idxs in instance.token_idxs]\n",
    "    feed_dec_input_idxs_list = [[config.BOS_IDX] + token_dropout(sent_idxs) for sent_idxs in feed_input_token_idxs_list]\n",
    "    feed_dec_target_idxs_list = [sent_idxs + [config.EOS_IDX]  for sent_idxs in feed_input_token_idxs_list]\n",
    "        \n",
    "    sent_l = np.array([len(sent_idxs) for sent_idxs in feed_input_token_idxs_list], np.int32)\n",
    "    batch_l = len(sent_l)\n",
    "    \n",
    "    feed_input_token_idxs = pad_sequences(feed_input_token_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_input_idxs = pad_sequences(feed_dec_input_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_target_idxs = pad_sequences(feed_dec_target_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    \n",
    "    if assertion:\n",
    "        index = 0\n",
    "        for instance in batch:\n",
    "            for line_idxs in instance.token_idxs:\n",
    "                assert feed_input_token_idxs_list[index] == line_idxs\n",
    "                index += 1\n",
    "        assert feed_input_token_idxs.shape[1] == np.max(sent_l)\n",
    "        assert feed_dec_input_idxs.shape[1] == np.max(sent_l) + 1\n",
    "        assert feed_dec_target_idxs.shape[1] == np.max(sent_l) + 1\n",
    "    \n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "\n",
    "    feed_dict = {\n",
    "                t_variables['bow']: bow, \n",
    "                t_variables['batch_l']: batch_l, t_variables['doc_l']: doc_l, t_variables['sent_l']: sent_l, \n",
    "                t_variables['input_token_idxs']: feed_input_token_idxs, t_variables['dec_input_idxs']: feed_dec_input_idxs, t_variables['dec_target_idxs']: feed_dec_target_idxs, \n",
    "                t_variables['keep_prob']: keep_prob\n",
    "    }\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     10,
     24,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "def check_shape(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()\n",
    "    \n",
    "def check_value(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()    \n",
    "    \n",
    "# sent_loss_kl_categ_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, tf_log(prob_topic_infer/prob_topic_sents)), 1))\n",
    "# debug_value([sent_loss_kl_categ, sent_loss_kl_categ_tmp])\n",
    "# sent_loss_kl_gauss_tmp = 0.5 * tf.reduce_sum(tf.exp(logvars_topic_infer-logvars_topic) + tf.square(means_topic - means_topic_infer) / tf.exp(logvars_topic) - 1 + (logvars_topic - logvars_topic_infer), -1)\n",
    "# sent_loss_kl_gmm_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss_tmp), -1))\n",
    "# debug_value([sent_loss_kl_gmm_tmp, sent_loss_kl_gmm])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encode bow\n",
    "with tf.variable_scope('topic/enc', reuse=False):\n",
    "    hidden_bow_ = tf.layers.Dense(units=config.dim_hidden_bow, activation=tf.nn.relu, name='hidden_bow')(t_variables['bow'])\n",
    "    hidden_bow = tf.layers.Dropout(t_variables['keep_prob'])(hidden_bow_)\n",
    "    means_bow = tf.layers.Dense(units=config.dim_latent_bow, name='mean_bow')(hidden_bow)\n",
    "    logvars_bow = tf.layers.Dense(units=config.dim_latent_bow, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_bow)\n",
    "    latents_bow = sample_latents(means_bow, logvars_bow) # sample latent vectors\n",
    "\n",
    "    prob_topic = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic')(latents_bow) # inference of topic probabilities\n",
    "\n",
    "# decode bow\n",
    "with tf.variable_scope('shared', reuse=False):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "bow_embeddings = tf.nn.embedding_lookup(embeddings, bow_idxs) # embeddings of each bow features\n",
    "\n",
    "with tf.variable_scope('topic/dec', reuse=False):\n",
    "    topic_embeddings = tf.get_variable('topic_emb', [config.n_topic, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of topics\n",
    "\n",
    "    topic_bow = tf.nn.softmax(tf.matmul(topic_embeddings, bow_embeddings, transpose_b=True), 1) # bow vectors for each topic\n",
    "    logits_bow = tf_log(tf.matmul(prob_topic, topic_bow)) # predicted bow distribution\n",
    "\n",
    "    # prior of each gaussian distribution (computed for each topic)\n",
    "    hidden_topic = tf.layers.Dense(units=config.dim_hidden_topic, activation=tf.nn.relu, name='hidden_topic')(topic_bow)\n",
    "    means_topic = tf.layers.Dense(units=config.dim_latent, name='mean_topic')(hidden_topic)\n",
    "    logvars_topic = tf.layers.Dense(units=config.dim_latent, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_topic)\n",
    "    sigma_topic = tf.exp(0.5 * logvars_topic)\n",
    "    gauss_topic = tfd.Normal(loc=means_topic, scale=sigma_topic)    \n",
    "    \n",
    "# define losses\n",
    "topic_losses_recon = -tf.reduce_sum(tf.multiply(t_variables['bow'], logits_bow), 1)\n",
    "topic_loss_recon = tf.reduce_mean(topic_losses_recon) # negative log likelihood of each words\n",
    "\n",
    "topic_loss_kl = compute_kl_loss(means_bow, logvars_bow) # KL divergence b/w latent dist & gaussian std\n",
    "\n",
    "topic_bow_norm = topic_bow / tf.norm(topic_bow, axis=1, keepdims=True)\n",
    "topic_dots = tf.clip_by_value(tf.matmul(topic_bow_norm, tf.transpose(topic_bow_norm)), -1., 1.)\n",
    "topic_loss_reg = tf.reduce_mean(tf.square(topic_dots - tf.eye(config.n_topic)))\n",
    "# topic_angles = tf.acos(topic_dots)\n",
    "# topic_angles_mean = tf.reduce_mean(topic_angles)\n",
    "# topic_angles_vars = tf.reduce_mean(tf.square(topic_angles - topic_angles_mean))\n",
    "# topic_loss_reg = tf.exp(topic_angles_vars - topic_angles_mean)\n",
    "\n",
    "# monitor\n",
    "n_bow = tf.reduce_sum(t_variables['bow'], 1)\n",
    "topic_ppls = tf.divide(topic_losses_recon, n_bow)\n",
    "topics_freq_bow_indices = tf.nn.top_k(topic_bow, 10, name='topic_freq_bow').indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "input_token_idxs = t_variables['input_token_idxs']\n",
    "batch_l = t_variables['batch_l']\n",
    "sent_l = t_variables['sent_l']\n",
    "max_sent_l = tf.reduce_max(sent_l)\n",
    "\n",
    "with tf.variable_scope('sent/enc', reuse=False):\n",
    "    # get word embedding\n",
    "    enc_input = tf.nn.embedding_lookup(embeddings, input_token_idxs)\n",
    "\n",
    "    # get sentence embedding\n",
    "    _, enc_state = dynamic_bi_rnn(enc_input, sent_l, config.dim_hidden, t_variables['keep_prob'])\n",
    "\n",
    "    # TODO House Holder flow\n",
    "    hidden_topic_infer =  tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='hidden_topic_infer')(enc_state)\n",
    "    prob_topic_infer = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic_infer')(hidden_topic_infer)\n",
    "\n",
    "    w_mean_topic_infer = tf.get_variable('mean_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32)\n",
    "    b_mean_topic_infer = tf.get_variable('mean_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32)\n",
    "    means_topic_infer = tf.tensordot(enc_state, w_mean_topic_infer, axes=[[1], [1]]) + b_mean_topic_infer\n",
    "    \n",
    "    w_logvar_topic_infer = tf.get_variable('logvar_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    b_logvar_topic_infer = tf.get_variable('logvar_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    logvars_topic_infer = tf.tensordot(enc_state, w_logvar_topic_infer, axes=[[1], [1]]) + b_logvar_topic_infer\n",
    "    sigma_topic_infer = tf.exp(0.5 * logvars_topic_infer)\n",
    "    gauss_topic_infer = tfd.Normal(loc=means_topic_infer, scale=sigma_topic_infer)\n",
    "    \n",
    "    # latent vectors from each gaussian dist.\n",
    "    latents_topic_infer = sample_latents(means_topic_infer, logvars_topic_infer) \n",
    "    # latent vector from gaussian mixture\n",
    "    latents_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), latents_topic_infer, transpose_a=True)\n",
    "    \n",
    "    # for beam search\n",
    "    means_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), means_topic_infer, transpose_a=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for decoding\n",
    "dec_sent_l = tf.add(sent_l, 1)\n",
    "dec_input_idxs = t_variables['dec_input_idxs']\n",
    "dec_input = tf.nn.embedding_lookup(embeddings, dec_input_idxs)\n",
    "\n",
    "dec_latents_input = tf.tile(latents_input, [1, tf.shape(dec_input)[1], 1])\n",
    "dec_concat_input = tf.concat([dec_input, dec_latents_input], -1)\n",
    "\n",
    "# decode for training\n",
    "with tf.variable_scope('sent/dec/rnn', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32, reuse=False):\n",
    "    dec_cell = tf.contrib.rnn.GRUCell(config.dim_hidden)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob = t_variables['keep_prob'])\n",
    "\n",
    "    dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(latents_input, 1))\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_concat_input, sequence_length=dec_sent_l)\n",
    "\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=dec_cell,\n",
    "        helper=helper,\n",
    "        initial_state=dec_initial_state)\n",
    "\n",
    "    dec_outputs, _, output_sent_l = tf.contrib.seq2seq.dynamic_decode(train_decoder)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(config.n_vocab, use_bias=False, name='out')\n",
    "    output_logits = output_layer(dec_outputs.rnn_output)\n",
    "    \n",
    "    output_token_idxs = tf.argmax(output_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tokens = tf.fill([batch_l], config.BOS_IDX)\n",
    "end_token = config.EOS_IDX\n",
    "\n",
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    infer_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(means_input, 1))\n",
    "    beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(infer_dec_initial_state, multiplier=config.beam_width)\n",
    "    beam_latents_input = tf.contrib.seq2seq.tile_batch(tf.squeeze(means_input, 1), multiplier=config.beam_width) # added\n",
    "    \n",
    "    beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=beam_latents_input)\n",
    "\n",
    "    beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    beam_output_token_idxs = beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    inter_means_input = tf.placeholder(tf.float32, [None, config.dim_latent])\n",
    "    \n",
    "    inter_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(inter_means_input)\n",
    "    inter_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(inter_dec_initial_state, multiplier=config.beam_width)\n",
    "    inter_beam_latents_input = tf.contrib.seq2seq.tile_batch(inter_means_input, multiplier=config.beam_width) # added\n",
    "    \n",
    "    inter_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=inter_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=inter_beam_latents_input)\n",
    "\n",
    "    inter_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inter_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    inter_beam_output_token_idxs = inter_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    topic_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic)\n",
    "    topic_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(topic_dec_initial_state, multiplier=config.beam_width)\n",
    "    topic_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic, multiplier=config.beam_width) # added\n",
    "    \n",
    "    topic_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=topic_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=topic_beam_latents_input)\n",
    "\n",
    "    topic_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        topic_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    topic_beam_output_token_idxs = topic_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language modeling cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target and mask\n",
    "dec_target_idxs = t_variables['dec_target_idxs']\n",
    "dec_mask_tokens = tf.sequence_mask(dec_sent_l, maxlen=max_sent_l+1, dtype=tf.float32)\n",
    "\n",
    "# nll for each token (averaged over batch & sentence)\n",
    "sent_loss_recon = tf.contrib.seq2seq.sequence_loss(output_logits, dec_target_idxs, dec_mask_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_l = t_variables['doc_l']\n",
    "mask_sents = tf.sequence_mask(doc_l)\n",
    "mask_sents_flatten = tf.reshape(mask_sents, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1]])\n",
    "\n",
    "prob_topic_tiled = tf.tile(tf.expand_dims(prob_topic, 1), [1, tf.shape(mask_sents)[1], 1])\n",
    "prob_topic_flatten = tf.reshape(prob_topic_tiled, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1], config.n_topic])\n",
    "prob_topic_sents = tf.boolean_mask(prob_topic_flatten, mask_sents_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferred mixture probabilities (computed for each sentence)\n",
    "categ_topic_infer = tfd.Categorical(probs=prob_topic_infer)\n",
    "\n",
    "# prior of mixture probabilities (computed for each document, tiled for each sentence)\n",
    "categ_topic = tfd.Categorical(probs=prob_topic_sents)\n",
    "\n",
    "sent_loss_kl_categ = tf.reduce_mean(tfd.kl_divergence(categ_topic_infer, categ_topic))\n",
    "\n",
    "# inference of each gaussian gaussribution (computed for each sentence)\n",
    "\n",
    "sent_loss_kl_gauss = tf.reduce_sum(tfd.kl_divergence(gauss_topic_infer, gauss_topic), -1)\n",
    "sent_loss_kl_gmm = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss), -1))\n",
    "\n",
    "sent_loss_kl = sent_loss_kl_categ + sent_loss_kl_gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "tau = tf.cast(tf.divide(tf.mod(global_step, tf.constant(config.cycle_steps)), tf.constant(config.cycle_steps)), dtype=tf.float32)\n",
    "beta = tf.minimum(1., tau/config.r_cycle)\n",
    "\n",
    "sent_loss = sent_loss_recon + beta * sent_loss_kl\n",
    "\n",
    "topic_loss = topic_loss_recon + topic_loss_kl + config.reg * topic_loss_reg\n",
    "loss = topic_loss + sent_loss\n",
    "\n",
    "# define optimizer\n",
    "if config.opt == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif config.opt == 'Adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "    \n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_sents(token_idxs, config, idx_to_word):\n",
    "    sents = []\n",
    "    for sent_idxs in token_idxs:\n",
    "        tokens = []\n",
    "        for idx in sent_idxs:\n",
    "            if idx == config.EOS_IDX: break\n",
    "            tokens.append(idx_to_word[idx])\n",
    "        sent = ' '.join(tokens)\n",
    "        sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_batch, sent_loss_batch, ppls_batch = sess.run([loss, topic_loss, sent_loss, topic_ppls], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_batch, sent_loss_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "    loss_mean, topic_loss_mean, sent_loss_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    return loss_mean, topic_loss_mean, sent_loss_mean, ppl_mean\n",
    "\n",
    "def get_all_losses(sess, batches):\n",
    "    losses = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch = \\\n",
    "        sess.run([loss, topic_loss_recon, topic_loss_kl, sent_loss_recon, sent_loss_kl], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "    print('LOSS %.2f | TM NLL: %.2f, KL: %.4f | LM NLL: %.2f, KL: %.4f' %  np.mean(losses, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_batch):\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    pred_token_idxs = sess.run(output_token_idxs, feed_dict = feed_dict)\n",
    "    true_token_idxs = [sent_idxs for instance in sample_batch for sent_idxs in instance.token_idxs]\n",
    "    \n",
    "    assert len(pred_token_idxs) == len(true_token_idxs)\n",
    "    \n",
    "    pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "    true_sents = idxs_to_sents(true_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    for i, (true_sent, pred_sent) in enumerate(zip(true_sents, pred_sents)):        \n",
    "        print(i, 'True: %s' % true_sent)\n",
    "        print(i, 'Pred: %s' % pred_sent)\n",
    "\n",
    "def print_topic_sample():\n",
    "    pred_topics_freq_bow_indices, pred_topic_token_idxs = sess.run([topics_freq_bow_indices, topic_beam_output_token_idxs], \n",
    "                                                                                                           feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "    pred_topic_sents = idxs_to_sents(pred_topic_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Topic Samples-----------')\n",
    "    for i, (topic_freq_bow_idxs, pred_topic_sent) in enumerate(zip(topics_freq_bow_idxs, pred_topic_sents)):\n",
    "        print(i, ' bow:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' sent:', pred_topic_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "logs = []\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "saver = tf.train.Saver(max_to_keep=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "010[s], Ep: 00, Ct: 00000|TR LOSS: 364, PPL: 2660|TM NLL: 352, KL: 0.76, REG:0.90 | LM NLL: 10.34, KL: 1.60|DE LOSS: 348, PPL: 2657, TM: 337, LM: 10.34|BETA: 0.000069\n",
      "082[s], Ep: 00, Ct: 00500|TR LOSS: 329, PPL: 1695|TM NLL: 320, KL: 0.51, REG:0.53 | LM NLL: 7.43, KL: 2.10|DE LOSS: 321, PPL: 1532, TM: 314, LM: 6.94|BETA: 0.034743\n",
      "081[s], Ep: 00, Ct: 01000|TR LOSS: 326, PPL: 1555|TM NLL: 317, KL: 1.35, REG:0.35 | LM NLL: 7.15, KL: 2.05|DE LOSS: 317, PPL: 1353, TM: 310, LM: 6.91|BETA: 0.069417\n",
      "082[s], Ep: 00, Ct: 01500|TR LOSS: 324, PPL: 1474|TM NLL: 315, KL: 1.92, REG:0.27 | LM NLL: 7.04, KL: 1.82|DE LOSS: 315, PPL: 1293, TM: 308, LM: 6.86|BETA: 0.104092\n",
      "082[s], Ep: 00, Ct: 02000|TR LOSS: 323, PPL: 1422|TM NLL: 313, KL: 2.16, REG:0.23 | LM NLL: 6.98, KL: 1.63|DE LOSS: 314, PPL: 1266, TM: 307, LM: 6.82|BETA: 0.138766\n",
      "083[s], Ep: 00, Ct: 02500|TR LOSS: 321, PPL: 1383|TM NLL: 312, KL: 2.33, REG:0.20 | LM NLL: 6.93, KL: 1.47|DE LOSS: 313, PPL: 1228, TM: 306, LM: 6.77|BETA: 0.173440\n",
      "067[s], Ep: 01, Ct: 00000|TR LOSS: 321, PPL: 1359|TM NLL: 311, KL: 2.44, REG:0.18 | LM NLL: 6.90, KL: 1.38|DE LOSS: 311, PPL: 1180, TM: 304, LM: 6.75|BETA: 0.200069\n",
      "0 True: an # year old is in critical condition after being shot during an attempted robbery in st. louis\n",
      "0 Pred: the say the in the <unk> the # in the <unk> # # <unk>\n",
      "1 True: the st. louis post dispatch ( http : //bit.ly/ # <unk> # ) reports the incident happened early saturday\n",
      "1 Pred: the state say ( http <unk> <unk> : //bit.ly/ : <unk> <unk> <unk> <unk> <unk> #\n",
      "2 True: police say the victim told them he was standing behind a man at a gas station when the man turned around and announced a robbery\n",
      "2 Pred: the say the the of the the <unk> <unk> <unk> <unk> the\n",
      "3 True: the teen said he refused to turn over property , and a struggle ensued\n",
      "3 Pred: the state is the the the the in the in the # #\n",
      "4 True: according to authorities , the suspect pulled out a gun and fired several shots\n",
      "4 Pred: the say the in the # of the <unk> the\n",
      "5 True: police say the victim was shot once , in the neck\n",
      "5 Pred: the say the # of the in the the the #\n",
      "6 True: according to police , the suspect fled in a black monte carlo with a red racing <unk>\n",
      "6 Pred: the say the in the # # a in the\n",
      "7 True: he is described as a # foot # inch black male with a thin build and <unk>\n",
      "7 Pred: the say the the the # of the in the\n",
      "8 True: police say he was wearing a red shirt and black shorts at the time\n",
      "8 Pred: the say the in the the the of in the in the the #\n",
      "9 True: kimberly brooke sawyer is the new miss wisconsin\n",
      "9 Pred: the say the the the the\n",
      "10 True: the # year old classical vocalist won the statewide competition saturday night\n",
      "10 Pred: the state is in the in the the\n",
      "11 True: in an unusual twist , her younger sister , # year old singer katie leigh sawyer , was first runner up\n",
      "11 Pred: the the is the the the # the <unk> the\n",
      "12 True: both are from egg harbor\n",
      "12 Pred: the say the the # the # the the <unk> in in in\n",
      "13 True: kimberly sawyer has a degree in business finance from st. norbert college\n",
      "13 Pred: the is the the the in the\n",
      "14 True: she wins a $ # scholarship and will compete in the # miss america pageant on jan. # in las vegas\n",
      "14 Pred: the say the the # in the the # the\n",
      "15 True: the second runner up was # year old laura <unk> of kenosha , followed by # year old desiree <unk> of oshkosh and # year old kathryn williams of wisconsin rapids\n",
      "15 Pred: the state is the <unk> <unk> in <unk> <unk> # <unk> the\n",
      "16 True: pageant officials say the only other time sisters competed in the annual pageant was in # , when julia anne and <unk> sue <unk> of burlington finished among the # finalists\n",
      "16 Pred: the say the the # of\n",
      "17 True: ( ap )\n",
      "17 Pred: the say the in the the in the\n",
      "18 True: heavy rains have <unk> to significant snowfall in west virginia as a storm system slowly winds its way through the state\n",
      "18 Pred: the say the is the the in in the\n",
      "19 True: the national weather service says up to a foot of snow is forecast in parts of the state by thursday night\n",
      "19 Pred: the state is is in the # the # of the\n",
      "20 True: appalachian power has crews prepared to respond to power outages , although the utility says no widespread power failures are anticipated\n",
      "20 Pred: the say in the <unk> the # <unk> <unk> in the the #\n",
      "21 True: on wednesday , high water from heavy rains forced roads to close in at least # of the state 's # counties\n",
      "21 Pred: the say is the in in the # in in the\n",
      "22 True: in logan county , the rain swollen <unk> river washed away a temporary causeway connecting madison creek and <unk> bottom\n",
      "22 Pred: the the is\n",
      "23 True: in northern west virginia , the national weather service says the <unk> valley river is forecast to crest nearly # feet above flood stage thursday in belington and <unk>\n",
      "23 Pred: the the say ( ( <unk> # of the in <unk> <unk> <unk> in the\n",
      "24 True: a newspaper is reporting that eight california air national guard pilots are under criminal investigation for allegedly padding their salaries by getting paid for more than one shift on the same day\n",
      "24 Pred: the say the the the the # in in in <unk> the <unk> # the\n",
      "25 True: the sacramento bee reported the investigation by the federal air force office of special investigations in sunday 's edition\n",
      "25 Pred: the state is the the <unk> in the # of <unk> <unk>\n",
      "26 True: the newspaper also says the practice may go back many years and could have cost taxpayers millions of dollars\n",
      "26 Pred: the state is the the # of the in in the in the # the\n",
      "27 True: it may also have potentially jeopardized public and pilot safety by failing to provide pilots with adequate rest\n",
      "27 Pred: the say the in the the in\n",
      "28 True: at issue is a practice that allows pilots to be placed on alert to respond to any emergencies\n",
      "28 Pred: the the is the in in the the in the\n",
      "29 True: the pilots , based in fresno , are accused of earning full pay for standby shifts spent at home\n",
      "29 Pred: the state is the is the of the\n",
      "30 True: they were also allegedly on standby for more hours than they were allowed\n",
      "30 Pred: the say the the in the # the in in the\n",
      "31 True: the bay city state recreation area is offering a halloween preview this weekend with a pumpkin lit nature trail for families\n",
      "31 Pred: the the is the the the in the # the\n",
      "32 True: the recreation area says the trail will feature actors dressed as bats , wolves , rats and spiders\n",
      "32 Pred: the # is in the in the the the\n",
      "33 True: radio station <unk> says visitors who encounter the halloween themed animals will learn about where their frightening <unk> come from and how they 're actually helpful to humans\n",
      "33 Pred: the say in the the # the in the #\n",
      "34 True: the hike is scheduled for # p.m. saturday on the andersen nature trail that begins at the saginaw bay visitors center\n",
      "34 Pred: the # is the in the in the in the #\n",
      "35 True: the event is free but requires either a day pass or a recreation passport for entry to the recreation area\n",
      "35 Pred: the <unk> is the ( <unk> <unk> <unk> # in <unk> <unk> <unk> #\n",
      "36 True: the recreation area is along lake huron in bay county 's bangor and <unk> townships , north of bay city\n",
      "36 Pred: the state is in the # the in the of in the # the\n",
      "37 True: anchorage police have released the name of the pilot of a helicopter that crashed at the <unk> airport in <unk>\n",
      "37 Pred: the say is the in the in the\n",
      "38 True: police say the pilot was # year old thomas moore of anchorage\n",
      "38 Pred: the say the <unk> the the <unk> <unk> <unk> # <unk> the\n",
      "39 True: moore was flying a robinson r # wednesday afternoon when the aircraft crashed and became engulfed in flames\n",
      "39 Pred: the say the the the <unk> the in <unk> : the # <unk> the the\n",
      "40 True: a bystander was taken to a hospital for treatment of burns sustained while he was trying to rescue moore\n",
      "40 Pred: the # is the the the the in the in the\n",
      "41 True: the bystander 's name has not been released\n",
      "41 Pred: the state is the in in\n",
      "42 True: the cause of the crash is under investigation\n",
      "42 Pred: the state is the # of in the in the in\n",
      "43 True: <unk> inc. of new orleans has been awarded a $ # million contract by nasa to provide a high pressure industrial water line at the stennis space center near bay st. louis , miss .\n",
      "43 Pred: the say is the # the <unk> <unk> <unk> # #\n",
      "44 True: the times picayune reports ( http : //bit.ly/ # <unk> ) that the line will provide water for cooling and to suppress noise at the stennis b test complex\n",
      "44 Pred: the state reports reports http http : //bit.ly/ <unk> ) <unk> ) <unk> # of the in\n",
      "45 True: the complex will be used in # to test rocket engines that will be part of nasa 's space launch system\n",
      "45 Pred: the state is the the in the # the in\n",
      "46 True: nasa 's <unk> heavy lift rocket and orion spacecraft , both being built at the <unk> assembly facility in eastern new orleans , will make up the agency 's new human exploration launch system\n",
      "46 Pred: the say the is <unk> the <unk> the # <unk> the in the\n",
      "47 True: the <unk> is scheduled for a <unk> test launch in #\n",
      "47 Pred: the say the the ( http # # <unk> <unk> <unk> of <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "48 True: the contract requires <unk> to complete work in # days , scheduled to begin in july\n",
      "48 Pred: the say of the # <unk> <unk> in the of <unk> <unk> <unk> the\n",
      "49 True: a junior at central high school in florence has been killed in a car accident , the third student at the school to die since the school year began\n",
      "49 Pred: the state say the ( ( http # # <unk> ) : the # of in the #\n",
      "50 True: officials say samantha price was killed saturday in a one car accident on a lauderdale county road\n",
      "50 Pred: the say the is the the in in the #\n",
      "51 True: coroner andy high says price was the only one person involved in the accident\n",
      "51 Pred: the say is in the to the the\n",
      "52 True: central high principal david <unk> says the school had its prom friday night and that he was told price had just given another student a ride home\n",
      "52 Pred: the say the the in the the # of in in\n",
      "53 True: senior britney michelle lanier was killed in december in a wreck on alabama # near florence\n",
      "53 Pred: the say the the the the in the in the #\n",
      "54 True: junior frank graham iii died in september while swimming with friends near waterloo\n",
      "54 Pred: the say the the the in the in the the the\n",
      "55 True: a # year old st. louis dance instructor faces up to # years in prison after his conviction for statutory rape of a # year old girl\n",
      "55 Pred: the state is the <unk> <unk> : <unk> <unk> <unk> <unk> the of\n",
      "56 True: kmov tv ( bit.ly / <unk> ) reports that a st. louis jury found <unk> caston guilty on thursday after a four day trial\n",
      "56 Pred: the say ( reports http <unk> <unk> <unk> <unk> <unk> <unk> <unk> # <unk> : <unk> <unk>\n",
      "57 True: the teen was a ballet student at caston 's webster groves dance studio\n",
      "57 Pred: the say the the the of <unk> the # the # in in\n",
      "58 True: police say the crime happened between # and #\n",
      "58 Pred: the say the # of old <unk> <unk> # # # the # in\n",
      "59 True: caston also has separate charges pending against him for allegedly physically assaulting a woman at a chapel on the saint louis university campus\n",
      "59 Pred: the say is the in the in the\n",
      "60 True: preliminary results from a study of the san juan and animas rivers in northwestern new mexico 's san juan county reportedly has found high levels of bacteria associated with human waste\n",
      "60 Pred: the say the the # of the # of\n",
      "61 True: the daily times reports ( http : <unk> # n ) that preliminary results of the study indicate that leaky septic tanks and illegal waste dumping are the sources of the pollution\n",
      "61 Pred: the state of in : http : <unk> <unk> <unk> <unk> <unk> the #\n",
      "62 True: the study is being conducted by two farmington area organizations\n",
      "62 Pred: the state is the in in the the\n",
      "63 True: the san juan watershed group and san juan soil and water conservation district the study is scheduled for completion in january or early february\n",
      "63 Pred: the state is the the in the\n",
      "64 True: more than # percent of all trading today is automated , the commodity futures trading commission said tuesday as it voted unanimously in favor of new registration standards for high speed traders\n",
      "64 Pred: the say the in the the\n",
      "65 True: the systems use algorithms to spot <unk> in market data , allowing trading firms to deliver buy and sell orders in <unk>\n",
      "65 Pred: the state <unk> reports ( http <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> the #\n",
      "66 True: that technology has led to a number of high profile glitches , including one this summer that shut down the new york stock exchange for almost half a day\n",
      "66 Pred: the say is the the the # of the\n",
      "-----------Topic Samples-----------\n",
      "0  bow: department people u.s. monday thursday ' ? wednesday week office\n",
      "0  sent: the # is the the # of the #\n",
      "1  bow: percent million company cents shares points billion report stock trading\n",
      "1  sent: the # is the the # of the #\n",
      "2  bow: found authorities death arrested woman reports murder http home charged\n",
      "2  sent: the # is the the # of the #\n",
      "3  bow: movement published forum honored feature chapter ordinance auction tested participants\n",
      "3  sent: the # is the the # of the #\n",
      "4  bow: fire area reported wednesday a.m. reports authorities morning monday sunday\n",
      "4  sent: the # is the the # of the #\n",
      "5  bow: school million program http university reports students public schools health\n",
      "5  sent: the # is the the # of the #\n",
      "6  bow: republican bill gov senate house lawmakers campaign democratic u.s. committee\n",
      "6  sent: the # is the the # of the #\n",
      "7  bow: court attorney judge federal years prosecutors trial case guilty prison\n",
      "7  sent: the # is the the # of the #\n",
      "8  bow: water river event residents air season wildlife airport emergency fish\n",
      "8  sent: the # is the the # of the #\n",
      "9  bow: detained drowned unidentified killings foul incidents kenneth confronted kicked defender\n",
      "9  sent: the # is the the # of the #\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan occured\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'all_model_checkpoint_paths'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-50e0e0534bec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Nan occured'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_checkpoint_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeldir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_model_checkpoint_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'all_model_checkpoint_paths'"
     ]
    }
   ],
   "source": [
    "if len(logs) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "while epoch < config.epochs:\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = get_feed_dict(batch)\n",
    "\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, sent_loss_kl_categ_batch, sent_loss_kl_gmm_batch, ppls_batch = \\\n",
    "        sess.run([opt, loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, sent_loss_kl_categ, sent_loss_kl_gmm, topic_ppls], feed_dict = feed_dict)\n",
    "   \n",
    "        if sent_loss_kl_batch == np.inf:\n",
    "            print('Nan occured')\n",
    "            ckpt = tf.train.get_checkpoint_state(config.modeldir)\n",
    "            model_checkpoint_path = ckpt.all_model_checkpoint_paths[-1]\n",
    "            saver.restore(sess, model_checkpoint_path)            \n",
    "            break\n",
    "            \n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if ct%config.log_period==0:\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train, sent_loss_recon_train, sent_loss_kl_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_dev, sent_loss_dev, ppl_dev = get_loss(sess, dev_batches)\n",
    "\n",
    "            if config.warmup: beta_eval = beta.eval(session=sess)\n",
    "            global_step_log = sess.run(tf.train.get_global_step())            \n",
    "            \n",
    "#             if loss_dev < loss_min:\n",
    "#                 loss_min = loss_dev\n",
    "#                 saver.save(sess, config.modelpath, global_step=global_step_log)\n",
    "\n",
    "            clear_output()\n",
    "\n",
    "            time_finish = time.time()\n",
    "            time_log = int(time_finish - time_start)\n",
    "            logs += [(time_log, epoch, ct, loss_train, ppl_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train, sent_loss_recon_train, sent_loss_kl_train, loss_dev, ppl_dev, topic_loss_dev, sent_loss_dev, beta_eval)]\n",
    "            for log in logs:\n",
    "                print('%03d[s], Ep: %02d, Ct: %05d|TR LOSS: %.0f, PPL: %.0f|TM NLL: %.0f, KL: %.2f, REG:%.2f | LM NLL: %.2f, KL: %.2f|DE LOSS: %.0f, PPL: %.0f, TM: %.0f, LM: %.2f|BETA: %.6f' %  log)\n",
    "\n",
    "            print_sample(batch)\n",
    "\n",
    "            time_start = time.time()\n",
    "            \n",
    "            print_topic_sample()\n",
    "                \n",
    "    epoch += 1\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46746"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ? <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "# e <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "# e <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "# e <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "# e boston scientific # ? ? ? # ? ? ? # ? ? ? # ? ? ? # ? ? ? # <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "% abbott # ? ? ? # ? ? ? # ? ? ? # ? ? ? # ? ? ? # <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "% medtronic # ? ? ? # ? ? ? # ? ? ? # ? ? ? # ? ? ? # <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "% johnson & johnson # ? ? ? # ? ? ? # ? ? ? ? # ? ? ? ? # ? ? ? ? # <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "% other # ? ? ? ? # ? ? ? ? # ? ? ? ? # ? ? ? ? # ? ? ? ? # <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "% e : <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "the developers of a $ # million casino at arundel mills mall in hanover say they are accepting job applications <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "robert norton , president and general manager of the maryland live <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "casino , says the casino has launched an online application system and plans to fill # positions by april <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "he says interviews for some of the positions are already under way <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "hundreds of full and part time positions will be available in all departments , including facilities , security , marketing , finance , environmental services , information technology and food and beverage <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "the casino will have # slots machines and electronic table games <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "most of the machines and table games will open in june and the rest will open at the end of next year <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "online : http : <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "catholic school teachers in the philadelphia area have rejected a contract offer and are going on strike <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "lay instructors at the archdiocese of philadelphia 's # high schools voted # on tuesday to walk off the job after rejecting the diocese 's most recent proposal <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "school is scheduled to begin wednesday <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "the contract , which affects # lay teachers , does not involve elementary schools in the five county archdiocese <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "the diocese says it expects school to go on as usual this week , a time already scheduled for student orientation <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "officials with the diocese said in conference call tuesday that administrators and nonunion employees will supervise students during that time <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "union president rita schwartz says she hopes to continue talks later this week <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "defense contractor lockheed martin says it will team with alaska 's state owned aerospace corporation to pursue a u.s. missile defense agency contract to maintain and improve the country 's ground based missile defense system <pad> <pad> <pad>\n",
      "the system is designed to defend against a limited attack by intermediate and long range ballistic missiles <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "it includes <unk> missiles stationed at fort <unk> , alaska , about # miles south of fairbanks , and at vandenberg air force base in california <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "boeing was picked over lockheed as the system 's original prime contractor <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "but the missile defense agency on may # issued an amended draft request for proposals <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "a final request is expected this summer , with a five year contract awarded early next year <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "the military has valued the contract at about $ # million per year <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "tamarack resort 's owner says he 's selected a buyer <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "jean pierre boespflug said tuesday he plans to announce a buyer for the failed central idaho vacation development on wednesday morning <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "there are at least three offers , including one from an idaho group , green valley holdings , for $ # million , and another $ # million offer from a real estate company in salt lake city\n",
      "in federal bankruptcy court documents , boespflug has also requested the judge 's permission to hire a lawyer to oversee the sale transaction <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "boespflug 's troubled resort is in chapter # reorganization , so u.s. bankruptcy judge terry myers holds sway over a final transaction <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "myers has yet to publish a decision on a motion from the biggest creditor , a credit suisse group led lender syndicate , to return tamarack to a court where it could be liquidated <pad> <pad> <pad> <pad>\n",
      "the massachusetts port authority has approved a five year , $ # billion spending plan designed to pay for # improvement projects and create # construction jobs <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "the largest project is the building of a consolidated rental car facility for boston logan international airport <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "the $ # million project is intended to improve customer service , ease airport roadway and terminal curbside congestion and replace rental car diesel buses with alternative fuel buses and improve air quality <pad> <pad> <pad> <pad> <pad>\n",
      "other projects include an expanded safety area for runway # l to safely stop an aircraft that <unk> the runway , and the renovation and rehabilitation of terminals b , c and e to accommodate airline growth <pad>\n",
      "funding comes from a range of sources including customer charges , federal grants and revenue bonds <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<unk> receives no state tax dollars <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "officials with the rhode island public transit authority are set to hold the second in a series of scheduled hearings on planned service cuts statewide <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "the hearing is scheduled for wednesday in the burnside building in bristol <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "testimony will be heard from # to # p.m. and # to # p.m. officials have proposed reductions in service that will affect # communities and # bus routes across rhode island <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "some routes would be eliminated , some service would run less frequently and holiday service would be discontinued <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "officials are seeking to address a $ # million budget shortfall for the current fiscal year caused by less revenue from the state gas tax and the high cost of fuel to run the system <pad> <pad> <pad>\n",
      "<unk> has eight more public hearings scheduled through aug. # <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "visitors to the greek resort of <unk> are facing candlelit dinners with warm beer after a power station fire left the entire island without electricity at the peak of the tourist season <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "greece 's power network administrator said tuesday 's blaze has been extinguished and efforts are being made to gradually restore power to the aegean sea island as soon as possible <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "no injuries were reported from the fire , the cause of which was not immediately known <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<unk> is one of greece 's main tourist destinations , attracting about # million visitors last year <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "it is famous for the view from its dramatic seaside cliffs at sunset <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "local authorities said the greek navy would ship generators to the island by early wednesday <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "freshmen entering the university of south carolina gather on the columbia campus to hear dave eggers , the author of the futuristic novel `` the circle . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "it is the # st time that students gather to hear an author discuss a book chosen to be read by the entire incoming class <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "the book deals with issues such as privacy and transparency in a corporate world <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "the event starts at # a.m. monday in the carolina coliseum <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "it is designed to prompt reflection among new students and lead to small group discussions taking place across the campus later in the day <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "the former chief of the disbanded las vegas township constable office is due before a local judge on friday after being accused of misdemeanor domestic battery and coercion <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "john bonaventura 's defense attorney , robert pool , did n't immediately respond thursday to messages about an arrest warrant issued monday for his client 's arrest <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "pool is due to ask justice of the peace melanie andress <unk> to withdraw the warrant on a case that was sealed when it was filed last week <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "the las vegas constable 's office had about # deputies serving court papers and carrying out <unk> before it was dissolved dec. # by the clark county commission <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "under bonaventura , the office was beset by controversies , including about how money was handled , allegations of improper wiretapping and efforts to produce a reality television show <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "for line_idxs in input_token_idxs.eval(session=sess, feed_dict=feed_dict):\n",
    "    print(' '.join([idx_to_word[idx] for idx in line_idxs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "_prob_topic, _prob_topic_sents, _prob_topic_infer, _means_topic_infer = debug_value([prob_topic, prob_topic_sents, prob_topic_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.2281889e-02, 6.7835855e-03, 2.0132679e-01, 1.1674699e-02,\n",
       "        6.5264981e-03, 1.1411838e-02, 6.9189700e-03, 2.8612482e-04,\n",
       "        7.4042326e-01, 2.3663496e-03], dtype=float32),\n",
       " array([0.09889801, 0.11335436, 0.12141278, 0.10027827, 0.13508864,\n",
       "        0.09455997, 0.0860846 , 0.08042429, 0.09635323, 0.07354581],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_i = 4\n",
    "_prob_topic_sents[batch_i], _prob_topic_infer[batch_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02824139, -0.22793637,  0.3788033 , -0.02814816],\n",
       "       [ 0.06249218, -0.08085692,  0.06756439,  0.2259818 ],\n",
       "       [-0.1377574 ,  0.28185233,  0.23656711, -0.3201025 ],\n",
       "       [-0.03711405,  0.16953555,  0.16389738, -0.16461828],\n",
       "       [ 0.27302447, -0.22462857,  0.37200606, -0.00549014],\n",
       "       [-0.16755986,  0.11964113,  0.17052333,  0.18083367],\n",
       "       [ 0.26351342, -0.33828875,  0.3920058 , -0.17029142],\n",
       "       [ 0.21033517, -0.1955087 ,  0.20963705, -0.24809986],\n",
       "       [ 0.01922028, -0.03185754,  0.28022933, -0.13928899],\n",
       "       [-0.26817918, -0.3364467 ,  0.36924574,  0.15818927]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_means_topic_infer[0][:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_means_topic, b_means_topic = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"topic/dec/mean_topic\")\n",
    "\n",
    "pred_topic_embeddings, pred_topic_bow, pred_means_topic, pred_logvars_topic, pred_token_idxs, _w_means_topic, _b_means_topic, _w_mean_topic_infer = \\\n",
    "                                sess.run([topic_embeddings, topic_bow, means_topic, logvars_topic, topic_beam_output_token_idxs, w_means_topic, b_means_topic, w_mean_topic_infer], \n",
    "                                         feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "\n",
    "pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "\n",
    "pred_topics_freq_bow_indices = np.argsort(pred_topic_bow, 1)[:, ::-1][:, :10]\n",
    "pred_topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['million', 'http', 'years', 'tuesday', 'federal', 'u.s.', 'month', 'office', 'people', 'monday']\n",
      "['reports', 'department', 'thursday', 'court', 'time', 'u.s.', 'years', 'monday', 'tuesday', \"'\"]\n",
      "['school', 'public', 'u.s.', 'board', 'university', 'federal', 'president', 'program', 'health', 'students']\n",
      "['fire', 'found', 'authorities', 'hospital', 'home', 'a.m.', 'dead', 'investigation', 'chief', 'morning']\n",
      "['service', 'national', 'department', 'http', 'water', 'park', 'area', 'center', 'wednesday', 'weather']\n",
      "['percent', 'company', 'million', 'billion', 'shares', 'inc.', 'revenue', 'average', 'cents', 'price']\n",
      "['design', 'researchers', 'donations', 'prize', 'encourage', 'peak', 'awards', 'feature', 'chapter', 'affect']\n",
      "['court', 'arrested', 'charged', 'attorney', 'death', 'authorities', 'found', 'prison', 'shooting', 'woman']\n",
      "['bill', 'senate', 'house', 'republican', 'committee', 'public', 'board', 'approved', 'u.s.', 'law']\n",
      "['todd', 'rear', 'drowned', 'confronted', 'foul', 'transported', 'assaulted', 'loaded', 'defender', 'unidentified']\n"
     ]
    }
   ],
   "source": [
    "for idxs in pred_topics_freq_bow_idxs:\n",
    "    print([idx_to_word[idx] for idx in idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09207956,  0.09077395,  0.06120839, -0.22176096,  0.19921999,\n",
       "        -0.05485373, -0.23069587,  0.05650459,  0.142239  , -0.17410392],\n",
       "       [ 0.29974467, -0.03281972,  0.25064805, -0.2893645 , -0.1312135 ,\n",
       "        -0.11136644,  0.02060958, -0.13598587,  0.24592783, -0.18463418],\n",
       "       [ 0.22819577,  0.14566512,  0.05597903, -0.1109414 , -0.00879419,\n",
       "        -0.1605561 ,  0.11445126,  0.00916692, -0.04847115, -0.0603605 ],\n",
       "       [ 0.3365141 , -0.06439152,  0.3631938 ,  0.14203475,  0.10052318,\n",
       "        -0.09119357, -0.08768138, -0.28562906,  0.3186355 , -0.08452626],\n",
       "       [ 0.26727033,  0.03540339,  0.28315246, -0.1286456 , -0.02444943,\n",
       "        -0.163507  , -0.02146725, -0.21729966,  0.26542294, -0.1589102 ],\n",
       "       [ 0.28826687, -0.18438952,  0.2760168 , -0.12692557,  0.05485712,\n",
       "         0.17983724,  0.14970489, -0.36168167,  0.30563408, -0.13128197],\n",
       "       [-0.32988435, -0.11005852, -0.27013943,  0.26166642, -0.3013388 ,\n",
       "        -0.2234488 ,  0.02287463,  0.20320751, -0.10510623,  0.28636736],\n",
       "       [ 0.37211177, -0.00585993,  0.2853141 , -0.05771174,  0.15170154,\n",
       "         0.05411884, -0.24091715, -0.01475959, -0.0408047 , -0.17167823],\n",
       "       [ 0.20512433,  0.18697639, -0.08326819,  0.03792854,  0.03214281,\n",
       "         0.05618311,  0.09544107,  0.03913466, -0.06514693, -0.04000437],\n",
       "       [-0.3745221 , -0.06385167, -0.29924905,  0.08158813, -0.28344202,\n",
       "        -0.01799835,  0.02471407,  0.408938  , -0.169172  ,  0.37273586]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_topic_embeddings[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0327697e-03, 4.6811462e-03, 1.7024261e-04, ..., 3.3795869e-04,\n",
       "        1.5871278e-04, 6.6570101e-05],\n",
       "       [5.9443718e-04, 7.4376534e-03, 2.0111131e-04, ..., 7.4048399e-04,\n",
       "        1.5028552e-04, 8.9953668e-05],\n",
       "       [8.5580017e-04, 2.8901210e-03, 1.6464434e-04, ..., 3.1770146e-04,\n",
       "        1.9349743e-04, 3.0012103e-04],\n",
       "       ...,\n",
       "       [1.3188391e-04, 2.6697975e-03, 2.3009101e-04, ..., 8.1575691e-04,\n",
       "        1.5104198e-04, 3.2140684e-05],\n",
       "       [3.8558390e-04, 4.0810513e-03, 1.4347673e-04, ..., 1.4157601e-04,\n",
       "        1.8342295e-04, 4.7484271e-05],\n",
       "       [7.1679087e-06, 2.9757638e-07, 1.8662996e-04, ..., 5.9841594e-05,\n",
       "        1.7972868e-04, 5.0798093e-04]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_topic_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09637171, -0.09069463,  0.09532599, ..., -0.02989542,\n",
       "        -0.02514938,  0.01415634],\n",
       "       [-0.02764424,  0.01254364,  0.03703775, ..., -0.01512817,\n",
       "         0.02590305,  0.01404281],\n",
       "       [-0.08408152, -0.00924051, -0.08365072, ...,  0.05623364,\n",
       "        -0.05008389,  0.0209127 ],\n",
       "       ...,\n",
       "       [-0.00629492, -0.05131948, -0.05206006, ..., -0.0955721 ,\n",
       "        -0.01980347, -0.02646225],\n",
       "       [-0.00839109, -0.05062511, -0.05731497, ...,  0.07618995,\n",
       "         0.03893617,  0.06835916],\n",
       "       [ 0.07883421, -0.02804748,  0.09326512, ..., -0.0734218 ,\n",
       "         0.0045918 , -0.10043538]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_w_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03207342, -0.08436021,  0.36105958, -0.02925379, -0.26779142,\n",
       "       -0.4809294 , -0.04713235, -0.6384421 ,  0.49655244, -0.18943405,\n",
       "        0.32139724,  0.00376519, -0.02733823,  0.06968141,  0.12723993,\n",
       "        0.17086434,  0.4321125 ,  0.2930171 ,  0.43750855, -0.32728267,\n",
       "       -0.29320472,  0.46633536,  0.03467208, -0.11661331,  0.02106199,\n",
       "        0.10828511,  0.14413024,  0.00801178, -0.16259423,  0.31461757,\n",
       "       -0.25328413,  0.11320477], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03563995, -0.3379975 ,  1.6953684 , -0.3394102 , -1.3313038 ,\n",
       "        -1.9561913 , -0.34439525, -3.1041431 ,  2.302101  , -0.80340064,\n",
       "         1.374892  ,  0.10505254,  0.13142577,  0.36230466,  0.5036737 ,\n",
       "         0.6154421 ,  1.8879794 ,  1.263777  ,  2.0281937 , -1.4289141 ,\n",
       "        -1.3110695 ,  2.0390706 ,  0.32288784, -0.39520237,  0.10320839,\n",
       "         0.35608286,  0.61754334, -0.1438482 , -0.74413097,  1.4731948 ,\n",
       "        -1.1249324 ,  0.33894244],\n",
       "       [ 0.03510073, -0.3374708 ,  1.6888778 , -0.3363807 , -1.3308815 ,\n",
       "        -1.9608293 , -0.34732953, -3.1081154 ,  2.3005512 , -0.80602133,\n",
       "         1.3695524 ,  0.10256523,  0.13283278,  0.36182955,  0.49822897,\n",
       "         0.6213894 ,  1.8883276 ,  1.266403  ,  2.023539  , -1.426166  ,\n",
       "        -1.3108103 ,  2.0448976 ,  0.32430837, -0.38903922,  0.10560946,\n",
       "         0.3596735 ,  0.6160856 , -0.14840832, -0.74532497,  1.4738762 ,\n",
       "        -1.1249926 ,  0.33331037],\n",
       "       [ 0.03490497, -0.33676392,  1.6840277 , -0.335459  , -1.3295547 ,\n",
       "        -1.95664   , -0.3462094 , -3.1003447 ,  2.296549  , -0.80408335,\n",
       "         1.367739  ,  0.10281808,  0.13425133,  0.36240244,  0.4972452 ,\n",
       "         0.6206323 ,  1.885982  ,  1.2645416 ,  2.0215812 , -1.4212072 ,\n",
       "        -1.3081043 ,  2.0386996 ,  0.32136053, -0.39039296,  0.1050597 ,\n",
       "         0.36023104,  0.6137113 , -0.15039542, -0.7433981 ,  1.4694595 ,\n",
       "        -1.1245916 ,  0.3360805 ],\n",
       "       [ 0.03728028, -0.33768234,  1.6832604 , -0.3353232 , -1.325423  ,\n",
       "        -1.9593027 , -0.34599763, -3.1006794 ,  2.2962196 , -0.80693865,\n",
       "         1.3678529 ,  0.10114851,  0.13371727,  0.36420822,  0.49783063,\n",
       "         0.62049437,  1.8832903 ,  1.2625886 ,  2.0200832 , -1.421174  ,\n",
       "        -1.3108637 ,  2.0388248 ,  0.32396334, -0.38813725,  0.10552703,\n",
       "         0.3586173 ,  0.6122677 , -0.15264365, -0.74335086,  1.468328  ,\n",
       "        -1.1214948 ,  0.337562  ],\n",
       "       [ 0.0355639 , -0.33673748,  1.6850188 , -0.334822  , -1.3297123 ,\n",
       "        -1.9579692 , -0.34517902, -3.0999303 ,  2.296747  , -0.8048171 ,\n",
       "         1.3683693 ,  0.10311563,  0.13507861,  0.36239594,  0.4968614 ,\n",
       "         0.61993766,  1.8848993 ,  1.2634866 ,  2.0213115 , -1.4220002 ,\n",
       "        -1.3081868 ,  2.0382977 ,  0.32180363, -0.39071375,  0.10513556,\n",
       "         0.35903966,  0.6132862 , -0.15032193, -0.74355686,  1.4703372 ,\n",
       "        -1.1240762 ,  0.3361809 ],\n",
       "       [ 0.03776056, -0.33906677,  1.6886759 , -0.33578348, -1.3277831 ,\n",
       "        -1.9574624 , -0.3511446 , -3.1008887 ,  2.298576  , -0.80058026,\n",
       "         1.3695161 ,  0.10116885,  0.13310117,  0.36374724,  0.49994904,\n",
       "         0.62002206,  1.8863395 ,  1.2639412 ,  2.0218256 , -1.4257591 ,\n",
       "        -1.3068638 ,  2.038707  ,  0.3251842 , -0.38946715,  0.10602362,\n",
       "         0.3615039 ,  0.6127277 , -0.15162012, -0.739843  ,  1.4696462 ,\n",
       "        -1.1235677 ,  0.34125343],\n",
       "       [ 0.03675649, -0.33748797,  1.6818695 , -0.3343631 , -1.3270382 ,\n",
       "        -1.9566132 , -0.34484407, -3.0962553 ,  2.2958052 , -0.8025343 ,\n",
       "         1.365838  ,  0.10129849,  0.13410446,  0.36187994,  0.49527365,\n",
       "         0.62073433,  1.8827194 ,  1.264018  ,  2.019868  , -1.4191431 ,\n",
       "        -1.3083122 ,  2.036423  ,  0.32277   , -0.38972613,  0.10532874,\n",
       "         0.36029965,  0.6137044 , -0.14923215, -0.7424054 ,  1.4679198 ,\n",
       "        -1.1228634 ,  0.33640632],\n",
       "       [ 0.03632384, -0.33631054,  1.6842166 , -0.33446336, -1.3275865 ,\n",
       "        -1.959459  , -0.34500512, -3.1014085 ,  2.2958436 , -0.804686  ,\n",
       "         1.3688817 ,  0.10245646,  0.13472772,  0.3634492 ,  0.49877107,\n",
       "         0.61891234,  1.8855013 ,  1.263787  ,  2.0223496 , -1.4211094 ,\n",
       "        -1.309351  ,  2.0364125 ,  0.32266062, -0.3891954 ,  0.10458721,\n",
       "         0.35922444,  0.61319816, -0.14912874, -0.7429457 ,  1.469351  ,\n",
       "        -1.1232082 ,  0.3378038 ],\n",
       "       [ 0.03574873, -0.3368752 ,  1.6819855 , -0.33521634, -1.3290966 ,\n",
       "        -1.9556532 , -0.34607896, -3.0987115 ,  2.2963123 , -0.80486095,\n",
       "         1.3669474 ,  0.10132913,  0.13473387,  0.36283588,  0.49758494,\n",
       "         0.6216788 ,  1.8858868 ,  1.2627757 ,  2.0219636 , -1.4208897 ,\n",
       "        -1.3082187 ,  2.0389626 ,  0.32159996, -0.39116064,  0.1062526 ,\n",
       "         0.35957128,  0.6122661 , -0.14942539, -0.74453473,  1.468337  ,\n",
       "        -1.123489  ,  0.33671775],\n",
       "       [ 0.03600421, -0.33810538,  1.6826388 , -0.33433414, -1.3267329 ,\n",
       "        -1.9560882 , -0.34396413, -3.096003  ,  2.2952938 , -0.8032101 ,\n",
       "         1.366725  ,  0.10274363,  0.13443008,  0.36315334,  0.4956261 ,\n",
       "         0.6213716 ,  1.882305  ,  1.2623034 ,  2.0204127 , -1.4200761 ,\n",
       "        -1.3082595 ,  2.034914  ,  0.32189733, -0.38998067,  0.10444504,\n",
       "         0.35928738,  0.6142209 , -0.1489011 , -0.74237704,  1.4684169 ,\n",
       "        -1.1216142 ,  0.3379858 ]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02438005, -0.02036242, -0.01039507,  0.00277665,  0.02350294,\n",
       "         0.01473409, -0.02157389,  0.02251465,  0.00814133,  0.01785252],\n",
       "       [ 0.00074128, -0.001453  , -0.01162554, -0.0014644 , -0.02097618,\n",
       "         0.00570552,  0.00674119, -0.01879225,  0.02161046,  0.01941887],\n",
       "       [ 0.01402811, -0.00613734, -0.0027943 , -0.01052572, -0.00041091,\n",
       "         0.0045091 , -0.01547336,  0.0194314 ,  0.00784631,  0.00505763],\n",
       "       [ 0.01655601, -0.02403533,  0.01607199, -0.01643238,  0.00686195,\n",
       "        -0.01788632,  0.01109112,  0.00087933,  0.00448566, -0.00823843],\n",
       "       [-0.00695892,  0.01631877,  0.01657149,  0.01772625,  0.02026216,\n",
       "         0.01811663,  0.02153757, -0.00020743, -0.02066918, -0.00025119],\n",
       "       [ 0.02195978,  0.00867016, -0.02278856,  0.0122384 , -0.02183604,\n",
       "         0.00798479,  0.00545774,  0.00923075,  0.01814621,  0.02018008],\n",
       "       [ 0.02242495, -0.00673729,  0.01447002,  0.02163557,  0.02312511,\n",
       "        -0.00920323, -0.01116639,  0.01708085, -0.01181559, -0.00763743],\n",
       "       [-0.01978129,  0.00916991, -0.00304236,  0.00106503,  0.00690803,\n",
       "        -0.01416942,  0.0063372 ,  0.00018093,  0.00393429, -0.00307162],\n",
       "       [-0.01821756,  0.01890057,  0.00228031, -0.01240117, -0.01108933,\n",
       "        -0.01674686, -0.01161283, -0.01693372,  0.00370204, -0.00271433],\n",
       "       [-0.0158878 ,  0.01951969, -0.01606018, -0.00457847,  0.00044555,\n",
       "        -0.0050344 , -0.0013686 ,  0.00539651,  0.02015766, -0.00510509]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_w_mean_topic_infer[:, :10, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03922943,  0.46112132, -0.08166478, -0.06917454, -0.01617851,\n",
       "        0.18225916, -0.28025913, -0.06620797,  0.03231472, -0.02211766,\n",
       "       -0.38082743,  0.11382294,  0.39363608, -0.4168986 ,  1.1677127 ,\n",
       "        0.08979508,  0.03593519, -0.07863234,  0.12493958,  0.03175303,\n",
       "        0.13351525,  0.8176425 , -0.28750482,  0.37927672, -0.01720515,\n",
       "       -0.21228473, -0.05377329, -0.10451841, -0.24399954, -0.25936088,\n",
       "        0.0845396 ,  0.33591354], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enc_state_infer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-ee7c3cd147b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_enc_state_infer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_means_topic_infer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdebug_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menc_state_infer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans_topic_infer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'enc_state_infer' is not defined"
     ]
    }
   ],
   "source": [
    "_enc_state_infer, _means_topic_infer = debug_value([enc_state_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67, 10, 1024)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_enc_state_infer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00],\n",
       "       [-4.4230181e-01, -8.2235533e-01, -3.9928532e-01,  3.3004081e-01,\n",
       "         1.2879696e+00,  2.5491878e-01,  5.9280634e+00,  1.8478313e-01,\n",
       "         1.1477946e-01,  1.1729380e+00, -3.8528564e+00, -1.1789608e+00,\n",
       "         9.1431034e-01, -1.7245387e+00, -2.9743382e-01,  1.0960317e+00,\n",
       "         5.1114732e-01,  8.8052756e-01,  5.0135303e-01,  8.0816686e-01,\n",
       "        -2.1616230e+00, -1.2552780e+00, -8.6165917e-01, -1.0416836e+00,\n",
       "         2.4590290e+00, -5.1620197e+00,  1.5814751e-03,  1.3141291e+00,\n",
       "        -6.7580324e-01,  2.8643382e+00, -2.3818936e+00, -2.5698123e+00]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_means_topic_infer[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
