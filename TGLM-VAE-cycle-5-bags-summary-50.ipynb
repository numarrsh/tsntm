{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from data_structure import get_batches, get_test_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "from topic_model import TopicModel\n",
    "\n",
    "from topic_beam_search_decoder import BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '3', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/bags/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/tglm_vae_tmp2', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'bags', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 50, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 500, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.1, 'lr')\n",
    "flags.DEFINE_float('reg', 1., 'regularization term')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_bool('warmup', True, 'flg of warming up')\n",
    "flags.DEFINE_integer('epochs_cycle', 5, 'number of epochs within a cycle')\n",
    "flags.DEFINE_float('r_cycle', 0.5, 'proportion used to increase beta within a cycle')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 50, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_bow', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_hidden_topic', 512, 'dim_hidden_topic')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "flags.DEFINE_bool('bidirectional', True, 'flg of bidirectional encoding')\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_test_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('PAD_IDX', word_to_idx[PAD], 'PAD_IDX')\n",
    "flags.DEFINE_integer('UNK_IDX', word_to_idx[UNK], 'UNK_IDX')\n",
    "flags.DEFINE_integer('BOS_IDX', word_to_idx[BOS], 'BOS_IDX')\n",
    "flags.DEFINE_integer('EOS_IDX', word_to_idx[EOS], 'EOS_IDX')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "maximum_iterations = max([max([instance.max_sent_l for instance in batch]) for ct, batch in dev_batches])\n",
    "flags.DEFINE_integer('maximum_iterations', maximum_iterations, 'maximum_iterations')\n",
    "\n",
    "flags.DEFINE_integer('cycle_steps', len(train_batches)*config.epochs_cycle, 'number of steps for each cycle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build language model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['bow'] = tf.placeholder(tf.float32, [None, config.dim_bow], name='bow')\n",
    "t_variables['input_token_idxs'] = tf.placeholder(tf.int32, [None, None], name='input_token_idxs')\n",
    "t_variables['dec_input_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_input_idxs')\n",
    "t_variables['dec_target_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_target_idxs')\n",
    "t_variables['batch_l'] = tf.placeholder(tf.int32, name='batch_l')\n",
    "t_variables['doc_l'] = tf.placeholder(tf.int32, [None], name='doc_l')\n",
    "t_variables['sent_l'] = tf.placeholder(tf.int32, [None], name='sent_l')\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train', assertion=False):\n",
    "    def token_dropout(sent_idxs):\n",
    "        sent_idxs_dropout = np.asarray(sent_idxs)\n",
    "        sent_idxs_dropout[np.random.rand(len(sent_idxs)) > config.word_keep_prob] = config.UNK_IDX\n",
    "        return list(sent_idxs_dropout)\n",
    "\n",
    "    bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "    \n",
    "    doc_l = np.array([len(instance.token_idxs) for instance in batch])\n",
    "    \n",
    "    feed_input_token_idxs_list = [sent_idxs for instance in batch for sent_idxs in instance.token_idxs]\n",
    "    feed_dec_input_idxs_list = [[config.BOS_IDX] + token_dropout(sent_idxs) for sent_idxs in feed_input_token_idxs_list]\n",
    "    feed_dec_target_idxs_list = [sent_idxs + [config.EOS_IDX]  for sent_idxs in feed_input_token_idxs_list]\n",
    "        \n",
    "    sent_l = np.array([len(sent_idxs) for sent_idxs in feed_input_token_idxs_list], np.int32)\n",
    "    batch_l = len(sent_l)\n",
    "    \n",
    "    feed_input_token_idxs = pad_sequences(feed_input_token_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_input_idxs = pad_sequences(feed_dec_input_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_target_idxs = pad_sequences(feed_dec_target_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    \n",
    "    if assertion:\n",
    "        index = 0\n",
    "        for instance in batch:\n",
    "            for line_idxs in instance.token_idxs:\n",
    "                assert feed_input_token_idxs_list[index] == line_idxs\n",
    "                index += 1\n",
    "        assert feed_input_token_idxs.shape[1] == np.max(sent_l)\n",
    "        assert feed_dec_input_idxs.shape[1] == np.max(sent_l) + 1\n",
    "        assert feed_dec_target_idxs.shape[1] == np.max(sent_l) + 1\n",
    "    \n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "\n",
    "    feed_dict = {\n",
    "                t_variables['bow']: bow, \n",
    "                t_variables['batch_l']: batch_l, t_variables['doc_l']: doc_l, t_variables['sent_l']: sent_l, \n",
    "                t_variables['input_token_idxs']: feed_input_token_idxs, t_variables['dec_input_idxs']: feed_dec_input_idxs, t_variables['dec_target_idxs']: feed_dec_target_idxs, \n",
    "                t_variables['keep_prob']: keep_prob\n",
    "    }\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     10,
     24,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables):\n",
    "    sample_batch = dev_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "def check_shape(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()\n",
    "    \n",
    "def check_value(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()    \n",
    "    \n",
    "# sent_loss_kl_categ_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, tf_log(prob_topic_infer/prob_topic_sents)), 1))\n",
    "# debug_value([sent_loss_kl_categ, sent_loss_kl_categ_tmp])\n",
    "# sent_loss_kl_gauss_tmp = 0.5 * tf.reduce_sum(tf.exp(logvars_topic_infer-logvars_topic) + tf.square(means_topic - means_topic_infer) / tf.exp(logvars_topic) - 1 + (logvars_topic - logvars_topic_infer), -1)\n",
    "# sent_loss_kl_gmm_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss_tmp), -1))\n",
    "# debug_value([sent_loss_kl_gmm_tmp, sent_loss_kl_gmm])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encode bow\n",
    "with tf.variable_scope('topic/enc', reuse=False):\n",
    "    hidden_bow_ = tf.layers.Dense(units=config.dim_hidden_bow, activation=tf.nn.relu, name='hidden_bow')(t_variables['bow'])\n",
    "    hidden_bow = tf.layers.Dropout(t_variables['keep_prob'])(hidden_bow_)\n",
    "    means_bow = tf.layers.Dense(units=config.dim_latent_bow, name='mean_bow')(hidden_bow)\n",
    "    logvars_bow = tf.layers.Dense(units=config.dim_latent_bow, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_bow)\n",
    "    latents_bow = sample_latents(means_bow, logvars_bow) # sample latent vectors\n",
    "\n",
    "    prob_topic = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic')(latents_bow) # inference of topic probabilities\n",
    "\n",
    "# decode bow\n",
    "with tf.variable_scope('shared', reuse=False):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "bow_embeddings = tf.nn.embedding_lookup(embeddings, bow_idxs) # embeddings of each bow features\n",
    "\n",
    "with tf.variable_scope('topic/dec', reuse=False):\n",
    "    topic_embeddings = tf.get_variable('topic_emb', [config.n_topic, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of topics\n",
    "\n",
    "    topic_bow = tf.nn.softmax(tf.matmul(topic_embeddings, bow_embeddings, transpose_b=True), 1) # bow vectors for each topic\n",
    "    logits_bow = tf_log(tf.matmul(prob_topic, topic_bow)) # predicted bow distribution\n",
    "\n",
    "    # prior of each gaussian distribution (computed for each topic)\n",
    "    hidden_topic = tf.layers.Dense(units=config.dim_hidden_topic, activation=tf.nn.relu, name='hidden_topic')(topic_bow)\n",
    "    means_topic = tf.layers.Dense(units=config.dim_latent, name='mean_topic')(hidden_topic)\n",
    "    logvars_topic = tf.layers.Dense(units=config.dim_latent, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_topic)\n",
    "    sigma_topic = tf.exp(0.5 * logvars_topic)\n",
    "    gauss_topic = tfd.Normal(loc=means_topic, scale=sigma_topic)    \n",
    "    \n",
    "# define losses\n",
    "topic_losses_recon = -tf.reduce_sum(tf.multiply(t_variables['bow'], logits_bow), 1)\n",
    "topic_loss_recon = tf.reduce_mean(topic_losses_recon) # negative log likelihood of each words\n",
    "\n",
    "topic_loss_kl = compute_kl_loss(means_bow, logvars_bow) # KL divergence b/w latent dist & gaussian std\n",
    "\n",
    "topic_bow_norm = topic_bow / tf.norm(topic_bow, axis=1, keepdims=True)\n",
    "topic_dots = tf.clip_by_value(tf.matmul(topic_bow_norm, tf.transpose(topic_bow_norm)), -1., 1.)\n",
    "topic_loss_reg = tf.reduce_mean(tf.square(topic_dots - tf.eye(config.n_topic)))\n",
    "# topic_angles = tf.acos(topic_dots)\n",
    "# topic_angles_mean = tf.reduce_mean(topic_angles)\n",
    "# topic_angles_vars = tf.reduce_mean(tf.square(topic_angles - topic_angles_mean))\n",
    "# topic_loss_reg = tf.exp(topic_angles_vars - topic_angles_mean)\n",
    "\n",
    "# monitor\n",
    "n_bow = tf.reduce_sum(t_variables['bow'], 1)\n",
    "topic_ppls = tf.divide(topic_losses_recon, tf.maximum(1e-5, n_bow))\n",
    "topics_freq_bow_indices = tf.nn.top_k(topic_bow, 10, name='topic_freq_bow').indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "input_token_idxs = t_variables['input_token_idxs']\n",
    "batch_l = t_variables['batch_l']\n",
    "sent_l = t_variables['sent_l']\n",
    "max_sent_l = tf.reduce_max(sent_l)\n",
    "\n",
    "with tf.variable_scope('sent/enc', reuse=False):\n",
    "    # get word embedding\n",
    "    enc_input = tf.nn.embedding_lookup(embeddings, input_token_idxs)\n",
    "\n",
    "    # get sentence embedding\n",
    "    _, enc_state = dynamic_bi_rnn(enc_input, sent_l, config.dim_hidden, t_variables['keep_prob'])\n",
    "\n",
    "    # TODO House Holder flow\n",
    "    hidden_topic_infer =  tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='hidden_topic_infer')(enc_state)\n",
    "    prob_topic_infer = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic_infer')(hidden_topic_infer)\n",
    "\n",
    "    w_mean_topic_infer = tf.get_variable('mean_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32)\n",
    "    b_mean_topic_infer = tf.get_variable('mean_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32)\n",
    "    means_topic_infer = tf.tensordot(enc_state, w_mean_topic_infer, axes=[[1], [1]]) + b_mean_topic_infer\n",
    "    \n",
    "    w_logvar_topic_infer = tf.get_variable('logvar_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    b_logvar_topic_infer = tf.get_variable('logvar_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    logvars_topic_infer = tf.tensordot(enc_state, w_logvar_topic_infer, axes=[[1], [1]]) + b_logvar_topic_infer\n",
    "    sigma_topic_infer = tf.exp(0.5 * logvars_topic_infer)\n",
    "    gauss_topic_infer = tfd.Normal(loc=means_topic_infer, scale=sigma_topic_infer)\n",
    "    \n",
    "    # latent vectors from each gaussian dist.\n",
    "    latents_topic_infer = sample_latents(means_topic_infer, logvars_topic_infer) \n",
    "    # latent vector from gaussian mixture\n",
    "    latents_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), latents_topic_infer, transpose_a=True)\n",
    "    \n",
    "    # for beam search\n",
    "    means_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), means_topic_infer, transpose_a=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for decoding\n",
    "dec_sent_l = tf.add(sent_l, 1)\n",
    "dec_input_idxs = t_variables['dec_input_idxs']\n",
    "dec_input = tf.nn.embedding_lookup(embeddings, dec_input_idxs)\n",
    "\n",
    "dec_latents_input = tf.tile(latents_input, [1, tf.shape(dec_input)[1], 1])\n",
    "dec_concat_input = tf.concat([dec_input, dec_latents_input], -1)\n",
    "\n",
    "# decode for training\n",
    "with tf.variable_scope('sent/dec/rnn', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32, reuse=False):\n",
    "    dec_cell = tf.contrib.rnn.GRUCell(config.dim_hidden)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob = t_variables['keep_prob'])\n",
    "\n",
    "    dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(latents_input, 1))\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_concat_input, sequence_length=dec_sent_l)\n",
    "\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=dec_cell,\n",
    "        helper=helper,\n",
    "        initial_state=dec_initial_state)\n",
    "\n",
    "    dec_outputs, _, output_sent_l = tf.contrib.seq2seq.dynamic_decode(train_decoder)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(config.n_vocab, use_bias=False, name='out')\n",
    "    output_logits = output_layer(dec_outputs.rnn_output)\n",
    "    \n",
    "    output_token_idxs = tf.argmax(output_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tokens = tf.fill([batch_l], config.BOS_IDX)\n",
    "end_token = config.EOS_IDX\n",
    "\n",
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    infer_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(means_input, 1))\n",
    "    beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(infer_dec_initial_state, multiplier=config.beam_width)\n",
    "    beam_latents_input = tf.contrib.seq2seq.tile_batch(tf.squeeze(means_input, 1), multiplier=config.beam_width) # added\n",
    "    \n",
    "    beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=beam_latents_input)\n",
    "\n",
    "    beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    beam_output_token_idxs = beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    inter_means_input = tf.placeholder(tf.float32, [None, config.dim_latent])\n",
    "    \n",
    "    inter_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(inter_means_input)\n",
    "    inter_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(inter_dec_initial_state, multiplier=config.beam_width)\n",
    "    inter_beam_latents_input = tf.contrib.seq2seq.tile_batch(inter_means_input, multiplier=config.beam_width) # added\n",
    "    \n",
    "    inter_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=inter_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=inter_beam_latents_input)\n",
    "\n",
    "    inter_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inter_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    inter_beam_output_token_idxs = inter_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    topic_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic)\n",
    "    topic_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(topic_dec_initial_state, multiplier=config.beam_width)\n",
    "    topic_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic, multiplier=config.beam_width) # added\n",
    "    \n",
    "    topic_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=topic_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=topic_beam_latents_input)\n",
    "\n",
    "    topic_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        topic_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    topic_beam_output_token_idxs = topic_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    means_topic_summary = tf.reduce_mean(means_topic_infer, 0)\n",
    "    \n",
    "    summary_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic_summary)\n",
    "    summary_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(summary_dec_initial_state, multiplier=config.beam_width)\n",
    "    summary_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic_summary, multiplier=config.beam_width) # added\n",
    "    \n",
    "    summary_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=summary_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width,\n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=summary_beam_latents_input)\n",
    "\n",
    "    summary_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        summary_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    summary_beam_output_token_idxs = summary_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language modeling cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target and mask\n",
    "dec_target_idxs = t_variables['dec_target_idxs']\n",
    "dec_mask_tokens = tf.sequence_mask(dec_sent_l, maxlen=max_sent_l+1, dtype=tf.float32)\n",
    "\n",
    "# nll for each token (averaged over batch & sentence)\n",
    "sent_loss_recon = tf.contrib.seq2seq.sequence_loss(output_logits, dec_target_idxs, dec_mask_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_l = t_variables['doc_l']\n",
    "mask_sents = tf.sequence_mask(doc_l)\n",
    "mask_sents_flatten = tf.reshape(mask_sents, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1]])\n",
    "\n",
    "prob_topic_tiled = tf.tile(tf.expand_dims(prob_topic, 1), [1, tf.shape(mask_sents)[1], 1])\n",
    "prob_topic_flatten = tf.reshape(prob_topic_tiled, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1], config.n_topic])\n",
    "prob_topic_sents = tf.boolean_mask(prob_topic_flatten, mask_sents_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferred mixture probabilities (computed for each sentence)\n",
    "categ_topic_infer = tfd.Categorical(probs=prob_topic_infer)\n",
    "\n",
    "# prior of mixture probabilities (computed for each document, tiled for each sentence)\n",
    "categ_topic = tfd.Categorical(probs=prob_topic_sents)\n",
    "\n",
    "sent_loss_kl_categ = tf.reduce_mean(tfd.kl_divergence(categ_topic_infer, categ_topic))\n",
    "\n",
    "# inference of each gaussian gaussribution (computed for each sentence)\n",
    "\n",
    "sent_loss_kl_gauss = tf.reduce_sum(tfd.kl_divergence(gauss_topic_infer, gauss_topic), -1)\n",
    "sent_loss_kl_gmm = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss), -1))\n",
    "\n",
    "sent_loss_kl = sent_loss_kl_categ + sent_loss_kl_gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "tau = tf.cast(tf.divide(tf.mod(global_step, tf.constant(config.cycle_steps)), tf.constant(config.cycle_steps)), dtype=tf.float32)\n",
    "beta = tf.minimum(1., tau/config.r_cycle)\n",
    "\n",
    "sent_loss = sent_loss_recon + beta * sent_loss_kl\n",
    "\n",
    "topic_loss = topic_loss_recon + topic_loss_kl + config.reg * topic_loss_reg\n",
    "loss = topic_loss + sent_loss\n",
    "\n",
    "# define optimizer\n",
    "if config.opt == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif config.opt == 'Adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "    \n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_sents(token_idxs, config, idx_to_word):\n",
    "    sents = []\n",
    "    for sent_idxs in token_idxs:\n",
    "        tokens = []\n",
    "        for idx in sent_idxs:\n",
    "            if idx == config.EOS_IDX: break\n",
    "            tokens.append(idx_to_word[idx])\n",
    "        sent = ' '.join(tokens)\n",
    "        sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, ppls_batch \\\n",
    "            = sess.run([loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, topic_ppls], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "    loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, sent_loss_recon_mean, sent_loss_kl_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    return loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, sent_loss_recon_mean, sent_loss_kl_mean, ppl_mean\n",
    "\n",
    "def get_all_losses(sess, batches):\n",
    "    losses = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch = \\\n",
    "        sess.run([loss, topic_loss_recon, topic_loss_kl, sent_loss_recon, sent_loss_kl], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "    print('LOSS %.2f | TM NLL: %.2f, KL: %.4f | LM NLL: %.2f, KL: %.4f' %  np.mean(losses, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_batch):\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    pred_token_idxs = sess.run(output_token_idxs, feed_dict = feed_dict)\n",
    "    true_token_idxs = [sent_idxs for instance in sample_batch for sent_idxs in instance.token_idxs]\n",
    "    \n",
    "    assert len(pred_token_idxs) == len(true_token_idxs)\n",
    "    \n",
    "    pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "    true_sents = idxs_to_sents(true_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    for i, (true_sent, pred_sent) in enumerate(zip(true_sents, pred_sents)):        \n",
    "        print(i, 'TRUE: %s' % true_sent)\n",
    "        print(i, 'PRED: %s' % pred_sent)\n",
    "\n",
    "def print_topic_sample():\n",
    "    pred_topics_freq_bow_indices, pred_topic_token_idxs = sess.run([topics_freq_bow_indices, topic_beam_output_token_idxs], \n",
    "                                                                                                           feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "    pred_topic_sents = idxs_to_sents(pred_topic_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Topic Samples-----------')\n",
    "    for i, (topic_freq_bow_idxs, pred_topic_sent) in enumerate(zip(topics_freq_bow_idxs, pred_topic_sents)):\n",
    "        print(i, ' BOW:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' SENTENCE:', pred_topic_sent)\n",
    "        \n",
    "def print_summary(test_batch):\n",
    "    feed_dict = get_feed_dict(test_batch)\n",
    "    feed_dict[t_variables['batch_l']] = config.n_topic\n",
    "    feed_dict[t_variables['keep_prob']] = 1.\n",
    "    pred_topics_freq_bow_indices, pred_summary_token_idxs = sess.run([topics_freq_bow_indices, summary_beam_output_token_idxs], feed_dict=feed_dict)\n",
    "    pred_summary_sents = idxs_to_sents(pred_summary_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Output sentences for each topic-----------')\n",
    "    print('Item idx:', test_batch[0].item_idx)\n",
    "    for i, (topic_freq_bow_idxs, pred_summary_sent) in enumerate(zip(topics_freq_bow_idxs, pred_summary_sents)):\n",
    "        print(i, ' BOW:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' SENTENCE:', pred_summary_sent)\n",
    "        \n",
    "    print('-----------Summaries-----------')\n",
    "    for i, summary in enumerate(test_batch[0].summaries):\n",
    "        print('SUMMARY %i :'%i, '\\n', summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "saver = tf.train.Saver(max_to_keep=10)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','TM','','','','LM','','VALID:','TM','','','','LM','', ''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL','REG','NLL','KL','LOSS','PPL','NLL','KL','REG','NLL','KL', 'Beta']]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>LM</th>\n",
       "      <th></th>\n",
       "      <th>VALID:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>LM</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>133.01</td>\n",
       "      <td>1035</td>\n",
       "      <td>122.48</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.98</td>\n",
       "      <td>9.13</td>\n",
       "      <td>0.80</td>\n",
       "      <td>126.55</td>\n",
       "      <td>1035</td>\n",
       "      <td>116.16</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.98</td>\n",
       "      <td>9.12</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>121.21</td>\n",
       "      <td>614</td>\n",
       "      <td>114.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.71</td>\n",
       "      <td>6.34</td>\n",
       "      <td>0.98</td>\n",
       "      <td>111.72</td>\n",
       "      <td>536</td>\n",
       "      <td>105.34</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.49</td>\n",
       "      <td>5.76</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>120.42</td>\n",
       "      <td>584</td>\n",
       "      <td>113.63</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.57</td>\n",
       "      <td>6.07</td>\n",
       "      <td>0.78</td>\n",
       "      <td>111.25</td>\n",
       "      <td>521</td>\n",
       "      <td>104.89</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.41</td>\n",
       "      <td>5.69</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>1500</td>\n",
       "      <td>119.95</td>\n",
       "      <td>570</td>\n",
       "      <td>113.25</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.51</td>\n",
       "      <td>5.96</td>\n",
       "      <td>0.66</td>\n",
       "      <td>111.21</td>\n",
       "      <td>521</td>\n",
       "      <td>104.78</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.33</td>\n",
       "      <td>5.62</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>119.55</td>\n",
       "      <td>560</td>\n",
       "      <td>112.89</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.46</td>\n",
       "      <td>5.88</td>\n",
       "      <td>0.59</td>\n",
       "      <td>110.76</td>\n",
       "      <td>513</td>\n",
       "      <td>104.53</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.28</td>\n",
       "      <td>5.48</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119.40</td>\n",
       "      <td>554</td>\n",
       "      <td>112.75</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.43</td>\n",
       "      <td>5.84</td>\n",
       "      <td>0.56</td>\n",
       "      <td>110.46</td>\n",
       "      <td>506</td>\n",
       "      <td>104.26</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.23</td>\n",
       "      <td>5.39</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2776</th>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>119.02</td>\n",
       "      <td>547</td>\n",
       "      <td>112.39</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.39</td>\n",
       "      <td>5.76</td>\n",
       "      <td>0.51</td>\n",
       "      <td>110.20</td>\n",
       "      <td>495</td>\n",
       "      <td>103.92</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.19</td>\n",
       "      <td>5.26</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3276</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>118.66</td>\n",
       "      <td>539</td>\n",
       "      <td>112.05</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.36</td>\n",
       "      <td>5.69</td>\n",
       "      <td>0.48</td>\n",
       "      <td>109.85</td>\n",
       "      <td>488</td>\n",
       "      <td>103.66</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.15</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3776</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>118.51</td>\n",
       "      <td>534</td>\n",
       "      <td>111.92</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.33</td>\n",
       "      <td>5.63</td>\n",
       "      <td>0.45</td>\n",
       "      <td>109.62</td>\n",
       "      <td>481</td>\n",
       "      <td>103.42</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.14</td>\n",
       "      <td>5.09</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4276</th>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>118.46</td>\n",
       "      <td>528</td>\n",
       "      <td>111.88</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.31</td>\n",
       "      <td>5.57</td>\n",
       "      <td>0.43</td>\n",
       "      <td>109.48</td>\n",
       "      <td>475</td>\n",
       "      <td>103.16</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.13</td>\n",
       "      <td>5.01</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4551</th>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>118.33</td>\n",
       "      <td>525</td>\n",
       "      <td>111.76</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.30</td>\n",
       "      <td>5.54</td>\n",
       "      <td>0.42</td>\n",
       "      <td>109.41</td>\n",
       "      <td>475</td>\n",
       "      <td>103.19</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.12</td>\n",
       "      <td>4.97</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5051</th>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>118.18</td>\n",
       "      <td>521</td>\n",
       "      <td>111.61</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.28</td>\n",
       "      <td>5.49</td>\n",
       "      <td>0.40</td>\n",
       "      <td>109.21</td>\n",
       "      <td>468</td>\n",
       "      <td>102.91</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.11</td>\n",
       "      <td>4.92</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5551</th>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>118.00</td>\n",
       "      <td>516</td>\n",
       "      <td>111.43</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.27</td>\n",
       "      <td>5.45</td>\n",
       "      <td>0.39</td>\n",
       "      <td>109.08</td>\n",
       "      <td>461</td>\n",
       "      <td>102.72</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.10</td>\n",
       "      <td>4.87</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6051</th>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>1500</td>\n",
       "      <td>117.82</td>\n",
       "      <td>512</td>\n",
       "      <td>111.24</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.25</td>\n",
       "      <td>5.41</td>\n",
       "      <td>0.38</td>\n",
       "      <td>109.19</td>\n",
       "      <td>462</td>\n",
       "      <td>102.81</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.09</td>\n",
       "      <td>4.82</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6551</th>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>117.75</td>\n",
       "      <td>508</td>\n",
       "      <td>111.16</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.24</td>\n",
       "      <td>5.37</td>\n",
       "      <td>0.38</td>\n",
       "      <td>108.88</td>\n",
       "      <td>457</td>\n",
       "      <td>102.56</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.79</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6826</th>\n",
       "      <td>44</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>117.67</td>\n",
       "      <td>506</td>\n",
       "      <td>111.08</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.23</td>\n",
       "      <td>5.35</td>\n",
       "      <td>0.37</td>\n",
       "      <td>108.87</td>\n",
       "      <td>459</td>\n",
       "      <td>102.60</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4.77</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7326</th>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>117.55</td>\n",
       "      <td>503</td>\n",
       "      <td>110.96</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.22</td>\n",
       "      <td>5.32</td>\n",
       "      <td>0.37</td>\n",
       "      <td>108.85</td>\n",
       "      <td>457</td>\n",
       "      <td>102.55</td>\n",
       "      <td>1.22</td>\n",
       "      <td>0.06</td>\n",
       "      <td>4.73</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7826</th>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>117.40</td>\n",
       "      <td>500</td>\n",
       "      <td>110.81</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.21</td>\n",
       "      <td>5.29</td>\n",
       "      <td>0.36</td>\n",
       "      <td>108.60</td>\n",
       "      <td>448</td>\n",
       "      <td>102.21</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4.70</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8326</th>\n",
       "      <td>78</td>\n",
       "      <td>3</td>\n",
       "      <td>1500</td>\n",
       "      <td>117.32</td>\n",
       "      <td>497</td>\n",
       "      <td>110.73</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.20</td>\n",
       "      <td>5.26</td>\n",
       "      <td>0.36</td>\n",
       "      <td>108.39</td>\n",
       "      <td>446</td>\n",
       "      <td>102.10</td>\n",
       "      <td>1.27</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4.68</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8826</th>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>117.24</td>\n",
       "      <td>494</td>\n",
       "      <td>110.65</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.19</td>\n",
       "      <td>5.23</td>\n",
       "      <td>0.35</td>\n",
       "      <td>108.60</td>\n",
       "      <td>449</td>\n",
       "      <td>102.31</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4.64</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9101</th>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>117.20</td>\n",
       "      <td>493</td>\n",
       "      <td>110.61</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.19</td>\n",
       "      <td>5.22</td>\n",
       "      <td>0.35</td>\n",
       "      <td>108.54</td>\n",
       "      <td>445</td>\n",
       "      <td>102.17</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4.62</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9601</th>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "      <td>117.14</td>\n",
       "      <td>490</td>\n",
       "      <td>110.55</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.18</td>\n",
       "      <td>5.19</td>\n",
       "      <td>0.35</td>\n",
       "      <td>108.46</td>\n",
       "      <td>443</td>\n",
       "      <td>102.10</td>\n",
       "      <td>1.42</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4.61</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10101</th>\n",
       "      <td>66</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>117.06</td>\n",
       "      <td>488</td>\n",
       "      <td>110.47</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.17</td>\n",
       "      <td>0.35</td>\n",
       "      <td>108.30</td>\n",
       "      <td>440</td>\n",
       "      <td>101.96</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10601</th>\n",
       "      <td>58</td>\n",
       "      <td>4</td>\n",
       "      <td>1500</td>\n",
       "      <td>117.00</td>\n",
       "      <td>486</td>\n",
       "      <td>110.40</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.14</td>\n",
       "      <td>0.34</td>\n",
       "      <td>108.36</td>\n",
       "      <td>442</td>\n",
       "      <td>102.03</td>\n",
       "      <td>1.42</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.55</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11101</th>\n",
       "      <td>81</td>\n",
       "      <td>4</td>\n",
       "      <td>2000</td>\n",
       "      <td>116.90</td>\n",
       "      <td>484</td>\n",
       "      <td>110.30</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.16</td>\n",
       "      <td>5.12</td>\n",
       "      <td>0.34</td>\n",
       "      <td>108.22</td>\n",
       "      <td>436</td>\n",
       "      <td>101.90</td>\n",
       "      <td>1.43</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.53</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11376</th>\n",
       "      <td>41</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>116.86</td>\n",
       "      <td>483</td>\n",
       "      <td>110.26</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.16</td>\n",
       "      <td>5.11</td>\n",
       "      <td>0.34</td>\n",
       "      <td>107.76</td>\n",
       "      <td>435</td>\n",
       "      <td>101.75</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.51</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11876</th>\n",
       "      <td>53</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>116.76</td>\n",
       "      <td>481</td>\n",
       "      <td>110.16</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.15</td>\n",
       "      <td>5.09</td>\n",
       "      <td>0.35</td>\n",
       "      <td>107.83</td>\n",
       "      <td>432</td>\n",
       "      <td>101.71</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.46</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12376</th>\n",
       "      <td>65</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>116.67</td>\n",
       "      <td>479</td>\n",
       "      <td>110.08</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.15</td>\n",
       "      <td>5.07</td>\n",
       "      <td>0.36</td>\n",
       "      <td>107.76</td>\n",
       "      <td>431</td>\n",
       "      <td>101.57</td>\n",
       "      <td>1.64</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.41</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12876</th>\n",
       "      <td>65</td>\n",
       "      <td>5</td>\n",
       "      <td>1500</td>\n",
       "      <td>116.61</td>\n",
       "      <td>477</td>\n",
       "      <td>110.03</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.14</td>\n",
       "      <td>5.05</td>\n",
       "      <td>0.37</td>\n",
       "      <td>107.68</td>\n",
       "      <td>423</td>\n",
       "      <td>101.44</td>\n",
       "      <td>1.66</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.40</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13376</th>\n",
       "      <td>53</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>116.55</td>\n",
       "      <td>475</td>\n",
       "      <td>109.97</td>\n",
       "      <td>1.21</td>\n",
       "      <td>0.14</td>\n",
       "      <td>5.03</td>\n",
       "      <td>0.38</td>\n",
       "      <td>107.89</td>\n",
       "      <td>431</td>\n",
       "      <td>101.67</td>\n",
       "      <td>1.63</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.36</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100101</th>\n",
       "      <td>30</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>113.59</td>\n",
       "      <td>396</td>\n",
       "      <td>106.87</td>\n",
       "      <td>2.45</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.83</td>\n",
       "      <td>0.61</td>\n",
       "      <td>105.81</td>\n",
       "      <td>377</td>\n",
       "      <td>99.62</td>\n",
       "      <td>2.59</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.97</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100601</th>\n",
       "      <td>54</td>\n",
       "      <td>44</td>\n",
       "      <td>500</td>\n",
       "      <td>113.59</td>\n",
       "      <td>396</td>\n",
       "      <td>106.86</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.82</td>\n",
       "      <td>0.61</td>\n",
       "      <td>105.73</td>\n",
       "      <td>376</td>\n",
       "      <td>99.54</td>\n",
       "      <td>2.59</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101101</th>\n",
       "      <td>54</td>\n",
       "      <td>44</td>\n",
       "      <td>1000</td>\n",
       "      <td>113.58</td>\n",
       "      <td>395</td>\n",
       "      <td>106.86</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.82</td>\n",
       "      <td>0.61</td>\n",
       "      <td>105.84</td>\n",
       "      <td>378</td>\n",
       "      <td>99.64</td>\n",
       "      <td>2.60</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101601</th>\n",
       "      <td>53</td>\n",
       "      <td>44</td>\n",
       "      <td>1500</td>\n",
       "      <td>113.58</td>\n",
       "      <td>395</td>\n",
       "      <td>106.85</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.81</td>\n",
       "      <td>0.61</td>\n",
       "      <td>105.68</td>\n",
       "      <td>372</td>\n",
       "      <td>99.45</td>\n",
       "      <td>2.64</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102101</th>\n",
       "      <td>53</td>\n",
       "      <td>44</td>\n",
       "      <td>2000</td>\n",
       "      <td>113.57</td>\n",
       "      <td>395</td>\n",
       "      <td>106.85</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.81</td>\n",
       "      <td>0.61</td>\n",
       "      <td>105.87</td>\n",
       "      <td>381</td>\n",
       "      <td>99.66</td>\n",
       "      <td>2.60</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.01</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102376</th>\n",
       "      <td>42</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>113.57</td>\n",
       "      <td>395</td>\n",
       "      <td>106.84</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.81</td>\n",
       "      <td>0.61</td>\n",
       "      <td>105.03</td>\n",
       "      <td>373</td>\n",
       "      <td>99.43</td>\n",
       "      <td>2.60</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102876</th>\n",
       "      <td>54</td>\n",
       "      <td>45</td>\n",
       "      <td>500</td>\n",
       "      <td>113.56</td>\n",
       "      <td>395</td>\n",
       "      <td>106.84</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.81</td>\n",
       "      <td>0.61</td>\n",
       "      <td>105.03</td>\n",
       "      <td>374</td>\n",
       "      <td>99.48</td>\n",
       "      <td>2.61</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103376</th>\n",
       "      <td>54</td>\n",
       "      <td>45</td>\n",
       "      <td>1000</td>\n",
       "      <td>113.55</td>\n",
       "      <td>395</td>\n",
       "      <td>106.83</td>\n",
       "      <td>2.47</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.80</td>\n",
       "      <td>0.62</td>\n",
       "      <td>105.10</td>\n",
       "      <td>373</td>\n",
       "      <td>99.42</td>\n",
       "      <td>2.66</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.86</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103876</th>\n",
       "      <td>54</td>\n",
       "      <td>45</td>\n",
       "      <td>1500</td>\n",
       "      <td>113.54</td>\n",
       "      <td>395</td>\n",
       "      <td>106.83</td>\n",
       "      <td>2.47</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.80</td>\n",
       "      <td>0.62</td>\n",
       "      <td>105.14</td>\n",
       "      <td>373</td>\n",
       "      <td>99.43</td>\n",
       "      <td>2.64</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.85</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104376</th>\n",
       "      <td>54</td>\n",
       "      <td>45</td>\n",
       "      <td>2000</td>\n",
       "      <td>113.53</td>\n",
       "      <td>395</td>\n",
       "      <td>106.82</td>\n",
       "      <td>2.47</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.79</td>\n",
       "      <td>0.62</td>\n",
       "      <td>105.30</td>\n",
       "      <td>376</td>\n",
       "      <td>99.57</td>\n",
       "      <td>2.59</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.87</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104651</th>\n",
       "      <td>31</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>113.53</td>\n",
       "      <td>395</td>\n",
       "      <td>106.82</td>\n",
       "      <td>2.47</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.79</td>\n",
       "      <td>0.62</td>\n",
       "      <td>105.28</td>\n",
       "      <td>375</td>\n",
       "      <td>99.53</td>\n",
       "      <td>2.59</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.85</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105151</th>\n",
       "      <td>54</td>\n",
       "      <td>46</td>\n",
       "      <td>500</td>\n",
       "      <td>113.52</td>\n",
       "      <td>395</td>\n",
       "      <td>106.81</td>\n",
       "      <td>2.47</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.79</td>\n",
       "      <td>0.62</td>\n",
       "      <td>105.42</td>\n",
       "      <td>372</td>\n",
       "      <td>99.36</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105651</th>\n",
       "      <td>53</td>\n",
       "      <td>46</td>\n",
       "      <td>1000</td>\n",
       "      <td>113.51</td>\n",
       "      <td>394</td>\n",
       "      <td>106.81</td>\n",
       "      <td>2.48</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.78</td>\n",
       "      <td>0.62</td>\n",
       "      <td>105.41</td>\n",
       "      <td>376</td>\n",
       "      <td>99.53</td>\n",
       "      <td>2.61</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.86</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106151</th>\n",
       "      <td>55</td>\n",
       "      <td>46</td>\n",
       "      <td>1500</td>\n",
       "      <td>113.50</td>\n",
       "      <td>394</td>\n",
       "      <td>106.80</td>\n",
       "      <td>2.48</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.78</td>\n",
       "      <td>0.62</td>\n",
       "      <td>105.56</td>\n",
       "      <td>376</td>\n",
       "      <td>99.53</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.87</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106651</th>\n",
       "      <td>55</td>\n",
       "      <td>46</td>\n",
       "      <td>2000</td>\n",
       "      <td>113.50</td>\n",
       "      <td>394</td>\n",
       "      <td>106.80</td>\n",
       "      <td>2.48</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.78</td>\n",
       "      <td>0.62</td>\n",
       "      <td>105.67</td>\n",
       "      <td>378</td>\n",
       "      <td>99.54</td>\n",
       "      <td>2.63</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.90</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106926</th>\n",
       "      <td>31</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>113.50</td>\n",
       "      <td>394</td>\n",
       "      <td>106.80</td>\n",
       "      <td>2.48</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.78</td>\n",
       "      <td>0.62</td>\n",
       "      <td>105.69</td>\n",
       "      <td>376</td>\n",
       "      <td>99.52</td>\n",
       "      <td>2.66</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.90</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107426</th>\n",
       "      <td>54</td>\n",
       "      <td>47</td>\n",
       "      <td>500</td>\n",
       "      <td>113.49</td>\n",
       "      <td>394</td>\n",
       "      <td>106.79</td>\n",
       "      <td>2.48</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.77</td>\n",
       "      <td>0.62</td>\n",
       "      <td>105.92</td>\n",
       "      <td>380</td>\n",
       "      <td>99.73</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.91</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107926</th>\n",
       "      <td>54</td>\n",
       "      <td>47</td>\n",
       "      <td>1000</td>\n",
       "      <td>113.49</td>\n",
       "      <td>394</td>\n",
       "      <td>106.79</td>\n",
       "      <td>2.48</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.77</td>\n",
       "      <td>0.62</td>\n",
       "      <td>105.83</td>\n",
       "      <td>374</td>\n",
       "      <td>99.48</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.97</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108426</th>\n",
       "      <td>54</td>\n",
       "      <td>47</td>\n",
       "      <td>1500</td>\n",
       "      <td>113.48</td>\n",
       "      <td>394</td>\n",
       "      <td>106.78</td>\n",
       "      <td>2.49</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.76</td>\n",
       "      <td>0.62</td>\n",
       "      <td>105.80</td>\n",
       "      <td>375</td>\n",
       "      <td>99.53</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.95</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108926</th>\n",
       "      <td>54</td>\n",
       "      <td>47</td>\n",
       "      <td>2000</td>\n",
       "      <td>113.48</td>\n",
       "      <td>394</td>\n",
       "      <td>106.78</td>\n",
       "      <td>2.49</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.76</td>\n",
       "      <td>0.62</td>\n",
       "      <td>105.69</td>\n",
       "      <td>374</td>\n",
       "      <td>99.44</td>\n",
       "      <td>2.63</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.93</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109201</th>\n",
       "      <td>30</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>113.48</td>\n",
       "      <td>394</td>\n",
       "      <td>106.78</td>\n",
       "      <td>2.49</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.76</td>\n",
       "      <td>0.62</td>\n",
       "      <td>105.58</td>\n",
       "      <td>372</td>\n",
       "      <td>99.35</td>\n",
       "      <td>2.68</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.92</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109701</th>\n",
       "      <td>54</td>\n",
       "      <td>48</td>\n",
       "      <td>500</td>\n",
       "      <td>113.47</td>\n",
       "      <td>393</td>\n",
       "      <td>106.77</td>\n",
       "      <td>2.49</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.76</td>\n",
       "      <td>0.62</td>\n",
       "      <td>105.58</td>\n",
       "      <td>372</td>\n",
       "      <td>99.35</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.92</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110201</th>\n",
       "      <td>54</td>\n",
       "      <td>48</td>\n",
       "      <td>1000</td>\n",
       "      <td>113.46</td>\n",
       "      <td>393</td>\n",
       "      <td>106.76</td>\n",
       "      <td>2.49</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.62</td>\n",
       "      <td>105.68</td>\n",
       "      <td>375</td>\n",
       "      <td>99.49</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.92</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110701</th>\n",
       "      <td>54</td>\n",
       "      <td>48</td>\n",
       "      <td>1500</td>\n",
       "      <td>113.46</td>\n",
       "      <td>393</td>\n",
       "      <td>106.76</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.62</td>\n",
       "      <td>105.54</td>\n",
       "      <td>372</td>\n",
       "      <td>99.36</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.91</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111201</th>\n",
       "      <td>54</td>\n",
       "      <td>48</td>\n",
       "      <td>2000</td>\n",
       "      <td>113.46</td>\n",
       "      <td>393</td>\n",
       "      <td>106.76</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.62</td>\n",
       "      <td>105.63</td>\n",
       "      <td>374</td>\n",
       "      <td>99.44</td>\n",
       "      <td>2.64</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.91</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111476</th>\n",
       "      <td>30</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>113.45</td>\n",
       "      <td>393</td>\n",
       "      <td>106.76</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.62</td>\n",
       "      <td>105.74</td>\n",
       "      <td>379</td>\n",
       "      <td>99.56</td>\n",
       "      <td>2.63</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.93</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111976</th>\n",
       "      <td>54</td>\n",
       "      <td>49</td>\n",
       "      <td>500</td>\n",
       "      <td>113.45</td>\n",
       "      <td>393</td>\n",
       "      <td>106.75</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.74</td>\n",
       "      <td>0.62</td>\n",
       "      <td>105.79</td>\n",
       "      <td>376</td>\n",
       "      <td>99.57</td>\n",
       "      <td>2.71</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.89</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112476</th>\n",
       "      <td>54</td>\n",
       "      <td>49</td>\n",
       "      <td>1000</td>\n",
       "      <td>113.44</td>\n",
       "      <td>393</td>\n",
       "      <td>106.75</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.74</td>\n",
       "      <td>0.62</td>\n",
       "      <td>105.61</td>\n",
       "      <td>373</td>\n",
       "      <td>99.45</td>\n",
       "      <td>2.64</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.89</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112976</th>\n",
       "      <td>54</td>\n",
       "      <td>49</td>\n",
       "      <td>1500</td>\n",
       "      <td>113.44</td>\n",
       "      <td>393</td>\n",
       "      <td>106.74</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.74</td>\n",
       "      <td>0.62</td>\n",
       "      <td>105.76</td>\n",
       "      <td>378</td>\n",
       "      <td>99.58</td>\n",
       "      <td>2.63</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.90</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113476</th>\n",
       "      <td>53</td>\n",
       "      <td>49</td>\n",
       "      <td>2000</td>\n",
       "      <td>113.43</td>\n",
       "      <td>393</td>\n",
       "      <td>106.74</td>\n",
       "      <td>2.51</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0.62</td>\n",
       "      <td>105.56</td>\n",
       "      <td>371</td>\n",
       "      <td>99.31</td>\n",
       "      <td>2.66</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.92</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows  18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       TRAIN:    TM                        LM        VALID:  \\\n",
       "       Time  Ep    Ct    LOSS   PPL     NLL    KL   REG   NLL    KL    LOSS   \n",
       "1        17   0     0  133.01  1035  122.48  0.42  0.98  9.13  0.80  126.55   \n",
       "501      72   0   500  121.21   614  114.07  0.05  0.71  6.34  0.98  111.72   \n",
       "1001     71   0  1000  120.42   584  113.63  0.09  0.57  6.07  0.78  111.25   \n",
       "1501     71   0  1500  119.95   570  113.25  0.16  0.51  5.96  0.66  111.21   \n",
       "2001     81   0  2000  119.55   560  112.89  0.25  0.46  5.88  0.59  110.76   \n",
       "2276     44   1     0  119.40   554  112.75  0.29  0.43  5.84  0.56  110.46   \n",
       "2776     72   1   500  119.02   547  112.39  0.37  0.39  5.76  0.51  110.20   \n",
       "3276     70   1  1000  118.66   539  112.05  0.45  0.36  5.69  0.48  109.85   \n",
       "3776     70   1  1500  118.51   534  111.92  0.52  0.33  5.63  0.45  109.62   \n",
       "4276     72   1  2000  118.46   528  111.88  0.57  0.31  5.57  0.43  109.48   \n",
       "4551     45   2     0  118.33   525  111.76  0.61  0.30  5.54  0.42  109.41   \n",
       "5051     70   2   500  118.18   521  111.61  0.66  0.28  5.49  0.40  109.21   \n",
       "5551     71   2  1000  118.00   516  111.43  0.71  0.27  5.45  0.39  109.08   \n",
       "6051     59   2  1500  117.82   512  111.24  0.76  0.25  5.41  0.38  109.19   \n",
       "6551     72   2  2000  117.75   508  111.16  0.81  0.24  5.37  0.38  108.88   \n",
       "6826     44   3     0  117.67   506  111.08  0.83  0.23  5.35  0.37  108.87   \n",
       "7326     70   3   500  117.55   503  110.96  0.87  0.22  5.32  0.37  108.85   \n",
       "7826     71   3  1000  117.40   500  110.81  0.91  0.21  5.29  0.36  108.60   \n",
       "8326     78   3  1500  117.32   497  110.73  0.94  0.20  5.26  0.36  108.39   \n",
       "8826     55   3  2000  117.24   494  110.65  0.97  0.19  5.23  0.35  108.60   \n",
       "9101     30   4     0  117.20   493  110.61  0.99  0.19  5.22  0.35  108.54   \n",
       "9601     53   4   500  117.14   490  110.55  1.02  0.18  5.19  0.35  108.46   \n",
       "10101    66   4  1000  117.06   488  110.47  1.05  0.17  5.17  0.35  108.30   \n",
       "10601    58   4  1500  117.00   486  110.40  1.07  0.17  5.14  0.34  108.36   \n",
       "11101    81   4  2000  116.90   484  110.30  1.10  0.16  5.12  0.34  108.22   \n",
       "11376    41   5     0  116.86   483  110.26  1.11  0.16  5.11  0.34  107.76   \n",
       "11876    53   5   500  116.76   481  110.16  1.14  0.15  5.09  0.35  107.83   \n",
       "12376    65   5  1000  116.67   479  110.08  1.16  0.15  5.07  0.36  107.76   \n",
       "12876    65   5  1500  116.61   477  110.03  1.19  0.14  5.05  0.37  107.68   \n",
       "13376    53   5  2000  116.55   475  109.97  1.21  0.14  5.03  0.38  107.89   \n",
       "...     ...  ..   ...     ...   ...     ...   ...   ...   ...   ...     ...   \n",
       "100101   30  44     0  113.59   396  106.87  2.45  0.03  3.83  0.61  105.81   \n",
       "100601   54  44   500  113.59   396  106.86  2.46  0.03  3.82  0.61  105.73   \n",
       "101101   54  44  1000  113.58   395  106.86  2.46  0.03  3.82  0.61  105.84   \n",
       "101601   53  44  1500  113.58   395  106.85  2.46  0.03  3.81  0.61  105.68   \n",
       "102101   53  44  2000  113.57   395  106.85  2.46  0.03  3.81  0.61  105.87   \n",
       "102376   42  45     0  113.57   395  106.84  2.46  0.03  3.81  0.61  105.03   \n",
       "102876   54  45   500  113.56   395  106.84  2.46  0.03  3.81  0.61  105.03   \n",
       "103376   54  45  1000  113.55   395  106.83  2.47  0.03  3.80  0.62  105.10   \n",
       "103876   54  45  1500  113.54   395  106.83  2.47  0.03  3.80  0.62  105.14   \n",
       "104376   54  45  2000  113.53   395  106.82  2.47  0.03  3.79  0.62  105.30   \n",
       "104651   31  46     0  113.53   395  106.82  2.47  0.03  3.79  0.62  105.28   \n",
       "105151   54  46   500  113.52   395  106.81  2.47  0.03  3.79  0.62  105.42   \n",
       "105651   53  46  1000  113.51   394  106.81  2.48  0.03  3.78  0.62  105.41   \n",
       "106151   55  46  1500  113.50   394  106.80  2.48  0.03  3.78  0.62  105.56   \n",
       "106651   55  46  2000  113.50   394  106.80  2.48  0.03  3.78  0.62  105.67   \n",
       "106926   31  47     0  113.50   394  106.80  2.48  0.03  3.78  0.62  105.69   \n",
       "107426   54  47   500  113.49   394  106.79  2.48  0.03  3.77  0.62  105.92   \n",
       "107926   54  47  1000  113.49   394  106.79  2.48  0.03  3.77  0.62  105.83   \n",
       "108426   54  47  1500  113.48   394  106.78  2.49  0.03  3.76  0.62  105.80   \n",
       "108926   54  47  2000  113.48   394  106.78  2.49  0.03  3.76  0.62  105.69   \n",
       "109201   30  48     0  113.48   394  106.78  2.49  0.03  3.76  0.62  105.58   \n",
       "109701   54  48   500  113.47   393  106.77  2.49  0.03  3.76  0.62  105.58   \n",
       "110201   54  48  1000  113.46   393  106.76  2.49  0.03  3.75  0.62  105.68   \n",
       "110701   54  48  1500  113.46   393  106.76  2.50  0.03  3.75  0.62  105.54   \n",
       "111201   54  48  2000  113.46   393  106.76  2.50  0.03  3.75  0.62  105.63   \n",
       "111476   30  49     0  113.45   393  106.76  2.50  0.03  3.75  0.62  105.74   \n",
       "111976   54  49   500  113.45   393  106.75  2.50  0.03  3.74  0.62  105.79   \n",
       "112476   54  49  1000  113.44   393  106.75  2.50  0.03  3.74  0.62  105.61   \n",
       "112976   54  49  1500  113.44   393  106.74  2.50  0.03  3.74  0.62  105.76   \n",
       "113476   53  49  2000  113.43   393  106.74  2.51  0.03  3.73  0.62  105.56   \n",
       "\n",
       "          TM                        LM               \n",
       "         PPL     NLL    KL   REG   NLL    KL   Beta  \n",
       "1       1035  116.16  0.29  0.98  9.12  0.76  0.000  \n",
       "501      536  105.34  0.06  0.49  5.76  0.77  0.088  \n",
       "1001     521  104.89  0.18  0.41  5.69  0.47  0.176  \n",
       "1501     521  104.78  0.39  0.33  5.62  0.36  0.264  \n",
       "2001     513  104.53  0.38  0.28  5.48  0.25  0.352  \n",
       "2276     506  104.26  0.45  0.23  5.39  0.29  0.400  \n",
       "2776     495  103.92  0.68  0.19  5.26  0.28  0.488  \n",
       "3276     488  103.66  0.72  0.17  5.15  0.27  0.576  \n",
       "3776     481  103.42  0.78  0.14  5.09  0.27  0.664  \n",
       "4276     475  103.16  0.97  0.13  5.01  0.28  0.752  \n",
       "4551     475  103.19  0.92  0.12  4.97  0.26  0.800  \n",
       "5051     468  102.91  1.03  0.11  4.92  0.27  0.888  \n",
       "5551     461  102.72  1.14  0.10  4.87  0.27  0.976  \n",
       "6051     462  102.81  1.16  0.09  4.82  0.30  1.000  \n",
       "6551     457  102.56  1.17  0.08  4.79  0.28  1.000  \n",
       "6826     459  102.60  1.17  0.07  4.77  0.26  1.000  \n",
       "7326     457  102.55  1.22  0.06  4.73  0.29  1.000  \n",
       "7826     448  102.21  1.35  0.05  4.70  0.29  1.000  \n",
       "8326     446  102.10  1.27  0.05  4.68  0.29  1.000  \n",
       "8826     449  102.31  1.30  0.05  4.64  0.31  1.000  \n",
       "9101     445  102.17  1.41  0.05  4.62  0.30  1.000  \n",
       "9601     443  102.10  1.42  0.04  4.61  0.30  1.000  \n",
       "10101    440  101.96  1.40  0.04  4.58  0.31  1.000  \n",
       "10601    442  102.03  1.42  0.03  4.55  0.32  1.000  \n",
       "11101    436  101.90  1.43  0.03  4.53  0.33  1.000  \n",
       "11376    435  101.75  1.46  0.03  4.51  0.31  0.000  \n",
       "11876    432  101.71  1.58  0.03  4.46  0.61  0.088  \n",
       "12376    431  101.57  1.64  0.03  4.41  0.56  0.176  \n",
       "12876    423  101.44  1.66  0.03  4.40  0.58  0.264  \n",
       "13376    431  101.67  1.63  0.03  4.36  0.57  0.352  \n",
       "...      ...     ...   ...   ...   ...   ...    ...  \n",
       "100101   377   99.62  2.59  0.01  2.97  0.62  1.000  \n",
       "100601   376   99.54  2.59  0.01  2.99  0.59  1.000  \n",
       "101101   378   99.64  2.60  0.01  2.98  0.61  1.000  \n",
       "101601   372   99.45  2.64  0.01  2.99  0.59  1.000  \n",
       "102101   381   99.66  2.60  0.01  3.01  0.60  1.000  \n",
       "102376   373   99.43  2.60  0.01  2.98  0.59  0.000  \n",
       "102876   374   99.48  2.61  0.01  2.85  1.00  0.088  \n",
       "103376   373   99.42  2.66  0.01  2.86  0.87  0.176  \n",
       "103876   373   99.43  2.64  0.01  2.85  0.81  0.264  \n",
       "104376   376   99.57  2.59  0.01  2.87  0.75  0.352  \n",
       "104651   375   99.53  2.59  0.01  2.85  0.74  0.400  \n",
       "105151   372   99.36  2.69  0.01  2.99  0.75  0.488  \n",
       "105651   376   99.53  2.61  0.01  2.86  0.71  0.576  \n",
       "106151   376   99.53  2.65  0.01  2.87  0.76  0.664  \n",
       "106651   378   99.54  2.63  0.01  2.90  0.79  0.752  \n",
       "106926   376   99.52  2.66  0.01  2.90  0.73  0.800  \n",
       "107426   380   99.73  2.65  0.01  2.91  0.70  0.888  \n",
       "107926   374   99.48  2.65  0.01  2.97  0.75  0.976  \n",
       "108426   375   99.53  2.65  0.01  2.95  0.65  1.000  \n",
       "108926   374   99.44  2.63  0.01  2.93  0.67  1.000  \n",
       "109201   372   99.35  2.68  0.01  2.92  0.62  1.000  \n",
       "109701   372   99.35  2.65  0.01  2.92  0.64  1.000  \n",
       "110201   375   99.49  2.65  0.01  2.92  0.62  1.000  \n",
       "110701   372   99.36  2.65  0.01  2.91  0.62  1.000  \n",
       "111201   374   99.44  2.64  0.01  2.91  0.63  1.000  \n",
       "111476   379   99.56  2.63  0.01  2.93  0.61  1.000  \n",
       "111976   376   99.57  2.71  0.01  2.89  0.62  1.000  \n",
       "112476   373   99.45  2.64  0.01  2.89  0.62  1.000  \n",
       "112976   378   99.58  2.63  0.01  2.90  0.64  1.000  \n",
       "113476   371   99.31  2.66  0.01  2.92  0.67  1.000  \n",
       "\n",
       "[250 rows x 18 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Output sentences for each topic-----------\n",
      "Item idx: B000VB7EFW\n",
      "0  BOW: nice feel bit works 'm pro air protect put feels\n",
      "0  SENTENCE: the soft material , soft , soft , smooth , and feels durable , and feels durable , so far , feels durable and easy to clean and easy to clean , durable , durable , and feels durable and\n",
      "1  BOW: perfectly protection sleeve protect chromebook soft perfect bought snug inch\n",
      "1  SENTENCE: this case fits my macbook pro perfectly perfectly with my samsung chromebook perfectly , fits perfectly with my mac air perfectly , protects the chromebook perfectly , protects the chromebook perfectly , protects the chromebook perfectly and protects the chromebook\n",
      "2  BOW: print surprise saved nicer comments personally ugly user wont realized\n",
      "2  SENTENCE: this item was a $ # . # and the price was right away from the product description to the description\n",
      "3  BOW: cover easily hard shell apple logo scratches nice easy rubberized\n",
      "3  SENTENCE: the rubber feet on the bottom cover does n't snap into place , but you can see the apple logo when you open the screen and you can see the apple logo when you open the lid and you can\n",
      "4  BOW: back carry school books work heavy years comfortable bags compartments\n",
      "4  SENTENCE: the wheels roll this backpack and i have been using this backpack for school , and it has held up well and i have lots of books and gear gear and i have lots of compliments on my shoulders\n",
      "5  BOW: plain neat lose user personally nicer fair listed hit photos\n",
      "5  SENTENCE: this item was a $ # . # , but i had to return it for a # & # # ; macbook pro\n",
      "6  BOW: compartment main pocket small side front hold access zippered top\n",
      "6  SENTENCE: the main compartment that hold the main compartment attached to the main compartment , but the main compartment does not hold the main flap flap to hold the flap closed\n",
      "7  BOW: local searched user lose fair surprise comments nicer realized wont\n",
      "7  SENTENCE: this item was $ # . # and the price was cheaper than the other reviews i had purchased for the macbook\n",
      "8  BOW: ! cute super beautiful awesome totally compliments wonderful absolutely unique\n",
      "8  SENTENCE: <unk> hassle\n",
      "9  BOW: bought buy ... recommend purchase worth time 'm happy money\n",
      "9  SENTENCE: i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i\n",
      "10  BOW: cheaply wont poorly caught realized happen ugly hit contacted horrible\n",
      "10  SENTENCE: it was a little pricey for the $ # and the smell was terrible\n",
      "11  BOW: ! loves awesome amazing daughter compliments college brand absolutely school\n",
      "11  SENTENCE: <unk> letters bubbles bubbles ipearl ipearl ipearl ipearl ipearl early ipearl\n",
      "12  BOW: color perfectly cover easy recommend mac apple perfect colors protects\n",
      "12  SENTENCE: the keyboard cover fits great ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "13  BOW: sleeve neoprene protection sleeves tight inside foam zipper snug thick\n",
      "13  SENTENCE: this this sleeve there is the usb usb port port port the usb port port the usb port port port the usb port port port the usb port port port the usb port port port port port the usb port\n",
      "14  BOW: cover keyboard protector screen typing keys type key love hard\n",
      "14  SENTENCE: however , the keyboard cover does n't snap the keyboard cover when you snap the keyboard cover , but the keyboard cover does n't snap on the keyboard cover , but the keyboard cover does n't snap the screen protector\n",
      "15  BOW: build canvas materials average single system waterproof stitching functionality empty\n",
      "15  SENTENCE: the # st one of the zipper broke after only a few weeks of use , and i had to return it because it was too small for my # `` laptop\n",
      "16  BOW: zipper flap velcro closed side open : zip close metal\n",
      "16  SENTENCE: the main zipper tabs that hold the flap closed , the zipper tabs that hold the flap closed to the zipper tabs to close to the zipper tabs to close the flap closed closed closed\n",
      "17  BOW: return amazon service item customer company seller received back order\n",
      "17  SENTENCE: the customer service was a replacement within # days after # days after ordering the customer service from the seller within # days after a few days later the seller sent me a replacement for a refund within a replacement\n",
      "18  BOW: padding zipper velcro zippers zip padded briefcase logic inside notebook\n",
      "18  SENTENCE: this this this this this this was this this had this one had an only one only had an issue only had an asus port only had an asus port port port only had an asus port port\n",
      "19  BOW: perfect recommend highly carry stylish bulky durable purse pleased safe\n",
      "19  SENTENCE: this product was perfect ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "20  BOW: bottom top piece part plastic corners rubber cracked feet corner\n",
      "20  SENTENCE: the zipper tabs that hold the tabs that hold the tabs that hold the tabs that hold the zipper tabs that hold the tabs that hold the zipper tabs coming off the corners started to fall apart at the top\n",
      "21  BOW: camera room carry lenses accessories lens pack gear equipment compartments\n",
      "21  SENTENCE: the camera compartment is plenty of room for my camera , lenses , lenses , flash , camera , flash , camera , flash , camera , flash , and all my accessories\n",
      "22  BOW: color picture blue pink bright purple green red black shown\n",
      "22  SENTENCE: the keyboard cover & keyboard cover ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "23  BOW: gift amazon christmas loves purchased husband $ wife bought daughter\n",
      "23  SENTENCE: ordered amazon for my order as a christmas gift for christmas as a christmas gift for christmas as a christmas gift for christmas ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! amazon\n",
      "24  BOW: air protect protection scratches scratch mbp thin snug surface bit\n",
      "24  SENTENCE: the outer outer flap snaps on the side and slides against your ports and you can slide your power cord and slide out of the cover and you can scratch your macbook from scratching the edges of the case and\n",
      "25  BOW: - material design -- pretty 'll 're stars : made\n",
      "25  SENTENCE: the zipper tabs are coming off , and then , so you ca n't open the back and you wo n't open the bottom part of the zipper falling off\n",
      "26  BOW: ! highly absolutely loves awesome amazing cute daughter totally wonderful\n",
      "26  SENTENCE: avoid letters bubbles bubbles bubbles bubbles bubbles bubbles bubbles bubbles bubbles bubbles bubbles bubbles bubbles bubbles bubbles bubbles bubbles bubbles bubbles bubbles bubbles bubbles bubbles bubbles bubbles bubbles bubbles garbage garbage garbage garbage garbage garbage garbage garbage garbage garbage garbage\n",
      "27  BOW: months broke started weeks year years week month zipper ago\n",
      "27  SENTENCE: the second one broke within the first week , the straps started coming apart within the first week after the first week of the zipper broke within the first week of the zipper started coming apart within the first week\n",
      "28  BOW: mac pro retina book hard air protects recommend perfectly display\n",
      "28  SENTENCE: the cover fits perfectly protects the mac book pro fits perfectly and protects perfectly protects my mac book pro perfectly and protects perfectly protects my mac book pro perfectly and protects perfectly protects my mac book pro perfectly and protects\n",
      "29  BOW: smell bad smells days strong returning odor disappointed reviews return\n",
      "29  SENTENCE: the second one lasted # weeks after a few weeks ago , i had to return it because the bottom piece had cracked in the first one of the first one of the case\n",
      "30  BOW: & ; ordered description laptops size item hp big amazon\n",
      "30  SENTENCE: i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i\n",
      "31  BOW: sturdy lot size carry work carrying space big things bags\n",
      "31  SENTENCE: lots of pockets and lots of pockets and compartments to carry all your stuff and books and books and books and books and books and books and books and books\n",
      "32  BOW: wont ugly realized nicer positive happen fair smells caught hit\n",
      "32  SENTENCE: my wife had the same model of the case , but after reading the reviews , the case was not a big deal\n",
      "33  BOW: strap straps handle shoulder back zippers - top : compartment\n",
      "33  SENTENCE: the shoulder strap zipper clips and the shoulder straps started to fall apart and the shoulder straps started to fall apart and the zipper pulls started to fall apart when the zipper pull the zipper pulls broke the main compartment\n",
      "34  BOW: pack clothes seat wheels plane overhead airport airplane security luggage\n",
      "34  SENTENCE: this backpack was a replacement for my first trip after a few weeks of use , the wheels were too heavy and i had to carry a heavy load of a backpack\n",
      "35  BOW: pretty hard cheap : plastic cases make put problem feel\n",
      "35  SENTENCE: the top clips and bottom clips are loose and wo n't stay closed , so you ca n't open the bottom part , so it wo n't stay on your back\n",
      "36  BOW: 've 'm money $ cheap thought ? back day thing\n",
      "36  SENTENCE: within first week i started to send it back , i contacted them and they sent me back to send me back to send it back and i would have to send it back after a week\n",
      "37  BOW: love perfect compliments recommend absolutely beautiful durable super awesome colors\n",
      "37  SENTENCE: this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this\n",
      "38  BOW: power drive adapter usb cord dvd player cable mouse external\n",
      "38  SENTENCE: two sd cards in the main pocket , but there is only one side pocket to hold the power adapter , which is a little tight squeeze to hold the power adapter , usb cable , usb cable , etc\n",
      "39  BOW: sewn caught hit test floor stitching weak happen pulled nylon\n",
      "39  SENTENCE: my previous case was a # star rating , but the only problem is that the case had a little bit of a chemical smell\n",
      "40  BOW: room ipad carry charger mouse cords pocket extra accessories perfect\n",
      "40  SENTENCE: plenty of room for my power cord , mouse , mouse , mouse , mouse , mouse , mouse , mouse , mouse , mouse , mouse , mouse , mouse , mouse , mouse , mouse , mouse ,\n",
      "41  BOW: quality ... made leather time high 'm find work ca\n",
      "41  SENTENCE: i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i\n",
      "42  BOW: pocket tablet inside side extra asus power cord front room\n",
      "42  SENTENCE: the front pocket pocket pocket can hold power cord , power supply , usb cable , usb cable , and you can also zip the front flap to hold the power cord and you can zip it up\n",
      "43  BOW: nice inside bit small handles hold big put pretty 'm\n",
      "43  SENTENCE: the side pockets inside , velcro , and zippers are smooth , so you ca n't open , so you can get your fingers and stay closed , and you are very careful with them\n",
      "44  BOW: price reviews item $ happy find 'm people found excellent\n",
      "44  SENTENCE: after reading when i decided i would glad i would would buy again i would would buy again i would would buy again i would would buy again i would would buy i would buy again i would highly buy\n",
      "45  BOW: ordered arrived fast shipping received order item days happy delivery\n",
      "45  SENTENCE: amazon customer service was a christmas customer service from amazon order from amazon customer service from amazon and i ordered this item as i ordered another order from amazon order from amazon order to order from amazon order from amazon\n",
      "46  BOW: inch netbook battery bought acer size hp small memory purchased\n",
      "46  SENTENCE: amazon $ # $ $ $ # for this item for my wife for an acer aspire # . # inch hp netbook as a christmas gift for my acer aspire # . # inch acer netbook for my wife\n",
      "47  BOW: pockets carry plenty comfortable easy space room storage items shoulder\n",
      "47  SENTENCE: lots of front pockets and front pockets can hold lenses , lenses , lenses , lenses , lenses , lenses , lenses , flash , camera , camera , camera , camera , camera , camera , and lots of\n",
      "48  BOW: apple white mcover model ipearl shell package install packaging clear\n",
      "48  SENTENCE: the speck case had a speck cover from the apple store , but the only reason i did n't give this # stars instead of the # . # screen cover that fits the macbook pro\n",
      "49  BOW: description dimensions amazon size toshiba inches inch advertised hp version\n",
      "49  SENTENCE: my $ # $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ # for the $ # $ # $ # case\n",
      "-----------Summaries-----------\n",
      "SUMMARY 0 : \n",
      " This is a very well made bag, nice construction, lots of pockets.\n",
      "the straps are very comfortable.\n",
      "and protects everything inside.\n",
      "It says\n",
      "it fits a 17inch notebook,\n",
      "however it did not.\n",
      "after using the pack for less than a month,\n",
      "it is ripping out already.\n",
      "SUMMARY 1 : \n",
      " This is a very well made bag, nice construction, lots of pockets.\n",
      "The quality is excellent\n",
      "and it is very durable.\n",
      "the straps are very comfortable.\n",
      "and protects everything inside.\n",
      "The color is a true red\n",
      "and it fits nicely.\n",
      "it is ripping out already.\n",
      "and it doesnt fit.\n",
      "It's just not the lightest backpack\n",
      "because some zipper teeth were not aligned.\n",
      "SUMMARY 2 : \n",
      " The quality is excellent\n",
      "and I love all the pockets and compartments.\n",
      "and it is very durable.\n",
      "and can't beleive the price\n",
      "and protects everything inside.\n",
      "The laptop\n",
      "doesn't fit in it.\n",
      "it is ripping out already.\n",
      "It's just not the lightest backpack\n",
      "0 TRUE: we have a macbook pro and bought the orange mcover to keep the aluminum from getting messed up\n",
      "0 PRED: i have a mac pro and and a apple cover and protect and apple and getting scratched up\n",
      "1 TRUE: after looking at a number of options , we landed on this one\n",
      "1 PRED: after using at a great of bags for this found this this one one\n",
      "2 TRUE: our daughter has a polycarbonate -lrb- white plastic -rrb- macbook and she bought the speck cover for about twice as much\n",
      "2 PRED: my daughter has a cover cover # -rrb- -rrb- and and i was the speck one and her macbook and i more\n",
      "3 TRUE: both covers do the job , but we -lrb- my wife , if you must know -rrb- really appreciate the mcover hard shell case\n",
      "3 PRED: these the are a great , and for bought the mac and and needed want the the the like the the hard shell case for\n",
      "4 TRUE: it fits well\n",
      "4 PRED: it fits perfectly\n",
      "5 TRUE: it 's tough and offers great protection\n",
      "5 PRED: it 's also , the good protection\n",
      "6 TRUE: it is very affordable . ipearl-inc . delivered the cover quickly and followed up with me after the sale\n",
      "6 PRED: it is a beautiful . the the fast on time and , i up to no to i time\n",
      "7 TRUE: you ca n't ask for better service\n",
      "7 PRED: you do n't be your the money\n",
      "8 TRUE: if i buy another macbook , i will also buy another mcover\n",
      "8 PRED: would i buy another cover if i would have be another cover if\n",
      "9 TRUE: i purchased this backpack , based on reviewers who also looked at other laptop maker brands who offer backpacks\n",
      "9 PRED: it was this bag as i on the , said it at the bags bags , , are more more and\n",
      "10 TRUE: this one is definitely well constructed , with plenty of handy , ergonomic pockets\n",
      "10 PRED: this is is very a built , lots lots of pockets , and pockets\n",
      "11 TRUE: i can easily carry a # `` work laptop , with charger , and other items in the backpack\n",
      "11 PRED: i can easily carry my # `` laptop , , a a and other a laptop in the bag\n",
      "12 TRUE: after taking it on a few trips , i have n't noticed any signs of wear or falling apart - no frayed straps , stuck zippers , etc .\n",
      "12 PRED: after i it to a trip times , , , no noticed any signs of wear , tear , , the signs or , , , are and ,\n",
      "13 TRUE: the laptop compartment , in the back , is fairly accessible , and easy to pull out the laptop at the airport for security screening\n",
      "13 PRED: the back compartment is the the front , the the comfortable and and the the carry out of back when the airport\n",
      "14 TRUE: since using this , i 've put away my shoulder bag , which i previously used to lug around my laptop\n",
      "14 PRED: after i it to i have to my my laptop and and and and 'm had it carry my my laptop\n",
      "15 TRUE: overall , very pleased with the purchase\n",
      "15 PRED: overall , very love a a purchase\n",
      "16 TRUE: would be nice if it were a bit cheaper , but even at this price , it 's worth it\n",
      "16 PRED: if n't nice if it was a little more but but i after i price was i 's worth it\n",
      "17 TRUE: product serves its purpose\n",
      "17 PRED: product is its purpose\n",
      "18 TRUE: it works great for watching videos and reading\n",
      "18 PRED: it a great for my a and !\n",
      "19 TRUE: i bought it for my kindle fire\n",
      "19 PRED: i bought this for my ipad fire\n",
      "20 TRUE: however i did have to do some maintenance on it when i received it\n",
      "20 PRED: but i i n't to to with research on it and i love it\n",
      "21 TRUE: just a little super glue fixed it right up\n",
      "21 PRED: while a little disappointed glue but it it it it\n",
      "22 TRUE: but for the price , i could n't expect more\n",
      "22 PRED: for for the price i i ca n't be more better\n",
      "23 TRUE: this is like carrying a <unk> for my computer\n",
      "23 PRED: it is a a a laptop laptop a laptop\n",
      "24 TRUE: it improves my attitude about work\n",
      "24 PRED: he had my laptop in this bag\n",
      "25 TRUE: it 's beautiful\n",
      "25 PRED: it 's awesome\n",
      "26 TRUE: it 's light\n",
      "26 PRED: it 's lightweight\n",
      "27 TRUE: it fits lots of companion gear and papers\n",
      "27 PRED: it is my airplane my equipment and well\n",
      "28 TRUE: the internal zip pouch protects my computer and is easy to unpack at airport security\n",
      "28 PRED: the inside pocket pocket the my laptop and is easy to use into the security\n",
      "29 TRUE: case has been perfect for my mac air #\n",
      "29 PRED: its was been my fit my macbook air # ``\n",
      "30 TRUE: small , lightweight , and easy to carry\n",
      "30 PRED: all , easy , and easy to carry\n",
      "31 TRUE: i just use it for short trips around town\n",
      "31 PRED: i have use it to me trips to and\n",
      "32 TRUE: i do n't need a heavy duty case or a lot of thick protection\n",
      "32 PRED: i do n't even to <unk> <unk> with for a <unk> of padding <unk>\n",
      "33 TRUE: wo n't hold notebook but folders slide in just fine\n",
      "33 PRED: does n't hold your or even in just it fine\n",
      "34 TRUE: this cover was just right for my new laptop\n",
      "34 PRED: this case was just right for my new macbook\n",
      "35 TRUE: protects well and very easy to put in place\n",
      "35 PRED: protects it and easy easy to put on place take\n",
      "36 TRUE: really like that there are feet for <unk> : does seem like they should be a little sturdier\n",
      "36 PRED: but it it it is a on the case it is is the other be a little more\n",
      "37 TRUE: color -lrb- red -rrb- is good , would like a deeper red if i had my preference\n",
      "37 PRED: great the the -rrb- is a , but be a better more to you had a apple\n",
      "38 TRUE: but , for the money , a good product\n",
      "38 PRED: overall , for the price price the good buy\n",
      "39 TRUE: overall i have been very happy with this case\n",
      "39 PRED: but i would been looking with for this product\n",
      "40 TRUE: i used for an asus eeepc\n",
      "40 PRED: i used for a asus eee\n",
      "41 TRUE: it has held up great , except for the corners\n",
      "41 PRED: it has been up fine for the the the smell\n",
      "42 TRUE: we turned the case inside out and sewed a small piece of denim into the bottom two corners to patch some small holes and to add reinforcement -lrb- maybe they should have made it that way -rrb-\n",
      "42 PRED: i tried out foam down to of the the little of the the to the corner of of of the the of <unk> , the the the , which the a -rrb- -rrb- of -rrb- -rrb- to but\n",
      "43 TRUE: i am really happy with my kuzy red cover\n",
      "43 PRED: i would really happy with my cover cover cover\n",
      "44 TRUE: i especially dig the feel of it !\n",
      "44 PRED: i also like the color of this .\n",
      "45 TRUE: it shipped in a very timely manner and is working out great\n",
      "45 PRED: it arrived great the great good delivery , is worth in great\n",
      "46 TRUE: i highly recommend it to anyone looking to protect their mac\n",
      "46 PRED: i am recommend it to anyone looking to protect their macbook\n",
      "47 TRUE: i bought this for my daughter who has a # & # # ; hp computer\n",
      "47 PRED: i bought this for my wife for needed a new & # # ; laptop laptop\n",
      "48 TRUE: this is perfect for her needs there is even extra room for a book if she needs it\n",
      "48 PRED: this is perfect for my needs to to a a enough for spare laptop and you can to to\n",
      "49 TRUE: this is built well very durable and has beautiful design\n",
      "49 PRED: this is very well , durable and has a leather\n",
      "50 TRUE: i am glad i found this product\n",
      "50 PRED: i am glad i found this product\n",
      "51 TRUE: we had high hopes for the sportfolio deluxe # `` macbook pro case after reading some of the reviews and knowing that it was designed specifically for the macbook pro\n",
      "51 PRED: i had a hopes for the # # # `` sleeve case # for a the of the reviews , the that the would fit for protect my macbook pro\n",
      "52 TRUE: however , we quickly ended up with a broken zipper pull and were plagued with the extremely squeaky swivels that attach the straps\n",
      "52 PRED: however , after have had up using the the , and and the too to the bag <unk> strap , are the bag\n",
      "53 TRUE: because of the broken zipper pull , we 've returned the case\n",
      "53 PRED: after of the <unk> zipper broke off to i had it product\n",
      "54 TRUE: this <unk> fits snugly and with some room left for you to keep your mouse 's wireless adapter plugged into your usb\n",
      "54 PRED: this sleeve fits perfectly , i even i for for for have store the dvd in power usb in in the\n",
      "55 TRUE: it is sleek and sharp looking to match your brand new samsung laptop\n",
      "55 PRED: it is great , colorful looking for my my laptop new laptop laptop\n",
      "56 TRUE: i like it\n",
      "56 PRED: i like it\n",
      "57 TRUE: this bag stores my computer safely and is very easy to use\n",
      "57 PRED: this bag fits my laptop and and it very well to use\n",
      "58 TRUE: i like the long strap on it that provides a comfortable way to carry my laptop\n",
      "58 PRED: it like the fact strap that it has has my my on my my my laptop\n",
      "59 TRUE: it is perfect if you have a bigger laptop like mine\n",
      "59 PRED: it is just if you are a smaller laptop and you\n",
      "60 TRUE: this is a large backpack so it may not be for everyone but it is perfect for my needs\n",
      "60 PRED: this is a very bag that i does be be perfect what as i was perfect for what needs\n",
      "61 TRUE: it 's very well built -lrb- the steel cable in the top handle is overkill but looks cool -rrb- and it has lots and lots of pockets of different sizes to hold all your odds and ends\n",
      "61 PRED: it 's a comfortable made , the laptop has the the back of of very , the great , has has has a pockets pockets pockets pockets for pockets sizes and pockets the the stuff and ends\n",
      "62 TRUE: fits like a glove and i love the orange color since is my favorite color and i have no problem finding my laptop now when i put it somewhere and have to go looking for\n",
      "62 PRED: love my the great and i love the color color and it is mac color and it it it scratches to it mac to i i do it on to it it to my for a\n"
     ]
    }
   ],
   "source": [
    "if len(log_df) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "while epoch < config.epochs:\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = get_feed_dict(batch)\n",
    "\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, sent_loss_kl_categ_batch, sent_loss_kl_gmm_batch, ppls_batch = \\\n",
    "        sess.run([opt, loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, sent_loss_kl_categ, sent_loss_kl_gmm, topic_ppls], feed_dict = feed_dict)\n",
    "   \n",
    "        if sent_loss_kl_batch == np.inf:\n",
    "            print('Nan occured')\n",
    "            ckpt = tf.train.get_checkpoint_state(config.modeldir)\n",
    "            model_checkpoint_path = ckpt.all_model_checkpoint_paths[-1]\n",
    "            saver.restore(sess, model_checkpoint_path)            \n",
    "            break\n",
    "            \n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if ct%config.log_period==0:\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train, sent_loss_recon_train, sent_loss_kl_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, sent_loss_recon_dev, sent_loss_kl_dev, ppl_dev = get_loss(sess, dev_batches)\n",
    "            global_step_log, beta_eval = sess.run([tf.train.get_global_step(), beta])\n",
    "            \n",
    "            if loss_dev < loss_min:\n",
    "                loss_min = loss_dev\n",
    "                saver.save(sess, config.modelpath, global_step=global_step_log)\n",
    "\n",
    "            clear_output()\n",
    "    \n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_reg_train, '%.2f'%sent_loss_recon_train, '%.2f'%sent_loss_kl_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_reg_dev, '%.2f'%sent_loss_recon_dev, '%.2f'%sent_loss_kl_dev,  '%.3f'%beta_eval],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "\n",
    "            print_summary(test_batches[1][1])\n",
    "            print_sample(batch)\n",
    "            \n",
    "            time_start = time.time()\n",
    "            \n",
    "    epoch += 1\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "_prob_topic, _prob_topic_sents, _prob_topic_infer, _means_topic_infer = debug_value([prob_topic, prob_topic_sents, prob_topic_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.02792162, 0.01464365, 0.0005151 , 0.00252109, 0.04934236,\n",
       "        0.00070111, 0.01627774, 0.00029658, 0.00991118, 0.02318782,\n",
       "        0.00018818, 0.0047924 , 0.00556466, 0.01821583, 0.00063281,\n",
       "        0.0041016 , 0.03139242, 0.00631432, 0.00920366, 0.02967596,\n",
       "        0.00700531, 0.01664687, 0.00617395, 0.00841429, 0.01440318,\n",
       "        0.07624064, 0.00418605, 0.0311865 , 0.0020076 , 0.01217666,\n",
       "        0.00486961, 0.07091897, 0.00025259, 0.07629693, 0.00262747,\n",
       "        0.01475961, 0.05211972, 0.00855024, 0.00333116, 0.00067762,\n",
       "        0.02526877, 0.03176381, 0.04822835, 0.07904603, 0.03668637,\n",
       "        0.01281309, 0.01135783, 0.08260436, 0.00019155, 0.00379475],\n",
       "       dtype=float32),\n",
       " array([2.1000540e-02, 4.3415143e-03, 2.3385076e-04, 1.4621429e-03,\n",
       "        7.4650727e-02, 4.5905221e-04, 1.7114475e-02, 2.0538093e-04,\n",
       "        3.4309898e-03, 1.9077362e-02, 7.0506288e-04, 3.3221017e-03,\n",
       "        3.2677110e-03, 6.0324292e-03, 9.5951243e-04, 7.2797793e-03,\n",
       "        4.9556013e-02, 1.2506019e-02, 5.8799256e-03, 1.4327163e-02,\n",
       "        2.2613412e-02, 7.9633072e-03, 1.8131834e-03, 5.4929480e-03,\n",
       "        9.2801098e-03, 5.9654322e-02, 2.6953323e-03, 1.4416735e-01,\n",
       "        1.6111437e-03, 9.3289940e-03, 1.2548212e-02, 5.3209376e-02,\n",
       "        2.5186199e-04, 1.3697386e-01, 8.0847396e-03, 1.3985891e-02,\n",
       "        6.0344320e-02, 2.0873952e-03, 1.1517768e-03, 1.5821292e-03,\n",
       "        6.1354060e-03, 2.2164719e-02, 2.1396196e-02, 5.6289196e-02,\n",
       "        2.8697830e-02, 5.4366901e-03, 2.6841559e-03, 5.4554574e-02,\n",
       "        1.0420008e-04, 1.8856138e-03], dtype=float32))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_i = 4\n",
    "_prob_topic_sents[batch_i], _prob_topic_infer[batch_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.77705145, -1.4867538 ,  0.45617026, -0.7698751 ],\n",
       "       [ 2.272597  , -1.779705  ,  0.67984253, -1.7813202 ],\n",
       "       [-0.4757314 , -0.71794784,  1.2470813 ,  0.3362029 ],\n",
       "       [-2.179429  , -2.423539  ,  2.2410502 , -0.13183467],\n",
       "       [-0.7224728 ,  0.25821558, -0.9037421 ,  1.0290488 ],\n",
       "       [-0.44092745, -0.74774486,  1.2310039 ,  0.36331385],\n",
       "       [-0.9691905 , -0.94298947,  0.17719796,  0.31528753],\n",
       "       [-0.3983578 , -0.5914751 ,  1.1418928 ,  0.36395875],\n",
       "       [14.998835  , 19.94313   , -6.1085777 , -6.024583  ],\n",
       "       [-0.6340552 ,  1.5837287 , -0.09692126, -1.7177402 ],\n",
       "       [ 0.07968902, -0.26739648,  1.0126684 ,  0.2657051 ],\n",
       "       [17.479574  , 23.29396   , -7.136926  , -6.938561  ],\n",
       "       [ 0.16300717, -2.5457444 ,  2.4619877 , -0.39026064],\n",
       "       [ 1.5104747 , -2.2272413 ,  1.0863245 , -2.0352528 ],\n",
       "       [-2.9354844 , -3.0115507 ,  3.5118918 ,  0.04232958],\n",
       "       [-1.4653643 , -0.9599113 ,  0.9310325 ,  0.6793495 ],\n",
       "       [-2.683891  , -1.880378  ,  1.5215523 ,  0.5777358 ],\n",
       "       [-2.4413695 , -1.0535339 ,  2.161121  ,  1.49208   ],\n",
       "       [-0.13714732, -0.7654175 ,  1.3509823 , -0.4301038 ],\n",
       "       [ 1.5494443 ,  0.26658654, -1.3737565 , -0.63128823],\n",
       "       [-3.2774055 , -2.177287  ,  1.9249027 ,  0.828049  ],\n",
       "       [ 0.8771448 ,  0.41135666, -1.5530939 ,  0.29232842],\n",
       "       [-1.208724  , -1.9784665 ,  2.7730198 ,  0.8018609 ],\n",
       "       [-1.4061399 , -0.09726995,  1.3515719 ,  0.96683216],\n",
       "       [-0.6841136 , -2.127636  ,  1.2844464 , -0.6200827 ],\n",
       "       [-2.6426291 , -1.3574742 ,  0.763146  ,  0.353448  ],\n",
       "       [20.717133  , 27.509645  , -8.444774  , -8.216221  ],\n",
       "       [-2.9755328 , -1.2035198 ,  1.4333663 ,  1.347302  ],\n",
       "       [ 1.8988967 , -2.9333496 ,  2.0942864 , -1.6642416 ],\n",
       "       [-2.5681953 , -1.2737485 ,  1.7792985 ,  0.9821467 ],\n",
       "       [21.062685  , -3.5146174 , 27.66444   , 15.63678   ],\n",
       "       [ 0.3346158 ,  0.2804401 , -1.5480012 ,  0.25334296],\n",
       "       [-0.6143118 , -0.72894543,  1.1835467 ,  0.36216626],\n",
       "       [-2.7241857 , -1.2173983 ,  0.5268272 ,  1.0705965 ],\n",
       "       [-0.80584824, -0.22308195,  0.0761517 ,  0.98696846],\n",
       "       [-2.6880631 , -1.8911804 ,  1.4954858 ,  0.30584788],\n",
       "       [-2.7732673 , -0.68451583,  0.8729768 ,  0.7999728 ],\n",
       "       [ 0.98368704, -2.6387017 , -2.0773807 , -0.4115757 ],\n",
       "       [ 0.40596312, -0.7585651 , -0.03849921, -0.32533616],\n",
       "       [-0.9923807 , -0.97887576,  1.2808061 ,  0.54027706],\n",
       "       [ 3.3094673 ,  0.3865604 , -2.2161849 , -0.9911884 ],\n",
       "       [ 1.7266061 ,  1.227104  , -0.6684884 , -2.2783308 ],\n",
       "       [ 0.41795558, -0.98861885, -0.47602612, -0.75824726],\n",
       "       [-0.9248423 , -1.0524898 , -0.13738531, -0.38236627],\n",
       "       [-2.3274097 ,  0.5323218 , -0.24956864, -0.72378695],\n",
       "       [-2.247323  , -0.5857186 ,  1.9468317 ,  1.1629409 ],\n",
       "       [ 0.91128457, -0.49438563,  1.4018289 ,  0.39456475],\n",
       "       [ 0.8831209 ,  0.48219174, -2.0592885 ,  0.2812129 ],\n",
       "       [-1.3794442 , -1.8051625 ,  2.397827  ,  0.30717683],\n",
       "       [-0.75478345, -0.8755799 ,  1.8171496 ,  0.90055287]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_means_topic_infer[0][:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_means_topic, b_means_topic = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"topic/dec/mean_topic\")\n",
    "\n",
    "pred_topic_embeddings, pred_topic_bow, pred_means_topic, pred_logvars_topic, pred_token_idxs, _w_means_topic, _b_means_topic, _w_mean_topic_infer = \\\n",
    "                                sess.run([topic_embeddings, topic_bow, means_topic, logvars_topic, topic_beam_output_token_idxs, w_means_topic, b_means_topic, w_mean_topic_infer], \n",
    "                                         feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "\n",
    "pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "\n",
    "pred_topics_freq_bow_indices = np.argsort(pred_topic_bow, 1)[:, ::-1][:, :10]\n",
    "pred_topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nice', 'feel', 'bit', 'works', 'pro', 'put', 'air', \"'m\", 'protect', 'lot']\n",
      "['perfectly', 'protection', 'sleeve', 'protect', 'bought', 'perfect', 'chromebook', 'inside', 'inch', 'soft']\n",
      "['print', 'surprise', 'saved', 'nicer', 'personally', 'comments', 'wont', 'ugly', 'user', 'fair']\n",
      "['cover', 'easily', 'apple', 'shell', 'hard', 'logo', 'nice', 'scratches', 'rubberized', 'easy']\n",
      "['back', 'carry', 'books', 'work', 'heavy', 'school', 'years', \"'ve\", 'pack', 'bags']\n",
      "['neat', 'plain', 'lose', 'user', 'nicer', 'personally', 'fair', 'listed', 'photos', 'hit']\n",
      "['compartment', 'main', 'pocket', 'small', 'front', 'side', 'access', 'hold', 'bottle', 'water']\n",
      "['local', 'searched', 'fair', 'user', 'lose', 'nicer', 'surprise', 'realized', 'wont', 'comments']\n",
      "['!', 'cute', 'super', 'beautiful', 'awesome', 'totally', 'compliments', 'unique', 'absolutely', 'wonderful']\n",
      "['bought', 'buy', '...', 'recommend', 'purchase', 'time', 'worth', \"'m\", 'happy', 'money']\n",
      "['cheaply', 'wont', 'poorly', 'realized', 'caught', 'happen', 'ugly', 'hit', 'contacted', 'fair']\n",
      "['!', 'daughter', 'loves', 'awesome', 'compliments', 'amazing', 'college', 'absolutely', 'brand', 'year']\n",
      "['color', 'perfectly', 'cover', 'easy', 'recommend', 'put', 'apple', 'mac', 'protects', 'perfect']\n",
      "['sleeve', 'neoprene', 'protection', 'foam', 'inside', 'sleeves', 'tight', 'zipper', 'snug', 'thick']\n",
      "['cover', 'keyboard', 'screen', 'protector', 'keys', 'typing', 'key', 'type', 'love', 'mouse']\n",
      "['average', 'build', 'canvas', 'materials', 'single', 'system', 'waterproof', 'stitching', 'empty', 'zippers']\n",
      "['zipper', 'flap', 'velcro', ':', 'open', 'close', 'side', 'closed', 'zip', 'metal']\n",
      "['return', 'service', 'customer', 'amazon', 'item', 'seller', 'received', 'back', 'company', 'money']\n",
      "['padding', 'zipper', 'zippers', 'velcro', 'zip', 'briefcase', 'logic', 'padded', 'inside', 'notebook']\n",
      "['perfect', 'recommend', 'highly', 'carry', 'stylish', 'bulky', 'durable', 'purse', 'size', 'pleased']\n",
      "['bottom', 'top', 'part', 'corners', 'piece', 'plastic', 'speck', 'back', 'cracked', 'feet']\n",
      "['camera', 'room', 'lenses', 'carry', 'lens', 'pack', 'equipment', 'accessories', 'gear', 'canon']\n",
      "['color', 'picture', 'blue', 'pink', 'bright', 'purple', 'red', 'green', 'orange', 'shown']\n",
      "['gift', 'amazon', 'christmas', 'purchased', 'loves', 'husband', '$', 'daughter', 'wife', 'bought']\n",
      "['air', 'protection', 'protect', 'scratches', 'mbp', 'scratch', 'snug', 'surface', 'thin', 'pro']\n",
      "['-', 'material', 'design', '--', 'made', ':', 'thing', 'pretty', \"'ll\", 'fabric']\n",
      "['!', 'highly', 'absolutely', 'loves', 'awesome', 'daughter', 'amazing', 'cute', 'totally', 'wonderful']\n",
      "['months', 'broke', 'year', 'started', 'years', 'weeks', 'zipper', 'week', 'bought', 'month']\n",
      "['pro', 'mac', 'retina', 'book', 'hard', 'display', 'air', 'protects', 'recommend', 'perfectly']\n",
      "['smell', 'strong', 'bad', 'smells', 'days', 'returning', 'return', 'chemical', 'close', 'odor']\n",
      "[';', '&', 'ordered', 'laptops', 'size', 'description', 'hp', 'big', 'item', 'amazon']\n",
      "['sturdy', 'work', 'lot', 'size', 'big', 'carry', 'carrying', 'things', 'space', 'stuff']\n",
      "['wont', 'ugly', 'realized', 'nicer', 'positive', 'happen', 'fair', 'smells', 'caught', 'hit']\n",
      "['strap', 'shoulder', 'handle', 'straps', 'back', 'zippers', '-', 'top', ':', 'compartment']\n",
      "['pack', 'clothes', 'seat', 'plane', 'airport', 'wheels', 'overhead', 'luggage', 'rolling', 'airplane']\n",
      "['hard', 'pretty', 'cheap', 'plastic', ':', 'cases', 'put', 'top', 'problem', 'feel']\n",
      "[\"'ve\", \"'m\", 'cheap', 'money', '$', 'back', 'thought', 'thing', 'day', 'long']\n",
      "['love', 'compliments', 'perfect', 'absolutely', 'recommend', 'beautiful', 'super', 'durable', 'colors', 'awesome']\n",
      "['power', 'adapter', 'usb', 'drive', 'cord', 'mouse', 'cable', 'dvd', 'cards', 'external']\n",
      "['sewn', 'caught', 'hit', 'floor', 'stitching', 'test', 'weak', 'happen', 'pulled', 'nylon']\n",
      "['room', 'ipad', 'mouse', 'charger', 'carry', 'cords', 'pocket', 'accessories', 'extra', 'perfect']\n",
      "['quality', '...', 'made', 'time', 'leather', 'high', \"'m\", 'find', 'work', 'long']\n",
      "['pocket', 'inside', 'tablet', 'side', 'extra', 'put', 'power', 'room', 'front', 'padded']\n",
      "['nice', 'inside', 'bit', 'small', 'big', 'put', 'hold', 'handles', 'inch', '-']\n",
      "['price', 'reviews', 'happy', 'item', '$', 'find', 'people', \"'m\", 'deal', 'expected']\n",
      "['ordered', 'arrived', 'shipping', 'received', 'item', 'fast', 'order', 'happy', 'days', 'quickly']\n",
      "['inch', 'netbook', 'battery', 'bought', 'memory', 'hp', 'size', 'mini', 'small', 'acer']\n",
      "['pockets', 'carry', 'plenty', 'storage', 'space', 'room', 'lots', 'stuff', 'shoulder', 'comfortable']\n",
      "['apple', 'white', 'model', 'shell', 'package', 'description', 'packaging', 'aluminum', 'ipearl', 'clear']\n",
      "['inches', 'dimensions', 'description', 'amazon', 'size', 'toshiba', 'inch', 'laptops', 'version', 'hp']\n"
     ]
    }
   ],
   "source": [
    "for idxs in pred_topics_freq_bow_idxs:\n",
    "    print([idx_to_word[idx] for idx in idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.92404228e-01,  1.35630518e-01,  5.12800477e-02,\n",
       "         1.03478752e-01, -4.03331339e-01,  1.75541773e-01,\n",
       "         1.34249315e-01, -1.34838015e-01,  1.26896665e-01,\n",
       "        -1.38889745e-01],\n",
       "       [-4.30931538e-01, -6.72928691e-02, -5.09441718e-02,\n",
       "         9.13887378e-03, -2.40449548e-01,  8.15488696e-02,\n",
       "        -1.36742562e-01, -2.07162485e-01,  1.10992536e-01,\n",
       "        -1.09735683e-01],\n",
       "       [ 1.22895226e-01,  1.78709701e-01,  6.87819161e-03,\n",
       "        -4.94722337e-01, -3.33423615e-01,  1.99662253e-01,\n",
       "        -1.32985845e-01, -5.23654819e-02,  7.04008043e-01,\n",
       "         4.42525029e-01],\n",
       "       [ 1.58089206e-01, -5.11043631e-02, -1.69768795e-01,\n",
       "         1.66817848e-02,  4.21261102e-01,  3.02397227e-03,\n",
       "         2.62937307e-01, -7.98380375e-03, -8.58561173e-02,\n",
       "        -1.37656063e-01],\n",
       "       [ 1.73268333e-01, -1.27717219e-02,  5.71838878e-02,\n",
       "         1.28342614e-01,  2.60495245e-02,  4.26307961e-05,\n",
       "         9.31888148e-02,  2.89289486e-02, -1.99505210e-01,\n",
       "         2.15498619e-02],\n",
       "       [-1.17141847e-02,  1.50988534e-01,  5.07274449e-01,\n",
       "        -2.35218316e-01, -6.33995831e-01,  1.09955445e-02,\n",
       "         2.68577605e-01, -3.04134171e-02,  2.69868910e-01,\n",
       "         9.36162993e-02],\n",
       "       [ 1.20521784e-01, -3.54568541e-01, -1.25059143e-01,\n",
       "        -1.84793193e-02,  3.99113953e-01, -3.32046603e-03,\n",
       "         1.61668938e-02, -3.06486916e-02, -4.97107357e-02,\n",
       "        -1.75880492e-01],\n",
       "       [-7.34063908e-02, -2.21850891e-02,  2.56521136e-01,\n",
       "        -1.35058567e-01, -1.75637171e-01, -2.78912306e-01,\n",
       "         3.28539982e-02,  4.13407505e-01,  6.41756654e-01,\n",
       "        -1.80056430e-02],\n",
       "       [-3.95781010e-01, -1.28887221e-03, -1.59408569e-01,\n",
       "         1.07766435e-01, -4.28042442e-01,  2.53677428e-01,\n",
       "        -2.24492639e-01,  6.38357401e-02,  2.78350234e-01,\n",
       "        -1.76432550e-01],\n",
       "       [-1.68702558e-01,  9.68484655e-02, -8.09761584e-02,\n",
       "         2.75611877e-02,  1.80193737e-01,  4.23833840e-02,\n",
       "         1.56984895e-01, -1.21225931e-01,  2.33409423e-02,\n",
       "        -2.77111351e-01],\n",
       "       [-1.87968403e-01, -5.18897288e-02,  5.03607690e-01,\n",
       "         1.24227116e-02, -3.33191045e-02, -2.60440379e-01,\n",
       "         2.06775278e-01, -7.58953094e-02, -8.87101050e-03,\n",
       "         5.70176065e-01],\n",
       "       [-2.50434816e-01, -3.35850455e-02,  8.66808966e-02,\n",
       "        -4.01436016e-02, -1.21109307e-01, -7.84071982e-02,\n",
       "        -1.54966697e-01,  2.63435468e-02,  1.49198055e-01,\n",
       "        -1.85887754e-01],\n",
       "       [-1.14605606e-01,  2.59907767e-02, -7.42064491e-02,\n",
       "        -3.02686751e-01, -3.00239861e-01, -5.81772067e-02,\n",
       "        -1.46930158e-01, -3.43125612e-01,  2.26415083e-01,\n",
       "        -2.90659249e-01],\n",
       "       [-4.39418972e-01,  5.65235317e-03,  5.24567291e-02,\n",
       "        -1.52930319e-02, -2.04612151e-01, -1.08427465e-01,\n",
       "         3.16776991e-01, -3.84788625e-02, -1.53476134e-01,\n",
       "         9.81270373e-02],\n",
       "       [-1.11350790e-01,  2.35271648e-01, -1.00518959e-02,\n",
       "        -1.83117017e-01,  3.61907333e-01, -4.79611233e-02,\n",
       "         2.24363253e-01,  7.01105222e-02, -9.37800184e-02,\n",
       "        -3.22749048e-01],\n",
       "       [ 4.50134307e-01, -7.69094452e-02,  4.37991954e-02,\n",
       "        -5.95165640e-02, -2.10706830e-01,  2.86598116e-01,\n",
       "        -3.19898278e-02, -5.07290810e-02, -9.19050053e-02,\n",
       "         1.17669858e-01],\n",
       "       [ 1.00317290e-02, -4.66013312e-01, -1.02658756e-01,\n",
       "         3.01171035e-01,  1.36123314e-01,  1.02156490e-01,\n",
       "        -9.55941454e-02,  1.61201462e-01, -2.29964674e-01,\n",
       "         7.73895904e-02],\n",
       "       [-2.16926113e-01,  2.26663444e-02,  2.22521778e-02,\n",
       "        -7.22108856e-02,  6.42194599e-02, -1.06154017e-01,\n",
       "        -5.57926260e-02,  1.24118477e-01, -2.04057321e-01,\n",
       "        -1.75320029e-01],\n",
       "       [ 1.39738694e-02, -5.20420790e-01, -1.74798667e-02,\n",
       "         1.86575204e-01, -5.35375953e-01,  1.81019217e-01,\n",
       "         2.30144247e-01, -1.27068579e-01, -1.37804717e-01,\n",
       "         7.19361752e-02],\n",
       "       [-1.08413249e-01,  2.48185650e-01, -7.89519474e-02,\n",
       "         1.99911254e-03,  6.21605329e-02,  2.45962236e-02,\n",
       "        -2.59336103e-02,  2.92226046e-01,  1.75222866e-02,\n",
       "        -2.25964457e-01],\n",
       "       [ 5.84295690e-02,  3.36621672e-01, -1.75215527e-01,\n",
       "        -4.24148887e-02,  3.53270441e-01, -1.79941654e-01,\n",
       "        -2.01837540e-01,  8.97524282e-02, -3.52088869e-01,\n",
       "        -7.23324865e-02],\n",
       "       [ 5.22739470e-01, -1.62763134e-01,  1.11824740e-02,\n",
       "        -7.38063827e-02,  4.52843010e-01,  6.74590766e-02,\n",
       "        -1.43896565e-02,  9.54558179e-02,  6.63999990e-02,\n",
       "         2.78516859e-02],\n",
       "       [-3.00795585e-01,  1.76328048e-01,  4.18270975e-02,\n",
       "        -3.79878551e-01, -5.27935982e-01,  1.28706113e-01,\n",
       "         4.14588988e-01, -1.41874671e-01,  1.40841559e-01,\n",
       "        -1.30469337e-01],\n",
       "       [-2.95595471e-02, -3.77544872e-02,  8.37041810e-02,\n",
       "        -2.47115687e-01,  8.54383260e-02, -8.57512131e-02,\n",
       "         3.56474593e-02,  1.31951958e-01,  7.52924308e-02,\n",
       "        -2.85227925e-01],\n",
       "       [-3.14331740e-01, -3.47948372e-01, -1.23094589e-01,\n",
       "        -1.27104824e-04,  6.01421222e-02,  7.48340180e-03,\n",
       "        -6.89065903e-02,  7.79233351e-02,  8.73809829e-02,\n",
       "        -4.16427851e-02],\n",
       "       [-3.87248322e-02,  2.96190716e-02, -1.98952183e-01,\n",
       "         9.99030843e-02, -1.62430987e-01,  6.15324080e-02,\n",
       "         1.18918322e-01,  1.61862820e-02, -4.22601812e-02,\n",
       "        -4.73872237e-02],\n",
       "       [-3.03117365e-01,  6.15839064e-02, -1.45381302e-01,\n",
       "         9.66021568e-02, -4.02712762e-01, -1.55798018e-01,\n",
       "        -3.04867357e-01,  2.22933084e-01,  3.37512612e-01,\n",
       "        -4.03762043e-01],\n",
       "       [-5.35386093e-02, -2.53067076e-01,  7.54628256e-02,\n",
       "        -5.98000512e-02,  5.86153492e-02,  1.30711138e-01,\n",
       "         9.07514244e-02, -2.25174978e-01, -8.99840668e-02,\n",
       "        -1.23774439e-01],\n",
       "       [-2.12086394e-01, -3.16706323e-03, -2.35709608e-01,\n",
       "         1.11675337e-01,  1.67495415e-01, -2.96922345e-02,\n",
       "         2.08392993e-01, -1.50109939e-02,  3.59136254e-01,\n",
       "        -3.72929007e-01],\n",
       "       [-1.23046733e-01, -2.52478093e-01, -2.39796620e-02,\n",
       "         8.46638307e-02,  5.68008050e-02,  7.11117266e-03,\n",
       "         2.96320379e-01, -3.85773815e-02,  3.38465095e-01,\n",
       "        -5.59484884e-02],\n",
       "       [-3.84697944e-01,  5.75665683e-02,  8.09690282e-02,\n",
       "         4.21424687e-01, -2.91402727e-01,  5.16250849e-01,\n",
       "         3.55413891e-02,  4.08319026e-01,  8.94573634e-04,\n",
       "        -2.19751775e-01],\n",
       "       [ 2.05822751e-01, -9.98558551e-02, -2.72013247e-01,\n",
       "         1.29938900e-01,  6.94628060e-02,  1.90054029e-01,\n",
       "         9.31162089e-02,  1.37254044e-01, -1.88997045e-01,\n",
       "        -2.39934191e-01],\n",
       "       [-7.70589635e-02, -7.40730315e-02, -3.08472328e-02,\n",
       "         1.33296149e-02, -1.44349203e-01, -7.44619220e-02,\n",
       "         2.59745181e-01, -1.86214209e-01,  3.37187588e-01,\n",
       "         1.97013855e-01],\n",
       "       [ 1.62421260e-02, -2.82071352e-01, -1.51442900e-01,\n",
       "         7.06166252e-02,  2.39023313e-01,  1.04504809e-01,\n",
       "         7.13484138e-02,  1.41563220e-03, -3.24384123e-01,\n",
       "        -2.55671561e-01],\n",
       "       [ 3.33340794e-01, -7.51186162e-02,  4.42507565e-01,\n",
       "         7.66261220e-02,  1.62679732e-01, -1.06306151e-01,\n",
       "        -4.65622917e-02, -8.82743299e-02, -2.55304843e-01,\n",
       "         2.59157807e-01],\n",
       "       [-1.90186620e-01, -1.39048487e-01, -3.82861376e-01,\n",
       "         7.59675577e-02,  3.39493598e-03,  1.83284958e-03,\n",
       "        -2.10598171e-01,  1.07462421e-01, -9.15716123e-03,\n",
       "         5.01392037e-02],\n",
       "       [ 1.16910867e-01,  7.35019892e-02, -2.20297247e-01,\n",
       "         5.83910458e-02, -5.76536357e-02, -1.22352690e-01,\n",
       "         1.09865300e-01, -7.98494071e-02, -6.36025667e-02,\n",
       "         1.96920801e-02],\n",
       "       [-3.70443225e-01, -3.40092301e-01, -1.00106299e-01,\n",
       "        -2.20818147e-02,  2.72656679e-01,  2.58000612e-01,\n",
       "         3.24077010e-01,  2.26189643e-02, -1.08204959e-02,\n",
       "        -2.35515818e-01],\n",
       "       [ 1.79298341e-01,  3.78704593e-02, -8.14548954e-02,\n",
       "        -1.81714460e-01,  1.87690854e-02,  6.81924149e-02,\n",
       "        -3.10354918e-01, -4.00724858e-02,  2.17155367e-01,\n",
       "         6.80824043e-03],\n",
       "       [ 2.74324626e-01,  1.16066700e-02, -3.72664779e-02,\n",
       "         8.03413093e-02, -3.43283415e-01,  1.67525187e-01,\n",
       "        -1.60222307e-01,  2.92935520e-01,  2.83794016e-01,\n",
       "         4.07103419e-01],\n",
       "       [ 4.04006839e-02,  3.31635416e-01,  1.24306791e-01,\n",
       "        -1.51699036e-02, -4.36317623e-01,  1.40869156e-01,\n",
       "        -1.05185755e-01,  6.24980666e-02, -1.78801537e-01,\n",
       "        -4.12948504e-02],\n",
       "       [ 5.12373773e-03, -9.55644324e-02, -8.48176703e-02,\n",
       "        -1.58824965e-01,  2.38508284e-01,  2.11599976e-01,\n",
       "        -2.73207903e-01, -1.98891480e-02, -5.14494143e-02,\n",
       "         3.04621514e-02],\n",
       "       [ 1.94606304e-01,  1.63836598e-01, -4.45152819e-02,\n",
       "        -5.06742764e-03, -2.81601548e-01,  1.75656602e-01,\n",
       "         5.05665392e-02,  9.96951293e-03,  1.33841813e-01,\n",
       "        -3.38595137e-02],\n",
       "       [ 2.38424540e-01,  4.34085503e-02, -2.77104676e-02,\n",
       "         1.51646048e-01, -3.93335253e-01, -3.84320840e-02,\n",
       "        -3.87748368e-02, -2.89154828e-01, -1.23101480e-01,\n",
       "        -1.10472910e-01],\n",
       "       [ 7.55143166e-02,  4.93344441e-02,  7.96661675e-02,\n",
       "        -6.83239773e-02,  9.69447047e-02,  3.65847111e-01,\n",
       "        -1.46515697e-01,  2.02526413e-02,  3.85857940e-01,\n",
       "         7.99160916e-04],\n",
       "       [-2.67124474e-01,  7.60893077e-02, -1.83682889e-01,\n",
       "        -2.03896984e-01, -2.28928635e-03,  3.13981511e-02,\n",
       "        -1.29455447e-01, -8.57260451e-02,  2.24290550e-01,\n",
       "        -1.17941678e-01],\n",
       "       [ 1.47285789e-01, -1.12484634e-01, -2.98231781e-01,\n",
       "        -9.98192877e-02, -2.58779168e-01, -3.59559618e-02,\n",
       "        -1.20807692e-01,  1.38797194e-01, -5.50963655e-02,\n",
       "         3.32978065e-03],\n",
       "       [ 9.52894613e-02,  1.82176039e-01,  3.57757621e-02,\n",
       "         5.49918488e-02,  2.89138854e-01,  6.32284135e-02,\n",
       "        -1.39537185e-01, -1.17931135e-01, -2.74732083e-01,\n",
       "        -4.03688662e-02],\n",
       "       [-4.10497546e-01,  2.13838264e-01, -2.30735138e-01,\n",
       "         2.13042963e-02,  5.46060801e-02, -2.08283976e-01,\n",
       "         4.35345709e-01, -1.21570468e-01,  5.37494540e-01,\n",
       "        -2.08534926e-01],\n",
       "       [ 2.15381440e-02, -3.59724611e-02,  1.15651257e-01,\n",
       "         1.08888082e-01, -5.04533172e-01,  9.77834463e-02,\n",
       "         2.61106074e-01,  2.49319136e-01,  3.31755072e-01,\n",
       "         1.35767311e-02]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_topic_embeddings[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.21909203e-04, 4.31783177e-04, 1.63430566e-04, ...,\n",
       "        1.38849282e-06, 1.36577855e-05, 2.89274203e-06],\n",
       "       [2.24819145e-04, 4.36413247e-05, 5.89604788e-05, ...,\n",
       "        1.33266058e-05, 1.01355006e-04, 1.80296411e-05],\n",
       "       [1.72764412e-05, 7.27603910e-05, 7.25599239e-05, ...,\n",
       "        7.69662165e-06, 1.10924339e-06, 3.18973216e-05],\n",
       "       ...,\n",
       "       [1.34782196e-04, 1.30009221e-05, 3.45027584e-05, ...,\n",
       "        4.39123774e-04, 1.09823566e-04, 1.07166161e-04],\n",
       "       [2.22593117e-05, 2.99593929e-04, 5.23408758e-04, ...,\n",
       "        3.85788184e-07, 2.65403116e-07, 9.07258311e-07],\n",
       "       [9.86009763e-06, 2.63822754e-03, 9.28363879e-04, ...,\n",
       "        4.59286202e-05, 3.70380498e-04, 2.99739389e-04]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_topic_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09029697, -0.3818669 ,  0.08883395, ..., -0.25656465,\n",
       "        -0.39281362, -0.18671207],\n",
       "       [ 0.08856896, -0.1262514 ,  0.14311399, ...,  0.001688  ,\n",
       "         0.06438643, -0.03520157],\n",
       "       [-0.24564894, -0.12563021, -0.00175388, ..., -0.19886872,\n",
       "         0.0355957 ,  0.08251901],\n",
       "       ...,\n",
       "       [-0.01543219, -0.14567626, -0.0943089 , ..., -0.09193935,\n",
       "         0.04260384,  0.05946982],\n",
       "       [ 0.3125463 ,  0.6641227 , -0.12975322, ..., -0.08004636,\n",
       "        -0.01782492, -0.05968362],\n",
       "       [-0.03915088,  0.01528641,  0.04972398, ..., -0.07354554,\n",
       "        -0.07732335, -0.01298761]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_w_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11835057, -0.13454448, -0.00337572,  0.05177502,  0.10295276,\n",
       "        0.02605309, -0.18366031,  0.05934381,  0.1732228 , -0.13531289,\n",
       "       -0.22492057, -0.12212984,  0.1266292 , -0.25644603,  0.17135005,\n",
       "        0.09541989, -0.02934371,  0.09172291,  0.04331908, -0.03151641,\n",
       "       -0.02083609,  0.04228181, -0.0005491 , -0.10296965, -0.17627637,\n",
       "       -0.18757704, -0.04487782,  0.06314798, -0.18511961,  0.15516071,\n",
       "        0.05649541, -0.14112212], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.76687163, -1.475438  ,  0.43883345, ..., -0.664001  ,\n",
       "        -2.0241804 , -0.14186974],\n",
       "       [ 2.2823424 , -1.7608914 ,  0.6601023 , ...,  0.25930965,\n",
       "        -4.106938  , -0.02110008],\n",
       "       [-0.46391395, -0.71264696,  1.2170411 , ...,  0.12201966,\n",
       "         0.16775817, -0.15185839],\n",
       "       ...,\n",
       "       [ 0.88815385,  0.49680227, -2.0771904 , ...,  0.29725865,\n",
       "        -1.712059  , -0.3213429 ],\n",
       "       [-1.3737322 , -1.8035553 ,  2.3982174 , ...,  0.338076  ,\n",
       "         0.14978433, -0.19695844],\n",
       "       [-0.7402571 , -0.86499304,  1.7953227 , ...,  0.5729351 ,\n",
       "         0.15811875, -0.28681582]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.64181136e-03,  8.20039306e-03, -5.96803380e-04,\n",
       "        -3.88584211e-02, -1.42097217e-03,  8.56163539e-03,\n",
       "        -7.66706653e-03,  1.61374628e-04,  8.14735238e-03,\n",
       "        -4.56584152e-03],\n",
       "       [ 1.93836596e-02, -4.16404977e-02,  1.03566749e-02,\n",
       "         1.00123785e-01,  1.06822690e-02, -1.79884192e-02,\n",
       "         2.48028599e-02, -2.62945611e-03, -2.12644991e-02,\n",
       "         3.45119052e-02],\n",
       "       [-9.00319591e-03,  1.09295929e-02,  6.26969908e-04,\n",
       "        -1.80125087e-02,  3.42475693e-03,  1.58313420e-02,\n",
       "         2.54083192e-03,  1.11510800e-02,  1.42021372e-03,\n",
       "        -9.55972914e-03],\n",
       "       [-2.21229624e-02,  4.82514352e-02, -9.89297125e-03,\n",
       "        -7.39470571e-02, -1.59238242e-02,  2.11052559e-02,\n",
       "        -2.00824663e-02, -6.25270186e-03,  2.71893814e-02,\n",
       "        -3.04618180e-02],\n",
       "       [-8.88055749e-03,  2.28155311e-02, -1.15662245e-02,\n",
       "        -1.88281313e-02, -1.12704057e-02,  5.43615967e-03,\n",
       "        -1.48752062e-02,  3.10412422e-03,  6.55456074e-03,\n",
       "        -1.39879081e-02],\n",
       "       [-5.77345770e-03,  1.30841520e-03, -6.79873442e-03,\n",
       "        -1.61254089e-02, -1.29281124e-03, -1.78626005e-03,\n",
       "        -7.86571391e-03, -8.66359938e-03,  1.09333945e-02,\n",
       "        -1.98763739e-02],\n",
       "       [-5.49685722e-03,  3.79082821e-02,  2.66198418e-03,\n",
       "        -4.19246480e-02, -1.19073596e-03,  1.27648609e-02,\n",
       "        -1.19321393e-02,  7.03927781e-03,  1.94281247e-03,\n",
       "        -3.48964846e-03],\n",
       "       [ 7.88358599e-03,  1.01250522e-02, -1.02529768e-02,\n",
       "        -1.78593453e-02,  5.92280552e-03,  1.22741787e-02,\n",
       "        -5.63342730e-03, -6.38499390e-03, -4.74125240e-03,\n",
       "        -1.46880988e-02],\n",
       "       [ 1.07218057e-01, -4.67116266e-01,  5.39149456e-02,\n",
       "         6.53615177e-01,  7.27701709e-02, -1.61960974e-01,\n",
       "         1.67146221e-01, -2.60547362e-02, -1.36573955e-01,\n",
       "         2.45182827e-01],\n",
       "       [-1.11695072e-02,  2.31574997e-02, -8.56497604e-03,\n",
       "        -1.95290744e-02, -1.56420108e-03,  1.00311311e-02,\n",
       "        -1.40167708e-02,  9.83624160e-03,  8.41066334e-03,\n",
       "        -8.11712816e-03],\n",
       "       [ 2.07599415e-03, -6.75915182e-03, -6.52700430e-03,\n",
       "         3.16838990e-03,  2.88286293e-03,  5.15590701e-03,\n",
       "        -1.12586459e-02, -4.99683805e-03, -3.27445101e-03,\n",
       "        -2.64859479e-03],\n",
       "       [ 1.19766615e-01, -5.17183185e-01,  6.26115799e-02,\n",
       "         7.41782546e-01,  8.03291872e-02, -1.74227893e-01,\n",
       "         1.89036891e-01, -2.73072142e-02, -1.58089742e-01,\n",
       "         2.69107491e-01],\n",
       "       [-7.83703057e-04, -1.84671320e-02,  7.90909212e-03,\n",
       "         1.78358890e-02, -2.34531262e-03,  2.42904574e-03,\n",
       "         1.07561576e-03,  2.22490542e-03,  2.19178176e-03,\n",
       "         1.32269226e-03],\n",
       "       [ 9.73725691e-03, -8.13518558e-03,  1.62048005e-02,\n",
       "         6.16411045e-02,  8.66022240e-03, -3.61769856e-03,\n",
       "         1.87495407e-02,  8.51653167e-04, -1.15563255e-02,\n",
       "         1.60146356e-02],\n",
       "       [-1.91604830e-02,  6.14807718e-02, -9.36130341e-03,\n",
       "        -1.06224865e-01, -1.92122497e-02,  3.08545232e-02,\n",
       "        -3.53375189e-02,  7.15119438e-03,  3.14251930e-02,\n",
       "        -5.47553301e-02],\n",
       "       [-1.55358408e-02,  4.22812290e-02, -9.38479044e-03,\n",
       "        -5.15630767e-02, -1.01721268e-02,  8.41321796e-03,\n",
       "        -1.83219016e-02,  4.16018628e-03,  1.38832182e-02,\n",
       "        -1.89558826e-02],\n",
       "       [-1.40016004e-02,  8.08396861e-02, -1.27661098e-02,\n",
       "        -1.28839225e-01, -1.84629075e-02,  3.83727066e-02,\n",
       "        -2.63124630e-02,  6.54044840e-03,  1.77013520e-02,\n",
       "        -3.86034660e-02],\n",
       "       [-2.12366041e-02,  6.42684624e-02, -1.28685194e-03,\n",
       "        -1.01662748e-01, -9.06432234e-03,  2.80175935e-02,\n",
       "        -2.01146509e-02, -2.68741813e-03,  2.50856820e-02,\n",
       "        -4.67343777e-02],\n",
       "       [ 4.40775743e-03,  2.29797475e-02, -2.75916886e-03,\n",
       "        -1.69292968e-02, -5.11823222e-03,  8.13815370e-03,\n",
       "         5.50407870e-03,  5.82670211e-04,  7.34696351e-03,\n",
       "         8.25493503e-03],\n",
       "       [-1.98129984e-03, -2.56651752e-02, -2.10047257e-03,\n",
       "         6.43752143e-02,  1.62610295e-03, -9.72902961e-03,\n",
       "         1.09996190e-02,  1.92137319e-03, -1.55495787e-02,\n",
       "         1.24167530e-02],\n",
       "       [-2.14927010e-02,  6.71672672e-02, -3.89199681e-03,\n",
       "        -1.32757515e-01, -1.27705066e-02,  3.35868336e-02,\n",
       "        -3.78457233e-02,  2.61975802e-03,  2.39089113e-02,\n",
       "        -4.58203889e-02],\n",
       "       [-1.29988475e-03,  1.33344000e-02, -1.58797530e-03,\n",
       "         3.35433036e-02, -6.54379430e-04, -4.48831310e-03,\n",
       "         2.92268442e-03,  1.07061518e-02,  2.06550444e-03,\n",
       "         5.46708936e-04],\n",
       "       [-2.37324304e-04,  2.94211581e-02, -8.55969638e-03,\n",
       "        -3.48790847e-02, -1.36090703e-02,  8.92604794e-03,\n",
       "        -1.22484881e-02, -4.17090580e-03,  1.65454168e-02,\n",
       "        -2.44118106e-02],\n",
       "       [-7.34040886e-03,  4.04352918e-02, -9.02046449e-03,\n",
       "        -6.60352781e-02, -1.10936081e-02,  2.28406526e-02,\n",
       "        -1.89743321e-02,  1.48829808e-02,  1.25222923e-02,\n",
       "        -2.15606336e-02],\n",
       "       [-2.31897086e-03,  1.14985630e-02, -9.60512727e-04,\n",
       "        -2.68573761e-02, -7.58812251e-03, -2.33057467e-03,\n",
       "        -5.35785966e-03,  4.18658747e-04,  2.70526367e-03,\n",
       "        -6.07111724e-03],\n",
       "       [-2.67577451e-02,  7.26446956e-02, -7.52151851e-03,\n",
       "        -1.19715884e-01, -1.42114945e-02,  2.92698313e-02,\n",
       "        -3.31940986e-02,  5.90287661e-03,  2.07338240e-02,\n",
       "        -4.58536446e-02],\n",
       "       [ 1.49700075e-01, -6.29805982e-01,  8.59377608e-02,\n",
       "         8.91013026e-01,  1.03340015e-01, -2.23852441e-01,\n",
       "         2.23215148e-01, -2.64386777e-02, -1.91828445e-01,\n",
       "         3.34086716e-01],\n",
       "       [-1.63887683e-02,  6.31997660e-02, -6.29101600e-03,\n",
       "        -1.25897661e-01, -1.17493263e-02,  2.95703132e-02,\n",
       "        -3.33335772e-02,  9.52907465e-03,  2.63068918e-02,\n",
       "        -4.41865548e-02],\n",
       "       [ 1.79830510e-02, -5.93079962e-02,  5.47652598e-03,\n",
       "         9.40249115e-02,  2.63335812e-03, -1.89017784e-02,\n",
       "         2.16119066e-02, -7.73053197e-03, -1.38749462e-02,\n",
       "         3.74961235e-02],\n",
       "       [-7.42066279e-03,  6.33344129e-02, -6.43894263e-03,\n",
       "        -1.15691006e-01, -9.85432882e-03,  2.76560616e-02,\n",
       "        -2.85538454e-02, -9.51283073e-05,  3.05334479e-02,\n",
       "        -4.85369824e-02],\n",
       "       [ 1.67696610e-01, -6.98278129e-01,  9.02588367e-02,\n",
       "         9.56308544e-01,  9.05695334e-02, -2.31803492e-01,\n",
       "         2.32783660e-01, -1.83100756e-02, -1.83005825e-01,\n",
       "         3.34577411e-01],\n",
       "       [-3.65513231e-04,  2.97299307e-03, -4.75531397e-03,\n",
       "         2.20490955e-02,  4.18807613e-03, -2.59633036e-03,\n",
       "        -3.22271232e-03,  4.66782739e-03,  1.00382732e-03,\n",
       "        -3.29000480e-03],\n",
       "       [ 2.48879986e-03,  1.23757338e-02, -1.37126697e-02,\n",
       "        -1.98315978e-02, -4.07536561e-03,  9.09615867e-03,\n",
       "        -2.62893224e-03,  1.03763351e-02,  1.42028322e-02,\n",
       "        -6.27030851e-03],\n",
       "       [-1.45129943e-02,  6.60484359e-02, -1.29962917e-02,\n",
       "        -1.20917231e-01, -1.51438676e-02,  2.07370669e-02,\n",
       "        -3.51772495e-02, -3.56522622e-04,  2.40390729e-02,\n",
       "        -4.80208695e-02],\n",
       "       [-1.17686968e-02,  3.61962467e-02, -5.23680262e-03,\n",
       "        -3.05720940e-02, -1.40001690e-02,  7.37598352e-03,\n",
       "        -7.82382488e-03, -6.51326554e-04,  1.71730239e-02,\n",
       "        -1.52694304e-02],\n",
       "       [-2.15620231e-02,  6.65714741e-02, -1.19123999e-02,\n",
       "        -1.11715205e-01, -2.07609460e-02,  2.93201376e-02,\n",
       "        -2.57376637e-02,  5.74379368e-03,  2.76122689e-02,\n",
       "        -4.16199341e-02],\n",
       "       [-3.16854380e-02,  6.03320450e-02, -1.14795780e-02,\n",
       "        -1.15805201e-01, -1.38329295e-02,  3.20201479e-02,\n",
       "        -3.31418701e-02,  1.28731718e-02,  2.33553499e-02,\n",
       "        -4.06951793e-02],\n",
       "       [ 1.41710890e-02,  1.61848851e-02,  1.07968254e-02,\n",
       "         2.31820140e-02,  4.78444900e-03, -4.73678112e-03,\n",
       "         1.61047503e-02, -5.07170195e-03, -2.23365966e-02,\n",
       "         1.45503944e-02],\n",
       "       [-2.67606648e-03,  2.47417297e-03,  4.15628497e-03,\n",
       "         1.57491546e-02, -4.43540933e-03,  5.86065790e-03,\n",
       "         3.12378770e-03,  6.04540575e-03, -1.28425341e-02,\n",
       "         1.03372475e-02],\n",
       "       [ 4.16483358e-03,  1.88468099e-02, -1.04940431e-02,\n",
       "        -2.95951441e-02, -2.54042097e-03,  7.19992351e-03,\n",
       "        -1.03648296e-02,  3.88290011e-03,  8.79763812e-03,\n",
       "        -1.58506837e-02],\n",
       "       [ 2.88903490e-02, -5.00082485e-02,  1.30719440e-02,\n",
       "         1.35259777e-01,  1.61473993e-02, -2.56136917e-02,\n",
       "         2.91693341e-02,  2.35593622e-03, -3.07668112e-02,\n",
       "         4.89044785e-02],\n",
       "       [ 1.86773911e-02, -6.73527569e-02,  5.50514041e-03,\n",
       "         8.81567597e-02,  8.17595981e-03, -1.97911691e-02,\n",
       "         2.19686422e-02,  4.37532458e-03, -1.88601576e-02,\n",
       "         2.71099415e-02],\n",
       "       [ 5.86620485e-03,  7.16463663e-03,  3.13132070e-03,\n",
       "         1.08800037e-02, -1.64596667e-03, -2.23791529e-03,\n",
       "         6.92079542e-04,  4.91306884e-03, -3.16039450e-03,\n",
       "         1.34092604e-03],\n",
       "       [-4.06439696e-03,  2.58561932e-02, -5.63223800e-03,\n",
       "        -3.16909105e-02, -1.36870528e-02,  1.01480670e-02,\n",
       "        -1.28327506e-02,  6.36188220e-03,  1.34462472e-02,\n",
       "        -1.45798875e-02],\n",
       "       [-1.56102944e-02,  5.05330563e-02, -6.86329184e-03,\n",
       "        -8.90486389e-02, -1.57645941e-02,  2.09993552e-02,\n",
       "        -2.51064152e-02,  1.30263029e-03,  2.31017694e-02,\n",
       "        -3.15410905e-02],\n",
       "       [-1.76425688e-02,  5.87905087e-02, -1.30155571e-02,\n",
       "        -9.14214030e-02, -3.52041516e-03,  3.31960656e-02,\n",
       "        -2.37902571e-02,  1.03175445e-02,  2.24433281e-02,\n",
       "        -4.31678295e-02],\n",
       "       [ 1.19440258e-02, -1.50302965e-02,  3.87178245e-03,\n",
       "         4.36117053e-02,  8.40736367e-03, -1.63753256e-02,\n",
       "         1.54952370e-02, -3.04191490e-03, -1.27191646e-02,\n",
       "         1.56825483e-02],\n",
       "       [ 4.78352467e-03, -3.77115444e-03,  4.04081307e-03,\n",
       "         4.15054448e-02,  4.73151589e-03, -1.08220673e-03,\n",
       "         5.29319514e-04,  3.61256505e-04, -1.20983850e-02,\n",
       "         9.77290701e-03],\n",
       "       [-2.28839298e-03,  2.79801562e-02, -7.49152107e-03,\n",
       "        -5.81175722e-02, -1.09504964e-02,  1.07187741e-02,\n",
       "        -1.39235985e-02,  3.84137011e-03,  2.34043859e-02,\n",
       "        -2.94274166e-02],\n",
       "       [ 2.03652889e-03,  2.08861642e-02, -1.39366854e-02,\n",
       "        -3.60004269e-02, -5.83486166e-04,  9.54378117e-03,\n",
       "        -4.51510819e-03, -2.97563663e-03,  5.79426391e-03,\n",
       "        -8.86533782e-03]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_w_mean_topic_infer[:, :10, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11835057, -0.13454448, -0.00337572,  0.05177502,  0.10295276,\n",
       "        0.02605309, -0.18366031,  0.05934381,  0.1732228 , -0.13531289,\n",
       "       -0.22492057, -0.12212984,  0.1266292 , -0.25644603,  0.17135005,\n",
       "        0.09541989, -0.02934371,  0.09172291,  0.04331908, -0.03151641,\n",
       "       -0.02083609,  0.04228181, -0.0005491 , -0.10296965, -0.17627637,\n",
       "       -0.18757704, -0.04487782,  0.06314798, -0.18511961,  0.15516071,\n",
       "        0.05649541, -0.14112212], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enc_state_infer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-ee7c3cd147b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_enc_state_infer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_means_topic_infer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdebug_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menc_state_infer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans_topic_infer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'enc_state_infer' is not defined"
     ]
    }
   ],
   "source": [
    "_enc_state_infer, _means_topic_infer = debug_value([enc_state_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_enc_state_infer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_means_topic_infer[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
