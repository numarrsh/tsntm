{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from data_structure import get_batches\n",
    "from hntm import HierarchicalNeuralTopicModel\n",
    "from nhdp import nestedHierarchicalNeuralTopicModel\n",
    "from tsgntm import TreeStructuredGaussianNeuralTopicModel\n",
    "from tree import get_descendant_idxs\n",
    "from evaluation import get_hierarchical_affinity, get_topic_specialization, print_topic_sample, print_topic_year\n",
    "from configure import get_config\n",
    "\n",
    "pd.set_option('display.max_columns', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(nb_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu\n",
    "np.random.seed(config.seed)\n",
    "random.seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.path_data,'rb'))\n",
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)\n",
    "config.dim_bow = len(bow_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     6
    ]
   },
   "outputs": [],
   "source": [
    "def debug(variable, sample_batch=None):\n",
    "    if sample_batch is None: sample_batch = test_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch, mode='test')\n",
    "    _variable = sess.run(variable, feed_dict=feed_dict)\n",
    "    return _variable\n",
    "\n",
    "def check(variable):\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sample_batch = test_batches[0]\n",
    "    feed_dict = model.get_feed_dict(sample_batch, mode='test', assertion=True)\n",
    "    _variable = sess.run(variable, feed_dict=feed_dict)\n",
    "    return _variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "TopicModels = {'hntm': HierarchicalNeuralTopicModel, 'nhdp': nestedHierarchicalNeuralTopicModel, 'tsgntm': TreeStructuredGaussianNeuralTopicModel}\n",
    "TopicModel = TopicModels[config.model]\n",
    "model = TopicModel(config)\n",
    "sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(max_to_keep=config.max_to_keep)\n",
    "update_tree_flg = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = []\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "ppl_min = np.inf\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','','','','','','VALID:','','','','','','TEST:','', 'SPEC:', '', '', 'HIER:', ''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL', 'GAUSS', 'REG','LOSS','PPL','NLL','KL', 'GAUSS','REG','LOSS','PPL', '1', '2', '3', 'CHILD', 'OTHER']]))))\n",
    "\n",
    "cmd_rm = 'rm -r %s' % config.dir_model\n",
    "res = subprocess.call(cmd_rm.split())\n",
    "cmd_mk = 'mkdir %s' % config.dir_model\n",
    "res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "def update_checkpoint(config, checkpoint, global_step):\n",
    "    checkpoint.append(config.path_model + '-%i' % global_step)\n",
    "    if len(checkpoint) > config.max_to_keep:\n",
    "        path_model = checkpoint.pop(0) + '.*'\n",
    "        for p in glob.glob(path_model):\n",
    "            os.remove(p)\n",
    "    cPickle.dump(checkpoint, open(config.path_checkpoint, 'wb'))\n",
    "    \n",
    "def validate(sess, batches, model):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    prob_topic_list = []\n",
    "    n_bow_list = []\n",
    "    n_topics_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = model.get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch, ppls_batch, prob_topic_batch, n_bow_batch, n_topics_batch \\\n",
    "            = sess.run([model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_gauss, model.topic_loss_reg, model.topic_ppls, model.prob_topic, model.n_bow, model.n_topics], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "        prob_topic_list.append(prob_topic_batch)\n",
    "        n_bow_list.append(n_bow_batch)\n",
    "        n_topics_list.append(n_topics_batch)\n",
    "    loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_gauss_mean, topic_loss_reg_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    \n",
    "    probs_topic = np.concatenate(prob_topic_list, 0)\n",
    "    \n",
    "    n_bow = np.concatenate(n_bow_list, 0)\n",
    "    n_topics = np.concatenate(n_topics_list, 0)\n",
    "    probs_topic_mean = np.sum(n_topics, 0) / np.sum(n_bow)\n",
    "    \n",
    "    return loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_gauss_mean, topic_loss_reg_mean, ppl_mean, probs_topic_mean    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train & validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th colspan=\"5\" halign=\"left\"></th>\n",
       "      <th>VALID:</th>\n",
       "      <th colspan=\"5\" halign=\"left\"></th>\n",
       "      <th>TEST:</th>\n",
       "      <th></th>\n",
       "      <th>SPEC:</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "      <th>HIER:</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>GAUSS</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>GAUSS</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>CHILD</th>\n",
       "      <th>OTHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>78</td>\n",
       "      <td>8</td>\n",
       "      <td>463</td>\n",
       "      <td>9296.33</td>\n",
       "      <td>2037</td>\n",
       "      <td>9261.69</td>\n",
       "      <td>14.30</td>\n",
       "      <td>20.34</td>\n",
       "      <td>0.14</td>\n",
       "      <td>9258.74</td>\n",
       "      <td>1906</td>\n",
       "      <td>9215.00</td>\n",
       "      <td>17.15</td>\n",
       "      <td>26.59</td>\n",
       "      <td>0.05</td>\n",
       "      <td>9258.81</td>\n",
       "      <td>1905</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>74</td>\n",
       "      <td>17</td>\n",
       "      <td>360</td>\n",
       "      <td>9242.14</td>\n",
       "      <td>1938</td>\n",
       "      <td>9199.85</td>\n",
       "      <td>16.53</td>\n",
       "      <td>25.75</td>\n",
       "      <td>0.09</td>\n",
       "      <td>9215.13</td>\n",
       "      <td>1829</td>\n",
       "      <td>9162.98</td>\n",
       "      <td>18.97</td>\n",
       "      <td>33.18</td>\n",
       "      <td>0.04</td>\n",
       "      <td>9214.32</td>\n",
       "      <td>1829</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>48</td>\n",
       "      <td>26</td>\n",
       "      <td>257</td>\n",
       "      <td>9213.92</td>\n",
       "      <td>1890</td>\n",
       "      <td>9168.31</td>\n",
       "      <td>17.47</td>\n",
       "      <td>28.17</td>\n",
       "      <td>0.07</td>\n",
       "      <td>9202.35</td>\n",
       "      <td>1811</td>\n",
       "      <td>9150.28</td>\n",
       "      <td>19.29</td>\n",
       "      <td>32.77</td>\n",
       "      <td>0.04</td>\n",
       "      <td>9203.23</td>\n",
       "      <td>1813</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>73</td>\n",
       "      <td>35</td>\n",
       "      <td>154</td>\n",
       "      <td>9196.61</td>\n",
       "      <td>1862</td>\n",
       "      <td>9149.24</td>\n",
       "      <td>17.97</td>\n",
       "      <td>29.42</td>\n",
       "      <td>0.06</td>\n",
       "      <td>9196.60</td>\n",
       "      <td>1801</td>\n",
       "      <td>9144.04</td>\n",
       "      <td>19.38</td>\n",
       "      <td>33.19</td>\n",
       "      <td>0.03</td>\n",
       "      <td>9198.12</td>\n",
       "      <td>1804</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>58</td>\n",
       "      <td>44</td>\n",
       "      <td>52</td>\n",
       "      <td>9184.68</td>\n",
       "      <td>1843</td>\n",
       "      <td>9136.24</td>\n",
       "      <td>18.28</td>\n",
       "      <td>30.18</td>\n",
       "      <td>0.06</td>\n",
       "      <td>9194.21</td>\n",
       "      <td>1799</td>\n",
       "      <td>9141.66</td>\n",
       "      <td>19.25</td>\n",
       "      <td>33.30</td>\n",
       "      <td>0.04</td>\n",
       "      <td>9191.46</td>\n",
       "      <td>1794</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>74</td>\n",
       "      <td>52</td>\n",
       "      <td>516</td>\n",
       "      <td>9176.03</td>\n",
       "      <td>1829</td>\n",
       "      <td>9126.91</td>\n",
       "      <td>18.49</td>\n",
       "      <td>30.68</td>\n",
       "      <td>0.05</td>\n",
       "      <td>9191.57</td>\n",
       "      <td>1796</td>\n",
       "      <td>9139.42</td>\n",
       "      <td>19.34</td>\n",
       "      <td>32.81</td>\n",
       "      <td>0.04</td>\n",
       "      <td>9187.63</td>\n",
       "      <td>1789</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35000</th>\n",
       "      <td>73</td>\n",
       "      <td>61</td>\n",
       "      <td>413</td>\n",
       "      <td>9169.44</td>\n",
       "      <td>1818</td>\n",
       "      <td>9119.86</td>\n",
       "      <td>18.65</td>\n",
       "      <td>31.02</td>\n",
       "      <td>0.05</td>\n",
       "      <td>9187.20</td>\n",
       "      <td>1788</td>\n",
       "      <td>9134.71</td>\n",
       "      <td>19.44</td>\n",
       "      <td>33.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>9187.27</td>\n",
       "      <td>1788</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40000</th>\n",
       "      <td>73</td>\n",
       "      <td>70</td>\n",
       "      <td>310</td>\n",
       "      <td>9164.03</td>\n",
       "      <td>1810</td>\n",
       "      <td>9114.23</td>\n",
       "      <td>18.76</td>\n",
       "      <td>31.25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>9186.36</td>\n",
       "      <td>1788</td>\n",
       "      <td>9133.93</td>\n",
       "      <td>19.49</td>\n",
       "      <td>32.93</td>\n",
       "      <td>0.04</td>\n",
       "      <td>9186.22</td>\n",
       "      <td>1787</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45000</th>\n",
       "      <td>74</td>\n",
       "      <td>79</td>\n",
       "      <td>207</td>\n",
       "      <td>9159.41</td>\n",
       "      <td>1803</td>\n",
       "      <td>9109.41</td>\n",
       "      <td>18.86</td>\n",
       "      <td>31.41</td>\n",
       "      <td>0.05</td>\n",
       "      <td>9185.94</td>\n",
       "      <td>1787</td>\n",
       "      <td>9133.62</td>\n",
       "      <td>19.52</td>\n",
       "      <td>32.80</td>\n",
       "      <td>0.04</td>\n",
       "      <td>9183.25</td>\n",
       "      <td>1784</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000</th>\n",
       "      <td>73</td>\n",
       "      <td>88</td>\n",
       "      <td>104</td>\n",
       "      <td>9155.50</td>\n",
       "      <td>1797</td>\n",
       "      <td>9105.32</td>\n",
       "      <td>18.94</td>\n",
       "      <td>31.52</td>\n",
       "      <td>0.05</td>\n",
       "      <td>9184.84</td>\n",
       "      <td>1786</td>\n",
       "      <td>9132.98</td>\n",
       "      <td>19.46</td>\n",
       "      <td>32.41</td>\n",
       "      <td>0.04</td>\n",
       "      <td>9182.20</td>\n",
       "      <td>1782</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55000</th>\n",
       "      <td>74</td>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "      <td>9152.06</td>\n",
       "      <td>1792</td>\n",
       "      <td>9101.70</td>\n",
       "      <td>19.00</td>\n",
       "      <td>31.60</td>\n",
       "      <td>0.05</td>\n",
       "      <td>9181.40</td>\n",
       "      <td>1782</td>\n",
       "      <td>9129.84</td>\n",
       "      <td>19.50</td>\n",
       "      <td>32.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>9183.13</td>\n",
       "      <td>1784</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60000</th>\n",
       "      <td>73</td>\n",
       "      <td>105</td>\n",
       "      <td>465</td>\n",
       "      <td>9149.18</td>\n",
       "      <td>1788</td>\n",
       "      <td>9098.68</td>\n",
       "      <td>19.05</td>\n",
       "      <td>31.65</td>\n",
       "      <td>0.05</td>\n",
       "      <td>9182.45</td>\n",
       "      <td>1782</td>\n",
       "      <td>9130.82</td>\n",
       "      <td>19.45</td>\n",
       "      <td>32.18</td>\n",
       "      <td>0.05</td>\n",
       "      <td>9183.13</td>\n",
       "      <td>1784</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65000</th>\n",
       "      <td>75</td>\n",
       "      <td>114</td>\n",
       "      <td>362</td>\n",
       "      <td>9146.74</td>\n",
       "      <td>1784</td>\n",
       "      <td>9096.11</td>\n",
       "      <td>19.10</td>\n",
       "      <td>31.69</td>\n",
       "      <td>0.05</td>\n",
       "      <td>9179.98</td>\n",
       "      <td>1780</td>\n",
       "      <td>9128.58</td>\n",
       "      <td>19.51</td>\n",
       "      <td>31.89</td>\n",
       "      <td>0.05</td>\n",
       "      <td>9182.46</td>\n",
       "      <td>1784</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70000</th>\n",
       "      <td>76</td>\n",
       "      <td>123</td>\n",
       "      <td>259</td>\n",
       "      <td>9144.47</td>\n",
       "      <td>1780</td>\n",
       "      <td>9093.77</td>\n",
       "      <td>19.14</td>\n",
       "      <td>31.71</td>\n",
       "      <td>0.05</td>\n",
       "      <td>9181.39</td>\n",
       "      <td>1781</td>\n",
       "      <td>9129.64</td>\n",
       "      <td>19.69</td>\n",
       "      <td>32.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>9182.46</td>\n",
       "      <td>1784</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75000</th>\n",
       "      <td>75</td>\n",
       "      <td>132</td>\n",
       "      <td>156</td>\n",
       "      <td>9142.41</td>\n",
       "      <td>1777</td>\n",
       "      <td>9091.63</td>\n",
       "      <td>19.18</td>\n",
       "      <td>31.73</td>\n",
       "      <td>0.05</td>\n",
       "      <td>9179.02</td>\n",
       "      <td>1779</td>\n",
       "      <td>9127.24</td>\n",
       "      <td>19.64</td>\n",
       "      <td>32.14</td>\n",
       "      <td>0.05</td>\n",
       "      <td>9180.81</td>\n",
       "      <td>1780</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       TRAIN:                                      VALID:  \\\n",
       "      Time   Ep   Ct     LOSS   PPL      NLL     KL  GAUSS   REG     LOSS   \n",
       "5000    78    8  463  9296.33  2037  9261.69  14.30  20.34  0.14  9258.74   \n",
       "10000   74   17  360  9242.14  1938  9199.85  16.53  25.75  0.09  9215.13   \n",
       "15000   48   26  257  9213.92  1890  9168.31  17.47  28.17  0.07  9202.35   \n",
       "20000   73   35  154  9196.61  1862  9149.24  17.97  29.42  0.06  9196.60   \n",
       "25000   58   44   52  9184.68  1843  9136.24  18.28  30.18  0.06  9194.21   \n",
       "30000   74   52  516  9176.03  1829  9126.91  18.49  30.68  0.05  9191.57   \n",
       "35000   73   61  413  9169.44  1818  9119.86  18.65  31.02  0.05  9187.20   \n",
       "40000   73   70  310  9164.03  1810  9114.23  18.76  31.25  0.05  9186.36   \n",
       "45000   74   79  207  9159.41  1803  9109.41  18.86  31.41  0.05  9185.94   \n",
       "50000   73   88  104  9155.50  1797  9105.32  18.94  31.52  0.05  9184.84   \n",
       "55000   74   97    1  9152.06  1792  9101.70  19.00  31.60  0.05  9181.40   \n",
       "60000   73  105  465  9149.18  1788  9098.68  19.05  31.65  0.05  9182.45   \n",
       "65000   75  114  362  9146.74  1784  9096.11  19.10  31.69  0.05  9179.98   \n",
       "70000   76  123  259  9144.47  1780  9093.77  19.14  31.71  0.05  9181.39   \n",
       "75000   75  132  156  9142.41  1777  9091.63  19.18  31.73  0.05  9179.02   \n",
       "\n",
       "                                            TEST:       SPEC:              \\\n",
       "        PPL      NLL     KL  GAUSS   REG     LOSS   PPL     1     2     3   \n",
       "5000   1906  9215.00  17.15  26.59  0.05  9258.81  1905  0.14  0.20  0.34   \n",
       "10000  1829  9162.98  18.97  33.18  0.04  9214.32  1829  0.16  0.27  0.41   \n",
       "15000  1811  9150.28  19.29  32.77  0.04  9203.23  1813  0.11  0.29  0.44   \n",
       "20000  1801  9144.04  19.38  33.19  0.03  9198.12  1804  0.15  0.26  0.44   \n",
       "25000  1799  9141.66  19.25  33.30  0.04  9191.46  1794  0.15  0.31  0.44   \n",
       "30000  1796  9139.42  19.34  32.81  0.04  9187.63  1789  0.18  0.29  0.46   \n",
       "35000  1788  9134.71  19.44  33.04  0.04  9187.27  1788  0.13  0.31  0.44   \n",
       "40000  1788  9133.93  19.49  32.93  0.04  9186.22  1787  0.10  0.32  0.45   \n",
       "45000  1787  9133.62  19.52  32.80  0.04  9183.25  1784  0.23  0.36  0.47   \n",
       "50000  1786  9132.98  19.46  32.41  0.04  9182.20  1782  0.17  0.33  0.46   \n",
       "55000  1782  9129.84  19.50  32.06  0.06  9183.13  1784  0.16  0.33  0.46   \n",
       "60000  1782  9130.82  19.45  32.18  0.05  9183.13  1784  0.16  0.35  0.46   \n",
       "65000  1780  9128.58  19.51  31.89  0.05  9182.46  1784  0.17  0.33  0.46   \n",
       "70000  1781  9129.64  19.69  32.06  0.04  9182.46  1784  0.22  0.35  0.48   \n",
       "75000  1779  9127.24  19.64  32.14  0.05  9180.81  1780  0.15  0.40  0.49   \n",
       "\n",
       "      HIER:        \n",
       "      CHILD OTHER  \n",
       "5000   0.78  0.50  \n",
       "10000  0.67  0.42  \n",
       "15000  0.62  0.37  \n",
       "20000  0.58  0.35  \n",
       "25000  0.61  0.34  \n",
       "30000  0.59  0.34  \n",
       "35000  0.52  0.30  \n",
       "40000  0.53  0.30  \n",
       "45000  0.54  0.30  \n",
       "50000  0.54  0.30  \n",
       "55000  0.51  0.28  \n",
       "60000  0.53  0.30  \n",
       "65000  0.49  0.27  \n",
       "70000  0.53  0.28  \n",
       "75000  0.48  0.28  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 R: 1.000 P: 0.014 texts corpora english list types systems domain analysis languages dictionary\n",
      "   1 R: 0.280 P: 0.097 features feature training performance learning accuracy models classification test classifier\n",
      "     11 R: 0.080 P: 0.080 translation english source alignment target phrase translations parallel training pairs\n",
      "     12 R: 0.103 P: 0.103 models training vector learning vectors performance dataset network embeddings features\n",
      "   2 R: 0.215 P: 0.057 morphological pos character segmentation errors languages tagging arabic tags tag\n",
      "     21 R: 0.076 P: 0.076 verb verbs noun syntactic argument semantic subject arguments head predicate\n",
      "     22 R: 0.082 P: 0.082 tree parsing grammar parser dependency node trees rules parse algorithm\n",
      "   3 R: 0.167 P: 0.053 human annotation evaluation level agreement study annotators test systems differences\n",
      "     31 R: 0.053 P: 0.053 speech user dialogue utterance utterances speaker recognition spoken human systems\n",
      "     32 R: 0.062 P: 0.062 topic sentiment question questions answer topics tweets sentences answers summary\n",
      "   4 R: 0.168 P: 0.018 semantic knowledge type types context domain structure representation relations examples\n",
      "     41 R: 0.074 P: 0.074 semantic similarity relations relation sense wordnet event pairs senses lexical\n",
      "     42 R: 0.076 P: 0.076 discourse semantic knowledge structure type relations relation meaning context expressions\n",
      "   5 R: 0.155 P: 0.022 texts corpora annotation evaluation english systems automatic languages quality research\n",
      "     51 R: 0.067 P: 0.067 annotation user project resources database tools tool linguistic processing research\n",
      "     52 R: 0.066 P: 0.066 document documents entity terms query domain term entities method extraction\n",
      "[ 11.960399  -24.380676   -9.006723    4.659896   -3.436652    1.5567079\n",
      " -54.10932   -54.61172   -44.53682   -46.0942    -36.01728   -33.79793\n",
      " -40.235367  -40.09468   -33.199257  -32.207695 ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3c2a1a9b3f54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_recon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_kl_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_gauss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_reg_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppls_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_log\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_loss_recon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_loss_kl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_loss_gauss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_loss_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_ppls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mlosses_train\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_recon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_kl_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_gauss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_reg_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "while epoch < config.n_epochs:\n",
    "    # train\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = model.get_feed_dict(batch)\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch, ppls_batch, global_step_log = \\\n",
    "        sess.run([model.opt, model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_gauss, model.topic_loss_reg, model.topic_ppls, tf.train.get_global_step()], feed_dict = feed_dict)\n",
    "\n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if global_step_log % config.log_period == 0:\n",
    "            # validate\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_gauss_train, topic_loss_reg_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_gauss_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "\n",
    "            # test\n",
    "            if ppl_dev < ppl_min:\n",
    "                ppl_min = ppl_dev\n",
    "                loss_test, _, _, _, _, ppl_test, _ = validate(sess, test_batches, model)\n",
    "                saver.save(sess, config.path_model, global_step=global_step_log)\n",
    "                cPickle.dump(config, open(config.path_config % global_step_log, 'wb'))\n",
    "                update_checkpoint(config, checkpoint, global_step_log)\n",
    "            \n",
    "            # visualize topic\n",
    "            topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "            topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "            topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "            topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "            descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "            recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "            \n",
    "            depth_specs = get_topic_specialization(sess, model, instances_test)\n",
    "            hierarchical_affinities = get_hierarchical_affinity(sess, model)\n",
    "            \n",
    "            # log\n",
    "            clear_output()\n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_gauss_train, '%.2f'%topic_loss_reg_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_gauss_dev, '%.2f'%topic_loss_reg_dev, \\\n",
    "                    '%.2f'%loss_test, '%.0f'%ppl_test, \\\n",
    "                    '%.2f'%depth_specs[1], '%.2f'%depth_specs[2], '%.2f'%depth_specs[3], \\\n",
    "                    '%.2f'%hierarchical_affinities[0], '%.2f'%hierarchical_affinities[1]],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "            cPickle.dump(log_df, open(os.path.join(config.path_log), 'wb'))\n",
    "            print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)\n",
    "            print(np.sum(debug(model.topic_logvars), 1))\n",
    "            \n",
    "            # update tree\n",
    "            if not config.static:\n",
    "                config.tree_idxs, update_tree_flg = model.update_tree(topic_prob_topic, recur_prob_topic)\n",
    "                if update_tree_flg:\n",
    "                    print(config.tree_idxs)\n",
    "                    name_variables = {tensor.name: variable for tensor, variable in zip(tf.global_variables(), sess.run(tf.global_variables()))} # store paremeters\n",
    "                    if 'sess' in globals(): sess.close()\n",
    "                    model = HierarchicalNeuralTopicModel(config)\n",
    "                    sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "                    name_tensors = {tensor.name: tensor for tensor in tf.global_variables()}\n",
    "                    sess.run([name_tensors[name].assign(variable) for name, variable in name_variables.items()]) # restore parameters\n",
    "                    saver = tf.train.Saver(max_to_keep=1)\n",
    "                \n",
    "            time_start = time.time()\n",
    "\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "    epoch += 1\n",
    "\n",
    "loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "display(log_df)\n",
    "print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_year(sample_batches):\n",
    "    probs_topics = []\n",
    "    years = []\n",
    "    for i, sample_batch in sample_batches:\n",
    "        probs_topics_batch = sess.run(model.prob_topic, feed_dict=model.get_feed_dict(sample_batch, mode='test'))\n",
    "        years_batch = [instance.year for instance in sample_batch]\n",
    "        probs_topics += [probs_topics_batch]\n",
    "        years += years_batch\n",
    "    probs_topics = np.concatenate(probs_topics)\n",
    "    years = np.array(years)\n",
    "\n",
    "    topic_years = years.dot(probs_topics) / np.sum(probs_topics, 0)\n",
    "    topic_year = {model.topic_idxs[i]: year for i, year in enumerate(topic_years)}\n",
    "    return topic_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Avg Year: 2008 texts corpora english list types systems domain analysis languages dictionary\n",
      "   1 Avg Year: 2010 features feature training performance learning accuracy models classification test classifier\n",
      "     11 Avg Year: 2010 translation english source alignment target phrase translations parallel training pairs\n",
      "     12 Avg Year: 2013 models training vector learning vectors performance dataset network embeddings features\n",
      "   2 Avg Year: 2007 morphological pos character segmentation errors languages tagging arabic tags tag\n",
      "     21 Avg Year: 2006 verb verbs noun syntactic argument semantic subject arguments head predicate\n",
      "     22 Avg Year: 2005 tree parsing grammar parser dependency node trees rules parse algorithm\n",
      "   3 Avg Year: 2011 human annotation evaluation level agreement study annotators test systems differences\n",
      "     31 Avg Year: 2007 speech user dialogue utterance utterances speaker recognition spoken human systems\n",
      "     32 Avg Year: 2012 topic sentiment question questions answer topics tweets sentences answers summary\n",
      "   4 Avg Year: 2007 semantic knowledge type types context domain structure representation relations examples\n",
      "     41 Avg Year: 2010 semantic similarity relations relation sense wordnet event pairs senses lexical\n",
      "     42 Avg Year: 2002 discourse semantic knowledge structure type relations relation meaning context expressions\n",
      "   5 Avg Year: 2009 texts corpora annotation evaluation english systems automatic languages quality research\n",
      "     51 Avg Year: 2007 annotation user project resources database tools tool linguistic processing research\n",
      "     52 Avg Year: 2009 document documents entity terms query domain term entities method extraction\n"
     ]
    }
   ],
   "source": [
    "sample_batches = get_batches(instances_train, config.batch_size)\n",
    "topic_year = get_topic_year(sample_batches)\n",
    "print_topic_year(sess, model, topic_freq_tokens=topic_freq_tokens, topic_year=topic_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
