{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import _pickle as cPickle\n",
    "from collections import OrderedDict, defaultdict, Counter\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pdb\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy.sparse import csr_matrix\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "from data_structure import Instance\n",
    "\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)\n",
    "\n",
    "pd.set_option('display.max_rows', 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('train_dir', 'data/yelp/train', 'path of train data')\n",
    "flags.DEFINE_string('val_dir', 'data/yelp/val', 'path of val data')\n",
    "flags.DEFINE_string('test_dir', 'data/yelp/test', 'path of test data')\n",
    "flags.DEFINE_integer('batch_l', 8, 'batch length')\n",
    "\n",
    "flags.DEFINE_string('stopwords_path', 'data/stopwords_mallet.txt', 'path of input data')\n",
    "\n",
    "flags.DEFINE_string('business_path', 'data/yelp/business.json', 'path of business metadata')\n",
    "flags.DEFINE_string('ref_path', 'data/yelp/references.csv', 'directory of ref summaries')\n",
    "flags.DEFINE_string('output_path', 'data/yelp/instances.pkl', 'path of output data')\n",
    "flags.DEFINE_string('output_small_path', 'data/yelp/instances_small.pkl', 'path of output data')\n",
    "flags.DEFINE_string('output_large_path', 'data/yelp/instances_large.pkl', 'path of output data')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', 50000, 'size of vocab')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "config = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special tokens\n",
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences\n",
    "dummy_tokens = [PAD, UNK, BOS, EOS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_parallel(datas, num_split, map_func):\n",
    "    p = multiprocessing.Pool(processes=num_split)\n",
    "    data_split = np.array_split(datas, num_split)\n",
    "    output_dfs = p.map(map_func, data_split)\n",
    "    p.close()\n",
    "    output_df = pd.concat(output_dfs)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_business_df(business_path):\n",
    "    f = open(business_path, 'r')\n",
    "    business_df = pd.read_json(f, lines=True)\n",
    "    f.close()\n",
    "    business_df['category_list'] = business_df['categories'].apply(lambda categories: categories.split(', ') if categories is not None else [])\n",
    "    return business_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df = get_business_df(config.business_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm data\n",
    "def get_category_cnt(business_df):\n",
    "    return pd.DataFrame.from_dict(Counter([category for category_list in business_df['category_list'] for category in category_list]), orient='index').sort_values(0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_category_cnt(business_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df = business_df[business_df['category_list'].apply(lambda category_list: 'Restaurants' in category_list)]\n",
    "# get_category_cnt(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load review raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(text):\n",
    "    def preprocess(text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[0-9]+.[0-9]+|[0-9]+,[0-9]+|[0-9]+', '#', text)\n",
    "        text = re.sub('-', ' ', text)\n",
    "        code_regex = re.compile('[\"\\%&\\\\\\\\\\()*+/:;<=>?@[\\\\]^_`{|}~「」〔〕“”〈〉『』【】＆＊・（）＄＠。、？！｀＋￥％]|-lrb-(.*?)-rrb-|-lsb-(.*?)-rsb-')\n",
    "        text = code_regex.sub('', text)\n",
    "        return text\n",
    "\n",
    "    tokenize = lambda lines: [word_tokenize(line) for line in lines]\n",
    "    filtering = lambda tokens: [line_tokens for line_tokens in tokens if len(line_tokens) > 2]\n",
    "\n",
    "    preprocessed_text = preprocess(text)\n",
    "    lines = [line.strip() for line in re.split('[.!]', preprocessed_text) if not line == '']\n",
    "    tokens = tokenize(lines)\n",
    "    tokens = filtering(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_raw_df(data_paths):\n",
    "    data_raw_dfs = []\n",
    "    for data_path in data_paths:\n",
    "        f = open(data_path, 'r')\n",
    "        item_raw_df = pd.read_json(f)\n",
    "        f.close()\n",
    "        data_raw_dfs.append(item_raw_df)\n",
    "    data_raw_df = pd.concat(data_raw_dfs)\n",
    "    data_raw_df['tokens'] = data_raw_df['text'].apply(get_tokens)\n",
    "    data_raw_df = data_raw_df[data_raw_df['tokens'].apply(lambda tokens: len(tokens) > 2)]\n",
    "    return data_raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data_paths = lambda data_dir: [os.path.join(data_dir, data_name) for data_name in os.listdir(data_dir) if not data_name == 'store-to-nreviews.json']\n",
    "train_data_paths = get_data_paths(config.train_dir)\n",
    "val_data_paths = get_data_paths(config.val_dir)\n",
    "test_data_paths = get_data_paths(config.test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get train & val raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40 s, sys: 15.1 s, total: 55.1 s\n",
      "Wall time: 2min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_raw_df = apply_parallel(train_data_paths, num_split=32, map_func=get_data_raw_df).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.24 s, sys: 7.01 s, total: 15.2 s\n",
      "Wall time: 47.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val_raw_df = apply_parallel(val_data_paths, num_split=8, map_func=get_data_raw_df).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_raw_df.tokens.apply(lambda tokens: len(tokens)==0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "le est sympathique et les prix sont raisonnables sum la dim de san ayce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get test raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.83 s, sys: 4.3 s, total: 7.12 s\n",
      "Wall time: 40.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_tmp_df = apply_parallel(test_data_paths, num_split=8, map_func=get_data_raw_df).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_raw_df(ref_path, train_raw_df, val_raw_df, test_raw_df, business_df):\n",
    "    def get_review_business_id_dict(ref_df, train_raw_df, val_raw_df, test_raw_df):\n",
    "        ref_review_ids = []\n",
    "        for _, row in ref_df.iterrows():\n",
    "            ref_review_ids += [row['Input.original_review_%i_id' % i] for i in range(config.batch_l)]\n",
    "        ref_review_id_df = pd.DataFrame(ref_review_ids, columns=['review_id']) # only review_id in reference.csv\n",
    "        concat_raw_df = pd.concat([train_raw_df, val_raw_df, test_raw_df])[['review_id', 'business_id']] # filter review_id and business_id pair\n",
    "        review_business_id_dict = {row.review_id: row.business_id for _, row in pd.merge(ref_review_id_df, concat_raw_df).iterrows()}\n",
    "        return review_business_id_dict\n",
    "    \n",
    "    ref_df = pd.read_csv(open(ref_path, 'r'))\n",
    "    ref_df['business_id_csv'] = ref_df.apply(lambda row: row['Input.business_id'] if row['Input.business_id'] != '#NAME?' else 'null_%i' % row.name, axis=1)\n",
    "    review_business_id_dict = get_review_business_id_dict(ref_df, train_raw_df, val_raw_df, test_raw_df)\n",
    "    \n",
    "    test_raw_dfs = []\n",
    "    for index, row in ref_df.iterrows():\n",
    "        business_id = None \n",
    "        texts, review_ids = [], []\n",
    "        summary = row['Answer.summary']\n",
    "        for i in range(config.batch_l):\n",
    "            text = row['Input.original_review_%i' % i]\n",
    "            review_id = row['Input.original_review_%i_id' % i]\n",
    "            texts.append(text)\n",
    "            review_ids.append(review_id)\n",
    "            if review_id == '#NAME?': review_id = None\n",
    "            if business_id is None and review_id in review_business_id_dict: business_id = review_business_id_dict[review_id] # get business_id from review_business_id_dict\n",
    "        \n",
    "        if business_id is None: business_id = row['business_id_csv'] # if all review_id not in review_business_id_dict, then business_id in csv is used\n",
    "        for text, review_id in zip(texts, review_ids):\n",
    "            test_raw_dfs.append({'business_id': business_id, 'text': text, 'review_id': review_id, 'summary': summary})\n",
    "        \n",
    "    test_raw_df = pd.DataFrame(test_raw_dfs)\n",
    "    test_raw_df['tokens'] = test_raw_df['text'].apply(get_tokens)\n",
    "    return test_raw_df, ref_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw_df, ref_df = get_test_raw_df(config.ref_path, train_raw_df, val_raw_df, test_tmp_df, business_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# group data by business id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_category(group_df, business_df):\n",
    "    group_df = pd.merge(group_df, business_df[['business_id', 'category_list']])\n",
    "    group_df = group_df[group_df['category_list'].apply(lambda category_list: 'Restaurants' in category_list)]\n",
    "    return group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_group_df(data_df, business_df):\n",
    "    group_df = data_df.groupby('business_id').agg({\n",
    "        'tokens': lambda token_idxs_list: [sent_idxs for token_idxs in list(token_idxs_list) for sent_idxs in token_idxs],\n",
    "        'summary': lambda summary_series: list(summary_series)[0] # only first column for each business id\n",
    "    })\n",
    "    group_df['doc_l'] = group_df['tokens'].apply(lambda tokens: len(tokens))\n",
    "    group_df['max_sent_l'] = group_df['tokens'].apply(lambda tokens: max([len(line) for line in tokens]))\n",
    "    group_df['sent_l'] = group_df['tokens'].apply(lambda tokens: [len(line) for line in tokens])\n",
    "    group_df = group_df.reset_index()\n",
    "    print('before filtering: %i' % len(group_df))\n",
    "    group_df = filter_category(group_df, business_df)\n",
    "    print('after filtering: %i' % len(group_df))\n",
    "    return group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before filtering: 200\n",
      "after filtering: 141\n"
     ]
    }
   ],
   "source": [
    "test_df = get_test_group_df(test_raw_df, business_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_df(data_df, business_df, test_df, filter_sent_l=np.inf, filter_doc_l=np.inf):\n",
    "    group_df = data_df.groupby('business_id').agg({\n",
    "        'tokens': lambda tokens_list: list(tokens_list)\n",
    "    })\n",
    "    test_business_ids = test_df['business_id'].values\n",
    "    batch_list = []\n",
    "    for business_id, row in group_df.iterrows():\n",
    "        if business_id in test_business_ids: continue\n",
    "        tokens = random.sample(row.tokens, len(row.tokens))\n",
    "        tokens = [sents for sents in tokens if max([len(sent) for sent in sents]) <= filter_sent_l]\n",
    "        tokens_list = list(zip(*[iter(tokens)]*config.batch_l))\n",
    "        for tokens in tokens_list:\n",
    "            tokens = [sent for sents in tokens for sent in sents]\n",
    "            doc_l = len(tokens)\n",
    "            sent_l = [len(sent) for sent in tokens]\n",
    "            max_sent_l = max(sent_l)\n",
    "            batch_list.append({'business_id': business_id, 'tokens': tokens, 'doc_l': doc_l, 'sent_l': sent_l, 'max_sent_l': max_sent_l})\n",
    "    group_df = pd.DataFrame(batch_list)\n",
    "    print('before filtering: %i' % len(group_df))\n",
    "    group_df = group_df[group_df['doc_l'] <= filter_doc_l]\n",
    "    group_df = filter_category(group_df, business_df)\n",
    "    print('after filtering: %i' % len(group_df))\n",
    "    return group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before filtering: 240433\n",
      "after filtering: 160626\n",
      "before filtering: 29917\n",
      "after filtering: 20063\n"
     ]
    }
   ],
   "source": [
    "train_df = get_group_df(train_raw_df, business_df, test_df, filter_sent_l=50, filter_doc_l=50)\n",
    "val_df = get_group_df(val_raw_df, business_df, test_df, filter_sent_l=50, filter_doc_l=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 26\n",
      "50 26\n",
      "65 28\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(max(train_df.doc_l), min(train_df.doc_l))\n",
    "print(max(val_df.doc_l), min(val_df.doc_l))\n",
    "print(max(test_df.doc_l), min(test_df.doc_l))\n",
    "\n",
    "train_df.doc_l.plot(bins=50, alpha=0.5, figsize=(8,4), kind='hist', range=(0, 100), density=True);\n",
    "test_df.doc_l.plot(bins=50, alpha=0.5, figsize=(8,4), kind='hist', range=(0, 100), density=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 13\n",
      "50 14\n",
      "129 18\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(max(train_df.max_sent_l), min(train_df.max_sent_l))\n",
    "print(max(val_df.max_sent_l), min(val_df.max_sent_l))\n",
    "print(max(test_df.max_sent_l), min(test_df.max_sent_l))\n",
    "\n",
    "train_df.max_sent_l.plot(bins=50, alpha=0.5, figsize=(8,4), kind='hist', range=(0, 150), density=True);\n",
    "test_df.max_sent_l.plot(bins=50, alpha=0.5, figsize=(8,4), kind='hist', range=(0, 150), density=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build token idxs for language modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.4 s, sys: 3 s, total: 25.4 s\n",
      "Wall time: 25.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "words_list = train_df['tokens'].apply(lambda tokens: [token for line in tokens for token in line])\n",
    "word_tf_dict = sorted(Counter([word for words in words_list for word in words]).items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.5 s, sys: 4.69 s, total: 45.2 s\n",
      "Wall time: 44.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "unique_words_list = train_raw_df['tokens'].apply(lambda tokens: list(set([token for line in tokens for token in line])))\n",
    "word_df_dict = sorted(Counter([word for unique_words in unique_words_list for word in unique_words]).items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_words(word_tf_dict, max_tf_rate=0., min_tf_rate=0., min_tf=None, min_df=None, stop_words=[]):\n",
    "    filtered_word_tf_dict = dict(word_tf_dict[int(min_tf_rate*len(word_tf_dict)):int((1-max_tf_rate)*len(word_tf_dict))][::-1])\n",
    "    if min_tf: filtered_word_tf_dict = {word: tf for word, tf in filtered_word_tf_dict.items() if tf >= min_tf}\n",
    "    filtered_words = [word for word, _ in filtered_word_tf_dict.items() if word not in stop_words]\n",
    "    \n",
    "    if min_df: \n",
    "        filtered_df_words = [word for word, df in word_df_dict if df >= min_df]\n",
    "        filtered_df_words = list(set(filtered_words) & set(filtered_df_words))\n",
    "        filtered_word_tf_df_dict = sorted([(word, filtered_word_tf_dict[word]) for word in filtered_df_words], key=lambda x:x[1])[::-1]\n",
    "        filtered_words = [word for word, _ in filtered_word_tf_df_dict]\n",
    "        \n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24425"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_words = [PAD, UNK, BOS, EOS]\n",
    "# lm_words += filter_words(word_tf_dict, min_tf=10) # large: 10 usual: 10 small: 20\n",
    "lm_words += filter_words(word_tf_dict, min_tf=20) # large: 10 usual: 10 small: 20\n",
    "idx_to_word = {idx: word for idx, word in enumerate(lm_words)}\n",
    "word_to_idx = {word: idx for idx, word in idx_to_word.items()}\n",
    "len(lm_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_token_idxs(tokens_series):\n",
    "    def get_token_idxs(tokens):\n",
    "        return [[word_to_idx[token] if token in word_to_idx else word_to_idx[UNK] for token in sent] for sent in tokens]\n",
    "    return tokens_series.apply(get_token_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.4 s, sys: 1min 6s, total: 2min\n",
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df['token_idxs'] = apply_parallel(train_df['tokens'], num_split=64, map_func=apply_token_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.87 s, sys: 47.2 s, total: 51 s\n",
      "Wall time: 51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val_df['token_idxs'] = apply_parallel(val_df['tokens'], num_split=64, map_func=apply_token_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['token_idxs'] = apply_token_idxs(test_df['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build bow for topic modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2721"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(config.stopwords_path, 'r') as f:\n",
    "    stop_words_mallet = [w.replace('\\n', '') for w in f.readlines()]\n",
    "stop_words_nltk = nltk.corpus.stopwords.words('english')\n",
    "# stop_words = list(set(stop_words_nltk + [',', '#', '$', \"n't\", \"'s\", \"'ve\", \"'m\",  \"'ll\", \"'re\", \"'d\", 'could', 'would', 'us', 'ca']))\n",
    "stop_words = list(set(stop_words_mallet + stop_words_nltk + [',', '#', '$', \"n't\", \"'s\", \"'ve\", \"'m\",  \"'ll\", \"'re\", \"'d\", 'could', 'would', 'us', 'ca']))\n",
    "# tm_words = filter_words(word_tf_dict, min_df=500, min_tf=10, stop_words=stop_words) # large: 500 usual: 1000 small: 2000\n",
    "# tm_words = filter_words(word_tf_dict, min_df=1000, min_tf=10, stop_words=stop_words) # large: 500 usual: 1000 small: 2000\n",
    "tm_words = filter_words(word_tf_dict, min_df=2000, min_tf=20, stop_words=stop_words) # large: 500 usual: 1000 small: 2000\n",
    "# tm_words = filter_words(word_tf_dict, min_tf=1000, stop_words=stop_words)\n",
    "word_to_bow = {word: idx for idx, word in enumerate(tm_words)}\n",
    "bow_idxs = np.array([word_to_idx[word] for word in tm_words])\n",
    "len(tm_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_bows(tokens_series):\n",
    "    def get_bows(tokens):\n",
    "        def line_to_bow(line):\n",
    "            counter = Counter(line)\n",
    "            bow = np.zeros(len(word_to_bow), dtype=np.int32)\n",
    "            indices_cnts = np.array([[word_to_bow[word], cnt] for word, cnt in counter.items() if word in word_to_bow])\n",
    "            if len(indices_cnts) > 0: bow[indices_cnts[:, 0]] = indices_cnts[:, 1]\n",
    "            return bow\n",
    "#         bows = np.array([line_to_bow(line) for line in tokens])\n",
    "        bows = csr_matrix([line_to_bow(line) for line in tokens])\n",
    "        return bows\n",
    "    return tokens_series.apply(get_bows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28 s, sys: 1min 2s, total: 1min 30s\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df['bows'] = apply_parallel(train_df['tokens'], num_split=64, map_func=apply_bows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.26 s, sys: 31.4 s, total: 34.7 s\n",
      "Wall time: 35.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val_df['bows'] = apply_parallel(val_df['tokens'], num_split=32, map_func=apply_bows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['bows'] = apply_bows(test_df['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirm\n",
    "for i in range(len(val_df)//100):\n",
    "    assert all([all([word in line for word in np.array(tm_words)[np.where(line_bows>0)[0]]]) for line_bows, line in zip(val_df['bows'].iloc[i*100].toarray(), val_df['tokens'].iloc[i*100])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_save_df(data_df, frac=1., summary=False):\n",
    "    save_df = data_df[['business_id', 'doc_l', 'sent_l', 'max_sent_l', 'bows', 'token_idxs']]\n",
    "    if summary: save_df = data_df[['business_id', 'doc_l', 'sent_l', 'max_sent_l', 'bows', 'token_idxs', 'summary']]\n",
    "    if frac < 1: save_df = save_df.sample(frac=0.01)\n",
    "    return save_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160626, 201, 141)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_save_df = get_save_df(train_df)\n",
    "val_save_df = get_save_df(val_df, frac=0.01)\n",
    "test_save_df = get_save_df(test_df, summary=True)\n",
    "len(train_save_df), len(val_save_df), len(test_save_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving preprocessed instances...\n"
     ]
    }
   ],
   "source": [
    "print('saving preprocessed instances...')\n",
    "# cPickle.dump((train_save_df, val_save_df, test_save_df, word_to_idx, idx_to_word, bow_idxs),open(config.output_large_path,'wb'))\n",
    "# cPickle.dump((train_save_df, val_save_df, test_save_df, word_to_idx, idx_to_word, bow_idxs),open(config.output_path,'wb'))\n",
    "cPickle.dump((train_save_df, val_save_df, test_save_df, word_to_idx, idx_to_word, bow_idxs),open(config.output_small_path,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_instances(data_df, word_to_idx):\n",
    "#     instances = []\n",
    "#     for index, doc in data_df.iterrows():\n",
    "#         instance = Instance()\n",
    "#         instance.business_id = doc.business_id\n",
    "#         doc_token_idxs = []\n",
    "#         for sent_tokens in doc.tokens:\n",
    "#             sent_token_idxs = [word_to_idx[token] if token in word_to_idx else word_to_idx[UNK] for token in sent_tokens]\n",
    "#             doc_token_idxs.append(sent_token_idxs)\n",
    "#         instance.token_idxs = doc_token_idxs\n",
    "#         instance.doc_l = doc.doc_l\n",
    "#         instance.sent_l = doc.sent_l\n",
    "#         instance.max_sent_l = doc.max_sent_l\n",
    "#         instance.bows = doc.bows\n",
    "#         if 'summary' in doc: instance.summary = doc.summary\n",
    "#         instances.append(instance)\n",
    "#     return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 48s, sys: 7.98 s, total: 1min 56s\n",
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# instances_train = prepare_instances(train_df, word_to_idx)\n",
    "# instances_val = prepare_instances(val_df, word_to_idx)\n",
    "# instances_test = prepare_instances(test_df, word_to_idx)\n",
    "# len(instances_train), len(instances_val), len(instances_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instances_val = random.sample(instances_val, len(instances_val)//100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print('saving preprocessed instances...')\n",
    "# cPickle.dump((instances_train, instances_val, instances_test, word_to_idx, idx_to_word, bow_idxs),open(config.output_path+'_tmp','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'strip'\n",
    "tokens_list = train_df[words_list.apply(lambda words: word in words)].tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have passed this shopping mall strip plenty of times but never actually ventured in\n"
     ]
    }
   ],
   "source": [
    "for line_words in tokens_list.iloc[3]:\n",
    "    if word in line_words:\n",
    "        print(' '.join(line_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "though i put # stars here , i think # would be more appropriate\n",
      "the chicken was tender and tasty , but it was n't the the most special chicken rotisserie ever\n",
      "like it was good , solid , satisfying , delicious , but i 'm not sure that another perhaps more obscure restaurant could n't do the same fries were very good , though again not necessarily exceptional overall still a solid experience\n",
      "portion is very generous so order less or you 'll have a ton of leftovers\n",
      "i think for one person # chicken is fine , and you can ask for less friesrice and more salad\n",
      "the half chicken is an amazing deal\n",
      "comes with fries and salad , and the chicken itself is very well seasoned and full of flavor\n",
      "the seating space is a little bit limited though , and the pastry section 's desserts seemed not very fresh\n",
      "altogether , a great experience because the chicken was top notch\n",
      "i will definitely revisit this place , but i suggest to avoid the desserts section in the evening\n",
      "loved the chicken , deserves the hype , price is good\n",
      "would definitely go back\n",
      "place is on the small side , be prepared for a bit of a wait to sit down\n",
      "pretty tasty chicken and fries\n",
      "they ran out of rice when i was there\n",
      "we also had a tart pastry i forgot which one , but it was pretty good\n",
      "overall a rather heavy meal\n",
      "très bon poulet bbq à la portuguaise\n",
      "assurez vous de prendre avec sauce piquante ai vous aimez un peu de piquant , vous pouvez aussi en acheter extra\n",
      "les frites maisons sont super , ainsi que la vinagrette de la salade maison\n",
      "la meilleure valeur si vous êtes deux c'est de prendre le demi poulet en combo avec frites et salade , versus deux repas individuels\n",
      "le chorizo grillé est très bon aussi\n",
      "all portugese chicken places are born equal , but some are more equal than others\n",
      "get the chicken in any shape or form\n",
      "if they made chicken smoothies , i am sure i would order them as well\n",
      "service is fast\n",
      "only downside side they are a chicken place that runs out of chicken\n",
      "cuit aux charbons cuisson toujours parfaite\n",
      "sauce de cuisson\n",
      "poutine au poulet la sauce est excellente\n",
      "vraiment très rassasiant\n"
     ]
    }
   ],
   "source": [
    "for line_words in tokens_list.iloc[0]:\n",
    "    print(' '.join(line_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
