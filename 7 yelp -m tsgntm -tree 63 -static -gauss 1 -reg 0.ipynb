{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from data_structure import get_batches\n",
    "from hntm import HierarchicalNeuralTopicModel\n",
    "from nhdp import nestedHierarchicalNeuralTopicModel\n",
    "from tsgntm import TreeStructuredGaussianNeuralTopicModel\n",
    "from tree import get_descendant_idxs\n",
    "from evaluation import get_hierarchical_affinity, get_topic_specialization, print_topic_sample, print_topic_year\n",
    "from configure import get_config\n",
    "\n",
    "pd.set_option('display.max_columns', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(nb_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu\n",
    "np.random.seed(config.seed)\n",
    "random.seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.path_data,'rb'))\n",
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)\n",
    "config.dim_bow = len(bow_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     6
    ]
   },
   "outputs": [],
   "source": [
    "def debug(variable, sample_batch=None):\n",
    "    if sample_batch is None: sample_batch = test_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch, mode='test')\n",
    "    _variable = sess.run(variable, feed_dict=feed_dict)\n",
    "    return _variable\n",
    "\n",
    "def check(variable):\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sample_batch = test_batches[0]\n",
    "    feed_dict = model.get_feed_dict(sample_batch, mode='test', assertion=True)\n",
    "    _variable = sess.run(variable, feed_dict=feed_dict)\n",
    "    return _variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "TopicModels = {'hntm': HierarchicalNeuralTopicModel, 'nhdp': nestedHierarchicalNeuralTopicModel, 'tsgntm': TreeStructuredGaussianNeuralTopicModel}\n",
    "TopicModel = TopicModels[config.model]\n",
    "model = TopicModel(config)\n",
    "sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(max_to_keep=config.max_to_keep)\n",
    "update_tree_flg = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     24
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = []\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "ppl_min = np.inf\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','','','','','','VALID:','','','','','','TEST:','', 'SPEC:', '', '', 'HIER:', ''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL', 'GAUSS', 'REG','LOSS','PPL','NLL','KL', 'GAUSS','REG','LOSS','PPL', '1', '2', '3', 'CHILD', 'OTHER']]))))\n",
    "\n",
    "cmd_rm = 'rm -r %s' % config.dir_model\n",
    "res = subprocess.call(cmd_rm.split())\n",
    "cmd_mk = 'mkdir %s' % config.dir_model\n",
    "res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "def update_checkpoint(config, checkpoint, global_step):\n",
    "    checkpoint.append(config.path_model + '-%i' % global_step)\n",
    "    if len(checkpoint) > config.max_to_keep:\n",
    "        path_model = checkpoint.pop(0) + '.*'\n",
    "        for p in glob.glob(path_model):\n",
    "            os.remove(p)\n",
    "    cPickle.dump(checkpoint, open(config.path_checkpoint, 'wb'))\n",
    "    \n",
    "def validate(sess, batches, model):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    prob_topic_list = []\n",
    "    n_bow_list = []\n",
    "    n_topics_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = model.get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch, ppls_batch, prob_topic_batch, n_bow_batch, n_topics_batch \\\n",
    "            = sess.run([model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_gauss, model.topic_loss_reg, model.topic_ppls, model.prob_topic, model.n_bow, model.n_topics], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "        prob_topic_list.append(prob_topic_batch)\n",
    "        n_bow_list.append(n_bow_batch)\n",
    "        n_topics_list.append(n_topics_batch)\n",
    "    loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_gauss_mean, topic_loss_reg_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    \n",
    "    probs_topic = np.concatenate(prob_topic_list, 0)\n",
    "    \n",
    "    n_bow = np.concatenate(n_bow_list, 0)\n",
    "    n_topics = np.concatenate(n_topics_list, 0)\n",
    "    probs_topic_mean = np.sum(n_topics, 0) / np.sum(n_bow)\n",
    "    \n",
    "    return loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_gauss_mean, topic_loss_reg_mean, ppl_mean, probs_topic_mean    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train & validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th colspan=\"5\" halign=\"left\"></th>\n",
       "      <th>VALID:</th>\n",
       "      <th colspan=\"5\" halign=\"left\"></th>\n",
       "      <th>TEST:</th>\n",
       "      <th></th>\n",
       "      <th>SPEC:</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "      <th>HIER:</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>GAUSS</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>GAUSS</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>CHILD</th>\n",
       "      <th>OTHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>999</td>\n",
       "      <td>929.08</td>\n",
       "      <td>2708</td>\n",
       "      <td>924.71</td>\n",
       "      <td>1.21</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.25</td>\n",
       "      <td>890.30</td>\n",
       "      <td>2597</td>\n",
       "      <td>885.69</td>\n",
       "      <td>1.49</td>\n",
       "      <td>3.12</td>\n",
       "      <td>0.22</td>\n",
       "      <td>888.58</td>\n",
       "      <td>2549</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1999</td>\n",
       "      <td>922.36</td>\n",
       "      <td>2556</td>\n",
       "      <td>917.09</td>\n",
       "      <td>1.99</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.24</td>\n",
       "      <td>880.71</td>\n",
       "      <td>2364</td>\n",
       "      <td>874.73</td>\n",
       "      <td>2.95</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0.30</td>\n",
       "      <td>879.79</td>\n",
       "      <td>2370</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>295</td>\n",
       "      <td>919.52</td>\n",
       "      <td>2477</td>\n",
       "      <td>913.80</td>\n",
       "      <td>2.42</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.24</td>\n",
       "      <td>878.20</td>\n",
       "      <td>2304</td>\n",
       "      <td>871.00</td>\n",
       "      <td>3.48</td>\n",
       "      <td>3.71</td>\n",
       "      <td>0.27</td>\n",
       "      <td>878.46</td>\n",
       "      <td>2321</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>1295</td>\n",
       "      <td>916.76</td>\n",
       "      <td>2425</td>\n",
       "      <td>910.70</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.37</td>\n",
       "      <td>0.24</td>\n",
       "      <td>873.94</td>\n",
       "      <td>2218</td>\n",
       "      <td>866.65</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.54</td>\n",
       "      <td>0.28</td>\n",
       "      <td>876.26</td>\n",
       "      <td>2291</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>2295</td>\n",
       "      <td>914.52</td>\n",
       "      <td>2387</td>\n",
       "      <td>908.26</td>\n",
       "      <td>2.85</td>\n",
       "      <td>3.41</td>\n",
       "      <td>0.23</td>\n",
       "      <td>873.04</td>\n",
       "      <td>2208</td>\n",
       "      <td>865.59</td>\n",
       "      <td>3.81</td>\n",
       "      <td>3.64</td>\n",
       "      <td>0.21</td>\n",
       "      <td>874.92</td>\n",
       "      <td>2263</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6000</th>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>591</td>\n",
       "      <td>913.71</td>\n",
       "      <td>2361</td>\n",
       "      <td>907.30</td>\n",
       "      <td>2.97</td>\n",
       "      <td>3.44</td>\n",
       "      <td>0.23</td>\n",
       "      <td>874.29</td>\n",
       "      <td>2224</td>\n",
       "      <td>866.97</td>\n",
       "      <td>3.64</td>\n",
       "      <td>3.68</td>\n",
       "      <td>0.15</td>\n",
       "      <td>874.92</td>\n",
       "      <td>2263</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7000</th>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>1591</td>\n",
       "      <td>912.61</td>\n",
       "      <td>2340</td>\n",
       "      <td>906.06</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.48</td>\n",
       "      <td>0.23</td>\n",
       "      <td>870.75</td>\n",
       "      <td>2156</td>\n",
       "      <td>863.24</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.77</td>\n",
       "      <td>0.27</td>\n",
       "      <td>872.67</td>\n",
       "      <td>2225</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8000</th>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>2591</td>\n",
       "      <td>911.56</td>\n",
       "      <td>2323</td>\n",
       "      <td>904.89</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.53</td>\n",
       "      <td>0.23</td>\n",
       "      <td>870.12</td>\n",
       "      <td>2151</td>\n",
       "      <td>862.39</td>\n",
       "      <td>3.93</td>\n",
       "      <td>3.80</td>\n",
       "      <td>0.19</td>\n",
       "      <td>871.15</td>\n",
       "      <td>2189</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9000</th>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>887</td>\n",
       "      <td>910.86</td>\n",
       "      <td>2307</td>\n",
       "      <td>904.05</td>\n",
       "      <td>3.23</td>\n",
       "      <td>3.59</td>\n",
       "      <td>0.23</td>\n",
       "      <td>871.73</td>\n",
       "      <td>2175</td>\n",
       "      <td>863.62</td>\n",
       "      <td>4.04</td>\n",
       "      <td>4.07</td>\n",
       "      <td>0.25</td>\n",
       "      <td>871.15</td>\n",
       "      <td>2189</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>1887</td>\n",
       "      <td>910.17</td>\n",
       "      <td>2293</td>\n",
       "      <td>903.20</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.66</td>\n",
       "      <td>0.23</td>\n",
       "      <td>870.81</td>\n",
       "      <td>2157</td>\n",
       "      <td>862.39</td>\n",
       "      <td>4.17</td>\n",
       "      <td>4.25</td>\n",
       "      <td>0.17</td>\n",
       "      <td>871.15</td>\n",
       "      <td>2189</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11000</th>\n",
       "      <td>52</td>\n",
       "      <td>4</td>\n",
       "      <td>183</td>\n",
       "      <td>909.58</td>\n",
       "      <td>2279</td>\n",
       "      <td>902.48</td>\n",
       "      <td>3.38</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0.22</td>\n",
       "      <td>867.57</td>\n",
       "      <td>2089</td>\n",
       "      <td>858.81</td>\n",
       "      <td>4.27</td>\n",
       "      <td>4.49</td>\n",
       "      <td>0.15</td>\n",
       "      <td>870.63</td>\n",
       "      <td>2169</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12000</th>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>1183</td>\n",
       "      <td>908.96</td>\n",
       "      <td>2267</td>\n",
       "      <td>901.71</td>\n",
       "      <td>3.45</td>\n",
       "      <td>3.81</td>\n",
       "      <td>0.22</td>\n",
       "      <td>868.33</td>\n",
       "      <td>2093</td>\n",
       "      <td>859.41</td>\n",
       "      <td>4.29</td>\n",
       "      <td>4.63</td>\n",
       "      <td>0.23</td>\n",
       "      <td>870.63</td>\n",
       "      <td>2169</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13000</th>\n",
       "      <td>51</td>\n",
       "      <td>4</td>\n",
       "      <td>2183</td>\n",
       "      <td>908.31</td>\n",
       "      <td>2255</td>\n",
       "      <td>900.92</td>\n",
       "      <td>3.51</td>\n",
       "      <td>3.88</td>\n",
       "      <td>0.22</td>\n",
       "      <td>866.51</td>\n",
       "      <td>2059</td>\n",
       "      <td>857.43</td>\n",
       "      <td>4.39</td>\n",
       "      <td>4.69</td>\n",
       "      <td>0.19</td>\n",
       "      <td>867.20</td>\n",
       "      <td>2105</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14000</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>479</td>\n",
       "      <td>907.92</td>\n",
       "      <td>2244</td>\n",
       "      <td>900.41</td>\n",
       "      <td>3.57</td>\n",
       "      <td>3.93</td>\n",
       "      <td>0.22</td>\n",
       "      <td>866.87</td>\n",
       "      <td>2069</td>\n",
       "      <td>857.65</td>\n",
       "      <td>4.46</td>\n",
       "      <td>4.77</td>\n",
       "      <td>0.16</td>\n",
       "      <td>867.20</td>\n",
       "      <td>2105</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>51</td>\n",
       "      <td>5</td>\n",
       "      <td>1479</td>\n",
       "      <td>907.49</td>\n",
       "      <td>2234</td>\n",
       "      <td>899.87</td>\n",
       "      <td>3.63</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.22</td>\n",
       "      <td>865.99</td>\n",
       "      <td>2054</td>\n",
       "      <td>856.70</td>\n",
       "      <td>4.48</td>\n",
       "      <td>4.80</td>\n",
       "      <td>0.21</td>\n",
       "      <td>867.81</td>\n",
       "      <td>2111</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16000</th>\n",
       "      <td>52</td>\n",
       "      <td>5</td>\n",
       "      <td>2479</td>\n",
       "      <td>906.93</td>\n",
       "      <td>2225</td>\n",
       "      <td>899.22</td>\n",
       "      <td>3.67</td>\n",
       "      <td>4.04</td>\n",
       "      <td>0.21</td>\n",
       "      <td>865.89</td>\n",
       "      <td>2050</td>\n",
       "      <td>856.61</td>\n",
       "      <td>4.53</td>\n",
       "      <td>4.75</td>\n",
       "      <td>0.16</td>\n",
       "      <td>865.50</td>\n",
       "      <td>2075</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17000</th>\n",
       "      <td>51</td>\n",
       "      <td>6</td>\n",
       "      <td>775</td>\n",
       "      <td>906.56</td>\n",
       "      <td>2217</td>\n",
       "      <td>898.76</td>\n",
       "      <td>3.72</td>\n",
       "      <td>4.08</td>\n",
       "      <td>0.21</td>\n",
       "      <td>866.56</td>\n",
       "      <td>2063</td>\n",
       "      <td>857.32</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.78</td>\n",
       "      <td>0.22</td>\n",
       "      <td>865.50</td>\n",
       "      <td>2075</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18000</th>\n",
       "      <td>51</td>\n",
       "      <td>6</td>\n",
       "      <td>1775</td>\n",
       "      <td>906.24</td>\n",
       "      <td>2210</td>\n",
       "      <td>898.35</td>\n",
       "      <td>3.76</td>\n",
       "      <td>4.13</td>\n",
       "      <td>0.21</td>\n",
       "      <td>867.98</td>\n",
       "      <td>2088</td>\n",
       "      <td>858.70</td>\n",
       "      <td>4.40</td>\n",
       "      <td>4.87</td>\n",
       "      <td>0.21</td>\n",
       "      <td>865.50</td>\n",
       "      <td>2075</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19000</th>\n",
       "      <td>51</td>\n",
       "      <td>7</td>\n",
       "      <td>71</td>\n",
       "      <td>905.93</td>\n",
       "      <td>2203</td>\n",
       "      <td>897.98</td>\n",
       "      <td>3.79</td>\n",
       "      <td>4.16</td>\n",
       "      <td>0.21</td>\n",
       "      <td>866.93</td>\n",
       "      <td>2067</td>\n",
       "      <td>857.49</td>\n",
       "      <td>4.53</td>\n",
       "      <td>4.92</td>\n",
       "      <td>0.13</td>\n",
       "      <td>865.50</td>\n",
       "      <td>2075</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>51</td>\n",
       "      <td>7</td>\n",
       "      <td>1071</td>\n",
       "      <td>905.54</td>\n",
       "      <td>2196</td>\n",
       "      <td>897.52</td>\n",
       "      <td>3.82</td>\n",
       "      <td>4.20</td>\n",
       "      <td>0.21</td>\n",
       "      <td>867.76</td>\n",
       "      <td>2086</td>\n",
       "      <td>858.47</td>\n",
       "      <td>4.43</td>\n",
       "      <td>4.86</td>\n",
       "      <td>0.14</td>\n",
       "      <td>865.50</td>\n",
       "      <td>2075</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21000</th>\n",
       "      <td>51</td>\n",
       "      <td>7</td>\n",
       "      <td>2071</td>\n",
       "      <td>905.20</td>\n",
       "      <td>2190</td>\n",
       "      <td>897.12</td>\n",
       "      <td>3.85</td>\n",
       "      <td>4.23</td>\n",
       "      <td>0.21</td>\n",
       "      <td>866.08</td>\n",
       "      <td>2053</td>\n",
       "      <td>856.74</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.83</td>\n",
       "      <td>0.22</td>\n",
       "      <td>865.50</td>\n",
       "      <td>2075</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22000</th>\n",
       "      <td>54</td>\n",
       "      <td>8</td>\n",
       "      <td>367</td>\n",
       "      <td>905.00</td>\n",
       "      <td>2184</td>\n",
       "      <td>896.86</td>\n",
       "      <td>3.88</td>\n",
       "      <td>4.26</td>\n",
       "      <td>0.21</td>\n",
       "      <td>864.14</td>\n",
       "      <td>2015</td>\n",
       "      <td>854.73</td>\n",
       "      <td>4.52</td>\n",
       "      <td>4.89</td>\n",
       "      <td>0.15</td>\n",
       "      <td>864.59</td>\n",
       "      <td>2054</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23000</th>\n",
       "      <td>51</td>\n",
       "      <td>8</td>\n",
       "      <td>1367</td>\n",
       "      <td>904.74</td>\n",
       "      <td>2179</td>\n",
       "      <td>896.55</td>\n",
       "      <td>3.91</td>\n",
       "      <td>4.29</td>\n",
       "      <td>0.20</td>\n",
       "      <td>864.71</td>\n",
       "      <td>2019</td>\n",
       "      <td>855.16</td>\n",
       "      <td>4.56</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>864.59</td>\n",
       "      <td>2054</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24000</th>\n",
       "      <td>51</td>\n",
       "      <td>8</td>\n",
       "      <td>2367</td>\n",
       "      <td>904.42</td>\n",
       "      <td>2174</td>\n",
       "      <td>896.18</td>\n",
       "      <td>3.93</td>\n",
       "      <td>4.31</td>\n",
       "      <td>0.20</td>\n",
       "      <td>864.77</td>\n",
       "      <td>2026</td>\n",
       "      <td>855.18</td>\n",
       "      <td>4.54</td>\n",
       "      <td>5.04</td>\n",
       "      <td>0.25</td>\n",
       "      <td>864.59</td>\n",
       "      <td>2054</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>52</td>\n",
       "      <td>9</td>\n",
       "      <td>663</td>\n",
       "      <td>904.27</td>\n",
       "      <td>2170</td>\n",
       "      <td>895.97</td>\n",
       "      <td>3.95</td>\n",
       "      <td>4.34</td>\n",
       "      <td>0.20</td>\n",
       "      <td>866.31</td>\n",
       "      <td>2069</td>\n",
       "      <td>856.70</td>\n",
       "      <td>4.63</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.17</td>\n",
       "      <td>864.59</td>\n",
       "      <td>2054</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26000</th>\n",
       "      <td>54</td>\n",
       "      <td>9</td>\n",
       "      <td>1663</td>\n",
       "      <td>904.05</td>\n",
       "      <td>2166</td>\n",
       "      <td>895.71</td>\n",
       "      <td>3.98</td>\n",
       "      <td>4.36</td>\n",
       "      <td>0.20</td>\n",
       "      <td>863.24</td>\n",
       "      <td>2002</td>\n",
       "      <td>853.61</td>\n",
       "      <td>4.61</td>\n",
       "      <td>5.02</td>\n",
       "      <td>0.18</td>\n",
       "      <td>864.99</td>\n",
       "      <td>2059</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27000</th>\n",
       "      <td>52</td>\n",
       "      <td>9</td>\n",
       "      <td>2663</td>\n",
       "      <td>903.82</td>\n",
       "      <td>2161</td>\n",
       "      <td>895.44</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.20</td>\n",
       "      <td>863.81</td>\n",
       "      <td>2011</td>\n",
       "      <td>854.32</td>\n",
       "      <td>4.51</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.14</td>\n",
       "      <td>864.99</td>\n",
       "      <td>2059</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28000</th>\n",
       "      <td>52</td>\n",
       "      <td>10</td>\n",
       "      <td>959</td>\n",
       "      <td>903.62</td>\n",
       "      <td>2158</td>\n",
       "      <td>895.20</td>\n",
       "      <td>4.02</td>\n",
       "      <td>4.40</td>\n",
       "      <td>0.20</td>\n",
       "      <td>866.12</td>\n",
       "      <td>2061</td>\n",
       "      <td>856.70</td>\n",
       "      <td>4.56</td>\n",
       "      <td>4.87</td>\n",
       "      <td>0.17</td>\n",
       "      <td>864.99</td>\n",
       "      <td>2059</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29000</th>\n",
       "      <td>52</td>\n",
       "      <td>10</td>\n",
       "      <td>1959</td>\n",
       "      <td>903.42</td>\n",
       "      <td>2154</td>\n",
       "      <td>894.96</td>\n",
       "      <td>4.04</td>\n",
       "      <td>4.42</td>\n",
       "      <td>0.20</td>\n",
       "      <td>864.39</td>\n",
       "      <td>2022</td>\n",
       "      <td>854.75</td>\n",
       "      <td>4.51</td>\n",
       "      <td>5.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>864.99</td>\n",
       "      <td>2059</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>52</td>\n",
       "      <td>11</td>\n",
       "      <td>255</td>\n",
       "      <td>903.31</td>\n",
       "      <td>2150</td>\n",
       "      <td>894.81</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.44</td>\n",
       "      <td>0.20</td>\n",
       "      <td>863.26</td>\n",
       "      <td>2004</td>\n",
       "      <td>853.55</td>\n",
       "      <td>4.67</td>\n",
       "      <td>5.04</td>\n",
       "      <td>0.14</td>\n",
       "      <td>864.99</td>\n",
       "      <td>2059</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31000</th>\n",
       "      <td>52</td>\n",
       "      <td>11</td>\n",
       "      <td>1255</td>\n",
       "      <td>903.11</td>\n",
       "      <td>2147</td>\n",
       "      <td>894.58</td>\n",
       "      <td>4.07</td>\n",
       "      <td>4.46</td>\n",
       "      <td>0.20</td>\n",
       "      <td>863.65</td>\n",
       "      <td>2012</td>\n",
       "      <td>854.05</td>\n",
       "      <td>4.55</td>\n",
       "      <td>5.04</td>\n",
       "      <td>0.12</td>\n",
       "      <td>864.99</td>\n",
       "      <td>2059</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32000</th>\n",
       "      <td>54</td>\n",
       "      <td>11</td>\n",
       "      <td>2255</td>\n",
       "      <td>902.90</td>\n",
       "      <td>2144</td>\n",
       "      <td>894.33</td>\n",
       "      <td>4.09</td>\n",
       "      <td>4.48</td>\n",
       "      <td>0.20</td>\n",
       "      <td>863.22</td>\n",
       "      <td>1996</td>\n",
       "      <td>853.46</td>\n",
       "      <td>4.63</td>\n",
       "      <td>5.12</td>\n",
       "      <td>0.15</td>\n",
       "      <td>866.09</td>\n",
       "      <td>2063</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      TRAIN:                                  VALID:        \\\n",
       "      Time  Ep    Ct    LOSS   PPL     NLL    KL GAUSS   REG    LOSS   PPL   \n",
       "1000    61   0   999  929.08  2708  924.71  1.21  3.16  0.25  890.30  2597   \n",
       "2000    50   0  1999  922.36  2556  917.09  1.99  3.28  0.24  880.71  2364   \n",
       "3000    51   1   295  919.52  2477  913.80  2.42  3.30  0.24  878.20  2304   \n",
       "4000    52   1  1295  916.76  2425  910.70  2.69  3.37  0.24  873.94  2218   \n",
       "5000    52   1  2295  914.52  2387  908.26  2.85  3.41  0.23  873.04  2208   \n",
       "6000    53   2   591  913.71  2361  907.30  2.97  3.44  0.23  874.29  2224   \n",
       "7000    51   2  1591  912.61  2340  906.06  3.06  3.48  0.23  870.75  2156   \n",
       "8000    52   2  2591  911.56  2323  904.89  3.15  3.53  0.23  870.12  2151   \n",
       "9000    50   3   887  910.86  2307  904.05  3.23  3.59  0.23  871.73  2175   \n",
       "10000   51   3  1887  910.17  2293  903.20  3.30  3.66  0.23  870.81  2157   \n",
       "11000   52   4   183  909.58  2279  902.48  3.38  3.73  0.22  867.57  2089   \n",
       "12000   50   4  1183  908.96  2267  901.71  3.45  3.81  0.22  868.33  2093   \n",
       "13000   51   4  2183  908.31  2255  900.92  3.51  3.88  0.22  866.51  2059   \n",
       "14000   50   5   479  907.92  2244  900.41  3.57  3.93  0.22  866.87  2069   \n",
       "15000   51   5  1479  907.49  2234  899.87  3.63  3.99  0.22  865.99  2054   \n",
       "16000   52   5  2479  906.93  2225  899.22  3.67  4.04  0.21  865.89  2050   \n",
       "17000   51   6   775  906.56  2217  898.76  3.72  4.08  0.21  866.56  2063   \n",
       "18000   51   6  1775  906.24  2210  898.35  3.76  4.13  0.21  867.98  2088   \n",
       "19000   51   7    71  905.93  2203  897.98  3.79  4.16  0.21  866.93  2067   \n",
       "20000   51   7  1071  905.54  2196  897.52  3.82  4.20  0.21  867.76  2086   \n",
       "21000   51   7  2071  905.20  2190  897.12  3.85  4.23  0.21  866.08  2053   \n",
       "22000   54   8   367  905.00  2184  896.86  3.88  4.26  0.21  864.14  2015   \n",
       "23000   51   8  1367  904.74  2179  896.55  3.91  4.29  0.20  864.71  2019   \n",
       "24000   51   8  2367  904.42  2174  896.18  3.93  4.31  0.20  864.77  2026   \n",
       "25000   52   9   663  904.27  2170  895.97  3.95  4.34  0.20  866.31  2069   \n",
       "26000   54   9  1663  904.05  2166  895.71  3.98  4.36  0.20  863.24  2002   \n",
       "27000   52   9  2663  903.82  2161  895.44  4.00  4.38  0.20  863.81  2011   \n",
       "28000   52  10   959  903.62  2158  895.20  4.02  4.40  0.20  866.12  2061   \n",
       "29000   52  10  1959  903.42  2154  894.96  4.04  4.42  0.20  864.39  2022   \n",
       "30000   52  11   255  903.31  2150  894.81  4.05  4.44  0.20  863.26  2004   \n",
       "31000   52  11  1255  903.11  2147  894.58  4.07  4.46  0.20  863.65  2012   \n",
       "32000   54  11  2255  902.90  2144  894.33  4.09  4.48  0.20  863.22  1996   \n",
       "\n",
       "                                  TEST:       SPEC:             HIER:        \n",
       "          NLL    KL GAUSS   REG    LOSS   PPL     1     2     3 CHILD OTHER  \n",
       "1000   885.69  1.49  3.12  0.22  888.58  2549  0.16  0.16  0.14  0.95  0.92  \n",
       "2000   874.73  2.95  3.03  0.30  879.79  2370  0.29  0.20  0.21  0.93  0.83  \n",
       "3000   871.00  3.48  3.71  0.27  878.46  2321  0.18  0.22  0.25  0.93  0.82  \n",
       "4000   866.65  3.75  3.54  0.28  876.26  2291  0.26  0.24  0.23  0.92  0.80  \n",
       "5000   865.59  3.81  3.64  0.21  874.92  2263  0.20  0.23  0.23  0.92  0.82  \n",
       "6000   866.97  3.64  3.68  0.15  874.92  2263  0.24  0.22  0.25  0.94  0.81  \n",
       "7000   863.24  3.75  3.77  0.27  872.67  2225  0.25  0.22  0.24  0.92  0.78  \n",
       "8000   862.39  3.93  3.80  0.19  871.15  2189  0.21  0.20  0.27  0.90  0.81  \n",
       "9000   863.62  4.04  4.07  0.25  871.15  2189  0.27  0.20  0.26  0.89  0.76  \n",
       "10000  862.39  4.17  4.25  0.17  871.15  2189  0.20  0.24  0.25  0.91  0.75  \n",
       "11000  858.81  4.27  4.49  0.15  870.63  2169  0.19  0.23  0.27  0.89  0.76  \n",
       "12000  859.41  4.29  4.63  0.23  870.63  2169  0.19  0.21  0.28  0.87  0.75  \n",
       "13000  857.43  4.39  4.69  0.19  867.20  2105  0.36  0.23  0.28  0.89  0.71  \n",
       "14000  857.65  4.46  4.77  0.16  867.20  2105  0.24  0.21  0.28  0.85  0.73  \n",
       "15000  856.70  4.48  4.80  0.21  867.81  2111  0.22  0.22  0.26  0.90  0.72  \n",
       "16000  856.61  4.53  4.75  0.16  865.50  2075  0.23  0.23  0.28  0.89  0.77  \n",
       "17000  857.32  4.45  4.78  0.22  865.50  2075  0.20  0.22  0.27  0.89  0.73  \n",
       "18000  858.70  4.40  4.87  0.21  865.50  2075  0.32  0.23  0.30  0.90  0.74  \n",
       "19000  857.49  4.53  4.92  0.13  865.50  2075  0.23  0.23  0.29  0.87  0.75  \n",
       "20000  858.47  4.43  4.86  0.14  865.50  2075  0.23  0.25  0.29  0.85  0.71  \n",
       "21000  856.74  4.50  4.83  0.22  865.50  2075  0.21  0.21  0.31  0.86  0.71  \n",
       "22000  854.73  4.52  4.89  0.15  864.59  2054  0.19  0.23  0.32  0.88  0.72  \n",
       "23000  855.16  4.56  5.00  0.20  864.59  2054  0.24  0.23  0.31  0.87  0.72  \n",
       "24000  855.18  4.54  5.04  0.25  864.59  2054  0.25  0.24  0.28  0.89  0.68  \n",
       "25000  856.70  4.63  4.98  0.17  864.59  2054  0.19  0.21  0.31  0.88  0.73  \n",
       "26000  853.61  4.61  5.02  0.18  864.99  2059  0.17  0.22  0.32  0.85  0.72  \n",
       "27000  854.32  4.51  4.98  0.14  864.99  2059  0.24  0.26  0.30  0.85  0.72  \n",
       "28000  856.70  4.56  4.87  0.17  864.99  2059  0.19  0.24  0.34  0.86  0.73  \n",
       "29000  854.75  4.51  5.13  0.13  864.99  2059  0.25  0.23  0.30  0.84  0.67  \n",
       "30000  853.55  4.67  5.04  0.14  864.99  2059  0.28  0.24  0.31  0.87  0.68  \n",
       "31000  854.05  4.55  5.04  0.12  864.99  2059  0.19  0.23  0.29  0.87  0.69  \n",
       "32000  853.46  4.63  5.12  0.15  866.09  2063  0.25  0.24  0.30  0.86  0.68  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 R: 1.000 P: 0.012 music kids room show hotel cool wings enjoyed games enjoy\n",
      "   1 R: 0.365 P: 0.065 fried soup beef meat flavor portions decent dishes huge fish\n",
      "     11 R: 0.137 P: 0.137 rice thai mexican spicy tacos chinese rolls soup beef noodles\n",
      "     12 R: 0.059 P: 0.059 years found fried decent fantastic beef meat full huge wo\n",
      "     13 R: 0.105 P: 0.105 enjoyed wonderful fantastic steak husband bread large wine meat absolutely\n",
      "   2 R: 0.204 P: 0.033 coffee sandwich ice free cream store stop inside options drive\n",
      "     21 R: 0.022 P: 0.022 coffee sandwich cream ice sandwiches bread options today french drive\n",
      "     22 R: 0.094 P: 0.094 coffee ice cream tea shop store chocolate free flavors line\n",
      "     23 R: 0.055 P: 0.055 sandwich bacon sandwiches eggs brunch burgers bread options french toast\n",
      "   3 R: 0.175 P: 0.026 wings music cool check decent waitress enjoyed patio enjoy game\n",
      "     31 R: 0.022 P: 0.022 wings wine attentive fantastic waitress manager enjoyed check music wonderful\n",
      "     32 R: 0.048 P: 0.048 wings burgers sandwich waitress patio wine enjoyed music large bread\n",
      "     33 R: 0.079 P: 0.079 years attentive asked fantastic decent found check full left wo\n",
      "   4 R: 0.003 P: 0.001 fantastic check sandwich asked huge free wo years full found\n",
      "     41 R: 0.001 P: 0.001 enjoyed large enjoy free huge kids check decent room cool\n",
      "     42 R: 0.000 P: 0.000 car job helpful store wonderful free beautiful care years room\n",
      "     43 R: 0.001 P: 0.001 cool free check stop inside local music lots open huge\n",
      "   5 R: 0.002 P: 0.000 cool sandwich enjoyed free music huge check wings enjoy large\n",
      "     51 R: 0.000 P: 0.000 shop drive found store car cream coffee helpful today free\n",
      "     52 R: 0.000 P: 0.000 free kids hotel show strip lots check room huge parking\n",
      "     53 R: 0.001 P: 0.001 room free check kids lots parking enjoyed decent enjoy close\n",
      "   6 R: 0.239 P: 0.005 room store helpful hotel stay show car free check rooms\n",
      "     61 R: 0.113 P: 0.113 car nails job salon nail massage appointment hair professional pedicure\n",
      "     62 R: 0.049 P: 0.049 store shop helpful room free buy show front line beautiful\n",
      "     63 R: 0.072 P: 0.072 room hotel show kids stay strip parking free check seats\n",
      "[31.594067 30.62826  31.205893 31.88715  31.758856 31.781458 32.\n",
      " 30.253134 30.57917  30.394709 31.174675 30.857908 30.973166 31.78585\n",
      " 31.773867 31.39916  31.746796 31.748009 31.770496 31.769539 31.76641\n",
      " 31.78883  32.       32.       32.      ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-01d99103f841>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mrecur_prob_topic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mparent_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic_prob_topic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchild_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchild_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecur_child_idxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparent_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecur_child_idxs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdescendant_idxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mdepth_specs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_topic_specialization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstances_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mhierarchical_affinities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_hierarchical_affinity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tsntm/evaluation.py\u001b[0m in \u001b[0;36mget_topic_specialization\u001b[0;34m(sess, model, instances, verbose)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_topic_specialization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mtopics_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_bow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mtopics_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_bow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "while epoch < config.n_epochs:\n",
    "    # train\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = model.get_feed_dict(batch)\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch, ppls_batch, global_step_log = \\\n",
    "        sess.run([model.opt, model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_gauss, model.topic_loss_reg, model.topic_ppls, tf.train.get_global_step()], feed_dict = feed_dict)\n",
    "\n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "#         if global_step_log % config.log_period == 0:\n",
    "        if global_step_log % 1000 == 0:\n",
    "            # validate\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_gauss_train, topic_loss_reg_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_gauss_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "\n",
    "            # test\n",
    "            if ppl_dev < ppl_min:\n",
    "                ppl_min = ppl_dev\n",
    "                loss_test, _, _, _, _, ppl_test, _ = validate(sess, test_batches, model)\n",
    "                saver.save(sess, config.path_model, global_step=global_step_log)\n",
    "                cPickle.dump(config, open(config.path_config % global_step_log, 'wb'))\n",
    "                update_checkpoint(config, checkpoint, global_step_log)\n",
    "            \n",
    "            # visualize topic\n",
    "            topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "            topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "            topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "            topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "            descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "            recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "            \n",
    "            depth_specs = get_topic_specialization(sess, model, instances_test)\n",
    "            hierarchical_affinities = get_hierarchical_affinity(sess, model)\n",
    "            \n",
    "            # log\n",
    "            clear_output()\n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_gauss_train, '%.2f'%topic_loss_reg_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_gauss_dev, '%.2f'%topic_loss_reg_dev, \\\n",
    "                    '%.2f'%loss_test, '%.0f'%ppl_test, \\\n",
    "                    '%.2f'%depth_specs[1], '%.2f'%depth_specs[2], '%.2f'%depth_specs[3], \\\n",
    "                    '%.2f'%hierarchical_affinities[0], '%.2f'%hierarchical_affinities[1]],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "            cPickle.dump(log_df, open(os.path.join(config.path_log), 'wb'))\n",
    "            print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)\n",
    "            print(np.sum(debug(model.topic_logvars), 1))\n",
    "            \n",
    "            # update tree\n",
    "            if not config.static:\n",
    "                config.tree_idxs, update_tree_flg = model.update_tree(topic_prob_topic, recur_prob_topic)\n",
    "                if update_tree_flg:\n",
    "                    print(config.tree_idxs)\n",
    "                    name_variables = {tensor.name: variable for tensor, variable in zip(tf.global_variables(), sess.run(tf.global_variables()))} # store paremeters\n",
    "                    if 'sess' in globals(): sess.close()\n",
    "                    model = HierarchicalNeuralTopicModel(config)\n",
    "                    sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "                    name_tensors = {tensor.name: tensor for tensor in tf.global_variables()}\n",
    "                    sess.run([name_tensors[name].assign(variable) for name, variable in name_variables.items()]) # restore parameters\n",
    "                    saver = tf.train.Saver(max_to_keep=1)\n",
    "                \n",
    "            time_start = time.time()\n",
    "\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "    epoch += 1\n",
    "\n",
    "loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "display(log_df)\n",
    "print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.8216894 , -3.0563464 ,  1.6908246 , -1.1562389 , -2.7861776 ,\n",
       "       -0.16849104, -2.979567  ,  2.3647585 , -0.24711254,  2.3434775 ,\n",
       "        1.2681077 , -2.037859  , -0.5537858 ,  3.1777048 , -2.3174355 ,\n",
       "        0.81516606,  1.3252438 ,  0.9859205 , -3.2249868 ,  0.00921591,\n",
       "       -1.2574761 ,  2.7307491 ,  2.9201877 , -2.4252245 ,  0.7486757 ,\n",
       "        2.6120846 , -0.5086255 , -2.6265562 , -1.9473915 , -2.147787  ,\n",
       "       -0.47880453, -2.537198  ], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug(model.topic_means)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.4633958e+00, -3.3375292e+00,  2.9749236e+00, -2.5223880e+00,\n",
       "       -3.1319203e+00,  4.4205226e-04, -3.4287007e+00,  2.8137615e+00,\n",
       "       -2.7193124e+00,  3.1666677e+00,  3.2885613e+00, -2.4302793e+00,\n",
       "       -1.6794885e+00,  3.0809908e+00, -3.2836900e+00,  2.2672522e+00,\n",
       "        1.5040021e+00,  1.7132781e+00, -3.0915594e+00, -2.8817942e+00,\n",
       "       -3.3942351e+00,  3.0325408e+00,  3.0610740e+00, -3.4285488e+00,\n",
       "       -2.4458025e+00,  2.6225791e+00, -2.7167537e+00, -2.6694596e+00,\n",
       "       -2.8730149e+00, -3.1766460e+00, -2.1321492e+00, -3.5230451e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug(model.topic_means)[model.topic_idxs.index(model.config.tree_idxs[1][0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_year(sample_batches):\n",
    "    probs_topics = []\n",
    "    years = []\n",
    "    for i, sample_batch in sample_batches:\n",
    "        probs_topics_batch = sess.run(model.prob_topic, feed_dict=model.get_feed_dict(sample_batch, mode='test'))\n",
    "        years_batch = [instance.year for instance in sample_batch]\n",
    "        probs_topics += [probs_topics_batch]\n",
    "        years += years_batch\n",
    "    probs_topics = np.concatenate(probs_topics)\n",
    "    years = np.array(years)\n",
    "\n",
    "    topic_years = years.dot(probs_topics) / np.sum(probs_topics, 0)\n",
    "    topic_year = {model.topic_idxs[i]: year for i, year in enumerate(topic_years)}\n",
    "    return topic_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Avg Year: 2005 level analysis english systems domain lexical linguistic test process lexicon\n",
      "   1 Avg Year: 2000 grammar structure rules semantic form representation rule lexical type structures\n",
      "     11 Avg Year: 2006 verb semantic syntactic verbs event relations discourse argument relation arguments\n",
      "     12 Avg Year: 2007 tree parsing dependency parser grammar trees node parse rules syntactic\n",
      "   2 Avg Year: 2008 english languages dictionary errors morphological chinese lexical rules corpora pos\n",
      "     21 Avg Year: 2009 entity sense relations relation semantic entities wordnet senses lexical patterns\n",
      "     22 Avg Year: 2010 translation english source alignment phrase target languages sentences systems parallel\n",
      "   3 Avg Year: 2008 models probability training algorithm probabilities search parameters sequence segmentation gram\n",
      "     31 Avg Year: 2011 features feature training performance learning classifier classification accuracy test class\n",
      "     32 Avg Year: 2015 models network vector embeddings neural training vectors representations embedding input\n",
      "   4 Avg Year: 2007 knowledge semantic terms term natural values context type representation concept\n",
      "     41 Avg Year: 2009 question query terms documents answer questions document domain term web\n",
      "     42 Avg Year: 2010 similarity topic document sentences method score scores measure clustering pairs\n",
      "   5 Avg Year: 2007 user annotation resources project tools tool database research interface linguistic\n",
      "     51 Avg Year: 2007 speech dialogue user speaker utterance utterances recognition spoken human speakers\n",
      "     52 Avg Year: 2013 sentiment tweets polarity twitter negative opinion positive social emotion annotators\n"
     ]
    }
   ],
   "source": [
    "sample_batches = get_batches(instances_train, config.batch_size)\n",
    "topic_year = get_topic_year(sample_batches)\n",
    "print_topic_year(sess, model, topic_freq_tokens=topic_freq_tokens, topic_year=topic_year)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
