{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from data_structure import get_batches, get_test_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "from topic_model import TopicModel\n",
    "\n",
    "from topic_beam_search_decoder import BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '1', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/bags/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/tglm_vae_tmp1', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'bags', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 50, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 500, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.05, 'lr')\n",
    "flags.DEFINE_float('reg', 1., 'regularization term')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_bool('warmup', True, 'flg of warming up')\n",
    "flags.DEFINE_integer('epochs_cycle', 5, 'number of epochs within a cycle')\n",
    "flags.DEFINE_float('r_cycle', 0.5, 'proportion used to increase beta within a cycle')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 10, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_bow', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_hidden_topic', 512, 'dim_hidden_topic')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "flags.DEFINE_bool('bidirectional', True, 'flg of bidirectional encoding')\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_test_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('PAD_IDX', word_to_idx[PAD], 'PAD_IDX')\n",
    "flags.DEFINE_integer('UNK_IDX', word_to_idx[UNK], 'UNK_IDX')\n",
    "flags.DEFINE_integer('BOS_IDX', word_to_idx[BOS], 'BOS_IDX')\n",
    "flags.DEFINE_integer('EOS_IDX', word_to_idx[EOS], 'EOS_IDX')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "maximum_iterations = max([max([instance.max_sent_l for instance in batch]) for ct, batch in dev_batches])\n",
    "flags.DEFINE_integer('maximum_iterations', maximum_iterations, 'maximum_iterations')\n",
    "\n",
    "flags.DEFINE_integer('cycle_steps', len(train_batches)*config.epochs_cycle, 'number of steps for each cycle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build language model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['bow'] = tf.placeholder(tf.float32, [None, config.dim_bow], name='bow')\n",
    "t_variables['input_token_idxs'] = tf.placeholder(tf.int32, [None, None], name='input_token_idxs')\n",
    "t_variables['dec_input_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_input_idxs')\n",
    "t_variables['dec_target_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_target_idxs')\n",
    "t_variables['batch_l'] = tf.placeholder(tf.int32, name='batch_l')\n",
    "t_variables['doc_l'] = tf.placeholder(tf.int32, [None], name='doc_l')\n",
    "t_variables['sent_l'] = tf.placeholder(tf.int32, [None], name='sent_l')\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train', assertion=False):\n",
    "    def token_dropout(sent_idxs):\n",
    "        sent_idxs_dropout = np.asarray(sent_idxs)\n",
    "        sent_idxs_dropout[np.random.rand(len(sent_idxs)) > config.word_keep_prob] = config.UNK_IDX\n",
    "        return list(sent_idxs_dropout)\n",
    "\n",
    "    bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "    \n",
    "    doc_l = np.array([len(instance.token_idxs) for instance in batch])\n",
    "    \n",
    "    feed_input_token_idxs_list = [sent_idxs for instance in batch for sent_idxs in instance.token_idxs]\n",
    "    feed_dec_input_idxs_list = [[config.BOS_IDX] + token_dropout(sent_idxs) for sent_idxs in feed_input_token_idxs_list]\n",
    "    feed_dec_target_idxs_list = [sent_idxs + [config.EOS_IDX]  for sent_idxs in feed_input_token_idxs_list]\n",
    "        \n",
    "    sent_l = np.array([len(sent_idxs) for sent_idxs in feed_input_token_idxs_list], np.int32)\n",
    "    batch_l = len(sent_l)\n",
    "    \n",
    "    feed_input_token_idxs = pad_sequences(feed_input_token_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_input_idxs = pad_sequences(feed_dec_input_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_target_idxs = pad_sequences(feed_dec_target_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    \n",
    "    if assertion:\n",
    "        index = 0\n",
    "        for instance in batch:\n",
    "            for line_idxs in instance.token_idxs:\n",
    "                assert feed_input_token_idxs_list[index] == line_idxs\n",
    "                index += 1\n",
    "        assert feed_input_token_idxs.shape[1] == np.max(sent_l)\n",
    "        assert feed_dec_input_idxs.shape[1] == np.max(sent_l) + 1\n",
    "        assert feed_dec_target_idxs.shape[1] == np.max(sent_l) + 1\n",
    "    \n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "\n",
    "    feed_dict = {\n",
    "                t_variables['bow']: bow, \n",
    "                t_variables['batch_l']: batch_l, t_variables['doc_l']: doc_l, t_variables['sent_l']: sent_l, \n",
    "                t_variables['input_token_idxs']: feed_input_token_idxs, t_variables['dec_input_idxs']: feed_dec_input_idxs, t_variables['dec_target_idxs']: feed_dec_target_idxs, \n",
    "                t_variables['keep_prob']: keep_prob\n",
    "    }\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     10,
     24,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables):\n",
    "    sample_batch = dev_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "def check_shape(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()\n",
    "    \n",
    "def check_value(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()    \n",
    "    \n",
    "# sent_loss_kl_categ_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, tf_log(prob_topic_infer/prob_topic_sents)), 1))\n",
    "# debug_value([sent_loss_kl_categ, sent_loss_kl_categ_tmp])\n",
    "# sent_loss_kl_gauss_tmp = 0.5 * tf.reduce_sum(tf.exp(logvars_topic_infer-logvars_topic) + tf.square(means_topic - means_topic_infer) / tf.exp(logvars_topic) - 1 + (logvars_topic - logvars_topic_infer), -1)\n",
    "# sent_loss_kl_gmm_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss_tmp), -1))\n",
    "# debug_value([sent_loss_kl_gmm_tmp, sent_loss_kl_gmm])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encode bow\n",
    "with tf.variable_scope('topic/enc', reuse=False):\n",
    "    hidden_bow_ = tf.layers.Dense(units=config.dim_hidden_bow, activation=tf.nn.relu, name='hidden_bow')(t_variables['bow'])\n",
    "    hidden_bow = tf.layers.Dropout(t_variables['keep_prob'])(hidden_bow_)\n",
    "    means_bow = tf.layers.Dense(units=config.dim_latent_bow, name='mean_bow')(hidden_bow)\n",
    "    logvars_bow = tf.layers.Dense(units=config.dim_latent_bow, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_bow)\n",
    "    latents_bow = sample_latents(means_bow, logvars_bow) # sample latent vectors\n",
    "\n",
    "    prob_topic = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic')(latents_bow) # inference of topic probabilities\n",
    "\n",
    "# decode bow\n",
    "with tf.variable_scope('shared', reuse=False):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "bow_embeddings = tf.nn.embedding_lookup(embeddings, bow_idxs) # embeddings of each bow features\n",
    "\n",
    "with tf.variable_scope('topic/dec', reuse=False):\n",
    "    topic_embeddings = tf.get_variable('topic_emb', [config.n_topic, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of topics\n",
    "\n",
    "    topic_bow = tf.nn.softmax(tf.matmul(topic_embeddings, bow_embeddings, transpose_b=True), 1) # bow vectors for each topic\n",
    "    logits_bow = tf_log(tf.matmul(prob_topic, topic_bow)) # predicted bow distribution\n",
    "\n",
    "    # prior of each gaussian distribution (computed for each topic)\n",
    "    hidden_topic = tf.layers.Dense(units=config.dim_hidden_topic, activation=tf.nn.relu, name='hidden_topic')(topic_bow)\n",
    "    means_topic = tf.layers.Dense(units=config.dim_latent, name='mean_topic')(hidden_topic)\n",
    "    logvars_topic = tf.layers.Dense(units=config.dim_latent, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_topic)\n",
    "    sigma_topic = tf.exp(0.5 * logvars_topic)\n",
    "    gauss_topic = tfd.Normal(loc=means_topic, scale=sigma_topic)    \n",
    "    \n",
    "# define losses\n",
    "topic_losses_recon = -tf.reduce_sum(tf.multiply(t_variables['bow'], logits_bow), 1)\n",
    "topic_loss_recon = tf.reduce_mean(topic_losses_recon) # negative log likelihood of each words\n",
    "\n",
    "topic_loss_kl = compute_kl_loss(means_bow, logvars_bow) # KL divergence b/w latent dist & gaussian std\n",
    "\n",
    "topic_bow_norm = topic_bow / tf.norm(topic_bow, axis=1, keepdims=True)\n",
    "topic_dots = tf.clip_by_value(tf.matmul(topic_bow_norm, tf.transpose(topic_bow_norm)), -1., 1.)\n",
    "topic_loss_reg = tf.reduce_mean(tf.square(topic_dots - tf.eye(config.n_topic)))\n",
    "# topic_angles = tf.acos(topic_dots)\n",
    "# topic_angles_mean = tf.reduce_mean(topic_angles)\n",
    "# topic_angles_vars = tf.reduce_mean(tf.square(topic_angles - topic_angles_mean))\n",
    "# topic_loss_reg = tf.exp(topic_angles_vars - topic_angles_mean)\n",
    "\n",
    "# monitor\n",
    "n_bow = tf.reduce_sum(t_variables['bow'], 1)\n",
    "topic_ppls = tf.divide(topic_losses_recon, tf.maximum(1e-5, n_bow))\n",
    "topics_freq_bow_indices = tf.nn.top_k(topic_bow, 10, name='topic_freq_bow').indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "input_token_idxs = t_variables['input_token_idxs']\n",
    "batch_l = t_variables['batch_l']\n",
    "sent_l = t_variables['sent_l']\n",
    "max_sent_l = tf.reduce_max(sent_l)\n",
    "\n",
    "with tf.variable_scope('sent/enc', reuse=False):\n",
    "    # get word embedding\n",
    "    enc_input = tf.nn.embedding_lookup(embeddings, input_token_idxs)\n",
    "\n",
    "    # get sentence embedding\n",
    "    _, enc_state = dynamic_bi_rnn(enc_input, sent_l, config.dim_hidden, t_variables['keep_prob'])\n",
    "\n",
    "    # TODO House Holder flow\n",
    "    hidden_topic_infer =  tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='hidden_topic_infer')(enc_state)\n",
    "    prob_topic_infer = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic_infer')(hidden_topic_infer)\n",
    "\n",
    "    w_mean_topic_infer = tf.get_variable('mean_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32)\n",
    "    b_mean_topic_infer = tf.get_variable('mean_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32)\n",
    "    means_topic_infer = tf.tensordot(enc_state, w_mean_topic_infer, axes=[[1], [1]]) + b_mean_topic_infer\n",
    "    \n",
    "    w_logvar_topic_infer = tf.get_variable('logvar_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    b_logvar_topic_infer = tf.get_variable('logvar_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    logvars_topic_infer = tf.tensordot(enc_state, w_logvar_topic_infer, axes=[[1], [1]]) + b_logvar_topic_infer\n",
    "    sigma_topic_infer = tf.exp(0.5 * logvars_topic_infer)\n",
    "    gauss_topic_infer = tfd.Normal(loc=means_topic_infer, scale=sigma_topic_infer)\n",
    "    \n",
    "    # latent vectors from each gaussian dist.\n",
    "    latents_topic_infer = sample_latents(means_topic_infer, logvars_topic_infer) \n",
    "    # latent vector from gaussian mixture\n",
    "    latents_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), latents_topic_infer, transpose_a=True)\n",
    "    \n",
    "    # for beam search\n",
    "    means_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), means_topic_infer, transpose_a=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for decoding\n",
    "dec_sent_l = tf.add(sent_l, 1)\n",
    "dec_input_idxs = t_variables['dec_input_idxs']\n",
    "dec_input = tf.nn.embedding_lookup(embeddings, dec_input_idxs)\n",
    "\n",
    "dec_latents_input = tf.tile(latents_input, [1, tf.shape(dec_input)[1], 1])\n",
    "dec_concat_input = tf.concat([dec_input, dec_latents_input], -1)\n",
    "\n",
    "# decode for training\n",
    "with tf.variable_scope('sent/dec/rnn', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32, reuse=False):\n",
    "    dec_cell = tf.contrib.rnn.GRUCell(config.dim_hidden)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob = t_variables['keep_prob'])\n",
    "\n",
    "    dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(latents_input, 1))\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_concat_input, sequence_length=dec_sent_l)\n",
    "\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=dec_cell,\n",
    "        helper=helper,\n",
    "        initial_state=dec_initial_state)\n",
    "\n",
    "    dec_outputs, _, output_sent_l = tf.contrib.seq2seq.dynamic_decode(train_decoder)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(config.n_vocab, use_bias=False, name='out')\n",
    "    output_logits = output_layer(dec_outputs.rnn_output)\n",
    "    \n",
    "    output_token_idxs = tf.argmax(output_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tokens = tf.fill([batch_l], config.BOS_IDX)\n",
    "end_token = config.EOS_IDX\n",
    "\n",
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    infer_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(means_input, 1))\n",
    "    beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(infer_dec_initial_state, multiplier=config.beam_width)\n",
    "    beam_latents_input = tf.contrib.seq2seq.tile_batch(tf.squeeze(means_input, 1), multiplier=config.beam_width) # added\n",
    "    \n",
    "    beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=beam_latents_input)\n",
    "\n",
    "    beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    beam_output_token_idxs = beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    inter_means_input = tf.placeholder(tf.float32, [None, config.dim_latent])\n",
    "    \n",
    "    inter_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(inter_means_input)\n",
    "    inter_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(inter_dec_initial_state, multiplier=config.beam_width)\n",
    "    inter_beam_latents_input = tf.contrib.seq2seq.tile_batch(inter_means_input, multiplier=config.beam_width) # added\n",
    "    \n",
    "    inter_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=inter_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=inter_beam_latents_input)\n",
    "\n",
    "    inter_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inter_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    inter_beam_output_token_idxs = inter_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    topic_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic)\n",
    "    topic_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(topic_dec_initial_state, multiplier=config.beam_width)\n",
    "    topic_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic, multiplier=config.beam_width) # added\n",
    "    \n",
    "    topic_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=topic_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=topic_beam_latents_input)\n",
    "\n",
    "    topic_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        topic_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    topic_beam_output_token_idxs = topic_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    means_topic_summary = tf.reduce_mean(means_topic_infer, 0)\n",
    "    \n",
    "    summary_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic_summary)\n",
    "    summary_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(summary_dec_initial_state, multiplier=config.beam_width)\n",
    "    summary_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic_summary, multiplier=config.beam_width) # added\n",
    "    \n",
    "    summary_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=summary_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width,\n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=summary_beam_latents_input)\n",
    "\n",
    "    summary_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        summary_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    summary_beam_output_token_idxs = summary_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language modeling cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target and mask\n",
    "dec_target_idxs = t_variables['dec_target_idxs']\n",
    "dec_mask_tokens = tf.sequence_mask(dec_sent_l, maxlen=max_sent_l+1, dtype=tf.float32)\n",
    "\n",
    "# nll for each token (averaged over batch & sentence)\n",
    "sent_loss_recon = tf.contrib.seq2seq.sequence_loss(output_logits, dec_target_idxs, dec_mask_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_l = t_variables['doc_l']\n",
    "mask_sents = tf.sequence_mask(doc_l)\n",
    "mask_sents_flatten = tf.reshape(mask_sents, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1]])\n",
    "\n",
    "prob_topic_tiled = tf.tile(tf.expand_dims(prob_topic, 1), [1, tf.shape(mask_sents)[1], 1])\n",
    "prob_topic_flatten = tf.reshape(prob_topic_tiled, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1], config.n_topic])\n",
    "prob_topic_sents = tf.boolean_mask(prob_topic_flatten, mask_sents_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferred mixture probabilities (computed for each sentence)\n",
    "categ_topic_infer = tfd.Categorical(probs=prob_topic_infer)\n",
    "\n",
    "# prior of mixture probabilities (computed for each document, tiled for each sentence)\n",
    "categ_topic = tfd.Categorical(probs=prob_topic_sents)\n",
    "\n",
    "sent_loss_kl_categ = tf.reduce_mean(tfd.kl_divergence(categ_topic_infer, categ_topic))\n",
    "\n",
    "# inference of each gaussian gaussribution (computed for each sentence)\n",
    "\n",
    "sent_loss_kl_gauss = tf.reduce_sum(tfd.kl_divergence(gauss_topic_infer, gauss_topic), -1)\n",
    "sent_loss_kl_gmm = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss), -1))\n",
    "\n",
    "sent_loss_kl = sent_loss_kl_categ + sent_loss_kl_gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "tau = tf.cast(tf.divide(tf.mod(global_step, tf.constant(config.cycle_steps)), tf.constant(config.cycle_steps)), dtype=tf.float32)\n",
    "beta = tf.minimum(1., tau/config.r_cycle)\n",
    "\n",
    "sent_loss = sent_loss_recon + beta * sent_loss_kl\n",
    "\n",
    "topic_loss = topic_loss_recon + topic_loss_kl + config.reg * topic_loss_reg\n",
    "loss = topic_loss + sent_loss\n",
    "\n",
    "# define optimizer\n",
    "if config.opt == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif config.opt == 'Adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "    \n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_sents(token_idxs, config, idx_to_word):\n",
    "    sents = []\n",
    "    for sent_idxs in token_idxs:\n",
    "        tokens = []\n",
    "        for idx in sent_idxs:\n",
    "            if idx == config.EOS_IDX: break\n",
    "            tokens.append(idx_to_word[idx])\n",
    "        sent = ' '.join(tokens)\n",
    "        sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, ppls_batch \\\n",
    "            = sess.run([loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, topic_ppls], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "    loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, sent_loss_recon_mean, sent_loss_kl_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    return loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, sent_loss_recon_mean, sent_loss_kl_mean, ppl_mean\n",
    "\n",
    "def get_all_losses(sess, batches):\n",
    "    losses = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch = \\\n",
    "        sess.run([loss, topic_loss_recon, topic_loss_kl, sent_loss_recon, sent_loss_kl], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "    print('LOSS %.2f | TM NLL: %.2f, KL: %.4f | LM NLL: %.2f, KL: %.4f' %  np.mean(losses, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_batch):\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    pred_token_idxs = sess.run(output_token_idxs, feed_dict = feed_dict)\n",
    "    true_token_idxs = [sent_idxs for instance in sample_batch for sent_idxs in instance.token_idxs]\n",
    "    \n",
    "    assert len(pred_token_idxs) == len(true_token_idxs)\n",
    "    \n",
    "    pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "    true_sents = idxs_to_sents(true_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    for i, (true_sent, pred_sent) in enumerate(zip(true_sents, pred_sents)):        \n",
    "        print(i, 'TRUE: %s' % true_sent)\n",
    "        print(i, 'PRED: %s' % pred_sent)\n",
    "\n",
    "def print_topic_sample():\n",
    "    pred_topics_freq_bow_indices, pred_topic_token_idxs = sess.run([topics_freq_bow_indices, topic_beam_output_token_idxs], \n",
    "                                                                                                           feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "    pred_topic_sents = idxs_to_sents(pred_topic_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Topic Samples-----------')\n",
    "    for i, (topic_freq_bow_idxs, pred_topic_sent) in enumerate(zip(topics_freq_bow_idxs, pred_topic_sents)):\n",
    "        print(i, ' BOW:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' SENTENCE:', pred_topic_sent)\n",
    "        \n",
    "def print_summary(test_batch):\n",
    "    feed_dict = get_feed_dict(test_batch)\n",
    "    feed_dict[t_variables['batch_l']] = config.n_topic\n",
    "    feed_dict[t_variables['keep_prob']] = 1.\n",
    "    pred_topics_freq_bow_indices, pred_summary_token_idxs = sess.run([topics_freq_bow_indices, summary_beam_output_token_idxs], feed_dict=feed_dict)\n",
    "    pred_summary_sents = idxs_to_sents(pred_summary_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Output sentences for each topic-----------')\n",
    "    print('Item idx:', test_batch[0].item_idx)\n",
    "    for i, (topic_freq_bow_idxs, pred_summary_sent) in enumerate(zip(topics_freq_bow_idxs, pred_summary_sents)):\n",
    "        print(i, ' BOW:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' SENTENCE:', pred_summary_sent)\n",
    "        \n",
    "    print('-----------Summaries-----------')\n",
    "    for i, summary in enumerate(test_batch[0].summaries):\n",
    "        print('SUMMARY %i :'%i, '\\n', summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "saver = tf.train.Saver(max_to_keep=10)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','TM','','','','LM','','VALID:','TM','','','','LM','', ''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL','REG','NLL','KL','LOSS','PPL','NLL','KL','REG','NLL','KL', 'Beta']]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>LM</th>\n",
       "      <th></th>\n",
       "      <th>VALID:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>LM</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>123.45</td>\n",
       "      <td>1035</td>\n",
       "      <td>113.05</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.90</td>\n",
       "      <td>9.13</td>\n",
       "      <td>1.46</td>\n",
       "      <td>126.48</td>\n",
       "      <td>1034</td>\n",
       "      <td>116.13</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.90</td>\n",
       "      <td>9.13</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>121.19</td>\n",
       "      <td>598</td>\n",
       "      <td>113.84</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.63</td>\n",
       "      <td>6.50</td>\n",
       "      <td>1.38</td>\n",
       "      <td>111.52</td>\n",
       "      <td>527</td>\n",
       "      <td>105.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.47</td>\n",
       "      <td>5.77</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>120.10</td>\n",
       "      <td>568</td>\n",
       "      <td>113.13</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.49</td>\n",
       "      <td>6.15</td>\n",
       "      <td>1.07</td>\n",
       "      <td>111.01</td>\n",
       "      <td>510</td>\n",
       "      <td>104.48</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.27</td>\n",
       "      <td>5.71</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>1500</td>\n",
       "      <td>119.68</td>\n",
       "      <td>553</td>\n",
       "      <td>112.78</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.03</td>\n",
       "      <td>0.91</td>\n",
       "      <td>110.78</td>\n",
       "      <td>507</td>\n",
       "      <td>104.24</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.69</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>119.22</td>\n",
       "      <td>542</td>\n",
       "      <td>112.33</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.33</td>\n",
       "      <td>5.96</td>\n",
       "      <td>0.80</td>\n",
       "      <td>110.57</td>\n",
       "      <td>497</td>\n",
       "      <td>103.96</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.13</td>\n",
       "      <td>5.67</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>118.98</td>\n",
       "      <td>536</td>\n",
       "      <td>112.10</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.31</td>\n",
       "      <td>5.93</td>\n",
       "      <td>0.75</td>\n",
       "      <td>110.24</td>\n",
       "      <td>488</td>\n",
       "      <td>103.61</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.11</td>\n",
       "      <td>5.67</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2776</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>118.70</td>\n",
       "      <td>529</td>\n",
       "      <td>111.80</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.27</td>\n",
       "      <td>5.90</td>\n",
       "      <td>0.68</td>\n",
       "      <td>110.28</td>\n",
       "      <td>483</td>\n",
       "      <td>103.52</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.66</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3276</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>118.41</td>\n",
       "      <td>523</td>\n",
       "      <td>111.50</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.24</td>\n",
       "      <td>5.87</td>\n",
       "      <td>0.63</td>\n",
       "      <td>110.10</td>\n",
       "      <td>478</td>\n",
       "      <td>103.30</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.09</td>\n",
       "      <td>5.64</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3776</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>118.35</td>\n",
       "      <td>517</td>\n",
       "      <td>111.42</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.22</td>\n",
       "      <td>5.85</td>\n",
       "      <td>0.58</td>\n",
       "      <td>109.95</td>\n",
       "      <td>474</td>\n",
       "      <td>103.13</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.07</td>\n",
       "      <td>5.63</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4276</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>118.21</td>\n",
       "      <td>512</td>\n",
       "      <td>111.27</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.20</td>\n",
       "      <td>5.83</td>\n",
       "      <td>0.55</td>\n",
       "      <td>109.83</td>\n",
       "      <td>469</td>\n",
       "      <td>103.04</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.07</td>\n",
       "      <td>5.56</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4551</th>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>118.16</td>\n",
       "      <td>510</td>\n",
       "      <td>111.21</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.20</td>\n",
       "      <td>5.81</td>\n",
       "      <td>0.53</td>\n",
       "      <td>109.75</td>\n",
       "      <td>467</td>\n",
       "      <td>102.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.06</td>\n",
       "      <td>5.52</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5051</th>\n",
       "      <td>62</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>118.02</td>\n",
       "      <td>506</td>\n",
       "      <td>111.07</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.18</td>\n",
       "      <td>5.79</td>\n",
       "      <td>0.50</td>\n",
       "      <td>109.62</td>\n",
       "      <td>462</td>\n",
       "      <td>102.83</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.45</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5551</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>117.88</td>\n",
       "      <td>503</td>\n",
       "      <td>110.93</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.76</td>\n",
       "      <td>0.48</td>\n",
       "      <td>109.61</td>\n",
       "      <td>465</td>\n",
       "      <td>102.89</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.04</td>\n",
       "      <td>5.38</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6051</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>1500</td>\n",
       "      <td>117.80</td>\n",
       "      <td>500</td>\n",
       "      <td>110.85</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.16</td>\n",
       "      <td>5.74</td>\n",
       "      <td>0.46</td>\n",
       "      <td>109.52</td>\n",
       "      <td>464</td>\n",
       "      <td>102.88</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>5.30</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6551</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>117.68</td>\n",
       "      <td>497</td>\n",
       "      <td>110.74</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.15</td>\n",
       "      <td>5.71</td>\n",
       "      <td>0.45</td>\n",
       "      <td>109.37</td>\n",
       "      <td>463</td>\n",
       "      <td>102.79</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>5.24</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6826</th>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>117.66</td>\n",
       "      <td>496</td>\n",
       "      <td>110.72</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.14</td>\n",
       "      <td>5.69</td>\n",
       "      <td>0.44</td>\n",
       "      <td>109.18</td>\n",
       "      <td>456</td>\n",
       "      <td>102.57</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.03</td>\n",
       "      <td>5.21</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7326</th>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>117.57</td>\n",
       "      <td>493</td>\n",
       "      <td>110.63</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.14</td>\n",
       "      <td>5.66</td>\n",
       "      <td>0.43</td>\n",
       "      <td>109.07</td>\n",
       "      <td>451</td>\n",
       "      <td>102.47</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.03</td>\n",
       "      <td>5.16</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7826</th>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>117.49</td>\n",
       "      <td>491</td>\n",
       "      <td>110.56</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.13</td>\n",
       "      <td>5.64</td>\n",
       "      <td>0.42</td>\n",
       "      <td>109.12</td>\n",
       "      <td>453</td>\n",
       "      <td>102.52</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.11</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8326</th>\n",
       "      <td>53</td>\n",
       "      <td>3</td>\n",
       "      <td>1500</td>\n",
       "      <td>117.43</td>\n",
       "      <td>489</td>\n",
       "      <td>110.51</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.12</td>\n",
       "      <td>5.61</td>\n",
       "      <td>0.41</td>\n",
       "      <td>109.18</td>\n",
       "      <td>458</td>\n",
       "      <td>102.65</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.07</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8826</th>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>117.37</td>\n",
       "      <td>487</td>\n",
       "      <td>110.45</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.12</td>\n",
       "      <td>5.58</td>\n",
       "      <td>0.40</td>\n",
       "      <td>109.08</td>\n",
       "      <td>457</td>\n",
       "      <td>102.57</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.05</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9101</th>\n",
       "      <td>39</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>117.31</td>\n",
       "      <td>487</td>\n",
       "      <td>110.40</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.11</td>\n",
       "      <td>5.57</td>\n",
       "      <td>0.40</td>\n",
       "      <td>108.90</td>\n",
       "      <td>450</td>\n",
       "      <td>102.37</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.03</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9601</th>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "      <td>117.23</td>\n",
       "      <td>485</td>\n",
       "      <td>110.33</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.11</td>\n",
       "      <td>5.55</td>\n",
       "      <td>0.39</td>\n",
       "      <td>109.11</td>\n",
       "      <td>455</td>\n",
       "      <td>102.52</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10101</th>\n",
       "      <td>52</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>117.13</td>\n",
       "      <td>483</td>\n",
       "      <td>110.24</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.52</td>\n",
       "      <td>0.38</td>\n",
       "      <td>109.01</td>\n",
       "      <td>455</td>\n",
       "      <td>102.55</td>\n",
       "      <td>1.21</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.97</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10601</th>\n",
       "      <td>71</td>\n",
       "      <td>4</td>\n",
       "      <td>1500</td>\n",
       "      <td>117.12</td>\n",
       "      <td>482</td>\n",
       "      <td>110.23</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.50</td>\n",
       "      <td>0.38</td>\n",
       "      <td>108.85</td>\n",
       "      <td>451</td>\n",
       "      <td>102.37</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.95</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11101</th>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>2000</td>\n",
       "      <td>117.05</td>\n",
       "      <td>480</td>\n",
       "      <td>110.16</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.48</td>\n",
       "      <td>0.37</td>\n",
       "      <td>108.91</td>\n",
       "      <td>453</td>\n",
       "      <td>102.43</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.93</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11376</th>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>117.04</td>\n",
       "      <td>480</td>\n",
       "      <td>110.16</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>5.47</td>\n",
       "      <td>0.37</td>\n",
       "      <td>108.64</td>\n",
       "      <td>455</td>\n",
       "      <td>102.48</td>\n",
       "      <td>1.24</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.90</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11876</th>\n",
       "      <td>62</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>116.97</td>\n",
       "      <td>478</td>\n",
       "      <td>110.11</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.09</td>\n",
       "      <td>5.45</td>\n",
       "      <td>0.37</td>\n",
       "      <td>108.46</td>\n",
       "      <td>445</td>\n",
       "      <td>102.19</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.88</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12376</th>\n",
       "      <td>65</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>116.91</td>\n",
       "      <td>477</td>\n",
       "      <td>110.06</td>\n",
       "      <td>1.13</td>\n",
       "      <td>0.09</td>\n",
       "      <td>5.43</td>\n",
       "      <td>0.37</td>\n",
       "      <td>108.39</td>\n",
       "      <td>443</td>\n",
       "      <td>102.12</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.86</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12876</th>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>1500</td>\n",
       "      <td>116.84</td>\n",
       "      <td>476</td>\n",
       "      <td>110.00</td>\n",
       "      <td>1.15</td>\n",
       "      <td>0.09</td>\n",
       "      <td>5.41</td>\n",
       "      <td>0.37</td>\n",
       "      <td>108.52</td>\n",
       "      <td>444</td>\n",
       "      <td>102.13</td>\n",
       "      <td>1.42</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.85</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13376</th>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>116.81</td>\n",
       "      <td>475</td>\n",
       "      <td>109.98</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.40</td>\n",
       "      <td>0.37</td>\n",
       "      <td>108.51</td>\n",
       "      <td>443</td>\n",
       "      <td>102.10</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.82</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100101</th>\n",
       "      <td>30</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>114.67</td>\n",
       "      <td>421</td>\n",
       "      <td>107.96</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.49</td>\n",
       "      <td>0.40</td>\n",
       "      <td>107.05</td>\n",
       "      <td>403</td>\n",
       "      <td>100.63</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100601</th>\n",
       "      <td>54</td>\n",
       "      <td>44</td>\n",
       "      <td>500</td>\n",
       "      <td>114.66</td>\n",
       "      <td>421</td>\n",
       "      <td>107.95</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.48</td>\n",
       "      <td>0.40</td>\n",
       "      <td>106.98</td>\n",
       "      <td>401</td>\n",
       "      <td>100.59</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101101</th>\n",
       "      <td>54</td>\n",
       "      <td>44</td>\n",
       "      <td>1000</td>\n",
       "      <td>114.66</td>\n",
       "      <td>421</td>\n",
       "      <td>107.95</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.48</td>\n",
       "      <td>0.40</td>\n",
       "      <td>107.11</td>\n",
       "      <td>405</td>\n",
       "      <td>100.70</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101601</th>\n",
       "      <td>59</td>\n",
       "      <td>44</td>\n",
       "      <td>1500</td>\n",
       "      <td>114.66</td>\n",
       "      <td>421</td>\n",
       "      <td>107.95</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.48</td>\n",
       "      <td>0.40</td>\n",
       "      <td>107.08</td>\n",
       "      <td>403</td>\n",
       "      <td>100.64</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102101</th>\n",
       "      <td>58</td>\n",
       "      <td>44</td>\n",
       "      <td>2000</td>\n",
       "      <td>114.65</td>\n",
       "      <td>421</td>\n",
       "      <td>107.94</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.48</td>\n",
       "      <td>0.40</td>\n",
       "      <td>106.94</td>\n",
       "      <td>401</td>\n",
       "      <td>100.53</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102376</th>\n",
       "      <td>33</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>114.65</td>\n",
       "      <td>420</td>\n",
       "      <td>107.94</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.48</td>\n",
       "      <td>0.40</td>\n",
       "      <td>106.67</td>\n",
       "      <td>406</td>\n",
       "      <td>100.74</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.93</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102876</th>\n",
       "      <td>69</td>\n",
       "      <td>45</td>\n",
       "      <td>500</td>\n",
       "      <td>114.64</td>\n",
       "      <td>420</td>\n",
       "      <td>107.94</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.48</td>\n",
       "      <td>0.40</td>\n",
       "      <td>106.61</td>\n",
       "      <td>405</td>\n",
       "      <td>100.65</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.92</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103376</th>\n",
       "      <td>69</td>\n",
       "      <td>45</td>\n",
       "      <td>1000</td>\n",
       "      <td>114.64</td>\n",
       "      <td>420</td>\n",
       "      <td>107.94</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.47</td>\n",
       "      <td>0.40</td>\n",
       "      <td>106.59</td>\n",
       "      <td>403</td>\n",
       "      <td>100.55</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.93</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103876</th>\n",
       "      <td>61</td>\n",
       "      <td>45</td>\n",
       "      <td>1500</td>\n",
       "      <td>114.63</td>\n",
       "      <td>420</td>\n",
       "      <td>107.93</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.47</td>\n",
       "      <td>0.40</td>\n",
       "      <td>106.72</td>\n",
       "      <td>404</td>\n",
       "      <td>100.67</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.91</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104376</th>\n",
       "      <td>61</td>\n",
       "      <td>45</td>\n",
       "      <td>2000</td>\n",
       "      <td>114.63</td>\n",
       "      <td>420</td>\n",
       "      <td>107.93</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.47</td>\n",
       "      <td>0.40</td>\n",
       "      <td>106.61</td>\n",
       "      <td>398</td>\n",
       "      <td>100.50</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.90</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104651</th>\n",
       "      <td>34</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>114.63</td>\n",
       "      <td>420</td>\n",
       "      <td>107.93</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.47</td>\n",
       "      <td>0.40</td>\n",
       "      <td>106.71</td>\n",
       "      <td>404</td>\n",
       "      <td>100.60</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.90</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105151</th>\n",
       "      <td>61</td>\n",
       "      <td>46</td>\n",
       "      <td>500</td>\n",
       "      <td>114.62</td>\n",
       "      <td>420</td>\n",
       "      <td>107.92</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.47</td>\n",
       "      <td>0.40</td>\n",
       "      <td>106.69</td>\n",
       "      <td>399</td>\n",
       "      <td>100.53</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.91</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105651</th>\n",
       "      <td>61</td>\n",
       "      <td>46</td>\n",
       "      <td>1000</td>\n",
       "      <td>114.62</td>\n",
       "      <td>420</td>\n",
       "      <td>107.92</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.46</td>\n",
       "      <td>0.40</td>\n",
       "      <td>106.92</td>\n",
       "      <td>404</td>\n",
       "      <td>100.69</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.90</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106151</th>\n",
       "      <td>63</td>\n",
       "      <td>46</td>\n",
       "      <td>1500</td>\n",
       "      <td>114.61</td>\n",
       "      <td>420</td>\n",
       "      <td>107.91</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.46</td>\n",
       "      <td>0.40</td>\n",
       "      <td>106.95</td>\n",
       "      <td>405</td>\n",
       "      <td>100.72</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.90</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106651</th>\n",
       "      <td>62</td>\n",
       "      <td>46</td>\n",
       "      <td>2000</td>\n",
       "      <td>114.61</td>\n",
       "      <td>420</td>\n",
       "      <td>107.91</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.46</td>\n",
       "      <td>0.40</td>\n",
       "      <td>106.81</td>\n",
       "      <td>400</td>\n",
       "      <td>100.56</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.89</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106926</th>\n",
       "      <td>34</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>114.61</td>\n",
       "      <td>420</td>\n",
       "      <td>107.91</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.46</td>\n",
       "      <td>0.40</td>\n",
       "      <td>106.91</td>\n",
       "      <td>402</td>\n",
       "      <td>100.59</td>\n",
       "      <td>2.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.91</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107426</th>\n",
       "      <td>58</td>\n",
       "      <td>47</td>\n",
       "      <td>500</td>\n",
       "      <td>114.61</td>\n",
       "      <td>420</td>\n",
       "      <td>107.91</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.46</td>\n",
       "      <td>0.40</td>\n",
       "      <td>107.06</td>\n",
       "      <td>405</td>\n",
       "      <td>100.69</td>\n",
       "      <td>2.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.90</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107926</th>\n",
       "      <td>57</td>\n",
       "      <td>47</td>\n",
       "      <td>1000</td>\n",
       "      <td>114.60</td>\n",
       "      <td>419</td>\n",
       "      <td>107.90</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.45</td>\n",
       "      <td>0.40</td>\n",
       "      <td>106.96</td>\n",
       "      <td>401</td>\n",
       "      <td>100.57</td>\n",
       "      <td>2.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.90</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108426</th>\n",
       "      <td>59</td>\n",
       "      <td>47</td>\n",
       "      <td>1500</td>\n",
       "      <td>114.60</td>\n",
       "      <td>419</td>\n",
       "      <td>107.90</td>\n",
       "      <td>1.94</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.45</td>\n",
       "      <td>0.40</td>\n",
       "      <td>106.94</td>\n",
       "      <td>404</td>\n",
       "      <td>100.57</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.89</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108926</th>\n",
       "      <td>58</td>\n",
       "      <td>47</td>\n",
       "      <td>2000</td>\n",
       "      <td>114.59</td>\n",
       "      <td>419</td>\n",
       "      <td>107.90</td>\n",
       "      <td>1.94</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.45</td>\n",
       "      <td>0.40</td>\n",
       "      <td>107.08</td>\n",
       "      <td>405</td>\n",
       "      <td>100.68</td>\n",
       "      <td>2.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.90</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109201</th>\n",
       "      <td>34</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>114.59</td>\n",
       "      <td>419</td>\n",
       "      <td>107.90</td>\n",
       "      <td>1.94</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.45</td>\n",
       "      <td>0.40</td>\n",
       "      <td>106.94</td>\n",
       "      <td>401</td>\n",
       "      <td>100.56</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.89</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109701</th>\n",
       "      <td>60</td>\n",
       "      <td>48</td>\n",
       "      <td>500</td>\n",
       "      <td>114.59</td>\n",
       "      <td>419</td>\n",
       "      <td>107.89</td>\n",
       "      <td>1.94</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.45</td>\n",
       "      <td>0.40</td>\n",
       "      <td>107.04</td>\n",
       "      <td>401</td>\n",
       "      <td>100.60</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.89</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110201</th>\n",
       "      <td>60</td>\n",
       "      <td>48</td>\n",
       "      <td>1000</td>\n",
       "      <td>114.58</td>\n",
       "      <td>419</td>\n",
       "      <td>107.89</td>\n",
       "      <td>1.94</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.44</td>\n",
       "      <td>0.40</td>\n",
       "      <td>106.95</td>\n",
       "      <td>403</td>\n",
       "      <td>100.62</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.88</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110701</th>\n",
       "      <td>62</td>\n",
       "      <td>48</td>\n",
       "      <td>1500</td>\n",
       "      <td>114.58</td>\n",
       "      <td>419</td>\n",
       "      <td>107.89</td>\n",
       "      <td>1.94</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.44</td>\n",
       "      <td>0.40</td>\n",
       "      <td>106.97</td>\n",
       "      <td>402</td>\n",
       "      <td>100.62</td>\n",
       "      <td>2.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.90</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111201</th>\n",
       "      <td>63</td>\n",
       "      <td>48</td>\n",
       "      <td>2000</td>\n",
       "      <td>114.58</td>\n",
       "      <td>419</td>\n",
       "      <td>107.88</td>\n",
       "      <td>1.94</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.44</td>\n",
       "      <td>0.40</td>\n",
       "      <td>107.03</td>\n",
       "      <td>404</td>\n",
       "      <td>100.69</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.89</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111476</th>\n",
       "      <td>35</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>114.58</td>\n",
       "      <td>419</td>\n",
       "      <td>107.88</td>\n",
       "      <td>1.94</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.44</td>\n",
       "      <td>0.40</td>\n",
       "      <td>107.02</td>\n",
       "      <td>402</td>\n",
       "      <td>100.61</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.88</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111976</th>\n",
       "      <td>62</td>\n",
       "      <td>49</td>\n",
       "      <td>500</td>\n",
       "      <td>114.57</td>\n",
       "      <td>419</td>\n",
       "      <td>107.88</td>\n",
       "      <td>1.94</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.44</td>\n",
       "      <td>0.40</td>\n",
       "      <td>107.00</td>\n",
       "      <td>404</td>\n",
       "      <td>100.65</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.87</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112476</th>\n",
       "      <td>62</td>\n",
       "      <td>49</td>\n",
       "      <td>1000</td>\n",
       "      <td>114.57</td>\n",
       "      <td>419</td>\n",
       "      <td>107.87</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.44</td>\n",
       "      <td>0.40</td>\n",
       "      <td>106.88</td>\n",
       "      <td>399</td>\n",
       "      <td>100.55</td>\n",
       "      <td>2.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.87</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112976</th>\n",
       "      <td>62</td>\n",
       "      <td>49</td>\n",
       "      <td>1500</td>\n",
       "      <td>114.56</td>\n",
       "      <td>419</td>\n",
       "      <td>107.87</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.43</td>\n",
       "      <td>0.40</td>\n",
       "      <td>107.12</td>\n",
       "      <td>399</td>\n",
       "      <td>100.52</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.87</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113476</th>\n",
       "      <td>63</td>\n",
       "      <td>49</td>\n",
       "      <td>2000</td>\n",
       "      <td>114.56</td>\n",
       "      <td>419</td>\n",
       "      <td>107.87</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.43</td>\n",
       "      <td>0.40</td>\n",
       "      <td>106.83</td>\n",
       "      <td>398</td>\n",
       "      <td>100.45</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.87</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       TRAIN:    TM                        LM        VALID:  \\\n",
       "       Time  Ep    Ct    LOSS   PPL     NLL    KL   REG   NLL    KL    LOSS   \n",
       "1        14   0     0  123.45  1035  113.05  0.37  0.90  9.13  1.46  126.48   \n",
       "501      62   0   500  121.19   598  113.84  0.17  0.63  6.50  1.38  111.52   \n",
       "1001     71   0  1000  120.10   568  113.13  0.26  0.49  6.15  1.07  111.01   \n",
       "1501     71   0  1500  119.68   553  112.78  0.39  0.40  6.03  0.91  110.78   \n",
       "2001     63   0  2000  119.22   542  112.33  0.49  0.33  5.96  0.80  110.57   \n",
       "2276     39   1     0  118.98   536  112.10  0.54  0.31  5.93  0.75  110.24   \n",
       "2776     52   1   500  118.70   529  111.80  0.61  0.27  5.90  0.68  110.28   \n",
       "3276     63   1  1000  118.41   523  111.50  0.67  0.24  5.87  0.63  110.10   \n",
       "3776     62   1  1500  118.35   517  111.42  0.72  0.22  5.85  0.58  109.95   \n",
       "4276     62   1  2000  118.21   512  111.27  0.77  0.20  5.83  0.55  109.83   \n",
       "4551     40   2     0  118.16   510  111.21  0.79  0.20  5.81  0.53  109.75   \n",
       "5051     62   2   500  118.02   506  111.07  0.82  0.18  5.79  0.50  109.62   \n",
       "5551     64   2  1000  117.88   503  110.93  0.86  0.17  5.76  0.48  109.61   \n",
       "6051     64   2  1500  117.80   500  110.85  0.89  0.16  5.74  0.46  109.52   \n",
       "6551     64   2  2000  117.68   497  110.74  0.91  0.15  5.71  0.45  109.37   \n",
       "6826     41   3     0  117.66   496  110.72  0.93  0.14  5.69  0.44  109.18   \n",
       "7326     64   3   500  117.57   493  110.63  0.95  0.14  5.66  0.43  109.07   \n",
       "7826     55   3  1000  117.49   491  110.56  0.97  0.13  5.64  0.42  109.12   \n",
       "8326     53   3  1500  117.43   489  110.51  0.99  0.12  5.61  0.41  109.18   \n",
       "8826     52   3  2000  117.37   487  110.45  1.01  0.12  5.58  0.40  109.08   \n",
       "9101     39   4     0  117.31   487  110.40  1.02  0.11  5.57  0.40  108.90   \n",
       "9601     53   4   500  117.23   485  110.33  1.04  0.11  5.55  0.39  109.11   \n",
       "10101    52   4  1000  117.13   483  110.24  1.06  0.10  5.52  0.38  109.01   \n",
       "10601    71   4  1500  117.12   482  110.23  1.08  0.10  5.50  0.38  108.85   \n",
       "11101    53   4  2000  117.05   480  110.16  1.09  0.10  5.48  0.37  108.91   \n",
       "11376    39   5     0  117.04   480  110.16  1.10  0.09  5.47  0.37  108.64   \n",
       "11876    62   5   500  116.97   478  110.11  1.12  0.09  5.45  0.37  108.46   \n",
       "12376    65   5  1000  116.91   477  110.06  1.13  0.09  5.43  0.37  108.39   \n",
       "12876    55   5  1500  116.84   476  110.00  1.15  0.09  5.41  0.37  108.52   \n",
       "13376    55   5  2000  116.81   475  109.98  1.16  0.08  5.40  0.37  108.51   \n",
       "...     ...  ..   ...     ...   ...     ...   ...   ...   ...   ...     ...   \n",
       "100101   30  44     0  114.67   421  107.96  1.91  0.02  4.49  0.40  107.05   \n",
       "100601   54  44   500  114.66   421  107.95  1.91  0.02  4.48  0.40  106.98   \n",
       "101101   54  44  1000  114.66   421  107.95  1.92  0.02  4.48  0.40  107.11   \n",
       "101601   59  44  1500  114.66   421  107.95  1.92  0.02  4.48  0.40  107.08   \n",
       "102101   58  44  2000  114.65   421  107.94  1.92  0.02  4.48  0.40  106.94   \n",
       "102376   33  45     0  114.65   420  107.94  1.92  0.02  4.48  0.40  106.67   \n",
       "102876   69  45   500  114.64   420  107.94  1.92  0.02  4.48  0.40  106.61   \n",
       "103376   69  45  1000  114.64   420  107.94  1.92  0.02  4.47  0.40  106.59   \n",
       "103876   61  45  1500  114.63   420  107.93  1.92  0.02  4.47  0.40  106.72   \n",
       "104376   61  45  2000  114.63   420  107.93  1.92  0.02  4.47  0.40  106.61   \n",
       "104651   34  46     0  114.63   420  107.93  1.93  0.02  4.47  0.40  106.71   \n",
       "105151   61  46   500  114.62   420  107.92  1.93  0.02  4.47  0.40  106.69   \n",
       "105651   61  46  1000  114.62   420  107.92  1.93  0.02  4.46  0.40  106.92   \n",
       "106151   63  46  1500  114.61   420  107.91  1.93  0.02  4.46  0.40  106.95   \n",
       "106651   62  46  2000  114.61   420  107.91  1.93  0.02  4.46  0.40  106.81   \n",
       "106926   34  47     0  114.61   420  107.91  1.93  0.02  4.46  0.40  106.91   \n",
       "107426   58  47   500  114.61   420  107.91  1.93  0.02  4.46  0.40  107.06   \n",
       "107926   57  47  1000  114.60   419  107.90  1.93  0.02  4.45  0.40  106.96   \n",
       "108426   59  47  1500  114.60   419  107.90  1.94  0.02  4.45  0.40  106.94   \n",
       "108926   58  47  2000  114.59   419  107.90  1.94  0.02  4.45  0.40  107.08   \n",
       "109201   34  48     0  114.59   419  107.90  1.94  0.02  4.45  0.40  106.94   \n",
       "109701   60  48   500  114.59   419  107.89  1.94  0.02  4.45  0.40  107.04   \n",
       "110201   60  48  1000  114.58   419  107.89  1.94  0.02  4.44  0.40  106.95   \n",
       "110701   62  48  1500  114.58   419  107.89  1.94  0.02  4.44  0.40  106.97   \n",
       "111201   63  48  2000  114.58   419  107.88  1.94  0.02  4.44  0.40  107.03   \n",
       "111476   35  49     0  114.58   419  107.88  1.94  0.02  4.44  0.40  107.02   \n",
       "111976   62  49   500  114.57   419  107.88  1.94  0.02  4.44  0.40  107.00   \n",
       "112476   62  49  1000  114.57   419  107.87  1.95  0.02  4.44  0.40  106.88   \n",
       "112976   62  49  1500  114.56   419  107.87  1.95  0.02  4.43  0.40  107.12   \n",
       "113476   63  49  2000  114.56   419  107.87  1.95  0.02  4.43  0.40  106.83   \n",
       "\n",
       "          TM                        LM               \n",
       "         PPL     NLL    KL   REG   NLL    KL   Beta  \n",
       "1       1034  116.13  0.33  0.90  9.13  1.40  0.000  \n",
       "501      527  105.05  0.15  0.47  5.77  0.78  0.088  \n",
       "1001     510  104.48  0.44  0.27  5.71  0.66  0.176  \n",
       "1501     507  104.24  0.56  0.17  5.69  0.46  0.264  \n",
       "2001     497  103.96  0.69  0.13  5.67  0.36  0.352  \n",
       "2276     488  103.61  0.71  0.11  5.67  0.37  0.400  \n",
       "2776     483  103.52  0.84  0.10  5.66  0.35  0.488  \n",
       "3276     478  103.30  0.90  0.09  5.64  0.30  0.576  \n",
       "3776     474  103.13  0.93  0.07  5.63  0.30  0.664  \n",
       "4276     469  103.04  0.97  0.07  5.56  0.26  0.752  \n",
       "4551     467  102.97  0.98  0.06  5.52  0.28  0.800  \n",
       "5051     462  102.83  1.04  0.05  5.45  0.28  0.888  \n",
       "5551     465  102.89  1.05  0.04  5.38  0.25  0.976  \n",
       "6051     464  102.88  1.04  0.04  5.30  0.26  1.000  \n",
       "6551     463  102.79  1.04  0.03  5.24  0.27  1.000  \n",
       "6826     456  102.57  1.09  0.03  5.21  0.27  1.000  \n",
       "7326     451  102.47  1.16  0.03  5.16  0.27  1.000  \n",
       "7826     453  102.52  1.18  0.02  5.11  0.28  1.000  \n",
       "8326     458  102.65  1.18  0.02  5.07  0.25  1.000  \n",
       "8826     457  102.57  1.18  0.02  5.05  0.26  1.000  \n",
       "9101     450  102.37  1.20  0.02  5.03  0.28  1.000  \n",
       "9601     455  102.52  1.28  0.02  5.00  0.28  1.000  \n",
       "10101    455  102.55  1.21  0.02  4.97  0.27  1.000  \n",
       "10601    451  102.37  1.25  0.02  4.95  0.27  1.000  \n",
       "11101    453  102.43  1.25  0.01  4.93  0.29  1.000  \n",
       "11376    455  102.48  1.24  0.02  4.90  0.29  0.000  \n",
       "11876    445  102.19  1.35  0.01  4.88  0.35  0.088  \n",
       "12376    443  102.12  1.34  0.01  4.86  0.34  0.176  \n",
       "12876    444  102.13  1.42  0.01  4.85  0.37  0.264  \n",
       "13376    443  102.10  1.45  0.02  4.82  0.35  0.352  \n",
       "...      ...     ...   ...   ...   ...   ...    ...  \n",
       "100101   403  100.63  1.99  0.02  3.95  0.47  1.000  \n",
       "100601   401  100.59  2.00  0.02  3.94  0.43  1.000  \n",
       "101101   405  100.70  2.03  0.02  3.95  0.42  1.000  \n",
       "101601   403  100.64  2.05  0.02  3.94  0.44  1.000  \n",
       "102101   401  100.53  2.03  0.02  3.94  0.42  1.000  \n",
       "102376   406  100.74  1.98  0.02  3.93  0.43  0.000  \n",
       "102876   405  100.65  1.97  0.02  3.92  0.53  0.088  \n",
       "103376   403  100.55  2.01  0.02  3.93  0.51  0.176  \n",
       "103876   404  100.67  1.98  0.02  3.91  0.51  0.264  \n",
       "104376   398  100.50  2.03  0.02  3.90  0.47  0.352  \n",
       "104651   404  100.60  2.00  0.02  3.90  0.46  0.400  \n",
       "105151   399  100.53  2.01  0.02  3.91  0.46  0.488  \n",
       "105651   404  100.69  2.03  0.02  3.90  0.48  0.576  \n",
       "106151   405  100.72  2.01  0.02  3.90  0.46  0.664  \n",
       "106651   400  100.56  2.01  0.02  3.89  0.45  0.752  \n",
       "106926   402  100.59  2.04  0.02  3.91  0.43  0.800  \n",
       "107426   405  100.69  2.04  0.02  3.90  0.45  0.888  \n",
       "107926   401  100.57  2.02  0.02  3.90  0.46  0.976  \n",
       "108426   404  100.57  2.03  0.02  3.89  0.43  1.000  \n",
       "108926   405  100.68  2.04  0.02  3.90  0.44  1.000  \n",
       "109201   401  100.56  2.03  0.02  3.89  0.44  1.000  \n",
       "109701   401  100.60  2.03  0.02  3.89  0.50  1.000  \n",
       "110201   403  100.62  2.01  0.02  3.88  0.42  1.000  \n",
       "110701   402  100.62  2.02  0.02  3.90  0.41  1.000  \n",
       "111201   404  100.69  2.01  0.02  3.89  0.42  1.000  \n",
       "111476   402  100.61  2.03  0.02  3.88  0.48  1.000  \n",
       "111976   404  100.65  2.01  0.02  3.87  0.45  1.000  \n",
       "112476   399  100.55  2.02  0.02  3.87  0.42  1.000  \n",
       "112976   399  100.52  2.01  0.02  3.87  0.70  1.000  \n",
       "113476   398  100.45  2.07  0.02  3.87  0.43  1.000  \n",
       "\n",
       "[250 rows x 18 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Output sentences for each topic-----------\n",
      "Item idx: B000VB7EFW\n",
      "0  BOW: bottom top plastic piece corners part edges speck cracked feet\n",
      "0  SENTENCE: it 's a great case for the price\n",
      "1  BOW: quality price made 'm nice ... time - thing put\n",
      "1  SENTENCE: i would recommend this to anyone looking for a laptop case\n",
      "2  BOW: cover color keyboard mac pro hard apple easy nice perfectly\n",
      "2  SENTENCE: the keyboard cover fits perfectly and is easy to put on\n",
      "3  BOW: ! love recommend perfect color perfectly highly bought ordered happy\n",
      "3  SENTENCE: i love the color and the color is great ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "4  BOW: sleeve & ; protection air pro protect inside inch snug\n",
      "4  SENTENCE: the sleeve is a little snug , but it does not fit my macbook pro # & # # ;\n",
      "5  BOW: zipper strap side - : padding zippers top compartment velcro\n",
      "5  SENTENCE: it is a little bigger than i expected\n",
      "6  BOW: carry pockets back compartments comfortable shoulder room stuff space work\n",
      "6  SENTENCE: it has plenty of room for my laptop , power cord , and a few other items\n",
      "7  BOW: bought time $ back money ... return buy received item\n",
      "7  SENTENCE: i would recommend this product\n",
      "8  BOW: handle months years straps broke strap wheels started zippers year\n",
      "8  SENTENCE: i bought this bag for my # . # `` laptop\n",
      "9  BOW: pocket room ipad power small charger mouse perfect cord carry\n",
      "9  SENTENCE: there is plenty of room for the power cord , mouse , power cord , mouse , power cord , etc .\n",
      "-----------Summaries-----------\n",
      "SUMMARY 0 : \n",
      " This is a very well made bag, nice construction, lots of pockets.\n",
      "the straps are very comfortable.\n",
      "and protects everything inside.\n",
      "It says\n",
      "it fits a 17inch notebook,\n",
      "however it did not.\n",
      "after using the pack for less than a month,\n",
      "it is ripping out already.\n",
      "SUMMARY 1 : \n",
      " This is a very well made bag, nice construction, lots of pockets.\n",
      "The quality is excellent\n",
      "and it is very durable.\n",
      "the straps are very comfortable.\n",
      "and protects everything inside.\n",
      "The color is a true red\n",
      "and it fits nicely.\n",
      "it is ripping out already.\n",
      "and it doesnt fit.\n",
      "It's just not the lightest backpack\n",
      "because some zipper teeth were not aligned.\n",
      "SUMMARY 2 : \n",
      " The quality is excellent\n",
      "and I love all the pockets and compartments.\n",
      "and it is very durable.\n",
      "and can't beleive the price\n",
      "and protects everything inside.\n",
      "The laptop\n",
      "doesn't fit in it.\n",
      "it is ripping out already.\n",
      "It's just not the lightest backpack\n",
      "0 TRUE: i bought this for my clumsy wives macbook pro\n",
      "0 PRED: i have n't case my macbook and macbook pro\n",
      "1 TRUE: the peace of mind literally makes this product pay for itself\n",
      "1 PRED: i is a the the the it case a of a\n",
      "2 TRUE: solid product , well made , and i did not experience any of the issues previous reviewers had\n",
      "2 PRED: i is is a great\n",
      "3 TRUE: i must have been lucky\n",
      "3 PRED: i would have it a to\n",
      "4 TRUE: the foam is <unk> to cut or punch out by hand\n",
      "4 PRED: i only is not and the the and\n",
      "5 TRUE: fits # in macbook pro ! !\n",
      "5 PRED: i perfectly macbook my pro perfectly\n",
      "6 TRUE: the foam is not like the foam in my pelican gun cases\n",
      "6 PRED: it only is very a thick\n",
      "7 TRUE: its made of a cube <unk> material\n",
      "7 PRED: i is of a good material , ,\n",
      "8 TRUE: so its really easy to work with\n",
      "8 PRED: i was a cute to get it it\n",
      "9 TRUE: when i bought this i thought i was going to have to use an electric knife -lrb- like the kind you use for <unk> -rrb-\n",
      "9 PRED: i i got it case was it would a to be a be it extra case to i i `` -rrb- get -rrb- -rrb- -rrb-\n",
      "10 TRUE: having owned a few macbooks , i 've had my share of macs with little dents in them\n",
      "10 PRED: i to for different of , but i the to macbook one a for this different\n",
      "11 TRUE: i always <unk> at paying $ # at the apple store for a shell , but after my wife dropped her macbook with a shell on it , the fact it lived changed my mind\n",
      "11 PRED: i bought this the the the the ; and and logo , my #\n",
      "12 TRUE: finding a one here for $ # made the purchase an easy decision\n",
      "12 PRED: i a great of\n",
      "13 TRUE: the shell arrived in two days -lrb- thanks , prime ! -rrb- and installation was a snap ... literally\n",
      "13 PRED: i color is quickly time time\n",
      "14 TRUE: the pictures accurately <unk> the shade of blue for this particular case\n",
      "14 PRED: the case is is is the of the\n",
      "15 TRUE: it looks spectacular , is lightweight , and has vents to keep your computer cool\n",
      "15 PRED: i only like , the a , but the a on the the laptop from\n",
      "16 TRUE: i took one star off because the tabs at the top show through a little bit on the back of the screen\n",
      "16 PRED: the bought the # on and it keyboard cover the the cover the the few\n",
      "17 TRUE: it 's a minor annoyance , but not enough to make me want to send it back\n",
      "17 PRED: i 's a a case , the i 's to be it to to get it out\n",
      "18 TRUE: i got this bag for christmas a year ago , and it has been great\n",
      "18 PRED: i bought this bag for my and year and and and it 's very great\n",
      "19 TRUE: it holds a ton of equipment , it 's smaller than most bags , and it 's really well designed\n",
      "19 PRED: the bag my lot of stuff and i is a than i of\n",
      "20 TRUE: my issue with the bag , and why it ended up receiving only # stars , is that i took it out in rain and it got completely soaked through\n",
      "20 PRED: i husband is this bag is the the i is up the the bag of , the a it can a to of a and the was a\n",
      "21 TRUE: every piece of my equipment got wet\n",
      "21 PRED: i time of the bag is this , , , and ,\n",
      "22 TRUE: none of it was ruined thankfully , and due to a long drying process , but i would suggest you consider whether or not you 'll be out in the elements with this bag\n",
      "22 PRED: i is the , a the and the i to the new use\n",
      "23 TRUE: had it not soaked through , i would have easily given it # stars\n",
      "23 PRED: i a for a and a\n",
      "24 TRUE: the product looked just as it was described however there were some issues\n",
      "24 PRED: i case is great as described as it\n",
      "25 TRUE: first issue , it does n't really provide very much protection\n",
      "25 PRED: the , , the case is fit a the well protection , ,\n",
      "26 TRUE: second issue , the graphics have faint shadows that should n't be there\n",
      "26 PRED: i , a i price is been\n",
      "27 TRUE: maybe it was printed wrong\n",
      "27 PRED: i i was a a\n",
      "28 TRUE: third issue , gets dirty easily\n",
      "28 PRED: the of , i a easily easily and\n",
      "29 TRUE: if cute is what you are going for then sure , but for the price it was n't the quality i expected\n",
      "29 PRED: i you , , i would looking to\n",
      "30 TRUE: there are a million laptop backpacks out there\n",
      "30 PRED: i is a of pockets bags in of the\n",
      "31 TRUE: i got this one because it 's checkpoint friendly - saves on needing # additional bins at the airport\n",
      "31 PRED: i have this for for my is a and , i the the back and compartments\n",
      "32 TRUE: has plenty of pockets , shoulders straps are comfortable enough\n",
      "32 PRED: the plenty of room for and , and\n",
      "33 TRUE: good padding and fit for laptop and accessories\n",
      "33 PRED: the for for the a my laptop a\n",
      "34 TRUE: the swiss gear brand lives up to it 's reputation\n",
      "34 PRED: i is gear are backpack up and the\n",
      "35 TRUE: just bought a # & # # ; mac air and wanted a leather case to protect it\n",
      "35 PRED: the is is little . # # ; macbook pro and it a case case for protect the\n",
      "36 TRUE: this case from casecrown has exceeded my expectations\n",
      "36 PRED: i is is the <unk> a the expectations\n",
      "37 TRUE: it protects my mac totally and makes it look very stylish\n",
      "37 PRED: i is a and and fast\n",
      "38 TRUE: nothing but positives on this case\n",
      "38 PRED: i to i n't the product\n",
      "39 TRUE: i highly recommend it\n",
      "39 PRED: i was recommend this\n",
      "40 TRUE: i had one of these , but the rollers finally wore out , so i had to replace it\n",
      "40 PRED: i bought this of these bags and i bag is is out\n",
      "41 TRUE: after looking all over , i chose the same one again -- it 's a good value for the price\n",
      "41 PRED: i reading for over time i would this case one of\n",
      "42 TRUE: i am a teacher and use this daily to carry a laptop , and two tablets , a multitude of assorted wires , papers , pens , notebooks , etc .\n",
      "42 PRED: it is is of and the for for use carry around laptop\n",
      "43 TRUE: it 's light -- easy for carrying up stairs , it has <unk> through a lot of snow , traveled as a carry-on on numerous flights , and was a pretty good bag for the money\n",
      "43 PRED: i is a and and to carry and and\n",
      "44 TRUE: this is a good case - i love the color and it 's nice that it 's cheaper than some of the other covers out there , but i think there is a reason\n",
      "44 PRED: the case a great case for i color the color and the is a\n",
      "45 TRUE: on the bottom case , there is a ridge that sticks up over the edge of the computer near the trackpad that is right underneath your wrists when you 're typing , and it 's annoying and kind of hurts\n",
      "45 PRED: the the bottom of is the case the little on the on to the bottom of the laptop\n",
      "46 TRUE: i am on this thing all day long and now i am thinking of buying another case to avoid this\n",
      "46 PRED: i have very this case i the and and i it 'm it it the this one for the the one\n",
      "47 TRUE: otherwise , it fits and looks really good\n",
      "47 PRED: i , it is my feels great nice\n",
      "48 TRUE: i like the case , the colors are cool and my acer # & # # ; fits perfectly\n",
      "48 PRED: i bought the material\n",
      "49 TRUE: i also like zipper pouch in the front\n",
      "49 PRED: i have like the the , the front pocket\n",
      "50 TRUE: i gave this three stars because the material is kind of thin , and my package took way too long to get here\n",
      "50 PRED: i case it case stars because because case of a of cheap\n",
      "51 TRUE: other than that , it 's pretty nice and not a bad deal for $ #\n",
      "51 PRED: i than that , it is a good\n",
      "52 TRUE: this bag has plenty of room for my # . # inch laptop and plenty of storage for my power cord/charger and other items\n",
      "52 PRED: the bag is plenty of room for all laptop , # , , , accessories\n",
      "53 TRUE: great product / well padded and good handles for carrying\n",
      "53 PRED: the is is # padded and has padding , the my\n",
      "54 TRUE: highly recommend this product\n",
      "54 PRED: i was this product\n",
      "55 TRUE: delays in the receipt of goods , a good package , i thought the whole leather\n",
      "55 PRED: i , the description of the , i is n't\n",
      "56 TRUE: fortunately , looks good , the capacity is large enough , can put on a lot of things out\n",
      "56 PRED: i , the great , the the , good enough\n",
      "57 TRUE: but prices generally\n",
      "57 PRED: i is , <unk>\n",
      "58 TRUE: the material is not as good as i expect\n",
      "58 PRED: i only is very a thick as the thought\n",
      "59 TRUE: it does n't feel very well , and looks too cheap for a <unk> gaming laptop\n",
      "59 PRED: i is not look like well\n",
      "60 TRUE: the only reason i bought it is because this is the only # `` case i could find\n",
      "60 PRED: i is a i have this bag a the bag a best one `` laptop\n",
      "61 TRUE: i like it , but its a little bulky to be considered a sleeve , but a little minimal to be a bag\n",
      "61 PRED: the have the , but it a little , , protect a\n",
      "62 TRUE: if it just had a top handle for picking up , it would be\n",
      "62 PRED: i you had need a little of , would it to i would be a\n",
      "63 TRUE: the case did not fit my new macbook pro which is # inches , when i got the case i tried to put it on and the case was too small\n",
      "63 PRED: the case is not fit my macbook macbook macbook # i a #\n"
     ]
    }
   ],
   "source": [
    "if len(log_df) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "while epoch < config.epochs:\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = get_feed_dict(batch)\n",
    "\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, sent_loss_kl_categ_batch, sent_loss_kl_gmm_batch, ppls_batch = \\\n",
    "        sess.run([opt, loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, sent_loss_kl_categ, sent_loss_kl_gmm, topic_ppls], feed_dict = feed_dict)\n",
    "   \n",
    "        if sent_loss_kl_batch == np.inf:\n",
    "            print('Nan occured')\n",
    "            ckpt = tf.train.get_checkpoint_state(config.modeldir)\n",
    "            model_checkpoint_path = ckpt.all_model_checkpoint_paths[-1]\n",
    "            saver.restore(sess, model_checkpoint_path)            \n",
    "            break\n",
    "            \n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if ct%config.log_period==0:\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train, sent_loss_recon_train, sent_loss_kl_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, sent_loss_recon_dev, sent_loss_kl_dev, ppl_dev = get_loss(sess, dev_batches)\n",
    "            global_step_log, beta_eval = sess.run([tf.train.get_global_step(), beta])\n",
    "            \n",
    "            if loss_dev < loss_min:\n",
    "                loss_min = loss_dev\n",
    "                saver.save(sess, config.modelpath, global_step=global_step_log)\n",
    "\n",
    "            clear_output()\n",
    "    \n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_reg_train, '%.2f'%sent_loss_recon_train, '%.2f'%sent_loss_kl_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_reg_dev, '%.2f'%sent_loss_recon_dev, '%.2f'%sent_loss_kl_dev,  '%.3f'%beta_eval],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "\n",
    "            print_summary(test_batches[1][1])\n",
    "            print_sample(batch)\n",
    "            \n",
    "            time_start = time.time()\n",
    "            \n",
    "    epoch += 1\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "_prob_topic, _prob_topic_sents, _prob_topic_infer, _means_topic_infer = debug_value([prob_topic, prob_topic_sents, prob_topic_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.02120299, 0.32619268, 0.02783219, 0.08510477, 0.04270138,\n",
       "        0.05854441, 0.17855252, 0.17987773, 0.05455504, 0.02543635],\n",
       "       dtype=float32),\n",
       " array([0.02938788, 0.24803634, 0.00798052, 0.01799672, 0.03427965,\n",
       "        0.15884441, 0.19315946, 0.1489072 , 0.1465453 , 0.01486251],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_i = 4\n",
    "_prob_topic_sents[batch_i], _prob_topic_infer[batch_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3878181 , -0.52203584, -1.9560823 ,  0.43151194],\n",
       "       [ 0.22494435, -0.10070819, -0.9306398 ,  0.24482146],\n",
       "       [-1.7151399 , -1.2774523 , -4.6163416 ,  0.35484877],\n",
       "       [-0.5554816 ,  0.38961172, -3.5819693 , -2.0564005 ],\n",
       "       [-0.6458729 , -1.167951  , -1.8318795 ,  1.5787079 ],\n",
       "       [ 0.67180693, -0.01855016,  0.19753097,  0.6514791 ],\n",
       "       [ 1.7514257 ,  0.7512727 ,  2.0913913 ,  0.32178992],\n",
       "       [ 0.05339959, -0.08493266, -1.3765373 , -0.00502022],\n",
       "       [ 0.65437484,  0.2057987 , -0.09499581,  0.10592981],\n",
       "       [ 1.4489527 ,  0.29247132,  1.894986  ,  0.97273844]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_means_topic_infer[0][:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_means_topic, b_means_topic = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"topic/dec/mean_topic\")\n",
    "\n",
    "pred_topic_embeddings, pred_topic_bow, pred_means_topic, pred_logvars_topic, pred_token_idxs, _w_means_topic, _b_means_topic, _w_mean_topic_infer = \\\n",
    "                                sess.run([topic_embeddings, topic_bow, means_topic, logvars_topic, topic_beam_output_token_idxs, w_means_topic, b_means_topic, w_mean_topic_infer], \n",
    "                                         feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "\n",
    "pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "\n",
    "pred_topics_freq_bow_indices = np.argsort(pred_topic_bow, 1)[:, ::-1][:, :10]\n",
    "pred_topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bottom', 'top', 'plastic', 'part', 'piece', 'speck', 'feet', 'corners', 'edges', 'rubber']\n",
      "['quality', 'price', 'made', \"'m\", 'nice', '...', 'time', '-', 'thing', 'lot']\n",
      "['cover', 'color', 'keyboard', 'mac', 'pro', 'apple', 'hard', 'easy', 'perfectly', 'air']\n",
      "['!', 'love', 'color', 'recommend', 'perfect', 'perfectly', 'bought', 'highly', 'buy', 'purchase']\n",
      "['sleeve', '&', ';', 'protection', 'pro', 'air', 'inside', 'protect', 'inch', 'snug']\n",
      "['zipper', 'strap', '-', 'side', ':', 'zippers', 'padding', 'top', 'velcro', 'compartment']\n",
      "['carry', 'pockets', 'back', 'compartments', 'space', 'room', 'comfortable', 'work', 'shoulder', 'stuff']\n",
      "['bought', 'time', '$', 'return', 'money', 'back', 'item', 'buy', 'received', '...']\n",
      "['handle', 'years', 'months', 'straps', 'broke', 'strap', 'wheels', 'started', 'zippers', 'year']\n",
      "['pocket', 'room', 'ipad', 'power', 'mouse', 'small', 'charger', 'perfect', 'carry', 'netbook']\n"
     ]
    }
   ],
   "source": [
    "for idxs in pred_topics_freq_bow_idxs:\n",
    "    print([idx_to_word[idx] for idx in idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.67144597e-01,  4.32745188e-01, -1.22095034e-01,\n",
       "        -1.03223495e-01,  1.78706244e-01, -1.67346388e-01,\n",
       "        -1.68974623e-01,  1.05840132e-01,  2.28138641e-01,\n",
       "        -4.37488645e-01],\n",
       "       [ 2.97464505e-02,  1.45410940e-01, -1.46664038e-01,\n",
       "         3.29389051e-02,  8.52175057e-02,  3.31251651e-01,\n",
       "         4.63549852e-01,  1.16298549e-01,  1.64733216e-01,\n",
       "         2.66190320e-01],\n",
       "       [-4.47264075e-01,  6.11892581e-01, -2.60540456e-01,\n",
       "         2.68532097e-01,  5.53756813e-03, -7.70700499e-02,\n",
       "         2.44523048e-01, -2.98057441e-02,  4.49646443e-01,\n",
       "         5.72897978e-02],\n",
       "       [ 4.71091345e-02, -2.23770499e-01, -3.51872534e-01,\n",
       "         3.92161198e-02, -2.27904618e-01, -8.74633193e-02,\n",
       "         4.43780005e-01,  1.71568878e-02, -2.76746869e-01,\n",
       "         4.64234948e-01],\n",
       "       [ 1.28112612e-02, -3.28971356e-01, -2.27453679e-01,\n",
       "         2.29710922e-01,  4.15110923e-02, -9.82860252e-02,\n",
       "        -2.48736888e-02, -5.07779300e-01, -3.58882606e-01,\n",
       "        -8.76430646e-02],\n",
       "       [-1.28862038e-01,  2.54263699e-01, -6.12929314e-02,\n",
       "         1.62618786e-01,  4.87120338e-02, -3.21024527e-05,\n",
       "        -2.13455617e-01, -3.30884218e-01, -8.26614350e-02,\n",
       "        -9.11088884e-02],\n",
       "       [ 5.32317907e-02, -7.59053454e-02, -6.98558092e-02,\n",
       "         6.25727847e-02, -4.23587590e-01, -1.20591208e-01,\n",
       "         4.02962193e-02,  7.70772249e-02,  8.06827992e-02,\n",
       "        -2.76510417e-01],\n",
       "       [ 6.65345043e-02,  3.09571445e-01, -2.03694627e-01,\n",
       "        -1.73277944e-01,  2.91147754e-02, -9.44335833e-02,\n",
       "         2.31558129e-01,  3.34705144e-01, -1.05971023e-01,\n",
       "         2.17835791e-02],\n",
       "       [ 1.87811732e-01,  2.87433207e-01, -7.54265562e-02,\n",
       "        -3.44863981e-02,  1.59536734e-01, -9.33663398e-02,\n",
       "         2.07849577e-01,  5.94476283e-01,  9.43783671e-02,\n",
       "         2.21876785e-01],\n",
       "       [ 2.38384873e-01, -5.37431121e-01,  2.29193568e-02,\n",
       "         1.57748580e-01,  1.57282874e-02,  8.72326791e-02,\n",
       "        -5.88206798e-02, -8.38808268e-02, -4.25985932e-01,\n",
       "         1.67721715e-02]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_topic_embeddings[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.63746061e-07, 1.27285850e-04, 1.01267084e-04, ...,\n",
       "        2.11540428e-05, 5.35642539e-05, 1.34317424e-05],\n",
       "       [2.02581621e-04, 5.98845724e-03, 6.64945692e-04, ...,\n",
       "        1.31005127e-05, 6.20674400e-04, 3.25565743e-05],\n",
       "       [3.54659511e-04, 1.93204498e-04, 1.91751606e-04, ...,\n",
       "        4.05486617e-06, 1.38744406e-06, 2.71618569e-06],\n",
       "       ...,\n",
       "       [2.07204619e-04, 2.19707973e-02, 1.25209999e-03, ...,\n",
       "        9.57436669e-07, 3.18203602e-05, 2.10850635e-06],\n",
       "       [3.60615218e-06, 1.41350058e-04, 1.86732781e-04, ...,\n",
       "        3.07424016e-05, 2.13968139e-02, 5.15784377e-05],\n",
       "       [9.11835741e-05, 2.65167218e-05, 1.20110635e-05, ...,\n",
       "        4.97434940e-03, 6.13421784e-04, 8.76899168e-04]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_topic_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.002856  , -0.12888564, -0.02525959, ...,  0.13936642,\n",
       "         0.06750863,  0.05765012],\n",
       "       [-0.04506291, -0.0032232 , -0.23664857, ...,  0.13413484,\n",
       "         0.00195352,  0.04439881],\n",
       "       [-0.20138465, -0.21933417, -0.45687827, ...,  0.27684236,\n",
       "        -0.0871457 ,  0.12629426],\n",
       "       ...,\n",
       "       [-0.1734638 ,  0.02492818, -0.16350251, ...,  0.15120973,\n",
       "         0.06725367,  0.02727391],\n",
       "       [ 0.21558905,  0.00539135,  0.21533233, ..., -0.0434513 ,\n",
       "         0.09895361,  0.04902754],\n",
       "       [ 0.21534637,  0.07677771,  0.08166326, ..., -0.14705855,\n",
       "        -0.02142379, -0.00574149]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_w_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.7616607e-01,  3.6224346e-03, -3.0754781e-01,  1.3990225e-01,\n",
       "        3.1391117e-01, -1.0993136e-01,  1.7876926e-01,  2.0730966e-01,\n",
       "        1.5003067e-01, -3.9625127e-02,  2.5470558e-01, -5.3844504e-02,\n",
       "        2.3800465e-01,  3.0333728e-01, -5.1428996e-02, -2.0459709e-01,\n",
       "        8.4517933e-02,  2.4236085e-01,  1.1645479e-01, -1.2791230e-01,\n",
       "        2.7641625e-05, -9.3722373e-02,  1.5119699e-01, -9.8118551e-02,\n",
       "        6.1052620e-02, -3.0996993e-01,  9.3275547e-02, -2.3359668e-02,\n",
       "        3.0665058e-01,  3.2136410e-02,  3.5929924e-01,  1.1415872e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.94786239e-01, -5.32271326e-01, -1.96238506e+00,\n",
       "         4.46755648e-01,  1.01060510e+00, -1.22109279e-01,\n",
       "         1.50481164e-02,  2.69290149e-01,  7.44255066e-01,\n",
       "        -8.88179123e-01,  4.17418182e-01,  3.42713386e-01,\n",
       "         7.09146023e-01,  1.00336218e+00, -2.45433211e-01,\n",
       "         7.27762282e-02, -1.27207696e-01,  9.71992254e-01,\n",
       "         5.58246493e-01, -3.56951237e-01,  2.25926831e-01,\n",
       "        -7.56059289e-01,  9.44043159e-01,  3.91685903e-01,\n",
       "        -1.83836803e-01, -6.32101893e-01,  6.59335405e-03,\n",
       "         2.12992758e-01,  1.09152269e+00,  7.77865648e-01,\n",
       "         5.01754999e-01,  2.74188370e-01],\n",
       "       [ 2.16301292e-01, -1.04559906e-01, -9.37525332e-01,\n",
       "         2.46304855e-01,  8.65785837e-01, -2.16787368e-01,\n",
       "         2.97387868e-01,  4.00805056e-01,  4.88129437e-01,\n",
       "        -2.57460028e-01,  4.59877938e-01,  4.12266478e-02,\n",
       "         5.00950575e-01,  7.02583432e-01, -4.87488061e-02,\n",
       "        -3.55852962e-01,  9.70679596e-02,  6.36684895e-01,\n",
       "         3.26907068e-01, -2.74714649e-01,  1.67324975e-01,\n",
       "        -3.03175926e-01,  3.75982702e-01, -4.50099409e-02,\n",
       "         1.10274360e-01, -6.15603209e-01,  3.63788828e-02,\n",
       "         3.44000496e-02,  6.92492783e-01,  2.21066296e-01,\n",
       "         6.63190961e-01,  1.74034923e-01],\n",
       "       [-1.70902407e+00, -1.27343678e+00, -4.59253502e+00,\n",
       "         3.55972052e-01,  1.74450040e+00,  3.63842025e-02,\n",
       "        -6.22988820e-01, -2.34669566e-01,  2.45287466e+00,\n",
       "        -2.85817528e+00,  1.04133233e-01,  1.01795030e+00,\n",
       "         1.05238748e+00,  1.37575841e+00, -4.02653247e-01,\n",
       "         7.10144997e-01, -5.05049348e-01,  1.62047291e+00,\n",
       "         1.10524797e+00, -3.26512456e-01,  6.07064605e-01,\n",
       "        -1.57933307e+00,  2.08110237e+00,  1.04252911e+00,\n",
       "        -6.60348415e-01, -6.56398058e-01, -1.38009012e-01,\n",
       "         6.55312359e-01,  1.78191924e+00,  1.91673326e+00,\n",
       "        -3.30049396e-02,  4.34487849e-01],\n",
       "       [-5.67534208e-01,  3.94355625e-01, -3.58622146e+00,\n",
       "        -2.06572819e+00,  2.51902103e+00, -7.09398538e-02,\n",
       "        -2.68246561e-01, -8.54798615e-01,  5.74635267e+00,\n",
       "        -3.74317479e+00, -6.24756098e-01,  5.20627916e-01,\n",
       "         8.10429007e-02, -5.55260479e-01,  8.26617122e-01,\n",
       "        -9.65790510e-01,  4.22601312e-01,  5.24646878e-01,\n",
       "         6.67036057e-01,  6.25044703e-01,  1.38483334e+00,\n",
       "        -1.70337409e-01,  4.42913800e-01, -7.63292968e-01,\n",
       "         6.10008001e-01, -3.52532387e-01, -3.18218708e-01,\n",
       "         4.80409980e-01,  1.45358965e-01,  4.38593268e-01,\n",
       "        -6.03197992e-01,  1.78936124e-03],\n",
       "       [-6.52791917e-01, -1.16858828e+00, -1.83570242e+00,\n",
       "         1.57636225e+00,  5.16950905e-01, -1.59414038e-01,\n",
       "        -5.22131026e-02,  6.65882587e-01, -1.04177284e+00,\n",
       "         1.71836205e-02,  7.95375705e-01,  4.30378348e-01,\n",
       "         1.06402063e+00,  1.83877087e+00, -7.69486427e-01,\n",
       "         7.01884985e-01, -4.75427896e-01,  1.39890003e+00,\n",
       "         6.50545359e-01, -8.55379343e-01, -1.64937094e-01,\n",
       "        -1.21193886e+00,  1.43524122e+00,  1.13000929e+00,\n",
       "        -6.37450635e-01, -7.67389417e-01,  9.60646495e-02,\n",
       "         2.36304626e-01,  1.67763591e+00,  1.22139263e+00,\n",
       "         9.16903913e-01,  4.54551786e-01],\n",
       "       [ 6.65229857e-01, -1.90398395e-02,  1.94967449e-01,\n",
       "         6.58294082e-01,  3.72071147e-01, -2.76011586e-01,\n",
       "         5.33012152e-01,  7.40930736e-01, -8.38484585e-01,\n",
       "         8.52164447e-01,  7.20560551e-01, -2.17797160e-01,\n",
       "         4.70395803e-01,  7.96882689e-01, -1.63458765e-01,\n",
       "        -3.96125823e-01,  1.31159604e-01,  5.02844810e-01,\n",
       "         1.24658398e-01, -4.38389301e-01, -1.12304427e-01,\n",
       "        -1.07268982e-01,  6.29671812e-02, -7.93701857e-02,\n",
       "         1.56132936e-01, -6.58085465e-01,  1.38674289e-01,\n",
       "        -1.40779763e-01,  5.95273018e-01, -1.32407814e-01,\n",
       "         1.00614285e+00,  1.65082648e-01],\n",
       "       [ 1.73975694e+00,  7.42083013e-01,  2.08391047e+00,\n",
       "         3.28850627e-01,  6.80004954e-02, -4.05521631e-01,\n",
       "         1.02944958e+00,  9.72487152e-01, -1.40440679e+00,\n",
       "         2.00900841e+00,  8.24826598e-01, -7.53986835e-01,\n",
       "         9.41617489e-02,  2.42287070e-01,  1.52316928e-01,\n",
       "        -1.12704933e+00,  5.30225217e-01, -9.99651104e-02,\n",
       "        -3.00232649e-01, -2.88046867e-01, -2.23003522e-01,\n",
       "         6.71151280e-01, -9.65194702e-01, -8.36596429e-01,\n",
       "         6.89077318e-01, -6.27234221e-01,  2.07130939e-01,\n",
       "        -4.65336680e-01, -1.19615197e-01, -1.13250566e+00,\n",
       "         1.29669905e+00, -1.92466080e-02],\n",
       "       [ 4.41385061e-02, -9.92087796e-02, -1.38210273e+00,\n",
       "        -2.74077058e-04,  1.06946576e+00, -1.77604288e-01,\n",
       "         1.96074754e-01,  2.37049356e-01,  1.13427949e+00,\n",
       "        -7.74825394e-01,  3.26722324e-01,  1.42181098e-01,\n",
       "         4.80167478e-01,  5.76295853e-01,  2.02738568e-02,\n",
       "        -3.62682641e-01,  1.06423140e-01,  6.48872852e-01,\n",
       "         3.90190214e-01, -1.54749900e-01,  3.01242679e-01,\n",
       "        -3.49081427e-01,  4.62909818e-01, -8.48934203e-02,\n",
       "         1.23390600e-01, -5.88925004e-01, -2.43765116e-03,\n",
       "         1.14437088e-01,  6.77190185e-01,  3.20817590e-01,\n",
       "         4.80066001e-01,  1.65320724e-01],\n",
       "       [ 6.63368225e-01,  2.05279157e-01, -9.57305431e-02,\n",
       "         1.30832583e-01,  6.68432236e-01, -2.63900638e-01,\n",
       "         4.93712127e-01,  5.14991999e-01,  1.58729732e-01,\n",
       "         2.61876255e-01,  5.10427833e-01, -1.89810425e-01,\n",
       "         3.40227246e-01,  4.48597848e-01,  5.48388660e-02,\n",
       "        -6.27925992e-01,  2.55628020e-01,  3.69924307e-01,\n",
       "         1.33927330e-01, -2.06011504e-01,  9.49746519e-02,\n",
       "         2.84313783e-02, -5.91041148e-02, -3.47157508e-01,\n",
       "         3.05459648e-01, -5.88236213e-01,  7.98241422e-02,\n",
       "        -1.01920083e-01,  3.91417146e-01, -2.13894069e-01,\n",
       "         7.70168900e-01,  9.91294980e-02],\n",
       "       [ 1.43782544e+00,  2.88893431e-01,  1.87839961e+00,\n",
       "         9.75441575e-01, -1.73951894e-01, -4.01863188e-01,\n",
       "         9.27049398e-01,  1.14366484e+00, -2.31713152e+00,\n",
       "         2.33615947e+00,  1.01368546e+00, -6.25245512e-01,\n",
       "         3.33938062e-01,  7.73097157e-01, -1.69705778e-01,\n",
       "        -6.69873178e-01,  2.84629226e-01,  2.15628922e-01,\n",
       "        -1.82897687e-01, -5.70435286e-01, -4.14206773e-01,\n",
       "         3.05642396e-01, -5.66172004e-01, -3.24597150e-01,\n",
       "         3.63994479e-01, -7.12094903e-01,  2.46743038e-01,\n",
       "        -4.04978216e-01,  2.98511058e-01, -7.58329511e-01,\n",
       "         1.47633076e+00,  1.08879074e-01]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.98322241e-03,  1.24624174e-03, -8.86734575e-03,\n",
       "        -1.33664701e-02,  2.19804631e-03,  1.40512912e-02,\n",
       "        -2.01842021e-02, -1.48272878e-02, -1.00363810e-02,\n",
       "         4.16810438e-03],\n",
       "       [-2.98862299e-03,  1.12401508e-03,  1.98944081e-02,\n",
       "         8.91935173e-03, -1.72090791e-02,  1.18043087e-02,\n",
       "         5.85581455e-03, -1.30022550e-03,  1.13905175e-02,\n",
       "         7.59256352e-03],\n",
       "       [-2.46382388e-03, -1.32139903e-02,  2.27825576e-03,\n",
       "        -2.34090518e-02,  1.94362942e-02, -5.95725654e-03,\n",
       "         1.02149220e-02,  1.48489103e-02, -4.36442345e-03,\n",
       "        -4.79104137e-03],\n",
       "       [-1.12793921e-02, -1.19534284e-02, -1.76024586e-02,\n",
       "         1.57898106e-02, -2.07192414e-02,  1.61588797e-03,\n",
       "         2.66234428e-02,  2.25266698e-03,  2.02661511e-02,\n",
       "        -1.05920928e-02],\n",
       "       [-4.38706949e-03, -4.19018278e-03,  2.24646833e-03,\n",
       "         5.32911252e-03,  1.03885671e-02,  7.60490214e-03,\n",
       "         1.11821210e-02,  1.38774859e-02, -7.36943027e-03,\n",
       "        -7.51953013e-03],\n",
       "       [-6.91507617e-03, -1.07760588e-03,  2.88234465e-03,\n",
       "        -3.85171245e-03, -1.72694377e-03,  1.17509663e-02,\n",
       "         5.55035844e-03, -7.95367640e-03, -4.35122434e-04,\n",
       "        -9.25600342e-03],\n",
       "       [ 2.48422660e-03,  2.36123931e-02,  1.66672450e-02,\n",
       "         2.06559971e-02,  5.86561812e-03,  2.54219654e-03,\n",
       "        -5.93110500e-03, -1.33939544e-02, -6.55411743e-04,\n",
       "        -7.39420648e-04],\n",
       "       [ 3.24546010e-03,  1.26509252e-03, -3.05984006e-03,\n",
       "        -9.00986325e-03, -4.09054570e-03, -1.20243744e-03,\n",
       "         1.52058126e-02, -1.29829757e-02, -1.84978247e-02,\n",
       "         3.41316778e-03],\n",
       "       [ 1.72783006e-02,  4.45432682e-03,  2.67749536e-03,\n",
       "         1.31414086e-03, -1.73559040e-02,  9.07525769e-04,\n",
       "         2.60450765e-02,  2.10193335e-03,  1.03126490e-03,\n",
       "         1.15628606e-02],\n",
       "       [-8.07545381e-04,  6.74150433e-05, -2.49653626e-02,\n",
       "         3.18168255e-04, -1.81656182e-02,  1.95341632e-02,\n",
       "         4.62608878e-03, -5.65420929e-03, -8.37449823e-03,\n",
       "         5.07245678e-03]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_w_mean_topic_infer[:, :10, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.7616607e-01,  3.6224346e-03, -3.0754781e-01,  1.3990225e-01,\n",
       "        3.1391117e-01, -1.0993136e-01,  1.7876926e-01,  2.0730966e-01,\n",
       "        1.5003067e-01, -3.9625127e-02,  2.5470558e-01, -5.3844504e-02,\n",
       "        2.3800465e-01,  3.0333728e-01, -5.1428996e-02, -2.0459709e-01,\n",
       "        8.4517933e-02,  2.4236085e-01,  1.1645479e-01, -1.2791230e-01,\n",
       "        2.7641625e-05, -9.3722373e-02,  1.5119699e-01, -9.8118551e-02,\n",
       "        6.1052620e-02, -3.0996993e-01,  9.3275547e-02, -2.3359668e-02,\n",
       "        3.0665058e-01,  3.2136410e-02,  3.5929924e-01,  1.1415872e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enc_state_infer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-ee7c3cd147b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_enc_state_infer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_means_topic_infer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdebug_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menc_state_infer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans_topic_infer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'enc_state_infer' is not defined"
     ]
    }
   ],
   "source": [
    "_enc_state_infer, _means_topic_infer = debug_value([enc_state_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_enc_state_infer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_means_topic_infer[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
