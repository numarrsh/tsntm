{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from data_structure import get_batches\n",
    "from hntm import HierarchicalNeuralTopicModel\n",
    "from nhdp import nestedHierarchicalNeuralTopicModel\n",
    "from tsgntm import TreeStructuredGaussianNeuralTopicModel\n",
    "from tree import get_descendant_idxs\n",
    "from evaluation import get_hierarchical_affinity, get_topic_specialization, print_topic_sample, print_topic_year\n",
    "from configure import get_config\n",
    "\n",
    "pd.set_option('display.max_columns', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(nb_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu\n",
    "np.random.seed(config.seed)\n",
    "random.seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.path_data,'rb'))\n",
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)\n",
    "config.dim_bow = len(bow_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     6
    ]
   },
   "outputs": [],
   "source": [
    "def debug(variable, sample_batch=None):\n",
    "    if sample_batch is None: sample_batch = test_batches[0][1]\n",
    "    feed_dict = model.get_feed_dict(sample_batch, mode='test')\n",
    "    _variable = sess.run(variable, feed_dict=feed_dict)\n",
    "    return _variable\n",
    "\n",
    "def check(variable):\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sample_batch = test_batches[0]\n",
    "    feed_dict = model.get_feed_dict(sample_batch, mode='test', assertion=True)\n",
    "    _variable = sess.run(variable, feed_dict=feed_dict)\n",
    "    return _variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "TopicModels = {'hntm': HierarchicalNeuralTopicModel, 'nhdp': nestedHierarchicalNeuralTopicModel, 'tsgntm': TreeStructuredGaussianNeuralTopicModel}\n",
    "TopicModel = TopicModels[config.model]\n",
    "model = TopicModel(config)\n",
    "sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(max_to_keep=config.max_to_keep)\n",
    "update_tree_flg = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     24
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = []\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "ppl_min = np.inf\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','','','','','','VALID:','','','','','','TEST:','', 'SPEC:', '', '', 'HIER:', ''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL', 'GAUSS', 'REG','LOSS','PPL','NLL','KL', 'GAUSS','REG','LOSS','PPL', '1', '2', '3', 'CHILD', 'OTHER']]))))\n",
    "\n",
    "cmd_rm = 'rm -r %s' % config.dir_model\n",
    "res = subprocess.call(cmd_rm.split())\n",
    "cmd_mk = 'mkdir %s' % config.dir_model\n",
    "res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "def update_checkpoint(config, checkpoint, global_step):\n",
    "    checkpoint.append(config.path_model + '-%i' % global_step)\n",
    "    if len(checkpoint) > config.max_to_keep:\n",
    "        path_model = checkpoint.pop(0) + '.*'\n",
    "        for p in glob.glob(path_model):\n",
    "            os.remove(p)\n",
    "    cPickle.dump(checkpoint, open(config.path_checkpoint, 'wb'))\n",
    "    \n",
    "def validate(sess, batches, model):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    prob_topic_list = []\n",
    "    n_bow_list = []\n",
    "    n_topics_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = model.get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch, ppls_batch, prob_topic_batch, n_bow_batch, n_topics_batch \\\n",
    "            = sess.run([model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_gauss, model.topic_loss_reg, model.topic_ppls, model.prob_topic, model.n_bow, model.n_topics], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "        prob_topic_list.append(prob_topic_batch)\n",
    "        n_bow_list.append(n_bow_batch)\n",
    "        n_topics_list.append(n_topics_batch)\n",
    "    loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_gauss_mean, topic_loss_reg_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    \n",
    "    probs_topic = np.concatenate(prob_topic_list, 0)\n",
    "    \n",
    "    n_bow = np.concatenate(n_bow_list, 0)\n",
    "    n_topics = np.concatenate(n_topics_list, 0)\n",
    "    probs_topic_mean = np.sum(n_topics, 0) / np.sum(n_bow)\n",
    "    \n",
    "    return loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_gauss_mean, topic_loss_reg_mean, ppl_mean, probs_topic_mean    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train & validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th colspan=\"5\" halign=\"left\"></th>\n",
       "      <th>VALID:</th>\n",
       "      <th colspan=\"5\" halign=\"left\"></th>\n",
       "      <th>TEST:</th>\n",
       "      <th></th>\n",
       "      <th>SPEC:</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "      <th>HIER:</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>GAUSS</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>GAUSS</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>CHILD</th>\n",
       "      <th>OTHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>82</td>\n",
       "      <td>8</td>\n",
       "      <td>463</td>\n",
       "      <td>9301.03</td>\n",
       "      <td>2041</td>\n",
       "      <td>9264.60</td>\n",
       "      <td>13.58</td>\n",
       "      <td>22.73</td>\n",
       "      <td>0.17</td>\n",
       "      <td>9246.92</td>\n",
       "      <td>1885</td>\n",
       "      <td>9199.21</td>\n",
       "      <td>18.07</td>\n",
       "      <td>29.60</td>\n",
       "      <td>0.04</td>\n",
       "      <td>9247.98</td>\n",
       "      <td>1887</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>76</td>\n",
       "      <td>17</td>\n",
       "      <td>360</td>\n",
       "      <td>9242.42</td>\n",
       "      <td>1937</td>\n",
       "      <td>9199.08</td>\n",
       "      <td>16.13</td>\n",
       "      <td>27.14</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9214.19</td>\n",
       "      <td>1830</td>\n",
       "      <td>9161.10</td>\n",
       "      <td>19.32</td>\n",
       "      <td>33.75</td>\n",
       "      <td>0.03</td>\n",
       "      <td>9211.51</td>\n",
       "      <td>1826</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>78</td>\n",
       "      <td>26</td>\n",
       "      <td>257</td>\n",
       "      <td>9212.76</td>\n",
       "      <td>1888</td>\n",
       "      <td>9166.12</td>\n",
       "      <td>17.29</td>\n",
       "      <td>29.25</td>\n",
       "      <td>0.07</td>\n",
       "      <td>9197.67</td>\n",
       "      <td>1805</td>\n",
       "      <td>9144.15</td>\n",
       "      <td>19.62</td>\n",
       "      <td>33.88</td>\n",
       "      <td>0.02</td>\n",
       "      <td>9196.90</td>\n",
       "      <td>1803</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>77</td>\n",
       "      <td>35</td>\n",
       "      <td>154</td>\n",
       "      <td>9194.21</td>\n",
       "      <td>1858</td>\n",
       "      <td>9145.85</td>\n",
       "      <td>17.89</td>\n",
       "      <td>30.39</td>\n",
       "      <td>0.06</td>\n",
       "      <td>9187.79</td>\n",
       "      <td>1790</td>\n",
       "      <td>9134.36</td>\n",
       "      <td>19.68</td>\n",
       "      <td>33.72</td>\n",
       "      <td>0.03</td>\n",
       "      <td>9189.85</td>\n",
       "      <td>1793</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>76</td>\n",
       "      <td>44</td>\n",
       "      <td>51</td>\n",
       "      <td>9181.13</td>\n",
       "      <td>1837</td>\n",
       "      <td>9131.68</td>\n",
       "      <td>18.26</td>\n",
       "      <td>31.11</td>\n",
       "      <td>0.05</td>\n",
       "      <td>9183.84</td>\n",
       "      <td>1784</td>\n",
       "      <td>9130.38</td>\n",
       "      <td>19.68</td>\n",
       "      <td>33.76</td>\n",
       "      <td>0.02</td>\n",
       "      <td>9184.60</td>\n",
       "      <td>1786</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>77</td>\n",
       "      <td>52</td>\n",
       "      <td>515</td>\n",
       "      <td>9171.45</td>\n",
       "      <td>1822</td>\n",
       "      <td>9121.26</td>\n",
       "      <td>18.53</td>\n",
       "      <td>31.60</td>\n",
       "      <td>0.05</td>\n",
       "      <td>9180.21</td>\n",
       "      <td>1778</td>\n",
       "      <td>9126.44</td>\n",
       "      <td>19.90</td>\n",
       "      <td>33.85</td>\n",
       "      <td>0.03</td>\n",
       "      <td>9179.61</td>\n",
       "      <td>1777</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35000</th>\n",
       "      <td>79</td>\n",
       "      <td>61</td>\n",
       "      <td>412</td>\n",
       "      <td>9164.02</td>\n",
       "      <td>1810</td>\n",
       "      <td>9113.27</td>\n",
       "      <td>18.73</td>\n",
       "      <td>31.91</td>\n",
       "      <td>0.04</td>\n",
       "      <td>9175.18</td>\n",
       "      <td>1771</td>\n",
       "      <td>9121.55</td>\n",
       "      <td>19.83</td>\n",
       "      <td>33.78</td>\n",
       "      <td>0.02</td>\n",
       "      <td>9177.27</td>\n",
       "      <td>1774</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40000</th>\n",
       "      <td>79</td>\n",
       "      <td>70</td>\n",
       "      <td>309</td>\n",
       "      <td>9157.99</td>\n",
       "      <td>1801</td>\n",
       "      <td>9106.87</td>\n",
       "      <td>18.90</td>\n",
       "      <td>32.13</td>\n",
       "      <td>0.04</td>\n",
       "      <td>9176.32</td>\n",
       "      <td>1774</td>\n",
       "      <td>9123.11</td>\n",
       "      <td>20.08</td>\n",
       "      <td>33.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>9177.27</td>\n",
       "      <td>1774</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45000</th>\n",
       "      <td>76</td>\n",
       "      <td>79</td>\n",
       "      <td>206</td>\n",
       "      <td>9152.78</td>\n",
       "      <td>1793</td>\n",
       "      <td>9101.42</td>\n",
       "      <td>19.03</td>\n",
       "      <td>32.26</td>\n",
       "      <td>0.04</td>\n",
       "      <td>9173.79</td>\n",
       "      <td>1770</td>\n",
       "      <td>9120.72</td>\n",
       "      <td>20.11</td>\n",
       "      <td>32.93</td>\n",
       "      <td>0.02</td>\n",
       "      <td>9175.14</td>\n",
       "      <td>1773</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000</th>\n",
       "      <td>75</td>\n",
       "      <td>88</td>\n",
       "      <td>103</td>\n",
       "      <td>9148.35</td>\n",
       "      <td>1786</td>\n",
       "      <td>9096.79</td>\n",
       "      <td>19.14</td>\n",
       "      <td>32.35</td>\n",
       "      <td>0.04</td>\n",
       "      <td>9171.75</td>\n",
       "      <td>1767</td>\n",
       "      <td>9118.39</td>\n",
       "      <td>20.03</td>\n",
       "      <td>33.31</td>\n",
       "      <td>0.02</td>\n",
       "      <td>9173.18</td>\n",
       "      <td>1768</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55000</th>\n",
       "      <td>77</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>9144.48</td>\n",
       "      <td>1781</td>\n",
       "      <td>9092.74</td>\n",
       "      <td>19.23</td>\n",
       "      <td>32.40</td>\n",
       "      <td>0.04</td>\n",
       "      <td>9171.57</td>\n",
       "      <td>1768</td>\n",
       "      <td>9118.70</td>\n",
       "      <td>20.16</td>\n",
       "      <td>32.69</td>\n",
       "      <td>0.03</td>\n",
       "      <td>9173.18</td>\n",
       "      <td>1768</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60000</th>\n",
       "      <td>79</td>\n",
       "      <td>105</td>\n",
       "      <td>464</td>\n",
       "      <td>9141.22</td>\n",
       "      <td>1776</td>\n",
       "      <td>9089.35</td>\n",
       "      <td>19.31</td>\n",
       "      <td>32.44</td>\n",
       "      <td>0.04</td>\n",
       "      <td>9172.06</td>\n",
       "      <td>1768</td>\n",
       "      <td>9119.72</td>\n",
       "      <td>20.01</td>\n",
       "      <td>32.31</td>\n",
       "      <td>0.02</td>\n",
       "      <td>9173.18</td>\n",
       "      <td>1768</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       TRAIN:                                      VALID:  \\\n",
       "      Time   Ep   Ct     LOSS   PPL      NLL     KL  GAUSS   REG     LOSS   \n",
       "5000    82    8  463  9301.03  2041  9264.60  13.58  22.73  0.17  9246.92   \n",
       "10000   76   17  360  9242.42  1937  9199.08  16.13  27.14  0.10  9214.19   \n",
       "15000   78   26  257  9212.76  1888  9166.12  17.29  29.25  0.07  9197.67   \n",
       "20000   77   35  154  9194.21  1858  9145.85  17.89  30.39  0.06  9187.79   \n",
       "25000   76   44   51  9181.13  1837  9131.68  18.26  31.11  0.05  9183.84   \n",
       "30000   77   52  515  9171.45  1822  9121.26  18.53  31.60  0.05  9180.21   \n",
       "35000   79   61  412  9164.02  1810  9113.27  18.73  31.91  0.04  9175.18   \n",
       "40000   79   70  309  9157.99  1801  9106.87  18.90  32.13  0.04  9176.32   \n",
       "45000   76   79  206  9152.78  1793  9101.42  19.03  32.26  0.04  9173.79   \n",
       "50000   75   88  103  9148.35  1786  9096.79  19.14  32.35  0.04  9171.75   \n",
       "55000   77   97    0  9144.48  1781  9092.74  19.23  32.40  0.04  9171.57   \n",
       "60000   79  105  464  9141.22  1776  9089.35  19.31  32.44  0.04  9172.06   \n",
       "\n",
       "                                            TEST:       SPEC:              \\\n",
       "        PPL      NLL     KL  GAUSS   REG     LOSS   PPL     1     2     3   \n",
       "5000   1885  9199.21  18.07  29.60  0.04  9247.98  1887  0.17  0.30  0.37   \n",
       "10000  1830  9161.10  19.32  33.75  0.03  9211.51  1826  0.19  0.28  0.41   \n",
       "15000  1805  9144.15  19.62  33.88  0.02  9196.90  1803  0.12  0.26  0.44   \n",
       "20000  1790  9134.36  19.68  33.72  0.03  9189.85  1793  0.13  0.32  0.45   \n",
       "25000  1784  9130.38  19.68  33.76  0.02  9184.60  1786  0.16  0.31  0.44   \n",
       "30000  1778  9126.44  19.90  33.85  0.03  9179.61  1777  0.15  0.36  0.45   \n",
       "35000  1771  9121.55  19.83  33.78  0.02  9177.27  1774  0.16  0.33  0.47   \n",
       "40000  1774  9123.11  20.08  33.11  0.02  9177.27  1774  0.20  0.35  0.45   \n",
       "45000  1770  9120.72  20.11  32.93  0.02  9175.14  1773  0.16  0.39  0.47   \n",
       "50000  1767  9118.39  20.03  33.31  0.02  9173.18  1768  0.20  0.41  0.45   \n",
       "55000  1768  9118.70  20.16  32.69  0.03  9173.18  1768  0.27  0.36  0.46   \n",
       "60000  1768  9119.72  20.01  32.31  0.02  9173.18  1768  0.20  0.35  0.46   \n",
       "\n",
       "      HIER:        \n",
       "      CHILD OTHER  \n",
       "5000   0.73  0.44  \n",
       "10000  0.66  0.35  \n",
       "15000  0.59  0.29  \n",
       "20000  0.60  0.35  \n",
       "25000  0.59  0.31  \n",
       "30000  0.53  0.28  \n",
       "35000  0.55  0.27  \n",
       "40000  0.55  0.28  \n",
       "45000  0.53  0.26  \n",
       "50000  0.52  0.27  \n",
       "55000  0.55  0.27  \n",
       "60000  0.49  0.25  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 R: 1.000 P: 0.024 level analysis english systems domain lexical linguistic test process lexicon\n",
      "   1 R: 0.227 P: 0.063 grammar structure rules semantic form representation rule lexical type structures\n",
      "     11 R: 0.090 P: 0.090 verb semantic syntactic verbs event relations discourse argument relation arguments\n",
      "     12 R: 0.074 P: 0.074 tree parsing dependency parser grammar trees node parse rules syntactic\n",
      "   2 R: 0.207 P: 0.070 english languages dictionary errors morphological chinese lexical rules corpora pos\n",
      "     21 R: 0.070 P: 0.070 entity sense relations relation semantic entities wordnet senses lexical patterns\n",
      "     22 R: 0.066 P: 0.066 translation english source alignment phrase target languages sentences systems parallel\n",
      "   3 R: 0.218 P: 0.060 models probability training algorithm probabilities search parameters sequence segmentation gram\n",
      "     31 R: 0.100 P: 0.100 features feature training performance learning classifier classification accuracy test class\n",
      "     32 R: 0.058 P: 0.058 models network vector embeddings neural training vectors representations embedding input\n",
      "   4 R: 0.141 P: 0.005 knowledge semantic terms term natural values context type representation concept\n",
      "     41 R: 0.066 P: 0.066 question query terms documents answer questions document domain term web\n",
      "     42 R: 0.070 P: 0.070 similarity topic document sentences method score scores measure clustering pairs\n",
      "   5 R: 0.183 P: 0.054 user annotation resources project tools tool database research interface linguistic\n",
      "     51 R: 0.065 P: 0.065 speech dialogue user speaker utterance utterances recognition spoken human speakers\n",
      "     52 R: 0.064 P: 0.064 sentiment tweets polarity twitter negative opinion positive social emotion annotators\n",
      "[ -6.160983 -39.796894 -28.63721  -28.080662 -17.295479 -26.55632\n",
      " -61.08841  -58.927925 -52.863724 -55.76008  -54.884583 -52.796886\n",
      " -45.185528 -48.437195 -53.50087  -52.298462]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3c2a1a9b3f54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_recon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_kl_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_gauss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_reg_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppls_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_log\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_loss_recon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_loss_kl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_loss_gauss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_loss_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_ppls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tsntm/tsgntm.py\u001b[0m in \u001b[0;36mget_feed_dict\u001b[0;34m(self, batch, mode)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbow\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0mkeep_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_prob\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         feed_dict = {\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "while epoch < config.n_epochs:\n",
    "    # train\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = model.get_feed_dict(batch)\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch, ppls_batch, global_step_log = \\\n",
    "        sess.run([model.opt, model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_gauss, model.topic_loss_reg, model.topic_ppls, tf.train.get_global_step()], feed_dict = feed_dict)\n",
    "\n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_gauss_batch, topic_loss_reg_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if global_step_log % config.log_period == 0:\n",
    "            # validate\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_gauss_train, topic_loss_reg_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_gauss_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "\n",
    "            # test\n",
    "            if ppl_dev < ppl_min:\n",
    "                ppl_min = ppl_dev\n",
    "                loss_test, _, _, _, _, ppl_test, _ = validate(sess, test_batches, model)\n",
    "                saver.save(sess, config.path_model, global_step=global_step_log)\n",
    "                cPickle.dump(config, open(config.path_config % global_step_log, 'wb'))\n",
    "                update_checkpoint(config, checkpoint, global_step_log)\n",
    "            \n",
    "            # visualize topic\n",
    "            topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "            topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "            topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "            topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "            descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "            recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "            \n",
    "            depth_specs = get_topic_specialization(sess, model, instances_test)\n",
    "            hierarchical_affinities = get_hierarchical_affinity(sess, model)\n",
    "            \n",
    "            # log\n",
    "            clear_output()\n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_gauss_train, '%.2f'%topic_loss_reg_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_gauss_dev, '%.2f'%topic_loss_reg_dev, \\\n",
    "                    '%.2f'%loss_test, '%.0f'%ppl_test, \\\n",
    "                    '%.2f'%depth_specs[1], '%.2f'%depth_specs[2], '%.2f'%depth_specs[3], \\\n",
    "                    '%.2f'%hierarchical_affinities[0], '%.2f'%hierarchical_affinities[1]],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "            cPickle.dump(log_df, open(os.path.join(config.path_log), 'wb'))\n",
    "            print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)\n",
    "            print(np.sum(debug(model.topic_logvars), 1))\n",
    "            \n",
    "            # update tree\n",
    "            if not config.static:\n",
    "                config.tree_idxs, update_tree_flg = model.update_tree(topic_prob_topic, recur_prob_topic)\n",
    "                if update_tree_flg:\n",
    "                    print(config.tree_idxs)\n",
    "                    name_variables = {tensor.name: variable for tensor, variable in zip(tf.global_variables(), sess.run(tf.global_variables()))} # store paremeters\n",
    "                    if 'sess' in globals(): sess.close()\n",
    "                    model = HierarchicalNeuralTopicModel(config)\n",
    "                    sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "                    name_tensors = {tensor.name: tensor for tensor in tf.global_variables()}\n",
    "                    sess.run([name_tensors[name].assign(variable) for name, variable in name_variables.items()]) # restore parameters\n",
    "                    saver = tf.train.Saver(max_to_keep=1)\n",
    "                \n",
    "            time_start = time.time()\n",
    "\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "    epoch += 1\n",
    "\n",
    "loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, model)\n",
    "topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "display(log_df)\n",
    "print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_year(sample_batches):\n",
    "    probs_topics = []\n",
    "    years = []\n",
    "    for i, sample_batch in sample_batches:\n",
    "        probs_topics_batch = sess.run(model.prob_topic, feed_dict=model.get_feed_dict(sample_batch, mode='test'))\n",
    "        years_batch = [instance.year for instance in sample_batch]\n",
    "        probs_topics += [probs_topics_batch]\n",
    "        years += years_batch\n",
    "    probs_topics = np.concatenate(probs_topics)\n",
    "    years = np.array(years)\n",
    "\n",
    "    topic_years = years.dot(probs_topics) / np.sum(probs_topics, 0)\n",
    "    topic_year = {model.topic_idxs[i]: year for i, year in enumerate(topic_years)}\n",
    "    return topic_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Avg Year: 2005 level analysis english systems domain lexical linguistic test process lexicon\n",
      "   1 Avg Year: 2000 grammar structure rules semantic form representation rule lexical type structures\n",
      "     11 Avg Year: 2006 verb semantic syntactic verbs event relations discourse argument relation arguments\n",
      "     12 Avg Year: 2007 tree parsing dependency parser grammar trees node parse rules syntactic\n",
      "   2 Avg Year: 2008 english languages dictionary errors morphological chinese lexical rules corpora pos\n",
      "     21 Avg Year: 2009 entity sense relations relation semantic entities wordnet senses lexical patterns\n",
      "     22 Avg Year: 2010 translation english source alignment phrase target languages sentences systems parallel\n",
      "   3 Avg Year: 2008 models probability training algorithm probabilities search parameters sequence segmentation gram\n",
      "     31 Avg Year: 2011 features feature training performance learning classifier classification accuracy test class\n",
      "     32 Avg Year: 2015 models network vector embeddings neural training vectors representations embedding input\n",
      "   4 Avg Year: 2007 knowledge semantic terms term natural values context type representation concept\n",
      "     41 Avg Year: 2009 question query terms documents answer questions document domain term web\n",
      "     42 Avg Year: 2010 similarity topic document sentences method score scores measure clustering pairs\n",
      "   5 Avg Year: 2007 user annotation resources project tools tool database research interface linguistic\n",
      "     51 Avg Year: 2007 speech dialogue user speaker utterance utterances recognition spoken human speakers\n",
      "     52 Avg Year: 2013 sentiment tweets polarity twitter negative opinion positive social emotion annotators\n"
     ]
    }
   ],
   "source": [
    "sample_batches = get_batches(instances_train, config.batch_size)\n",
    "topic_year = get_topic_year(sample_batches)\n",
    "print_topic_year(sess, model, topic_freq_tokens=topic_freq_tokens, topic_year=topic_year)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
