{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from data_structure import get_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "from topic_model import TopicModel\n",
    "\n",
    "from topic_beam_search_decoder import BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '4', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/bags/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/tglm_vae', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'bags', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 50, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 500, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.1, 'lr')\n",
    "flags.DEFINE_float('reg', 1., 'regularization term')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_bool('warmup', True, 'flg of warming up')\n",
    "flags.DEFINE_integer('epochs_cycle', 5, 'number of epochs within a cycle')\n",
    "flags.DEFINE_float('r_cycle', 0.5, 'proportion used to increase beta within a cycle')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 10, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_bow', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_hidden_topic', 512, 'dim_hidden_topic')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "flags.DEFINE_bool('bidirectional', True, 'flg of bidirectional encoding')\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('PAD_IDX', word_to_idx[PAD], 'PAD_IDX')\n",
    "flags.DEFINE_integer('UNK_IDX', word_to_idx[UNK], 'UNK_IDX')\n",
    "flags.DEFINE_integer('BOS_IDX', word_to_idx[BOS], 'BOS_IDX')\n",
    "flags.DEFINE_integer('EOS_IDX', word_to_idx[EOS], 'EOS_IDX')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "maximum_iterations = max([max([instance.max_sent_l for instance in batch]) for ct, batch in dev_batches])\n",
    "flags.DEFINE_integer('maximum_iterations', maximum_iterations, 'maximum_iterations')\n",
    "\n",
    "flags.DEFINE_integer('cycle_steps', len(train_batches)*config.epochs_cycle, 'number of steps for each cycle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build language model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['bow'] = tf.placeholder(tf.float32, [None, config.dim_bow], name='bow')\n",
    "t_variables['input_token_idxs'] = tf.placeholder(tf.int32, [None, None], name='input_token_idxs')\n",
    "t_variables['dec_input_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_input_idxs')\n",
    "t_variables['dec_target_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_target_idxs')\n",
    "t_variables['batch_l'] = tf.placeholder(tf.int32, name='batch_l')\n",
    "t_variables['doc_l'] = tf.placeholder(tf.int32, [None], name='doc_l')\n",
    "t_variables['sent_l'] = tf.placeholder(tf.int32, [None], name='sent_l')\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train', assertion=False):\n",
    "    def token_dropout(sent_idxs):\n",
    "        sent_idxs_dropout = np.asarray(sent_idxs)\n",
    "        sent_idxs_dropout[np.random.rand(len(sent_idxs)) > config.word_keep_prob] = config.UNK_IDX\n",
    "        return list(sent_idxs_dropout)\n",
    "\n",
    "    bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "    \n",
    "    doc_l = np.array([len(instance.token_idxs) for instance in batch])\n",
    "    \n",
    "    feed_input_token_idxs_list = [sent_idxs for instance in batch for sent_idxs in instance.token_idxs]\n",
    "    feed_dec_input_idxs_list = [[config.BOS_IDX] + token_dropout(sent_idxs) for sent_idxs in feed_input_token_idxs_list]\n",
    "    feed_dec_target_idxs_list = [sent_idxs + [config.EOS_IDX]  for sent_idxs in feed_input_token_idxs_list]\n",
    "        \n",
    "    sent_l = np.array([len(sent_idxs) for sent_idxs in feed_input_token_idxs_list], np.int32)\n",
    "    batch_l = len(sent_l)\n",
    "    \n",
    "    feed_input_token_idxs = pad_sequences(feed_input_token_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_input_idxs = pad_sequences(feed_dec_input_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_target_idxs = pad_sequences(feed_dec_target_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    \n",
    "    if assertion:\n",
    "        index = 0\n",
    "        for instance in batch:\n",
    "            for line_idxs in instance.token_idxs:\n",
    "                assert feed_input_token_idxs_list[index] == line_idxs\n",
    "                index += 1\n",
    "        assert feed_input_token_idxs.shape[1] == np.max(sent_l)\n",
    "        assert feed_dec_input_idxs.shape[1] == np.max(sent_l) + 1\n",
    "        assert feed_dec_target_idxs.shape[1] == np.max(sent_l) + 1\n",
    "    \n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "\n",
    "    feed_dict = {\n",
    "                t_variables['bow']: bow, \n",
    "                t_variables['batch_l']: batch_l, t_variables['doc_l']: doc_l, t_variables['sent_l']: sent_l, \n",
    "                t_variables['input_token_idxs']: feed_input_token_idxs, t_variables['dec_input_idxs']: feed_dec_input_idxs, t_variables['dec_target_idxs']: feed_dec_target_idxs, \n",
    "                t_variables['keep_prob']: keep_prob\n",
    "    }\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     10,
     24,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "def check_shape(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()\n",
    "    \n",
    "def check_value(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()    \n",
    "    \n",
    "# sent_loss_kl_categ_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, tf_log(prob_topic_infer/prob_topic_sents)), 1))\n",
    "# debug_value([sent_loss_kl_categ, sent_loss_kl_categ_tmp])\n",
    "# sent_loss_kl_gauss_tmp = 0.5 * tf.reduce_sum(tf.exp(logvars_topic_infer-logvars_topic) + tf.square(means_topic - means_topic_infer) / tf.exp(logvars_topic) - 1 + (logvars_topic - logvars_topic_infer), -1)\n",
    "# sent_loss_kl_gmm_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss_tmp), -1))\n",
    "# debug_value([sent_loss_kl_gmm_tmp, sent_loss_kl_gmm])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encode bow\n",
    "with tf.variable_scope('topic/enc', reuse=False):\n",
    "    hidden_bow_ = tf.layers.Dense(units=config.dim_hidden_bow, activation=tf.nn.relu, name='hidden_bow')(t_variables['bow'])\n",
    "    hidden_bow = tf.layers.Dropout(t_variables['keep_prob'])(hidden_bow_)\n",
    "    means_bow = tf.layers.Dense(units=config.dim_latent_bow, name='mean_bow')(hidden_bow)\n",
    "    logvars_bow = tf.layers.Dense(units=config.dim_latent_bow, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_bow)\n",
    "    latents_bow = sample_latents(means_bow, logvars_bow) # sample latent vectors\n",
    "\n",
    "    prob_topic = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic')(latents_bow) # inference of topic probabilities\n",
    "\n",
    "# decode bow\n",
    "with tf.variable_scope('shared', reuse=False):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "bow_embeddings = tf.nn.embedding_lookup(embeddings, bow_idxs) # embeddings of each bow features\n",
    "\n",
    "with tf.variable_scope('topic/dec', reuse=False):\n",
    "    topic_embeddings = tf.get_variable('topic_emb', [config.n_topic, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of topics\n",
    "\n",
    "    topic_bow = tf.nn.softmax(tf.matmul(topic_embeddings, bow_embeddings, transpose_b=True), 1) # bow vectors for each topic\n",
    "    logits_bow = tf_log(tf.matmul(prob_topic, topic_bow)) # predicted bow distribution\n",
    "\n",
    "    # prior of each gaussian distribution (computed for each topic)\n",
    "    hidden_topic = tf.layers.Dense(units=config.dim_hidden_topic, activation=tf.nn.relu, name='hidden_topic')(topic_bow)\n",
    "    means_topic = tf.layers.Dense(units=config.dim_latent, name='mean_topic')(hidden_topic)\n",
    "    logvars_topic = tf.layers.Dense(units=config.dim_latent, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_topic)\n",
    "    sigma_topic = tf.exp(0.5 * logvars_topic)\n",
    "    gauss_topic = tfd.Normal(loc=means_topic, scale=sigma_topic)    \n",
    "    \n",
    "# define losses\n",
    "topic_losses_recon = -tf.reduce_sum(tf.multiply(t_variables['bow'], logits_bow), 1)\n",
    "topic_loss_recon = tf.reduce_mean(topic_losses_recon) # negative log likelihood of each words\n",
    "\n",
    "topic_loss_kl = compute_kl_loss(means_bow, logvars_bow) # KL divergence b/w latent dist & gaussian std\n",
    "\n",
    "topic_bow_norm = topic_bow / tf.norm(topic_bow, axis=1, keepdims=True)\n",
    "topic_dots = tf.clip_by_value(tf.matmul(topic_bow_norm, tf.transpose(topic_bow_norm)), -1., 1.)\n",
    "topic_loss_reg = tf.reduce_mean(tf.square(topic_dots - tf.eye(config.n_topic)))\n",
    "# topic_angles = tf.acos(topic_dots)\n",
    "# topic_angles_mean = tf.reduce_mean(topic_angles)\n",
    "# topic_angles_vars = tf.reduce_mean(tf.square(topic_angles - topic_angles_mean))\n",
    "# topic_loss_reg = tf.exp(topic_angles_vars - topic_angles_mean)\n",
    "\n",
    "# monitor\n",
    "n_bow = tf.reduce_sum(t_variables['bow'], 1)\n",
    "topic_ppls = tf.divide(topic_losses_recon, tf.maximum(1e-5, n_bow))\n",
    "topics_freq_bow_indices = tf.nn.top_k(topic_bow, 10, name='topic_freq_bow').indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "input_token_idxs = t_variables['input_token_idxs']\n",
    "batch_l = t_variables['batch_l']\n",
    "sent_l = t_variables['sent_l']\n",
    "max_sent_l = tf.reduce_max(sent_l)\n",
    "\n",
    "with tf.variable_scope('sent/enc', reuse=False):\n",
    "    # get word embedding\n",
    "    enc_input = tf.nn.embedding_lookup(embeddings, input_token_idxs)\n",
    "\n",
    "    # get sentence embedding\n",
    "    _, enc_state = dynamic_bi_rnn(enc_input, sent_l, config.dim_hidden, t_variables['keep_prob'])\n",
    "\n",
    "    # TODO House Holder flow\n",
    "    hidden_topic_infer =  tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='hidden_topic_infer')(enc_state)\n",
    "    prob_topic_infer = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic_infer')(hidden_topic_infer)\n",
    "\n",
    "    w_mean_topic_infer = tf.get_variable('mean_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32)\n",
    "    b_mean_topic_infer = tf.get_variable('mean_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32)\n",
    "    means_topic_infer = tf.tensordot(enc_state, w_mean_topic_infer, axes=[[1], [1]]) + b_mean_topic_infer\n",
    "    \n",
    "    w_logvar_topic_infer = tf.get_variable('logvar_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    b_logvar_topic_infer = tf.get_variable('logvar_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    logvars_topic_infer = tf.tensordot(enc_state, w_logvar_topic_infer, axes=[[1], [1]]) + b_logvar_topic_infer\n",
    "    sigma_topic_infer = tf.exp(0.5 * logvars_topic_infer)\n",
    "    gauss_topic_infer = tfd.Normal(loc=means_topic_infer, scale=sigma_topic_infer)\n",
    "    \n",
    "    # latent vectors from each gaussian dist.\n",
    "    latents_topic_infer = sample_latents(means_topic_infer, logvars_topic_infer) \n",
    "    # latent vector from gaussian mixture\n",
    "    latents_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), latents_topic_infer, transpose_a=True)\n",
    "    \n",
    "    # for beam search\n",
    "    means_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), means_topic_infer, transpose_a=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for decoding\n",
    "dec_sent_l = tf.add(sent_l, 1)\n",
    "dec_input_idxs = t_variables['dec_input_idxs']\n",
    "dec_input = tf.nn.embedding_lookup(embeddings, dec_input_idxs)\n",
    "\n",
    "dec_latents_input = tf.tile(latents_input, [1, tf.shape(dec_input)[1], 1])\n",
    "dec_concat_input = tf.concat([dec_input, dec_latents_input], -1)\n",
    "\n",
    "# decode for training\n",
    "with tf.variable_scope('sent/dec/rnn', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32, reuse=False):\n",
    "    dec_cell = tf.contrib.rnn.GRUCell(config.dim_hidden)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob = t_variables['keep_prob'])\n",
    "\n",
    "    dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(latents_input, 1))\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_concat_input, sequence_length=dec_sent_l)\n",
    "\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=dec_cell,\n",
    "        helper=helper,\n",
    "        initial_state=dec_initial_state)\n",
    "\n",
    "    dec_outputs, _, output_sent_l = tf.contrib.seq2seq.dynamic_decode(train_decoder)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(config.n_vocab, use_bias=False, name='out')\n",
    "    output_logits = output_layer(dec_outputs.rnn_output)\n",
    "    \n",
    "    output_token_idxs = tf.argmax(output_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tokens = tf.fill([batch_l], config.BOS_IDX)\n",
    "end_token = config.EOS_IDX\n",
    "\n",
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    infer_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(means_input, 1))\n",
    "    beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(infer_dec_initial_state, multiplier=config.beam_width)\n",
    "    beam_latents_input = tf.contrib.seq2seq.tile_batch(tf.squeeze(means_input, 1), multiplier=config.beam_width) # added\n",
    "    \n",
    "    beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=beam_latents_input)\n",
    "\n",
    "    beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    beam_output_token_idxs = beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    inter_means_input = tf.placeholder(tf.float32, [None, config.dim_latent])\n",
    "    \n",
    "    inter_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(inter_means_input)\n",
    "    inter_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(inter_dec_initial_state, multiplier=config.beam_width)\n",
    "    inter_beam_latents_input = tf.contrib.seq2seq.tile_batch(inter_means_input, multiplier=config.beam_width) # added\n",
    "    \n",
    "    inter_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=inter_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=inter_beam_latents_input)\n",
    "\n",
    "    inter_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inter_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    inter_beam_output_token_idxs = inter_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    topic_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic)\n",
    "    topic_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(topic_dec_initial_state, multiplier=config.beam_width)\n",
    "    topic_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic, multiplier=config.beam_width) # added\n",
    "    \n",
    "    topic_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=topic_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=topic_beam_latents_input)\n",
    "\n",
    "    topic_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        topic_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    topic_beam_output_token_idxs = topic_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Topic Samples-----------\n",
      "0  bow: ! love color perfect recommend highly perfectly absolutely ordered awesome\n",
      "0  sent: love love ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "1  bow: bottom top plastic speck back part piece corners feet rubber\n",
      "1  sent: the only problem i have is that the bottom part of the case is not a little loose\n",
      "2  bow: zipper months strap handle years broke year zippers straps back\n",
      "2  sent: the only reason i did n't give it # stars is because it is a little heavy\n",
      "3  bow: pocket room ipad perfect mouse power small charger perfectly carry\n",
      "3  sent: there is plenty of room for the power cord , mouse , power cord , mouse , and a few other small items\n",
      "4  bow: carry pockets strap room back shoulder space comfortable plenty compartment\n",
      "4  sent: plenty of room for my laptop , files , and other items\n",
      "5  bow: nylon sewn floor closure test waterproof empty average trust thinner\n",
      "5  sent: the only reason i did n't give it # stars is because i wish it had a little more room for the charger\n",
      "6  bow: ; & pro big size air perfectly small laptops made\n",
      "6  sent: it fits my # . # inch laptop perfectly\n",
      "7  bow: quality bought price 'm ... buy time $ made 've\n",
      "7  sent: i 've had it for about # months now and it has already broken off\n",
      "8  bow: cover color keyboard mac pro apple perfectly hard easy air\n",
      "8  sent: the keyboard cover fits perfectly and the keyboard cover is a little loose\n",
      "9  bow: sleeve protection air inside inch pro protect bit nice zipper\n",
      "9  sent: the neoprene sleeve is a little tight for the macbook air # `` , but it does n't fit the charger in the sleeve\n"
     ]
    }
   ],
   "source": [
    "print_topic_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language modeling cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target and mask\n",
    "dec_target_idxs = t_variables['dec_target_idxs']\n",
    "dec_mask_tokens = tf.sequence_mask(dec_sent_l, maxlen=max_sent_l+1, dtype=tf.float32)\n",
    "\n",
    "# nll for each token (averaged over batch & sentence)\n",
    "sent_loss_recon = tf.contrib.seq2seq.sequence_loss(output_logits, dec_target_idxs, dec_mask_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_l = t_variables['doc_l']\n",
    "mask_sents = tf.sequence_mask(doc_l)\n",
    "mask_sents_flatten = tf.reshape(mask_sents, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1]])\n",
    "\n",
    "prob_topic_tiled = tf.tile(tf.expand_dims(prob_topic, 1), [1, tf.shape(mask_sents)[1], 1])\n",
    "prob_topic_flatten = tf.reshape(prob_topic_tiled, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1], config.n_topic])\n",
    "prob_topic_sents = tf.boolean_mask(prob_topic_flatten, mask_sents_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferred mixture probabilities (computed for each sentence)\n",
    "categ_topic_infer = tfd.Categorical(probs=prob_topic_infer)\n",
    "\n",
    "# prior of mixture probabilities (computed for each document, tiled for each sentence)\n",
    "categ_topic = tfd.Categorical(probs=prob_topic_sents)\n",
    "\n",
    "sent_loss_kl_categ = tf.reduce_mean(tfd.kl_divergence(categ_topic_infer, categ_topic))\n",
    "\n",
    "# inference of each gaussian gaussribution (computed for each sentence)\n",
    "\n",
    "sent_loss_kl_gauss = tf.reduce_sum(tfd.kl_divergence(gauss_topic_infer, gauss_topic), -1)\n",
    "sent_loss_kl_gmm = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss), -1))\n",
    "\n",
    "sent_loss_kl = sent_loss_kl_categ + sent_loss_kl_gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "tau = tf.cast(tf.divide(tf.mod(global_step, tf.constant(config.cycle_steps)), tf.constant(config.cycle_steps)), dtype=tf.float32)\n",
    "beta = tf.minimum(1., tau/config.r_cycle)\n",
    "\n",
    "sent_loss = sent_loss_recon + beta * sent_loss_kl\n",
    "\n",
    "topic_loss = topic_loss_recon + topic_loss_kl + config.reg * topic_loss_reg\n",
    "loss = topic_loss + sent_loss\n",
    "\n",
    "# define optimizer\n",
    "if config.opt == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif config.opt == 'Adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "    \n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_sents(token_idxs, config, idx_to_word):\n",
    "    sents = []\n",
    "    for sent_idxs in token_idxs:\n",
    "        tokens = []\n",
    "        for idx in sent_idxs:\n",
    "            if idx == config.EOS_IDX: break\n",
    "            tokens.append(idx_to_word[idx])\n",
    "        sent = ' '.join(tokens)\n",
    "        sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, ppls_batch \\\n",
    "            = sess.run([loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, topic_ppls], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "    loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, sent_loss_recon_mean, sent_loss_kl_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    return loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, sent_loss_recon_mean, sent_loss_kl_mean, ppl_mean\n",
    "\n",
    "def get_all_losses(sess, batches):\n",
    "    losses = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch = \\\n",
    "        sess.run([loss, topic_loss_recon, topic_loss_kl, sent_loss_recon, sent_loss_kl], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "    print('LOSS %.2f | TM NLL: %.2f, KL: %.4f | LM NLL: %.2f, KL: %.4f' %  np.mean(losses, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_batch):\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    pred_token_idxs = sess.run(output_token_idxs, feed_dict = feed_dict)\n",
    "    true_token_idxs = [sent_idxs for instance in sample_batch for sent_idxs in instance.token_idxs]\n",
    "    \n",
    "    assert len(pred_token_idxs) == len(true_token_idxs)\n",
    "    \n",
    "    pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "    true_sents = idxs_to_sents(true_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    for i, (true_sent, pred_sent) in enumerate(zip(true_sents, pred_sents)):        \n",
    "        print(i, 'True: %s' % true_sent)\n",
    "        print(i, 'Pred: %s' % pred_sent)\n",
    "\n",
    "def print_topic_sample():\n",
    "    pred_topics_freq_bow_indices, pred_topic_token_idxs = sess.run([topics_freq_bow_indices, topic_beam_output_token_idxs], \n",
    "                                                                                                           feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "    pred_topic_sents = idxs_to_sents(pred_topic_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Topic Samples-----------')\n",
    "    for i, (topic_freq_bow_idxs, pred_topic_sent) in enumerate(zip(topics_freq_bow_idxs, pred_topic_sents)):\n",
    "        print(i, ' bow:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' sent:', pred_topic_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "saver = tf.train.Saver(max_to_keep=10)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','TM','','','','LM','','VALID:','TM','','','','LM','', ''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL','REG','NLL','KL','LOSS','PPL','NLL','KL','REG','NLL','KL', 'Beta']]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>LM</th>\n",
       "      <th></th>\n",
       "      <th>VALID:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>LM</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>141.53</td>\n",
       "      <td>1035</td>\n",
       "      <td>130.91</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.90</td>\n",
       "      <td>9.13</td>\n",
       "      <td>1.36</td>\n",
       "      <td>126.43</td>\n",
       "      <td>1031</td>\n",
       "      <td>116.10</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.90</td>\n",
       "      <td>9.12</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>120.76</td>\n",
       "      <td>575</td>\n",
       "      <td>113.80</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.41</td>\n",
       "      <td>6.22</td>\n",
       "      <td>1.36</td>\n",
       "      <td>111.16</td>\n",
       "      <td>522</td>\n",
       "      <td>104.82</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.73</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>119.65</td>\n",
       "      <td>548</td>\n",
       "      <td>112.81</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.27</td>\n",
       "      <td>5.99</td>\n",
       "      <td>1.06</td>\n",
       "      <td>110.52</td>\n",
       "      <td>494</td>\n",
       "      <td>103.85</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5.68</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>1500</td>\n",
       "      <td>119.20</td>\n",
       "      <td>533</td>\n",
       "      <td>112.33</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.21</td>\n",
       "      <td>5.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>110.25</td>\n",
       "      <td>485</td>\n",
       "      <td>103.57</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.61</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>118.61</td>\n",
       "      <td>522</td>\n",
       "      <td>111.72</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.84</td>\n",
       "      <td>0.78</td>\n",
       "      <td>109.82</td>\n",
       "      <td>476</td>\n",
       "      <td>103.16</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.49</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>118.33</td>\n",
       "      <td>517</td>\n",
       "      <td>111.43</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.16</td>\n",
       "      <td>5.80</td>\n",
       "      <td>0.73</td>\n",
       "      <td>109.83</td>\n",
       "      <td>471</td>\n",
       "      <td>103.04</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.04</td>\n",
       "      <td>5.41</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2776</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>118.02</td>\n",
       "      <td>508</td>\n",
       "      <td>111.11</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.14</td>\n",
       "      <td>5.73</td>\n",
       "      <td>0.68</td>\n",
       "      <td>109.68</td>\n",
       "      <td>464</td>\n",
       "      <td>102.87</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.03</td>\n",
       "      <td>5.26</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3276</th>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>117.75</td>\n",
       "      <td>502</td>\n",
       "      <td>110.84</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.12</td>\n",
       "      <td>5.67</td>\n",
       "      <td>0.63</td>\n",
       "      <td>109.29</td>\n",
       "      <td>458</td>\n",
       "      <td>102.61</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.16</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3776</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>117.59</td>\n",
       "      <td>496</td>\n",
       "      <td>110.68</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.11</td>\n",
       "      <td>5.61</td>\n",
       "      <td>0.60</td>\n",
       "      <td>109.03</td>\n",
       "      <td>454</td>\n",
       "      <td>102.43</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.06</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4276</th>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>117.40</td>\n",
       "      <td>492</td>\n",
       "      <td>110.50</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>5.55</td>\n",
       "      <td>0.57</td>\n",
       "      <td>109.13</td>\n",
       "      <td>456</td>\n",
       "      <td>102.47</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5.01</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4551</th>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>117.39</td>\n",
       "      <td>490</td>\n",
       "      <td>110.49</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.09</td>\n",
       "      <td>5.52</td>\n",
       "      <td>0.56</td>\n",
       "      <td>108.93</td>\n",
       "      <td>453</td>\n",
       "      <td>102.37</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.97</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5051</th>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>117.27</td>\n",
       "      <td>486</td>\n",
       "      <td>110.38</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.47</td>\n",
       "      <td>0.54</td>\n",
       "      <td>109.10</td>\n",
       "      <td>458</td>\n",
       "      <td>102.54</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.92</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5551</th>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>117.18</td>\n",
       "      <td>483</td>\n",
       "      <td>110.29</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.43</td>\n",
       "      <td>0.52</td>\n",
       "      <td>108.91</td>\n",
       "      <td>452</td>\n",
       "      <td>102.39</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.87</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6051</th>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>1500</td>\n",
       "      <td>117.11</td>\n",
       "      <td>480</td>\n",
       "      <td>110.22</td>\n",
       "      <td>1.22</td>\n",
       "      <td>0.07</td>\n",
       "      <td>5.39</td>\n",
       "      <td>0.51</td>\n",
       "      <td>108.72</td>\n",
       "      <td>446</td>\n",
       "      <td>102.17</td>\n",
       "      <td>1.37</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.82</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6551</th>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>116.99</td>\n",
       "      <td>478</td>\n",
       "      <td>110.11</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.07</td>\n",
       "      <td>5.36</td>\n",
       "      <td>0.49</td>\n",
       "      <td>108.77</td>\n",
       "      <td>450</td>\n",
       "      <td>102.24</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.78</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6826</th>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>116.92</td>\n",
       "      <td>477</td>\n",
       "      <td>110.03</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.06</td>\n",
       "      <td>5.34</td>\n",
       "      <td>0.49</td>\n",
       "      <td>108.54</td>\n",
       "      <td>443</td>\n",
       "      <td>102.00</td>\n",
       "      <td>1.43</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.76</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7326</th>\n",
       "      <td>61</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>116.83</td>\n",
       "      <td>474</td>\n",
       "      <td>109.95</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.06</td>\n",
       "      <td>5.30</td>\n",
       "      <td>0.48</td>\n",
       "      <td>108.57</td>\n",
       "      <td>443</td>\n",
       "      <td>101.98</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.72</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7826</th>\n",
       "      <td>61</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>116.77</td>\n",
       "      <td>472</td>\n",
       "      <td>109.89</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.06</td>\n",
       "      <td>5.27</td>\n",
       "      <td>0.47</td>\n",
       "      <td>108.48</td>\n",
       "      <td>437</td>\n",
       "      <td>101.84</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.70</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8326</th>\n",
       "      <td>61</td>\n",
       "      <td>3</td>\n",
       "      <td>1500</td>\n",
       "      <td>116.73</td>\n",
       "      <td>470</td>\n",
       "      <td>109.85</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.24</td>\n",
       "      <td>0.46</td>\n",
       "      <td>108.61</td>\n",
       "      <td>440</td>\n",
       "      <td>102.02</td>\n",
       "      <td>1.56</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.67</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8826</th>\n",
       "      <td>61</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>116.64</td>\n",
       "      <td>469</td>\n",
       "      <td>109.76</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.22</td>\n",
       "      <td>0.45</td>\n",
       "      <td>108.28</td>\n",
       "      <td>433</td>\n",
       "      <td>101.69</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.65</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9101</th>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>116.58</td>\n",
       "      <td>467</td>\n",
       "      <td>109.70</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.45</td>\n",
       "      <td>108.27</td>\n",
       "      <td>432</td>\n",
       "      <td>101.74</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.62</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9601</th>\n",
       "      <td>62</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "      <td>116.51</td>\n",
       "      <td>465</td>\n",
       "      <td>109.63</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.18</td>\n",
       "      <td>0.44</td>\n",
       "      <td>108.44</td>\n",
       "      <td>436</td>\n",
       "      <td>101.77</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.61</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10101</th>\n",
       "      <td>61</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>116.47</td>\n",
       "      <td>464</td>\n",
       "      <td>109.59</td>\n",
       "      <td>1.42</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.16</td>\n",
       "      <td>0.44</td>\n",
       "      <td>108.35</td>\n",
       "      <td>434</td>\n",
       "      <td>101.74</td>\n",
       "      <td>1.63</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10601</th>\n",
       "      <td>59</td>\n",
       "      <td>4</td>\n",
       "      <td>1500</td>\n",
       "      <td>116.39</td>\n",
       "      <td>462</td>\n",
       "      <td>109.51</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5.13</td>\n",
       "      <td>0.44</td>\n",
       "      <td>108.08</td>\n",
       "      <td>423</td>\n",
       "      <td>101.39</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.55</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11101</th>\n",
       "      <td>58</td>\n",
       "      <td>4</td>\n",
       "      <td>2000</td>\n",
       "      <td>116.36</td>\n",
       "      <td>461</td>\n",
       "      <td>109.47</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.04</td>\n",
       "      <td>5.11</td>\n",
       "      <td>0.43</td>\n",
       "      <td>108.15</td>\n",
       "      <td>427</td>\n",
       "      <td>101.49</td>\n",
       "      <td>1.73</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.54</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11376</th>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>116.31</td>\n",
       "      <td>460</td>\n",
       "      <td>109.42</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.04</td>\n",
       "      <td>5.10</td>\n",
       "      <td>0.43</td>\n",
       "      <td>107.75</td>\n",
       "      <td>428</td>\n",
       "      <td>101.51</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.53</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11876</th>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>116.28</td>\n",
       "      <td>458</td>\n",
       "      <td>109.40</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.04</td>\n",
       "      <td>5.08</td>\n",
       "      <td>0.43</td>\n",
       "      <td>107.80</td>\n",
       "      <td>425</td>\n",
       "      <td>101.50</td>\n",
       "      <td>1.74</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.51</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12376</th>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>116.21</td>\n",
       "      <td>457</td>\n",
       "      <td>109.34</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.04</td>\n",
       "      <td>5.06</td>\n",
       "      <td>0.44</td>\n",
       "      <td>107.61</td>\n",
       "      <td>421</td>\n",
       "      <td>101.25</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.49</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12876</th>\n",
       "      <td>59</td>\n",
       "      <td>5</td>\n",
       "      <td>1500</td>\n",
       "      <td>116.14</td>\n",
       "      <td>455</td>\n",
       "      <td>109.28</td>\n",
       "      <td>1.53</td>\n",
       "      <td>0.04</td>\n",
       "      <td>5.05</td>\n",
       "      <td>0.44</td>\n",
       "      <td>107.72</td>\n",
       "      <td>419</td>\n",
       "      <td>101.25</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.47</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13376</th>\n",
       "      <td>59</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>116.07</td>\n",
       "      <td>454</td>\n",
       "      <td>109.21</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.04</td>\n",
       "      <td>5.03</td>\n",
       "      <td>0.44</td>\n",
       "      <td>107.50</td>\n",
       "      <td>415</td>\n",
       "      <td>101.03</td>\n",
       "      <td>1.82</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.44</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79626</th>\n",
       "      <td>33</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>114.35</td>\n",
       "      <td>413</td>\n",
       "      <td>107.61</td>\n",
       "      <td>2.12</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.29</td>\n",
       "      <td>0.44</td>\n",
       "      <td>106.50</td>\n",
       "      <td>403</td>\n",
       "      <td>100.66</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80126</th>\n",
       "      <td>59</td>\n",
       "      <td>35</td>\n",
       "      <td>500</td>\n",
       "      <td>114.34</td>\n",
       "      <td>413</td>\n",
       "      <td>107.60</td>\n",
       "      <td>2.12</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.29</td>\n",
       "      <td>0.45</td>\n",
       "      <td>106.44</td>\n",
       "      <td>401</td>\n",
       "      <td>100.52</td>\n",
       "      <td>2.12</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.74</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80626</th>\n",
       "      <td>59</td>\n",
       "      <td>35</td>\n",
       "      <td>1000</td>\n",
       "      <td>114.34</td>\n",
       "      <td>413</td>\n",
       "      <td>107.60</td>\n",
       "      <td>2.12</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.28</td>\n",
       "      <td>0.45</td>\n",
       "      <td>106.40</td>\n",
       "      <td>399</td>\n",
       "      <td>100.46</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81126</th>\n",
       "      <td>59</td>\n",
       "      <td>35</td>\n",
       "      <td>1500</td>\n",
       "      <td>114.33</td>\n",
       "      <td>413</td>\n",
       "      <td>107.60</td>\n",
       "      <td>2.12</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.28</td>\n",
       "      <td>0.45</td>\n",
       "      <td>106.49</td>\n",
       "      <td>398</td>\n",
       "      <td>100.46</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.71</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81626</th>\n",
       "      <td>59</td>\n",
       "      <td>35</td>\n",
       "      <td>2000</td>\n",
       "      <td>114.32</td>\n",
       "      <td>413</td>\n",
       "      <td>107.59</td>\n",
       "      <td>2.12</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.28</td>\n",
       "      <td>0.45</td>\n",
       "      <td>106.58</td>\n",
       "      <td>400</td>\n",
       "      <td>100.49</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81901</th>\n",
       "      <td>33</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>114.32</td>\n",
       "      <td>413</td>\n",
       "      <td>107.59</td>\n",
       "      <td>2.12</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.28</td>\n",
       "      <td>0.45</td>\n",
       "      <td>106.52</td>\n",
       "      <td>397</td>\n",
       "      <td>100.44</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82401</th>\n",
       "      <td>59</td>\n",
       "      <td>36</td>\n",
       "      <td>500</td>\n",
       "      <td>114.31</td>\n",
       "      <td>413</td>\n",
       "      <td>107.59</td>\n",
       "      <td>2.12</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.27</td>\n",
       "      <td>0.45</td>\n",
       "      <td>106.55</td>\n",
       "      <td>398</td>\n",
       "      <td>100.39</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82901</th>\n",
       "      <td>59</td>\n",
       "      <td>36</td>\n",
       "      <td>1000</td>\n",
       "      <td>114.31</td>\n",
       "      <td>412</td>\n",
       "      <td>107.58</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.27</td>\n",
       "      <td>0.45</td>\n",
       "      <td>106.70</td>\n",
       "      <td>401</td>\n",
       "      <td>100.49</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83401</th>\n",
       "      <td>59</td>\n",
       "      <td>36</td>\n",
       "      <td>1500</td>\n",
       "      <td>114.31</td>\n",
       "      <td>412</td>\n",
       "      <td>107.58</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.27</td>\n",
       "      <td>0.45</td>\n",
       "      <td>106.67</td>\n",
       "      <td>400</td>\n",
       "      <td>100.45</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83901</th>\n",
       "      <td>61</td>\n",
       "      <td>36</td>\n",
       "      <td>2000</td>\n",
       "      <td>114.30</td>\n",
       "      <td>412</td>\n",
       "      <td>107.58</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.27</td>\n",
       "      <td>0.45</td>\n",
       "      <td>106.81</td>\n",
       "      <td>401</td>\n",
       "      <td>100.47</td>\n",
       "      <td>2.19</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84176</th>\n",
       "      <td>34</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>114.30</td>\n",
       "      <td>412</td>\n",
       "      <td>107.58</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.26</td>\n",
       "      <td>0.45</td>\n",
       "      <td>106.79</td>\n",
       "      <td>400</td>\n",
       "      <td>100.48</td>\n",
       "      <td>2.19</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.71</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84676</th>\n",
       "      <td>59</td>\n",
       "      <td>37</td>\n",
       "      <td>500</td>\n",
       "      <td>114.29</td>\n",
       "      <td>412</td>\n",
       "      <td>107.57</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.26</td>\n",
       "      <td>0.45</td>\n",
       "      <td>106.79</td>\n",
       "      <td>399</td>\n",
       "      <td>100.44</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85176</th>\n",
       "      <td>59</td>\n",
       "      <td>37</td>\n",
       "      <td>1000</td>\n",
       "      <td>114.29</td>\n",
       "      <td>412</td>\n",
       "      <td>107.57</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.26</td>\n",
       "      <td>0.45</td>\n",
       "      <td>106.64</td>\n",
       "      <td>393</td>\n",
       "      <td>100.27</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85676</th>\n",
       "      <td>59</td>\n",
       "      <td>37</td>\n",
       "      <td>1500</td>\n",
       "      <td>114.29</td>\n",
       "      <td>412</td>\n",
       "      <td>107.57</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.26</td>\n",
       "      <td>0.45</td>\n",
       "      <td>106.78</td>\n",
       "      <td>400</td>\n",
       "      <td>100.42</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86176</th>\n",
       "      <td>59</td>\n",
       "      <td>37</td>\n",
       "      <td>2000</td>\n",
       "      <td>114.28</td>\n",
       "      <td>412</td>\n",
       "      <td>107.56</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.25</td>\n",
       "      <td>0.45</td>\n",
       "      <td>106.66</td>\n",
       "      <td>394</td>\n",
       "      <td>100.31</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86451</th>\n",
       "      <td>33</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>114.28</td>\n",
       "      <td>412</td>\n",
       "      <td>107.56</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.25</td>\n",
       "      <td>0.45</td>\n",
       "      <td>106.80</td>\n",
       "      <td>399</td>\n",
       "      <td>100.46</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86951</th>\n",
       "      <td>60</td>\n",
       "      <td>38</td>\n",
       "      <td>500</td>\n",
       "      <td>114.28</td>\n",
       "      <td>412</td>\n",
       "      <td>107.56</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.25</td>\n",
       "      <td>0.45</td>\n",
       "      <td>106.80</td>\n",
       "      <td>398</td>\n",
       "      <td>100.42</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.71</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87451</th>\n",
       "      <td>60</td>\n",
       "      <td>38</td>\n",
       "      <td>1000</td>\n",
       "      <td>114.27</td>\n",
       "      <td>412</td>\n",
       "      <td>107.55</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.25</td>\n",
       "      <td>0.45</td>\n",
       "      <td>106.84</td>\n",
       "      <td>400</td>\n",
       "      <td>100.51</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.71</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87951</th>\n",
       "      <td>60</td>\n",
       "      <td>38</td>\n",
       "      <td>1500</td>\n",
       "      <td>114.27</td>\n",
       "      <td>412</td>\n",
       "      <td>107.55</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.25</td>\n",
       "      <td>0.45</td>\n",
       "      <td>106.88</td>\n",
       "      <td>403</td>\n",
       "      <td>100.57</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.71</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88451</th>\n",
       "      <td>60</td>\n",
       "      <td>38</td>\n",
       "      <td>2000</td>\n",
       "      <td>114.27</td>\n",
       "      <td>412</td>\n",
       "      <td>107.56</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.24</td>\n",
       "      <td>0.45</td>\n",
       "      <td>106.86</td>\n",
       "      <td>400</td>\n",
       "      <td>100.54</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.71</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88726</th>\n",
       "      <td>34</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>114.27</td>\n",
       "      <td>412</td>\n",
       "      <td>107.55</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.24</td>\n",
       "      <td>0.45</td>\n",
       "      <td>106.94</td>\n",
       "      <td>407</td>\n",
       "      <td>100.67</td>\n",
       "      <td>2.12</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.69</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89226</th>\n",
       "      <td>60</td>\n",
       "      <td>39</td>\n",
       "      <td>500</td>\n",
       "      <td>114.27</td>\n",
       "      <td>412</td>\n",
       "      <td>107.55</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.24</td>\n",
       "      <td>0.45</td>\n",
       "      <td>106.87</td>\n",
       "      <td>403</td>\n",
       "      <td>100.52</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.69</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89726</th>\n",
       "      <td>60</td>\n",
       "      <td>39</td>\n",
       "      <td>1000</td>\n",
       "      <td>114.26</td>\n",
       "      <td>412</td>\n",
       "      <td>107.55</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.24</td>\n",
       "      <td>0.45</td>\n",
       "      <td>106.75</td>\n",
       "      <td>399</td>\n",
       "      <td>100.45</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.70</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90226</th>\n",
       "      <td>59</td>\n",
       "      <td>39</td>\n",
       "      <td>1500</td>\n",
       "      <td>114.26</td>\n",
       "      <td>411</td>\n",
       "      <td>107.55</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.23</td>\n",
       "      <td>0.45</td>\n",
       "      <td>106.92</td>\n",
       "      <td>404</td>\n",
       "      <td>100.63</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.69</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90726</th>\n",
       "      <td>60</td>\n",
       "      <td>39</td>\n",
       "      <td>2000</td>\n",
       "      <td>114.25</td>\n",
       "      <td>411</td>\n",
       "      <td>107.54</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.23</td>\n",
       "      <td>0.45</td>\n",
       "      <td>106.69</td>\n",
       "      <td>398</td>\n",
       "      <td>100.41</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91001</th>\n",
       "      <td>34</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>114.25</td>\n",
       "      <td>411</td>\n",
       "      <td>107.54</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.23</td>\n",
       "      <td>0.45</td>\n",
       "      <td>106.49</td>\n",
       "      <td>402</td>\n",
       "      <td>100.64</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91501</th>\n",
       "      <td>59</td>\n",
       "      <td>40</td>\n",
       "      <td>500</td>\n",
       "      <td>114.24</td>\n",
       "      <td>411</td>\n",
       "      <td>107.54</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.23</td>\n",
       "      <td>0.45</td>\n",
       "      <td>106.50</td>\n",
       "      <td>402</td>\n",
       "      <td>100.64</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.65</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92001</th>\n",
       "      <td>61</td>\n",
       "      <td>40</td>\n",
       "      <td>1000</td>\n",
       "      <td>114.24</td>\n",
       "      <td>411</td>\n",
       "      <td>107.54</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.23</td>\n",
       "      <td>0.45</td>\n",
       "      <td>106.50</td>\n",
       "      <td>403</td>\n",
       "      <td>100.57</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.64</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92501</th>\n",
       "      <td>64</td>\n",
       "      <td>40</td>\n",
       "      <td>1500</td>\n",
       "      <td>114.24</td>\n",
       "      <td>411</td>\n",
       "      <td>107.53</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.22</td>\n",
       "      <td>0.45</td>\n",
       "      <td>106.44</td>\n",
       "      <td>402</td>\n",
       "      <td>100.55</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.64</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93001</th>\n",
       "      <td>61</td>\n",
       "      <td>40</td>\n",
       "      <td>2000</td>\n",
       "      <td>114.23</td>\n",
       "      <td>411</td>\n",
       "      <td>107.53</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.22</td>\n",
       "      <td>0.45</td>\n",
       "      <td>106.39</td>\n",
       "      <td>396</td>\n",
       "      <td>100.36</td>\n",
       "      <td>2.19</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.63</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>205 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      TRAIN:    TM                        LM        VALID:  \\\n",
       "      Time  Ep    Ct    LOSS   PPL     NLL    KL   REG   NLL    KL    LOSS   \n",
       "1        4   0     0  141.53  1035  130.91  0.60  0.90  9.13  1.36  126.43   \n",
       "501     74   0   500  120.76   575  113.80  0.27  0.41  6.22  1.36  111.16   \n",
       "1001    69   0  1000  119.65   548  112.81  0.51  0.27  5.99  1.06  110.52   \n",
       "1501    61   0  1500  119.20   533  112.33  0.67  0.21  5.90  0.90  110.25   \n",
       "2001    61   0  2000  118.61   522  111.72  0.78  0.17  5.84  0.78  109.82   \n",
       "2276    35   1     0  118.33   517  111.43  0.83  0.16  5.80  0.73  109.83   \n",
       "2776    61   1   500  118.02   508  111.11  0.92  0.14  5.73  0.68  109.68   \n",
       "3276    60   1  1000  117.75   502  110.84  0.99  0.12  5.67  0.63  109.29   \n",
       "3776    61   1  1500  117.59   496  110.68  1.05  0.11  5.61  0.60  109.03   \n",
       "4276    60   1  2000  117.40   492  110.50  1.09  0.09  5.55  0.57  109.13   \n",
       "4551    34   2     0  117.39   490  110.49  1.12  0.09  5.52  0.56  108.93   \n",
       "5051    60   2   500  117.27   486  110.38  1.16  0.08  5.47  0.54  109.10   \n",
       "5551    61   2  1000  117.18   483  110.29  1.19  0.08  5.43  0.52  108.91   \n",
       "6051    60   2  1500  117.11   480  110.22  1.22  0.07  5.39  0.51  108.72   \n",
       "6551    61   2  2000  116.99   478  110.11  1.25  0.07  5.36  0.49  108.77   \n",
       "6826    34   3     0  116.92   477  110.03  1.26  0.06  5.34  0.49  108.54   \n",
       "7326    61   3   500  116.83   474  109.95  1.29  0.06  5.30  0.48  108.57   \n",
       "7826    61   3  1000  116.77   472  109.89  1.32  0.06  5.27  0.47  108.48   \n",
       "8326    61   3  1500  116.73   470  109.85  1.34  0.05  5.24  0.46  108.61   \n",
       "8826    61   3  2000  116.64   469  109.76  1.36  0.05  5.22  0.45  108.28   \n",
       "9101    34   4     0  116.58   467  109.70  1.38  0.05  5.20  0.45  108.27   \n",
       "9601    62   4   500  116.51   465  109.63  1.40  0.05  5.18  0.44  108.44   \n",
       "10101   61   4  1000  116.47   464  109.59  1.42  0.05  5.16  0.44  108.35   \n",
       "10601   59   4  1500  116.39   462  109.51  1.44  0.05  5.13  0.44  108.08   \n",
       "11101   58   4  2000  116.36   461  109.47  1.46  0.04  5.11  0.43  108.15   \n",
       "11376   33   5     0  116.31   460  109.42  1.47  0.04  5.10  0.43  107.75   \n",
       "11876   60   5   500  116.28   458  109.40  1.49  0.04  5.08  0.43  107.80   \n",
       "12376   61   5  1000  116.21   457  109.34  1.51  0.04  5.06  0.44  107.61   \n",
       "12876   59   5  1500  116.14   455  109.28  1.53  0.04  5.05  0.44  107.72   \n",
       "13376   59   5  2000  116.07   454  109.21  1.55  0.04  5.03  0.44  107.50   \n",
       "...    ...  ..   ...     ...   ...     ...   ...   ...   ...   ...     ...   \n",
       "79626   33  35     0  114.35   413  107.61  2.12  0.02  4.29  0.44  106.50   \n",
       "80126   59  35   500  114.34   413  107.60  2.12  0.02  4.29  0.45  106.44   \n",
       "80626   59  35  1000  114.34   413  107.60  2.12  0.02  4.28  0.45  106.40   \n",
       "81126   59  35  1500  114.33   413  107.60  2.12  0.02  4.28  0.45  106.49   \n",
       "81626   59  35  2000  114.32   413  107.59  2.12  0.02  4.28  0.45  106.58   \n",
       "81901   33  36     0  114.32   413  107.59  2.12  0.02  4.28  0.45  106.52   \n",
       "82401   59  36   500  114.31   413  107.59  2.12  0.02  4.27  0.45  106.55   \n",
       "82901   59  36  1000  114.31   412  107.58  2.13  0.02  4.27  0.45  106.70   \n",
       "83401   59  36  1500  114.31   412  107.58  2.13  0.02  4.27  0.45  106.67   \n",
       "83901   61  36  2000  114.30   412  107.58  2.13  0.02  4.27  0.45  106.81   \n",
       "84176   34  37     0  114.30   412  107.58  2.13  0.02  4.26  0.45  106.79   \n",
       "84676   59  37   500  114.29   412  107.57  2.13  0.02  4.26  0.45  106.79   \n",
       "85176   59  37  1000  114.29   412  107.57  2.13  0.02  4.26  0.45  106.64   \n",
       "85676   59  37  1500  114.29   412  107.57  2.13  0.02  4.26  0.45  106.78   \n",
       "86176   59  37  2000  114.28   412  107.56  2.13  0.02  4.25  0.45  106.66   \n",
       "86451   33  38     0  114.28   412  107.56  2.13  0.02  4.25  0.45  106.80   \n",
       "86951   60  38   500  114.28   412  107.56  2.13  0.02  4.25  0.45  106.80   \n",
       "87451   60  38  1000  114.27   412  107.55  2.14  0.02  4.25  0.45  106.84   \n",
       "87951   60  38  1500  114.27   412  107.55  2.14  0.02  4.25  0.45  106.88   \n",
       "88451   60  38  2000  114.27   412  107.56  2.14  0.02  4.24  0.45  106.86   \n",
       "88726   34  39     0  114.27   412  107.55  2.14  0.02  4.24  0.45  106.94   \n",
       "89226   60  39   500  114.27   412  107.55  2.14  0.02  4.24  0.45  106.87   \n",
       "89726   60  39  1000  114.26   412  107.55  2.14  0.02  4.24  0.45  106.75   \n",
       "90226   59  39  1500  114.26   411  107.55  2.14  0.02  4.23  0.45  106.92   \n",
       "90726   60  39  2000  114.25   411  107.54  2.14  0.02  4.23  0.45  106.69   \n",
       "91001   34  40     0  114.25   411  107.54  2.14  0.02  4.23  0.45  106.49   \n",
       "91501   59  40   500  114.24   411  107.54  2.14  0.02  4.23  0.45  106.50   \n",
       "92001   61  40  1000  114.24   411  107.54  2.14  0.02  4.23  0.45  106.50   \n",
       "92501   64  40  1500  114.24   411  107.53  2.15  0.02  4.22  0.45  106.44   \n",
       "93001   61  40  2000  114.23   411  107.53  2.15  0.02  4.22  0.45  106.39   \n",
       "\n",
       "         TM                        LM               \n",
       "        PPL     NLL    KL   REG   NLL    KL   Beta  \n",
       "1      1031  116.10  0.31  0.90  9.12  1.41  0.000  \n",
       "501     522  104.82  0.37  0.17  5.73  0.82  0.088  \n",
       "1001    494  103.85  0.77  0.10  5.68  0.64  0.176  \n",
       "1501    485  103.57  0.88  0.08  5.61  0.44  0.264  \n",
       "2001    476  103.16  0.97  0.05  5.49  0.41  0.352  \n",
       "2276    471  103.04  1.17  0.04  5.41  0.43  0.400  \n",
       "2776    464  102.87  1.32  0.03  5.26  0.43  0.488  \n",
       "3276    458  102.61  1.26  0.02  5.16  0.42  0.576  \n",
       "3776    454  102.43  1.28  0.02  5.06  0.37  0.664  \n",
       "4276    456  102.47  1.36  0.02  5.01  0.37  0.752  \n",
       "4551    453  102.37  1.30  0.02  4.97  0.35  0.800  \n",
       "5051    458  102.54  1.32  0.01  4.92  0.35  0.888  \n",
       "5551    452  102.39  1.32  0.01  4.87  0.33  0.976  \n",
       "6051    446  102.17  1.37  0.01  4.82  0.35  1.000  \n",
       "6551    450  102.24  1.41  0.01  4.78  0.33  1.000  \n",
       "6826    443  102.00  1.43  0.01  4.76  0.34  1.000  \n",
       "7326    443  101.98  1.52  0.01  4.72  0.34  1.000  \n",
       "7826    437  101.84  1.61  0.01  4.70  0.33  1.000  \n",
       "8326    440  102.02  1.56  0.01  4.67  0.35  1.000  \n",
       "8826    433  101.69  1.57  0.01  4.65  0.36  1.000  \n",
       "9101    432  101.74  1.54  0.01  4.62  0.36  1.000  \n",
       "9601    436  101.77  1.70  0.02  4.61  0.35  1.000  \n",
       "10101   434  101.74  1.63  0.02  4.58  0.39  1.000  \n",
       "10601   423  101.39  1.75  0.02  4.55  0.37  1.000  \n",
       "11101   427  101.49  1.73  0.02  4.54  0.38  1.000  \n",
       "11376   428  101.51  1.69  0.02  4.53  0.35  0.000  \n",
       "11876   425  101.50  1.74  0.02  4.51  0.50  0.088  \n",
       "12376   421  101.25  1.76  0.02  4.49  0.51  0.176  \n",
       "12876   419  101.25  1.85  0.02  4.47  0.48  0.264  \n",
       "13376   415  101.03  1.82  0.02  4.44  0.52  0.352  \n",
       "...     ...     ...   ...   ...   ...   ...    ...  \n",
       "79626   403  100.66  2.08  0.01  3.75  0.59  0.000  \n",
       "80126   401  100.52  2.12  0.01  3.74  0.55  0.088  \n",
       "80626   399  100.46  2.11  0.02  3.72  0.53  0.176  \n",
       "81126   398  100.46  2.15  0.02  3.71  0.55  0.264  \n",
       "81626   400  100.49  2.17  0.02  3.72  0.54  0.352  \n",
       "81901   397  100.44  2.15  0.02  3.72  0.50  0.400  \n",
       "82401   398  100.39  2.16  0.02  3.73  0.52  0.488  \n",
       "82901   401  100.49  2.20  0.02  3.72  0.48  0.576  \n",
       "83401   400  100.45  2.15  0.02  3.72  0.49  0.664  \n",
       "83901   401  100.47  2.19  0.02  3.73  0.55  0.752  \n",
       "84176   400  100.48  2.19  0.01  3.71  0.50  0.800  \n",
       "84676   399  100.44  2.18  0.02  3.72  0.49  0.888  \n",
       "85176   393  100.27  2.17  0.01  3.73  0.46  0.976  \n",
       "85676   400  100.42  2.16  0.02  3.72  0.47  1.000  \n",
       "86176   394  100.31  2.17  0.01  3.72  0.45  1.000  \n",
       "86451   399  100.46  2.15  0.01  3.72  0.45  1.000  \n",
       "86951   398  100.42  2.16  0.01  3.71  0.50  1.000  \n",
       "87451   400  100.51  2.13  0.01  3.71  0.47  1.000  \n",
       "87951   403  100.57  2.13  0.01  3.71  0.46  1.000  \n",
       "88451   400  100.54  2.15  0.01  3.71  0.45  1.000  \n",
       "88726   407  100.67  2.12  0.01  3.69  0.44  1.000  \n",
       "89226   403  100.52  2.15  0.01  3.69  0.49  1.000  \n",
       "89726   399  100.45  2.14  0.01  3.70  0.45  1.000  \n",
       "90226   404  100.63  2.14  0.01  3.69  0.45  1.000  \n",
       "90726   398  100.41  2.13  0.01  3.67  0.45  1.000  \n",
       "91001   402  100.64  2.16  0.01  3.67  0.45  0.000  \n",
       "91501   402  100.64  2.15  0.01  3.65  0.55  0.088  \n",
       "92001   403  100.57  2.17  0.02  3.64  0.55  0.176  \n",
       "92501   402  100.55  2.10  0.01  3.64  0.52  0.264  \n",
       "93001   396  100.36  2.19  0.02  3.63  0.54  0.352  \n",
       "\n",
       "[205 rows x 18 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 True: i was looking for a lightweight case that i could carry the my new surface tablet in - take it on a trip , to a trade show , even just to the coffee shop or library\n",
      "0 Pred: it is a for a sleeve sleeve that would could n't my ipad # asus pro , the it it out the go to the work it out\n",
      "1 True: it 's great for all that\n",
      "1 Pred: i 's a\n",
      "2 True: the surface fits in the tablet pouch perfectly\n",
      "2 Pred: the is is my the sleeve pocket with , with and , , , , , , , , , , the\n",
      "3 True: room for paper pad , pens , maybe some literature if take it to a trade show , a book for on the airplane\n",
      "3 Pred: the has the and , pens , etc a other\n",
      "4 True: perfect for what i want\n",
      "4 Pred: i bought this i needed for to\n",
      "5 True: very plain , though\n",
      "5 Pred: i nice with and ,\n",
      "6 True: some kind of small accent or logo would be nice\n",
      "6 Pred: it of of have for i\n",
      "7 True: maybe a red stripe along the side with the zippered side pouch\n",
      "7 Pred: the the is fit the with side pocket the power ,\n",
      "8 True: so i got the shell case as ordered , it was exactly what i wanted\n",
      "8 Pred: i love love the cover case for my it and it very what i was\n",
      "9 True: i had it on my laptop for maybe two weeks , including the handy little keyboard protector they gave you as well ... when i noticed my keys were sticking when i tried to type\n",
      "9 Pred: i have a on the macbook and the the of on and the keyboard keyboard and cover , had me to a as the i opened the computer\n",
      "10 True: i lift my key protector to find that my keyboard had melted\n",
      "10 Pred: the color the keyboard board and the a\n",
      "11 True: i took it to the apple store and they examined the inside and determined that the heat had come from the outside because the internal of the computer was not melted\n",
      "11 Pred: the have the on be bottom store , the the the case of the the the case would to the the case of the the case\n",
      "12 True: so they <unk> me at fault and wo n't cover it\n",
      "12 Pred: i had n't the to the and the n't be the up\n",
      "13 True: awesome . # $ to fix a laptop because of a # $ case\n",
      "13 Pred: i , it stars #\n",
      "14 True: otherwise , great case\n",
      "14 Pred: i , it case\n",
      "15 True: i was greatly satisfied with the briefcase\n",
      "15 Pred: i bought looking surprised with this bag\n",
      "16 True: it is of excellent quality and more than i expected\n",
      "16 Pred: i is a a quality and the\n",
      "17 True: it is n't bulky , is easy to carry and adjustable\n",
      "17 Pred: it is very a enough has easy to to for my\n",
      "18 True: it was the only one i could find that would accommodate my laptop without spending alot\n",
      "18 Pred: i is a best thing i have had for it can my #\n",
      "19 True: great deal for the price\n",
      "19 Pred: i would for my price\n",
      "20 True: caselogic tnc- # -inch laptop briefcase -lrb- black\n",
      "20 Pred: the is # . laptop bag # # -rrb- <unk> -rrb- <unk> <unk> <unk> <unk> <unk> <unk> <unk> -rrb-\n",
      "21 True: i took a long time deciding which case to get for my # `` macbook air\n",
      "21 Pred: i was a # and to it i was my a\n",
      "22 True: i have the late # version\n",
      "22 Pred: i would had it # # of and and\n",
      "23 True: this case is amazing -- very easy to put on , very protective\n",
      "23 Pred: it case is a , fits easy to put on\n",
      "24 True: it does not interfere with the screen angle at all , which was my main concern looking for a case\n",
      "24 Pred: the the n't fit with the keyboard protector\n",
      "25 True: the case is very sturdy\n",
      "25 Pred: it case is very nice and\n",
      "26 True: the only possible negative is that my macbook does feel a bit heavier now , but it 's definitely worth it for the protection\n",
      "26 Pred: the case thing thing is that that that is n't like it\n",
      "27 True: i absolutely love this case for my little laptop\n",
      "27 Pred: i love this\n",
      "28 True: it 's fun and always gets a lot of attention\n",
      "28 Pred: i is a\n",
      "29 True: especially at airports , where everybody always just have black , boring ones\n",
      "29 Pred: i for the , it i are have need to\n",
      "30 True: it 's nice and soft and definately protects the computer\n",
      "30 Pred: it fits a a protects and\n",
      "31 True: i can fit all the necessary cords , mouse etc in the side pocket\n",
      "31 Pred: it can fit my the cords cards in and , . the case pocket ,\n",
      "32 True: exactly what i expected\n",
      "32 Pred: i what i expected\n",
      "33 True: bag is great and has plenty of compartments to hold everything you need\n",
      "33 Pred: the is a enough has a of pockets\n",
      "34 True: this size only fits a # & # # ; inch laptop , and it fits very securely\n",
      "34 Pred: this is is fits my # . # `` ; laptop\n",
      "35 True: if you have a bigger laptop than this it will not fit\n",
      "35 Pred: the you 're a # laptop , this is it probably fit\n",
      "36 True: would definitely recommend this to anyone looking for a new laptop/messenger style bag\n",
      "36 Pred: i highly recommend this\n",
      "37 True: this is an excellent laptop sleeve\n",
      "37 Pred: this is a excellent case for for\n",
      "38 True: the fact that it has handles makes it much more convenient than other sleeves i have owned\n",
      "38 Pred: it only that it is a for it easy easier secure than my than\n",
      "39 True: it is a nice , snug fit for my # . # `` laptop\n",
      "39 Pred: it is a for , and fit\n",
      "40 True: the memory foam seems convincing too\n",
      "40 Pred: the 's foam is to and the\n",
      "41 True: this is a workable rolling case\n",
      "41 Pred: i is is great and bag\n",
      "42 True: a good value for the price\n",
      "42 Pred: i good fit for the price\n",
      "43 True: the handle structure could be a bit more tight , the interior -lrb- really cavernous -rrb- could have used another divider -lrb- to keep things from moving around within -rrb- or side rails that would have <unk> hanging files\n",
      "43 Pred: the only is the be a bit of but , the laptop , which <unk> -rrb- , be a the problem\n",
      "44 True: but is looks good and gets me where i am going .\n",
      "44 Pred: i was a like and i dirty\n",
      "45 True: i 'm very pleased with this backpack\n",
      "45 Pred: i bought very pleased with this bag\n",
      "46 True: i was concerned by some reviews about fabric tears , and bought it because of the price\n",
      "46 Pred: i was looking to the of that the , the but i this as it the quality\n",
      "47 True: i 've used it extensively over the last few months and it 's served me well\n",
      "47 Pred: i have the the for for the month and the the it has still me well\n",
      "48 True: travelling with my # & # # ; laptop in this as opposed to a sleeve has been a blessing\n",
      "48 Pred: it is the # . # # ; laptop laptop the case it is fit #\n",
      "49 True: i picked the right size , but it still is too big for my # . # & # # ; aspire e # - # laptop\n",
      "49 Pred: it is it bag size for it it it fit a small for my # . # `` # # ; laptop laptop #\n",
      "50 True: you will have plenty of room for other stuff , like documents etc .\n",
      "50 Pred: it can have a of room for a items\n",
      "51 True: other than that , product quality is quite good\n",
      "51 Pred: i than that , it is is great good\n",
      "52 True: this was a great deal\n",
      "52 Pred: i is a what deal\n",
      "53 True: i had been putting off buying a cover for my mac but i 'm so glad i finally did\n",
      "53 Pred: i love to looking my on a new for my macbook book i love glad glad i did bought this\n",
      "54 True: the cover does n't make my laptop weigh much more and i love the <unk> of it ... perfect fit\n",
      "54 Pred: the keyboard is not fit it look look a more\n",
      "55 True: this fits perfectly with my hp laptop\n",
      "55 Pred: this is my bill my laptop\n",
      "56 True: the zipper pocket comes in handy for flash drives and dvds when on the go\n",
      "56 Pred: the 's pocket is with handy for the for and mouse\n",
      "57 True: the only problem is that it looks a little cheap , other than that good product\n",
      "57 Pred: the case thing is that it does like like the\n",
      "58 True: the bag looks and feels very nice\n",
      "58 Pred: it 's is like feels good sturdy\n",
      "59 True: has good sized pockets to hold all of my gear , has a nice adjustable strap and the <unk> on the inside and outside look very nice\n",
      "59 Pred: the plenty storage pockets pockets hold the the the other , and the few point , , the front strap the side\n",
      "60 True: this bag will swallow so much gear you may question that it has straps and not wheels\n",
      "60 Pred: it is is a , the more\n",
      "61 True: all in all it is a great bag for holding a lot of equipment\n",
      "61 Pred: i in all , is very great bag bag the my laptop of stuff\n",
      "-----------Topic Samples-----------\n",
      "0  bow: ! love perfect color recommend perfectly highly ordered absolutely shipping\n",
      "0  sent: love love ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
      "1  bow: bottom top plastic speck part piece back corners feet rubber\n",
      "1  sent: the only thing i did n't like about the case is that the bottom part of the case does not fit properly\n",
      "2  bow: zipper months strap broke handle years year started zippers back\n",
      "2  sent: the only thing i did n't like about the bag is that the zipper broke within a week\n",
      "3  bow: pocket room power mouse ipad charger perfect small netbook cord\n",
      "3  sent: there is plenty of room for the power cord , mouse , power cord , mouse , mouse , etc .\n",
      "4  bow: pockets carry strap room shoulder back space compartment work stuff\n",
      "4  sent: plenty of room for my laptop , files , etc .\n",
      "5  bow: sewn nylon closure test floor waterproof average empty trust thinner\n",
      "5  sent: the only reason i did n't give it # stars is because it is a little bigger than i expected\n",
      "6  bow: ; & pro big size air perfectly small made laptops\n",
      "6  sent: it fits my # . # inch laptop perfectly\n",
      "7  bow: quality bought price 'm ... buy time made $ 've\n",
      "7  sent: i had to return it because i had to return it because i had to return it because i had to return it because i had to return it\n",
      "8  bow: cover color keyboard mac pro apple easy hard perfectly nice\n",
      "8  sent: the keyboard cover is a little difficult to type on\n",
      "9  bow: sleeve protection inside inch air nice pro protect bit padding\n",
      "9  sent: the sleeve is a little tight , but it does n't slide around inside the sleeve\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-fd566e493c27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_recon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_kl_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_reg_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_recon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_kl_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_kl_categ_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_kl_gmm_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppls_batch\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_recon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_kl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_loss_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_recon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_kl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_kl_categ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_loss_kl_gmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_ppls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msent_loss_kl_batch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if len(log_df) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "while epoch < config.epochs:\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = get_feed_dict(batch)\n",
    "\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, sent_loss_kl_categ_batch, sent_loss_kl_gmm_batch, ppls_batch = \\\n",
    "        sess.run([opt, loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, sent_loss_kl_categ, sent_loss_kl_gmm, topic_ppls], feed_dict = feed_dict)\n",
    "   \n",
    "        if sent_loss_kl_batch == np.inf:\n",
    "            print('Nan occured')\n",
    "            ckpt = tf.train.get_checkpoint_state(config.modeldir)\n",
    "            model_checkpoint_path = ckpt.all_model_checkpoint_paths[-1]\n",
    "            saver.restore(sess, model_checkpoint_path)            \n",
    "            break\n",
    "            \n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if ct%config.log_period==0:\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train, sent_loss_recon_train, sent_loss_kl_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, sent_loss_recon_dev, sent_loss_kl_dev, ppl_dev = get_loss(sess, dev_batches)\n",
    "            global_step_log, beta_eval = sess.run([tf.train.get_global_step(), beta])\n",
    "            \n",
    "#             if loss_dev < loss_min:\n",
    "#                 loss_min = loss_dev\n",
    "#                 saver.save(sess, config.modelpath, global_step=global_step_log)\n",
    "\n",
    "            clear_output()\n",
    "    \n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_reg_train, '%.2f'%sent_loss_recon_train, '%.2f'%sent_loss_kl_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_reg_dev, '%.2f'%sent_loss_recon_dev, '%.2f'%sent_loss_kl_dev,  '%.3f'%beta_eval],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "\n",
    "            print_sample(batch)\n",
    "            print_topic_sample()\n",
    "\n",
    "            time_start = time.time()\n",
    "            \n",
    "    epoch += 1\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "_prob_topic, _prob_topic_sents, _prob_topic_infer, _means_topic_infer = debug_value([prob_topic, prob_topic_sents, prob_topic_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.03536562, 0.00465868, 0.13373102, 0.02336785, 0.53393006,\n",
       "        0.0016737 , 0.02880868, 0.18370587, 0.00324288, 0.05151561],\n",
       "       dtype=float32),\n",
       " array([0.02469287, 0.00214033, 0.02091056, 0.2218967 , 0.40444553,\n",
       "        0.00085018, 0.0050975 , 0.152743  , 0.01029719, 0.1569261 ],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_i = 4\n",
    "_prob_topic_sents[batch_i], _prob_topic_infer[batch_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.7514409 , -0.02080724, -0.08754342,  2.9888659 ],\n",
       "       [ 0.7502979 ,  0.21387438, -0.21948494,  0.18009447],\n",
       "       [-0.07566352,  0.01762117, -0.02816802, -0.14794706],\n",
       "       [-0.24080695, -0.13825059,  0.41813016, -0.46455345],\n",
       "       [-1.0908539 , -0.26050574,  0.346231  , -0.6510836 ],\n",
       "       [ 0.17045614,  0.01826609, -0.01467549,  0.03143167],\n",
       "       [ 0.5033383 ,  0.0995406 ,  0.01209442,  0.21555611],\n",
       "       [ 0.50759584,  0.17584282, -0.22801438,  0.27870443],\n",
       "       [ 2.488893  ,  0.61977255, -0.53907454,  1.2611938 ],\n",
       "       [ 1.3132436 ,  0.26736495, -0.01388951,  0.16877195]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_means_topic_infer[0][:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_means_topic, b_means_topic = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"topic/dec/mean_topic\")\n",
    "\n",
    "pred_topic_embeddings, pred_topic_bow, pred_means_topic, pred_logvars_topic, pred_token_idxs, _w_means_topic, _b_means_topic, _w_mean_topic_infer = \\\n",
    "                                sess.run([topic_embeddings, topic_bow, means_topic, logvars_topic, topic_beam_output_token_idxs, w_means_topic, b_means_topic, w_mean_topic_infer], \n",
    "                                         feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "\n",
    "pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "\n",
    "pred_topics_freq_bow_indices = np.argsort(pred_topic_bow, 1)[:, ::-1][:, :10]\n",
    "pred_topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.52565742e-01, -1.33226626e-02, -8.45437944e-02,\n",
       "         2.99679995e+00,  1.44721472e+00, -6.55502033e+00,\n",
       "        -2.32751513e+00,  5.91111183e-01,  2.81779438e-01,\n",
       "         2.72793269e+00, -9.56127524e-01, -1.09030294e+00,\n",
       "         1.30759096e+00, -1.82090020e+00, -6.06395721e+00,\n",
       "        -1.48787630e+00, -3.89525056e+00, -1.45023775e+00,\n",
       "         1.95832759e-01,  1.97800130e-01, -3.61347735e-01,\n",
       "         8.49354506e-01, -9.01915932e+00,  4.58554626e-01,\n",
       "         1.66233563e+00,  1.80969310e+00,  1.30634201e+00,\n",
       "        -1.86664283e+00, -1.29454470e+00,  8.89331937e-01,\n",
       "        -2.18768215e+00, -2.42628002e+00],\n",
       "       [ 7.55044699e-01,  2.25778595e-01, -2.10457146e-01,\n",
       "         1.85192049e-01,  5.14446616e-01,  1.70179963e+00,\n",
       "        -3.59601378e-02, -5.91997385e-01,  3.97833884e-02,\n",
       "        -3.00024599e-01, -1.72570556e-01, -6.47799224e-02,\n",
       "        -3.95595640e-01,  1.90289974e-01, -3.47300947e-01,\n",
       "        -3.46785448e-02, -1.77649975e-01,  1.91410780e-01,\n",
       "         3.10804754e-01,  5.88759482e-02, -4.08443175e-02,\n",
       "        -4.65696633e-01,  6.18761778e-01, -2.88964093e-01,\n",
       "        -8.39534178e-02, -2.60046244e-01, -2.35720016e-02,\n",
       "         7.37678632e-02,  1.16820231e-01,  5.75327873e-01,\n",
       "         4.89302501e-02, -4.89990711e-01],\n",
       "       [-8.19532871e-02,  2.25753337e-02, -1.80968717e-02,\n",
       "        -1.47912204e-01, -1.66836157e-01, -9.38534364e-02,\n",
       "         3.53601903e-01,  5.26284873e-01, -5.84653258e-01,\n",
       "         9.12436247e-02, -3.20071667e-01,  1.55838951e-01,\n",
       "         8.94663557e-02, -3.03583816e-02, -3.29492092e-01,\n",
       "         4.40296531e-02,  1.58707410e-01,  3.43607903e-01,\n",
       "        -2.22983152e-01, -2.62850076e-02, -5.69496639e-02,\n",
       "         4.77437153e-02,  1.21313000e+00,  1.38558447e-01,\n",
       "         2.09363371e-01, -2.45513260e-01, -1.06658511e-01,\n",
       "        -6.43447042e-04,  1.29768848e-01, -1.41352434e-02,\n",
       "         2.97659099e-01,  2.43231669e-01],\n",
       "       [-2.47082263e-01, -1.27552599e-01,  4.26582724e-01,\n",
       "        -4.64522362e-01, -1.74074459e+00, -2.40167618e+00,\n",
       "         2.74904537e+00,  1.85965085e+00, -7.60190725e-01,\n",
       "        -1.25717819e-02, -1.43048489e+00,  6.14055812e-01,\n",
       "         1.68127060e-01,  1.60068065e-01,  3.89224267e+00,\n",
       "         1.06691611e+00,  6.72213018e-01,  7.30563641e-01,\n",
       "        -1.31468821e+00,  7.46384144e-01, -2.63170838e-01,\n",
       "         8.22323084e-01,  1.84379101e+00, -4.34337109e-01,\n",
       "        -1.52482438e+00, -6.39384687e-01, -5.12555361e-01,\n",
       "         4.94461209e-01,  4.64558192e-02, -1.61340153e+00,\n",
       "         1.73451722e+00,  2.26671886e+00],\n",
       "       [-1.10044563e+00, -2.51290828e-01,  3.58681381e-01,\n",
       "        -6.55616462e-01, -1.56611872e+00, -3.04870534e+00,\n",
       "         1.77632749e+00,  2.32729721e+00, -1.37192893e+00,\n",
       "         4.61123496e-01, -9.34595108e-01,  6.00279987e-01,\n",
       "         6.48963392e-01, -1.72896773e-01,  1.40204298e+00,\n",
       "         5.37843049e-01,  7.65979588e-01,  6.65097117e-01,\n",
       "        -1.27802229e+00,  1.86127573e-01, -1.71112329e-01,\n",
       "         9.27196503e-01,  2.16596317e+00,  3.97165775e-01,\n",
       "        -1.35653317e-01, -3.98448050e-01, -3.79777402e-01,\n",
       "         1.46370113e-01,  1.24489099e-01, -1.32163239e+00,\n",
       "         1.18885338e+00,  1.89692259e+00],\n",
       "       [ 1.57677352e-01,  3.50100733e-02,  2.25152820e-02,\n",
       "         1.62364841e-02, -2.49605209e-01, -2.72099972e-01,\n",
       "         6.38608575e-01,  4.71142113e-01, -4.01371837e-01,\n",
       "         6.49495125e-02, -5.43520272e-01,  1.24270871e-01,\n",
       "         9.73722991e-03,  1.45867467e-04,  4.33445036e-01,\n",
       "         1.89460903e-01, -2.54309177e-03,  3.08291018e-01,\n",
       "        -3.02097023e-01,  1.90895513e-01, -1.16939738e-01,\n",
       "         8.40579569e-02,  8.03793132e-01, -8.40446725e-02,\n",
       "        -2.18527317e-01, -2.43354619e-01, -1.14803530e-01,\n",
       "         7.43041858e-02,  4.32423614e-02, -1.70901924e-01,\n",
       "         4.41696197e-01,  3.65223259e-01],\n",
       "       [ 4.98636842e-01,  9.96657610e-02,  1.54079869e-02,\n",
       "         2.43335783e-01, -4.11391109e-01, -6.51156187e-01,\n",
       "         1.11103857e+00,  4.19318199e-01, -9.44855064e-02,\n",
       "        -5.79596460e-02, -8.99846792e-01,  4.92075607e-02,\n",
       "        -1.10783383e-01,  2.96255350e-02,  1.89542079e+00,\n",
       "         3.04682881e-01, -2.33959436e-01,  3.41812789e-01,\n",
       "        -3.46117139e-01,  4.91705507e-01, -2.25052804e-01,\n",
       "         1.95639789e-01,  1.76574126e-01, -3.98114443e-01,\n",
       "        -9.75063682e-01, -3.41502607e-01, -1.58675164e-01,\n",
       "         2.46177047e-01, -1.16263315e-01, -4.50962275e-01,\n",
       "         5.58147788e-01,  5.75673819e-01],\n",
       "       [ 5.11213064e-01,  1.86364084e-01, -2.21007630e-01,\n",
       "         2.85865456e-01,  6.07047379e-01,  1.10239923e+00,\n",
       "        -4.00829941e-01, -3.59083652e-01, -1.09341756e-01,\n",
       "         1.63500011e-02, -1.01464301e-01, -1.33794859e-01,\n",
       "        -1.56941369e-01, -3.47014517e-02, -1.33550930e+00,\n",
       "        -2.53736466e-01, -3.45748186e-01,  6.20219260e-02,\n",
       "         3.02092791e-01, -9.22171772e-02, -2.58896686e-02,\n",
       "        -3.31216037e-01,  2.10861266e-01, -3.48318964e-02,\n",
       "         3.90661746e-01, -8.10671076e-02,  7.22247511e-02,\n",
       "        -1.76043302e-01,  5.91074415e-02,  6.68273330e-01,\n",
       "        -2.22742289e-01, -6.97681427e-01],\n",
       "       [ 2.48691893e+00,  6.24517143e-01, -5.36843181e-01,\n",
       "         1.26683509e+00,  1.93868101e+00,  4.10826635e+00,\n",
       "        -9.96438384e-01, -2.63486481e+00,  1.38195121e+00,\n",
       "        -6.37818217e-01, -9.69472826e-02, -6.53301895e-01,\n",
       "        -1.09042633e+00,  3.00143957e-01, -9.44175959e-01,\n",
       "        -3.52568239e-01, -1.38472414e+00, -3.81049663e-01,\n",
       "         1.35197306e+00,  3.23426098e-01, -5.21752834e-02,\n",
       "        -1.25540090e+00, -1.98718190e+00, -1.11954558e+00,\n",
       "        -5.61906099e-01,  3.32860649e-03,  3.08790714e-01,\n",
       "        -7.47221112e-02, -1.69080809e-01,  1.72242546e+00,\n",
       "        -6.88383222e-01, -2.19920111e+00],\n",
       "       [ 1.32513857e+00,  2.76965857e-01, -1.01696029e-02,\n",
       "         1.76905990e-01, -1.63454860e-01,  1.50284636e+00,\n",
       "         1.50841808e+00, -5.01805782e-01,  4.44041789e-01,\n",
       "        -7.27930546e-01, -8.90547514e-01,  9.47208703e-02,\n",
       "        -7.26988614e-01,  5.11883020e-01,  2.91967845e+00,\n",
       "         7.20722020e-01, -1.54182315e-02,  3.59919190e-01,\n",
       "        -9.38798264e-02,  7.46873617e-01, -1.68985024e-01,\n",
       "        -3.08203816e-01,  6.89112842e-01, -1.10661101e+00,\n",
       "        -1.73939395e+00, -6.04618907e-01, -2.95623749e-01,\n",
       "         5.28078556e-01,  4.56437208e-02, -1.67823538e-01,\n",
       "         9.72209930e-01,  4.77726936e-01]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_w_mean_topic_infer[:, :10, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_enc_state_infer, _means_topic_infer = debug_value([enc_state_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_enc_state_infer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_means_topic_infer[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
