{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "# %matplotlib nbagg\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from data_structure import get_batches, get_test_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "\n",
    "from topic_beam_search_decoder import BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '2', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/synthetic/instances_ncrp.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/topic_vae', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'bags', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 1000, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 1000, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "# flags.DEFINE_string('opt', 'Adam', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.01, 'lr')\n",
    "flags.DEFINE_float('reg', 10., 'regularization term')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_bool('warmup', True, 'flg of warming up')\n",
    "flags.DEFINE_integer('epochs_cycle', 5, 'number of epochs within a cycle')\n",
    "flags.DEFINE_float('r_cycle', 0.5, 'proportion used to increase beta within a cycle')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 20, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_bow', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_hidden_topic', 512, 'dim_hidden_topic')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "flags.DEFINE_bool('bidirectional', True, 'flg of bidirectional encoding')\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(instances, batch_size, iterator=False):\n",
    "    iter_instances = iter(instances)\n",
    "    n_batch = len(instances)//batch_size\n",
    "    \n",
    "    batches = [(i_batch, [next(iter_instances) for i_doc in range(batch_size)]) for i_batch in range(n_batch)]\n",
    "    \n",
    "    if iterator: batches = iter(batches)\n",
    "    return batches\n",
    "\n",
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "flags.DEFINE_integer('cycle_steps', len(train_batches)*config.epochs_cycle, 'number of steps for each cycle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0,
     10,
     18,
     24,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables):\n",
    "    sample_batch = dev_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "def check_shape(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()\n",
    "    \n",
    "def check_value(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train'):\n",
    "    bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "    feed_dict = {\n",
    "                t_variables['bow']: bow, \n",
    "                t_variables['keep_prob']: keep_prob\n",
    "    }\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def get_depth(parent_idx=0, tree_depth=None, depth=1):\n",
    "    if tree_depth is None: tree_depth={0: depth}\n",
    "\n",
    "    child_idxs = tree_idxs[parent_idx]\n",
    "    depth +=1\n",
    "    for child_idx in child_idxs:\n",
    "        tree_depth[child_idx] = depth\n",
    "        if child_idx in tree_idxs: get_depth(child_idx, tree_depth, depth)\n",
    "    return tree_depth\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['bow'] = tf.placeholder(tf.float32, [None, config.dim_bow])\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32)\n",
    "\n",
    "# tree_idxs = {0:[1, 2, 3], \n",
    "#                       1:[10, 11], 2:[20, 21], 3:[30, 31]}\n",
    "\n",
    "tree_idxs = {0:[1, 2, 3], \n",
    "              1:[10, 11, 12], 2:[20, 21, 22], 3:[30, 31, 32]}\n",
    "\n",
    "# tree_idxs = {0:[1, 2], \n",
    "#              1:[11, 12], 2:[21]}\n",
    "\n",
    "# tree_idxs = {0:[1, 2, 3], \n",
    "#                       1:[10, 11], 2:[20, 21], 3:[30, 31],\n",
    "#                       10: [100, 101], 11: [110, 111], 20: [200, 201], 21: [210, 211], 30:[300, 301], 31:[310, 311]}\n",
    "\n",
    "topic_idxs = [0] + [idx for child_idxs in tree_idxs.values() for idx in child_idxs]\n",
    "\n",
    "child_to_parent_idxs = {child_idx: parent_idx for parent_idx, child_idxs in tree_idxs.items() for child_idx in child_idxs}\n",
    "\n",
    "tree_depth = get_depth()\n",
    "max_depth = max(tree_depth.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doubly rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoublyRNNCell:\n",
    "    def __init__(self, dim_hidden, output_layer=None):\n",
    "        self.dim_hidden = dim_hidden\n",
    "        \n",
    "        self.ancestral_layer=tf.layers.Dense(units=dim_hidden, activation=tf.nn.tanh, name='ancestral')\n",
    "        self.fraternal_layer=tf.layers.Dense(units=dim_hidden, activation=tf.nn.tanh, name='fraternal')\n",
    "#         self.hidden_layer = tf.layers.Dense(units=dim_hidden, activation=tf.nn.tanh, name='hidden')\n",
    "        self.hidden_layer = tf.layers.Dense(units=dim_hidden, name='hidden')\n",
    "        \n",
    "        self.output_layer=output_layer\n",
    "        \n",
    "    def __call__(self, state_ancestral, state_fraternal, reuse=True):\n",
    "        with tf.variable_scope('input', reuse=reuse):\n",
    "            state_ancestral = self.ancestral_layer(state_ancestral)\n",
    "            state_fraternal = self.fraternal_layer(state_fraternal)\n",
    "\n",
    "        with tf.variable_scope('output', reuse=reuse):\n",
    "            state_hidden = self.hidden_layer(state_ancestral + state_fraternal)\n",
    "            if self.output_layer is not None: \n",
    "                output = self.output_layer(state_hidden)\n",
    "            else:\n",
    "                output = state_hidden\n",
    "            \n",
    "        return output, state_hidden\n",
    "    \n",
    "    def get_initial_state(self, name):\n",
    "        initial_state = tf.get_variable(name, [1, self.dim_hidden], dtype=tf.float32)\n",
    "        return initial_state\n",
    "    \n",
    "    def get_zero_state(self, name):\n",
    "        zero_state = tf.zeros([1, self.dim_hidden], dtype=tf.float32, name=name)\n",
    "        return zero_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubly_rnn(dim_hidden, tree_idxs, initial_state_parent=None, initial_state_sibling=None, output_layer=None, name=''):\n",
    "    outputs, states_parent = {}, {}\n",
    "    \n",
    "    with tf.variable_scope(name, reuse=False):\n",
    "        doubly_rnn_cell = DoublyRNNCell(dim_hidden, output_layer)\n",
    "\n",
    "        if initial_state_parent is None: \n",
    "            initial_state_parent = doubly_rnn_cell.get_initial_state('init_state_parent')\n",
    "#             initial_state_parent = doubly_rnn_cell.get_zero_state('init_state_parent')\n",
    "        if initial_state_sibling is None: \n",
    "#             initial_state_sibling = doubly_rnn_cell.get_initial_state('init_state_sibling')\n",
    "            initial_state_sibling = doubly_rnn_cell.get_zero_state('init_state_sibling')\n",
    "        output, state_sibling = doubly_rnn_cell(initial_state_parent, initial_state_sibling, reuse=False)\n",
    "        outputs[0], states_parent[0] = output, state_sibling\n",
    "\n",
    "        for parent_idx, child_idxs in tree_idxs.items():\n",
    "            state_parent = states_parent[parent_idx]\n",
    "            state_sibling = initial_state_sibling\n",
    "            for child_idx in child_idxs:\n",
    "                output, state_sibling = doubly_rnn_cell(state_parent, state_sibling)\n",
    "                outputs[child_idx], states_parent[child_idx] = output, state_sibling\n",
    "\n",
    "    return outputs, states_parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell:\n",
    "    def __init__(self, dim_hidden, output_layer=None):\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.hidden_layer = tf.layers.Dense(units=dim_hidden, activation=tf.nn.tanh, name='hidden')\n",
    "        self.output_layer=output_layer\n",
    "        \n",
    "    def __call__(self, state, reuse=True):\n",
    "        with tf.variable_scope('output', reuse=reuse):\n",
    "            state_hidden = self.hidden_layer(state)\n",
    "            if self.output_layer is not None: \n",
    "                output = self.output_layer(state_hidden)\n",
    "            else:\n",
    "                output = state_hidden\n",
    "            \n",
    "        return output, state_hidden\n",
    "    \n",
    "    def get_initial_state(self, name):\n",
    "        initial_state = tf.get_variable(name, [1, self.dim_hidden], dtype=tf.float32)\n",
    "        return initial_state\n",
    "    \n",
    "    def get_zero_state(self, name):\n",
    "        zero_state = tf.zeros([1, self.dim_hidden], dtype=tf.float32, name=name)\n",
    "        return zero_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(dim_hidden, max_depth, initial_state=None, output_layer=None, name=''):\n",
    "    outputs, states_hidden = [], []\n",
    "    \n",
    "    with tf.variable_scope(name, reuse=False):\n",
    "        rnn_cell = RNNCell(dim_hidden, output_layer)\n",
    "\n",
    "        if initial_state is not None: \n",
    "            state_hidden = initial_state\n",
    "        else:\n",
    "            state_hidden = rnn_cell.get_initial_state('init_state')\n",
    "#             state_hidden = rnn_cell.get_zero_state('init_state_parent')\n",
    "        \n",
    "        for depth in range(max_depth):\n",
    "            if depth == 0:                \n",
    "                output, state_hidden = rnn_cell(state_hidden, reuse=False)\n",
    "            else:\n",
    "                output, state_hidden = rnn_cell(state_hidden, reuse=True)\n",
    "            outputs.append(output)\n",
    "            states_hidden.append(state_hidden)\n",
    "\n",
    "    outputs = tf.concat(outputs, 1)\n",
    "    states_hidden = tf.concat(states_hidden, 0)\n",
    "    return outputs, states_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stick break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nCRP(tree_sticks_topic):\n",
    "    tree_prob_topic = {}\n",
    "    tree_prob_leaf = {}\n",
    "    # calculate topic probability and save\n",
    "    tree_prob_topic[0] = 1.\n",
    "    \n",
    "    for parent_idx, child_idxs in tree_idxs.items():\n",
    "        rest_prob_topic = tree_prob_topic[parent_idx]\n",
    "        for child_idx in child_idxs:\n",
    "            stick_topic = tree_sticks_topic[child_idx]\n",
    "            if child_idx == child_idxs[-1]:\n",
    "                prob_topic = rest_prob_topic * 1.\n",
    "            else:\n",
    "                prob_topic = rest_prob_topic * stick_topic\n",
    "            \n",
    "            if not child_idx in tree_idxs: # leaf childs\n",
    "                tree_prob_leaf[child_idx] = prob_topic\n",
    "            else:\n",
    "                tree_prob_topic[child_idx] = prob_topic\n",
    "                \n",
    "            rest_prob_topic -= prob_topic\n",
    "            \n",
    "    return tree_prob_leaf\n",
    "\n",
    "def sbp(sticks_depth, max_depth):\n",
    "    prob_depth_list = []\n",
    "    rest_prob_depth = 1.\n",
    "    for depth in range(max_depth):\n",
    "        stick_depth = tf.expand_dims(sticks_depth[:, depth], 1)\n",
    "        if depth == max_depth -1:\n",
    "            prob_depth = rest_prob_depth * 1.\n",
    "        else:\n",
    "            prob_depth = rest_prob_depth * stick_depth\n",
    "        prob_depth_list.append(prob_depth)\n",
    "        rest_prob_depth -= prob_depth\n",
    "    \n",
    "    prob_depth = tf.concat(prob_depth_list, 1)\n",
    "    return prob_depth\n",
    "\n",
    "def get_ancestor_idxs(leaf_idx, ancestor_idxs = None):\n",
    "    if ancestor_idxs is None: ancestor_idxs = [leaf_idx]\n",
    "    \n",
    "    parent_idx = child_to_parent_idxs[leaf_idx]\n",
    "    ancestor_idxs += [parent_idx]\n",
    "    if parent_idx in child_to_parent_idxs: get_ancestor_idxs(parent_idx, ancestor_idxs)\n",
    "    return ancestor_idxs[::-1]\n",
    "\n",
    "def get_prob_topic(tree_prob_leaf, prob_depth):\n",
    "    tree_prob_topic = defaultdict(float)\n",
    "    \n",
    "    leaf_ancestor_idxs = {leaf_idx: get_ancestor_idxs(leaf_idx) for leaf_idx in tree_prob_leaf}\n",
    "    for leaf_idx, ancestor_idxs in leaf_ancestor_idxs.items():\n",
    "        prob_leaf = tree_prob_leaf[leaf_idx]\n",
    "        for i, ancestor_idx in enumerate(ancestor_idxs):\n",
    "            prob_ancestor = prob_leaf * tf.expand_dims(prob_depth[:, i], -1)\n",
    "            tree_prob_topic[ancestor_idx] += prob_ancestor\n",
    "    prob_topic = tf.concat([tree_prob_topic[topic_idx] for topic_idx in topic_idxs], -1)\n",
    "    return prob_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree_topic_bow(tree_topic_embeddings):\n",
    "    def softmax_with_temperature(logits, axis=None, name=None, temperature=1.):\n",
    "        if axis is None:\n",
    "            axis = -1\n",
    "        return tf.exp(logits / temperature) / tf.reduce_sum(tf.exp(logits / temperature), axis=axis)\n",
    "\n",
    "    tree_topic_bow = {}\n",
    "    for topic_idx, depth in tree_depth.items():\n",
    "        topic_embedding = tree_topic_embeddings[topic_idx]\n",
    "        temperature = tf.constant(10. * (10 ** (1./depth)), dtype=tf.float32)\n",
    "        logits = tf.matmul(topic_embedding, bow_embeddings, transpose_b=True)\n",
    "        tree_topic_bow[topic_idx] = softmax_with_temperature(logits, axis=-1, temperature=temperature)\n",
    "    \n",
    "    return tree_topic_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latents_bow(bow, name):\n",
    "    with tf.variable_scope(name, reuse=False):\n",
    "        hidden_bow_ = tf.layers.Dense(units=config.dim_hidden_bow, activation=tf.nn.tanh, name='hidden_bow')(bow)\n",
    "        hidden_bow = tf.layers.Dropout(t_variables['keep_prob'])(hidden_bow_)\n",
    "        means_bow = tf.layers.Dense(units=config.dim_latent_bow, name='mean_bow')(hidden_bow)\n",
    "        logvars_bow = tf.layers.Dense(units=config.dim_latent_bow, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_bow')(hidden_bow)        \n",
    "    return means_bow, logvars_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode bow\n",
    "with tf.variable_scope('topic/enc', reuse=False):\n",
    "    means_tree, logvars_tree = get_latents_bow(t_variables['bow'], 'tree') # sample latent vectors of tree path\n",
    "    means_depth, logvars_depth = get_latents_bow(t_variables['bow'], 'depth') # sample latent vectors  of depth\n",
    "    \n",
    "    latents_tree = sample_latents(means_tree, logvars_tree) # sample latent vectors\n",
    "    latents_depth = sample_latents(means_depth, logvars_depth) # sample latent vectors\n",
    "    \n",
    "    tree_prob_layer = lambda h: tf.nn.sigmoid(tf.matmul(latents_tree, h, transpose_b=True))\n",
    "    depth_prob_layer = lambda h: tf.nn.sigmoid(tf.matmul(latents_depth, h, transpose_b=True))\n",
    "    \n",
    "    tree_sticks_topic, tree_states_sticks_topic = doubly_rnn(config.dim_latent_bow, tree_idxs, output_layer=tree_prob_layer, name='sticks_topic')\n",
    "    tree_prob_leaf = nCRP(tree_sticks_topic)\n",
    "#     prob_depth = tf.layers.Dense(units=max_depth, activation=tf.nn.softmax, name='prob_depth')(latents_bow) # inference of topic probabilities\n",
    "    sticks_depth, _ = rnn(config.dim_latent_bow, max_depth, output_layer=depth_prob_layer, name='prob_depth')\n",
    "    prob_depth = sbp(sticks_depth, max_depth)\n",
    "    \n",
    "    prob_topic = get_prob_topic(tree_prob_leaf, prob_depth)\n",
    "\n",
    "# decode bow\n",
    "with tf.variable_scope('shared', reuse=False):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "bow_embeddings = tf.nn.embedding_lookup(embeddings, bow_idxs) # embeddings of each bow features\n",
    "\n",
    "with tf.variable_scope('topic/dec', reuse=False):\n",
    "#     tree_topic_embeddings, tree_states_topic_embeddings = doubly_rnn(config.dim_emb, tree_idxs, name='emb_topic')\n",
    "    emb_layer = lambda h: tf.layers.Dense(units=config.dim_emb, name='output')(tf.nn.tanh(h))\n",
    "    tree_topic_embeddings, tree_states_topic_embeddings = doubly_rnn(config.dim_emb, tree_idxs, output_layer=emb_layer, name='emb_topic')\n",
    "#     topic_embeddings = tf.get_variable('topic_emb', [len(topic_idxs), config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of topics\n",
    "#     tree_topic_embeddings = {topic_idx: tf.expand_dims(topic_embeddings[topic_idxs.index(topic_idx)], 0) for topic_idx in topic_idxs}\n",
    "\n",
    "    tree_topic_bow = get_tree_topic_bow(tree_topic_embeddings) # bow vectors for each topic\n",
    "    \n",
    "    topic_bow = tf.concat([tree_topic_bow[topic_idx] for topic_idx in topic_idxs], 0)\n",
    "    logits_bow = tf_log(tf.matmul(prob_topic, topic_bow)) # predicted bow distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree_mask_reg(tree_idxs):\n",
    "    tree_mask_reg = np.ones([len(topic_idxs), len(topic_idxs)], dtype=np.float32)\n",
    "    parent_to_descendant_idxs = {parent_idx: get_descendant_idxs(parent_idx) for parent_idx in tree_idxs}\n",
    "    \n",
    "    for parent_idx, descendant_idxs in parent_to_descendant_idxs.items():\n",
    "        for descendant_idx in descendant_idxs:\n",
    "            tree_mask_reg[topic_idxs.index(parent_idx), topic_idxs.index(descendant_idx)] = tree_mask_reg[topic_idxs.index(descendant_idx), topic_idxs.index(parent_idx)] = 0.\n",
    "            \n",
    "    return tree_mask_reg\n",
    "\n",
    "def get_depth_mask_reg(tree_idxs):\n",
    "    depth_mask_reg = np.zeros([len(topic_idxs), len(topic_idxs)], dtype=np.float32)\n",
    "\n",
    "    depth_topic_idxs = defaultdict(list)\n",
    "    for topic_idx, _depth in tree_depth.items():\n",
    "        depth_topic_idxs[_depth].append(topic_idx)\n",
    "    \n",
    "    for _depth, child_idxs in depth_topic_idxs.items():\n",
    "        for child_idx1 in child_idxs:\n",
    "            for child_idx2 in child_idxs:\n",
    "                depth_mask_reg[topic_idxs.index(child_idx1), topic_idxs.index(child_idx2)] = 1.\n",
    "                \n",
    "    return depth_mask_reg\n",
    "\n",
    "def get_descendant_idxs(parent_idx, descendant_idxs = None):\n",
    "    if descendant_idxs is None: descendant_idxs = []\n",
    "    \n",
    "    child_idxs = tree_idxs[parent_idx]\n",
    "    descendant_idxs += child_idxs\n",
    "    for child_idx in child_idxs:\n",
    "        if child_idx in tree_idxs: get_descendant_idxs(child_idx, descendant_idxs)\n",
    "    return descendant_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define losses\n",
    "topic_losses_recon = -tf.reduce_sum(tf.multiply(t_variables['bow'], logits_bow), 1)\n",
    "topic_loss_recon = tf.reduce_mean(topic_losses_recon) # negative log likelihood of each words\n",
    "\n",
    "topic_loss_kl_tree = compute_kl_loss(means_tree, logvars_tree) # KL divergence b/w latent dist & gaussian std\n",
    "topic_loss_kl_depth = compute_kl_loss(means_depth, logvars_depth) # KL divergence b/w latent dist & gaussian std\n",
    "topic_loss_kl = topic_loss_kl_tree + topic_loss_kl_depth\n",
    "\n",
    "topic_bow_norm = topic_bow / tf.norm(topic_bow, axis=1, keepdims=True)\n",
    "topic_dots = tf.clip_by_value(tf.matmul(topic_bow_norm, tf.transpose(topic_bow_norm)), -1., 1.)\n",
    "# topic_loss_reg = tf.reduce_mean(tf.square(topic_dots - tf.eye(len(topic_idxs))))\n",
    "tree_mask_reg = get_tree_mask_reg(tree_idxs)\n",
    "# tree_mask_reg = get_depth_mask_reg(tree_idxs)\n",
    "topic_losses_reg = tf.square(topic_dots - tf.eye(len(topic_idxs))) * tree_mask_reg\n",
    "topic_loss_reg = tf.reduce_sum(topic_losses_reg) / tf.reduce_sum(tree_mask_reg)\n",
    "\n",
    "# topic_embeddings = tf.concat([tree_topic_embeddings[topic_idx] for topic_idx in topic_idxs], 0)\n",
    "# topic_embeddings_norm = topic_embeddings / tf.norm(topic_embeddings, axis=1, keepdims=True)\n",
    "# topic_dots = tf.clip_by_value(tf.matmul(topic_embeddings_norm, tf.transpose(topic_embeddings_norm)), -1., 1.)\n",
    "# tree_mask_reg = get_tree_mask_reg(tree_idxs)\n",
    "# topic_loss_reg = tf.reduce_sum(tf.square(topic_dots - tf.eye(len(topic_idxs))) * tree_mask_reg) / tf.reduce_sum(tree_mask_reg)\n",
    "\n",
    "# topic_embeddings = tf.concat([tree_topic_embeddings[topic_idx] for topic_idx in topic_idxs], 0)\n",
    "# topic_embeddings_norm = topic_embeddings / tf.norm(topic_embeddings, axis=1, keepdims=True)\n",
    "# topic_dots = tf.clip_by_value(tf.matmul(topic_embeddings_norm, tf.transpose(topic_embeddings_norm)), -1., 1.)\n",
    "# topic_angles = tf.acos(topic_dots)\n",
    "# tree_mask_reg = get_tree_mask_reg(tree_idxs)\n",
    "\n",
    "# mean_angles = tf.reduce_sum(topic_angles*tree_mask_reg) / tf.reduce_sum(tree_mask_reg)\n",
    "# var_angles = tf.reduce_sum(tf.square(topic_angles-mean_angles)*tree_mask_reg) / tf.reduce_sum(tree_mask_reg)\n",
    "# mean_angles = tf.reduce_mean(topic_angles)\n",
    "# var_angles = tf.reduce_mean(tf.square(topic_angles-mean_angles))\n",
    "# mean_angles = tf.asin(tf.sqrt(tf.linalg.det(topic_dots * tree_mask_reg)))\n",
    "# var_angles = tf.square(tf.constant(np.pi/2., dtype=tf.float32)-mean_angles)\n",
    "\n",
    "# topic_loss_reg = var_angles - mean_angles\n",
    "\n",
    "global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "\n",
    "loss = topic_loss_recon + topic_loss_kl + config.reg * topic_loss_reg\n",
    "\n",
    "# define optimizer\n",
    "if config.opt == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif config.opt == 'Adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "\n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars, global_step=global_step)\n",
    "\n",
    "# monitor\n",
    "n_bow = tf.reduce_sum(t_variables['bow'], 1)\n",
    "topic_ppls = tf.divide(topic_losses_recon, tf.maximum(1e-5, n_bow))\n",
    "topics_freq_bow_indices = tf.nn.top_k(topic_bow, 5, name='topic_freq_bow').indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, ppls_batch \\\n",
    "            = sess.run([loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, topic_ppls], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "    loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    return loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, ppl_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import Normalize # Normalizeã‚’import\n",
    "\n",
    "def print_topic_sample():\n",
    "    _topics_bow = sess.run(topic_bow)\n",
    "    plt.figure(figsize=(9, 15))\n",
    "    \n",
    "    _topic_bow = _topics_bow[0]\n",
    "    plt.subplot(5,3,2)\n",
    "    plt.axis('off')\n",
    "    plt.title(topic_idxs[0])\n",
    "    plt.imshow(_topic_bow.reshape(30,30), cmap='Wistia', norm=Normalize(vmin=0., vmax=0.01))\n",
    "    \n",
    "    for i in range(1, len(topic_idxs)):\n",
    "        _topic_bow = _topics_bow[i]\n",
    "        plt.subplot(5,3,i+3)\n",
    "        plt.axis('off')\n",
    "        plt.title(topic_idxs[i])\n",
    "        plt.imshow(_topic_bow.reshape(30,30), cmap='Wistia', norm=Normalize(vmin=0., vmax=0.01))\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "saver = tf.train.Saver(max_to_keep=10)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','TM','','','','VALID:','TM','','',''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL','REG','LOSS','PPL','NLL','KL','REG']]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>VALID:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>63</td>\n",
       "      <td>686.79</td>\n",
       "      <td>874</td>\n",
       "      <td>677.35</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.82</td>\n",
       "      <td>682.65</td>\n",
       "      <td>854</td>\n",
       "      <td>675.04</td>\n",
       "      <td>1.15</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>127</td>\n",
       "      <td>683.51</td>\n",
       "      <td>853</td>\n",
       "      <td>674.87</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.72</td>\n",
       "      <td>679.73</td>\n",
       "      <td>827</td>\n",
       "      <td>671.75</td>\n",
       "      <td>1.83</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "      <td>35</td>\n",
       "      <td>681.68</td>\n",
       "      <td>841</td>\n",
       "      <td>673.48</td>\n",
       "      <td>1.64</td>\n",
       "      <td>0.66</td>\n",
       "      <td>677.90</td>\n",
       "      <td>825</td>\n",
       "      <td>671.53</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>14</td>\n",
       "      <td>25</td>\n",
       "      <td>99</td>\n",
       "      <td>680.34</td>\n",
       "      <td>834</td>\n",
       "      <td>672.61</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.60</td>\n",
       "      <td>676.46</td>\n",
       "      <td>816</td>\n",
       "      <td>670.43</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>15</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>679.20</td>\n",
       "      <td>828</td>\n",
       "      <td>671.88</td>\n",
       "      <td>1.89</td>\n",
       "      <td>0.54</td>\n",
       "      <td>675.08</td>\n",
       "      <td>810</td>\n",
       "      <td>669.75</td>\n",
       "      <td>2.23</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6000</th>\n",
       "      <td>15</td>\n",
       "      <td>38</td>\n",
       "      <td>71</td>\n",
       "      <td>678.29</td>\n",
       "      <td>823</td>\n",
       "      <td>671.27</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.50</td>\n",
       "      <td>674.61</td>\n",
       "      <td>806</td>\n",
       "      <td>669.16</td>\n",
       "      <td>2.39</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7000</th>\n",
       "      <td>15</td>\n",
       "      <td>44</td>\n",
       "      <td>135</td>\n",
       "      <td>677.59</td>\n",
       "      <td>819</td>\n",
       "      <td>670.78</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.47</td>\n",
       "      <td>674.26</td>\n",
       "      <td>802</td>\n",
       "      <td>668.76</td>\n",
       "      <td>2.48</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8000</th>\n",
       "      <td>13</td>\n",
       "      <td>51</td>\n",
       "      <td>43</td>\n",
       "      <td>677.03</td>\n",
       "      <td>815</td>\n",
       "      <td>670.37</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.45</td>\n",
       "      <td>674.28</td>\n",
       "      <td>802</td>\n",
       "      <td>668.69</td>\n",
       "      <td>2.61</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9000</th>\n",
       "      <td>15</td>\n",
       "      <td>57</td>\n",
       "      <td>107</td>\n",
       "      <td>676.57</td>\n",
       "      <td>813</td>\n",
       "      <td>670.02</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>673.75</td>\n",
       "      <td>799</td>\n",
       "      <td>668.29</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>14</td>\n",
       "      <td>64</td>\n",
       "      <td>15</td>\n",
       "      <td>676.11</td>\n",
       "      <td>810</td>\n",
       "      <td>669.73</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.41</td>\n",
       "      <td>672.57</td>\n",
       "      <td>797</td>\n",
       "      <td>668.09</td>\n",
       "      <td>2.80</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11000</th>\n",
       "      <td>16</td>\n",
       "      <td>70</td>\n",
       "      <td>79</td>\n",
       "      <td>675.69</td>\n",
       "      <td>808</td>\n",
       "      <td>669.47</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.39</td>\n",
       "      <td>672.60</td>\n",
       "      <td>796</td>\n",
       "      <td>668.00</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12000</th>\n",
       "      <td>14</td>\n",
       "      <td>76</td>\n",
       "      <td>143</td>\n",
       "      <td>675.33</td>\n",
       "      <td>806</td>\n",
       "      <td>669.25</td>\n",
       "      <td>2.38</td>\n",
       "      <td>0.37</td>\n",
       "      <td>672.58</td>\n",
       "      <td>797</td>\n",
       "      <td>668.07</td>\n",
       "      <td>2.88</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13000</th>\n",
       "      <td>14</td>\n",
       "      <td>83</td>\n",
       "      <td>51</td>\n",
       "      <td>675.03</td>\n",
       "      <td>805</td>\n",
       "      <td>669.06</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.35</td>\n",
       "      <td>672.39</td>\n",
       "      <td>795</td>\n",
       "      <td>667.84</td>\n",
       "      <td>2.92</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14000</th>\n",
       "      <td>14</td>\n",
       "      <td>89</td>\n",
       "      <td>115</td>\n",
       "      <td>674.76</td>\n",
       "      <td>803</td>\n",
       "      <td>668.89</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.34</td>\n",
       "      <td>672.38</td>\n",
       "      <td>795</td>\n",
       "      <td>667.88</td>\n",
       "      <td>2.87</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>15</td>\n",
       "      <td>96</td>\n",
       "      <td>23</td>\n",
       "      <td>674.53</td>\n",
       "      <td>802</td>\n",
       "      <td>668.74</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.33</td>\n",
       "      <td>672.48</td>\n",
       "      <td>796</td>\n",
       "      <td>667.97</td>\n",
       "      <td>2.89</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16000</th>\n",
       "      <td>16</td>\n",
       "      <td>102</td>\n",
       "      <td>87</td>\n",
       "      <td>674.32</td>\n",
       "      <td>801</td>\n",
       "      <td>668.61</td>\n",
       "      <td>2.53</td>\n",
       "      <td>0.32</td>\n",
       "      <td>672.49</td>\n",
       "      <td>796</td>\n",
       "      <td>667.91</td>\n",
       "      <td>2.95</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17000</th>\n",
       "      <td>16</td>\n",
       "      <td>108</td>\n",
       "      <td>151</td>\n",
       "      <td>674.14</td>\n",
       "      <td>800</td>\n",
       "      <td>668.49</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0.31</td>\n",
       "      <td>672.30</td>\n",
       "      <td>794</td>\n",
       "      <td>667.74</td>\n",
       "      <td>2.93</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18000</th>\n",
       "      <td>14</td>\n",
       "      <td>115</td>\n",
       "      <td>59</td>\n",
       "      <td>673.97</td>\n",
       "      <td>799</td>\n",
       "      <td>668.38</td>\n",
       "      <td>2.58</td>\n",
       "      <td>0.30</td>\n",
       "      <td>672.25</td>\n",
       "      <td>794</td>\n",
       "      <td>667.70</td>\n",
       "      <td>2.92</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19000</th>\n",
       "      <td>14</td>\n",
       "      <td>121</td>\n",
       "      <td>123</td>\n",
       "      <td>673.82</td>\n",
       "      <td>799</td>\n",
       "      <td>668.28</td>\n",
       "      <td>2.60</td>\n",
       "      <td>0.29</td>\n",
       "      <td>672.38</td>\n",
       "      <td>795</td>\n",
       "      <td>667.84</td>\n",
       "      <td>2.92</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>31</td>\n",
       "      <td>673.69</td>\n",
       "      <td>798</td>\n",
       "      <td>668.19</td>\n",
       "      <td>2.62</td>\n",
       "      <td>0.29</td>\n",
       "      <td>672.26</td>\n",
       "      <td>794</td>\n",
       "      <td>667.69</td>\n",
       "      <td>2.96</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21000</th>\n",
       "      <td>15</td>\n",
       "      <td>134</td>\n",
       "      <td>95</td>\n",
       "      <td>673.56</td>\n",
       "      <td>797</td>\n",
       "      <td>668.11</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.28</td>\n",
       "      <td>672.16</td>\n",
       "      <td>793</td>\n",
       "      <td>667.59</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22000</th>\n",
       "      <td>14</td>\n",
       "      <td>141</td>\n",
       "      <td>3</td>\n",
       "      <td>673.45</td>\n",
       "      <td>797</td>\n",
       "      <td>668.03</td>\n",
       "      <td>2.67</td>\n",
       "      <td>0.28</td>\n",
       "      <td>672.24</td>\n",
       "      <td>794</td>\n",
       "      <td>667.71</td>\n",
       "      <td>2.95</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23000</th>\n",
       "      <td>16</td>\n",
       "      <td>147</td>\n",
       "      <td>67</td>\n",
       "      <td>673.34</td>\n",
       "      <td>796</td>\n",
       "      <td>667.96</td>\n",
       "      <td>2.68</td>\n",
       "      <td>0.27</td>\n",
       "      <td>672.14</td>\n",
       "      <td>793</td>\n",
       "      <td>667.62</td>\n",
       "      <td>2.95</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24000</th>\n",
       "      <td>13</td>\n",
       "      <td>153</td>\n",
       "      <td>131</td>\n",
       "      <td>673.25</td>\n",
       "      <td>795</td>\n",
       "      <td>667.89</td>\n",
       "      <td>2.70</td>\n",
       "      <td>0.27</td>\n",
       "      <td>672.34</td>\n",
       "      <td>795</td>\n",
       "      <td>667.78</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>16</td>\n",
       "      <td>160</td>\n",
       "      <td>39</td>\n",
       "      <td>673.16</td>\n",
       "      <td>795</td>\n",
       "      <td>667.83</td>\n",
       "      <td>2.72</td>\n",
       "      <td>0.26</td>\n",
       "      <td>672.21</td>\n",
       "      <td>794</td>\n",
       "      <td>667.66</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26000</th>\n",
       "      <td>14</td>\n",
       "      <td>166</td>\n",
       "      <td>103</td>\n",
       "      <td>673.08</td>\n",
       "      <td>794</td>\n",
       "      <td>667.77</td>\n",
       "      <td>2.73</td>\n",
       "      <td>0.26</td>\n",
       "      <td>672.08</td>\n",
       "      <td>792</td>\n",
       "      <td>667.45</td>\n",
       "      <td>3.07</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27000</th>\n",
       "      <td>15</td>\n",
       "      <td>173</td>\n",
       "      <td>11</td>\n",
       "      <td>673.00</td>\n",
       "      <td>794</td>\n",
       "      <td>667.71</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>672.06</td>\n",
       "      <td>792</td>\n",
       "      <td>667.40</td>\n",
       "      <td>3.09</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28000</th>\n",
       "      <td>17</td>\n",
       "      <td>179</td>\n",
       "      <td>75</td>\n",
       "      <td>672.93</td>\n",
       "      <td>794</td>\n",
       "      <td>667.66</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.25</td>\n",
       "      <td>672.14</td>\n",
       "      <td>793</td>\n",
       "      <td>667.55</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29000</th>\n",
       "      <td>13</td>\n",
       "      <td>185</td>\n",
       "      <td>139</td>\n",
       "      <td>672.86</td>\n",
       "      <td>793</td>\n",
       "      <td>667.61</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0.25</td>\n",
       "      <td>672.03</td>\n",
       "      <td>792</td>\n",
       "      <td>667.45</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>16</td>\n",
       "      <td>192</td>\n",
       "      <td>47</td>\n",
       "      <td>672.80</td>\n",
       "      <td>793</td>\n",
       "      <td>667.56</td>\n",
       "      <td>2.79</td>\n",
       "      <td>0.24</td>\n",
       "      <td>672.14</td>\n",
       "      <td>793</td>\n",
       "      <td>667.54</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127000</th>\n",
       "      <td>16</td>\n",
       "      <td>814</td>\n",
       "      <td>15</td>\n",
       "      <td>671.18</td>\n",
       "      <td>782</td>\n",
       "      <td>666.06</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0.16</td>\n",
       "      <td>670.96</td>\n",
       "      <td>785</td>\n",
       "      <td>666.52</td>\n",
       "      <td>3.40</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128000</th>\n",
       "      <td>16</td>\n",
       "      <td>820</td>\n",
       "      <td>79</td>\n",
       "      <td>671.17</td>\n",
       "      <td>782</td>\n",
       "      <td>666.05</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0.16</td>\n",
       "      <td>671.17</td>\n",
       "      <td>786</td>\n",
       "      <td>666.69</td>\n",
       "      <td>3.43</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129000</th>\n",
       "      <td>16</td>\n",
       "      <td>826</td>\n",
       "      <td>143</td>\n",
       "      <td>671.17</td>\n",
       "      <td>782</td>\n",
       "      <td>666.04</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0.16</td>\n",
       "      <td>671.07</td>\n",
       "      <td>785</td>\n",
       "      <td>666.61</td>\n",
       "      <td>3.43</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130000</th>\n",
       "      <td>16</td>\n",
       "      <td>833</td>\n",
       "      <td>51</td>\n",
       "      <td>671.16</td>\n",
       "      <td>782</td>\n",
       "      <td>666.03</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0.16</td>\n",
       "      <td>670.98</td>\n",
       "      <td>784</td>\n",
       "      <td>666.48</td>\n",
       "      <td>3.46</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131000</th>\n",
       "      <td>15</td>\n",
       "      <td>839</td>\n",
       "      <td>115</td>\n",
       "      <td>671.16</td>\n",
       "      <td>782</td>\n",
       "      <td>666.02</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0.16</td>\n",
       "      <td>671.05</td>\n",
       "      <td>785</td>\n",
       "      <td>666.60</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132000</th>\n",
       "      <td>17</td>\n",
       "      <td>846</td>\n",
       "      <td>23</td>\n",
       "      <td>671.16</td>\n",
       "      <td>782</td>\n",
       "      <td>666.00</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.16</td>\n",
       "      <td>670.96</td>\n",
       "      <td>784</td>\n",
       "      <td>666.50</td>\n",
       "      <td>3.43</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133000</th>\n",
       "      <td>16</td>\n",
       "      <td>852</td>\n",
       "      <td>87</td>\n",
       "      <td>671.15</td>\n",
       "      <td>782</td>\n",
       "      <td>665.99</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.16</td>\n",
       "      <td>670.90</td>\n",
       "      <td>784</td>\n",
       "      <td>666.47</td>\n",
       "      <td>3.40</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134000</th>\n",
       "      <td>15</td>\n",
       "      <td>858</td>\n",
       "      <td>151</td>\n",
       "      <td>671.15</td>\n",
       "      <td>782</td>\n",
       "      <td>665.98</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.16</td>\n",
       "      <td>671.00</td>\n",
       "      <td>785</td>\n",
       "      <td>666.55</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135000</th>\n",
       "      <td>16</td>\n",
       "      <td>865</td>\n",
       "      <td>59</td>\n",
       "      <td>671.14</td>\n",
       "      <td>781</td>\n",
       "      <td>665.97</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.16</td>\n",
       "      <td>671.08</td>\n",
       "      <td>785</td>\n",
       "      <td>666.62</td>\n",
       "      <td>3.44</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136000</th>\n",
       "      <td>15</td>\n",
       "      <td>871</td>\n",
       "      <td>123</td>\n",
       "      <td>671.14</td>\n",
       "      <td>781</td>\n",
       "      <td>665.97</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.16</td>\n",
       "      <td>670.99</td>\n",
       "      <td>785</td>\n",
       "      <td>666.53</td>\n",
       "      <td>3.44</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137000</th>\n",
       "      <td>19</td>\n",
       "      <td>878</td>\n",
       "      <td>31</td>\n",
       "      <td>671.13</td>\n",
       "      <td>781</td>\n",
       "      <td>665.96</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.16</td>\n",
       "      <td>670.89</td>\n",
       "      <td>784</td>\n",
       "      <td>666.43</td>\n",
       "      <td>3.44</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138000</th>\n",
       "      <td>15</td>\n",
       "      <td>884</td>\n",
       "      <td>95</td>\n",
       "      <td>671.13</td>\n",
       "      <td>781</td>\n",
       "      <td>665.95</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.16</td>\n",
       "      <td>670.99</td>\n",
       "      <td>785</td>\n",
       "      <td>666.52</td>\n",
       "      <td>3.44</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139000</th>\n",
       "      <td>15</td>\n",
       "      <td>891</td>\n",
       "      <td>3</td>\n",
       "      <td>671.12</td>\n",
       "      <td>781</td>\n",
       "      <td>665.94</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.16</td>\n",
       "      <td>670.98</td>\n",
       "      <td>785</td>\n",
       "      <td>666.51</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140000</th>\n",
       "      <td>15</td>\n",
       "      <td>897</td>\n",
       "      <td>67</td>\n",
       "      <td>671.12</td>\n",
       "      <td>781</td>\n",
       "      <td>665.93</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.16</td>\n",
       "      <td>671.04</td>\n",
       "      <td>785</td>\n",
       "      <td>666.58</td>\n",
       "      <td>3.44</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141000</th>\n",
       "      <td>15</td>\n",
       "      <td>903</td>\n",
       "      <td>131</td>\n",
       "      <td>671.12</td>\n",
       "      <td>781</td>\n",
       "      <td>665.92</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.16</td>\n",
       "      <td>671.14</td>\n",
       "      <td>785</td>\n",
       "      <td>666.63</td>\n",
       "      <td>3.48</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142000</th>\n",
       "      <td>15</td>\n",
       "      <td>910</td>\n",
       "      <td>39</td>\n",
       "      <td>671.11</td>\n",
       "      <td>781</td>\n",
       "      <td>665.91</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.16</td>\n",
       "      <td>671.01</td>\n",
       "      <td>785</td>\n",
       "      <td>666.56</td>\n",
       "      <td>3.44</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143000</th>\n",
       "      <td>16</td>\n",
       "      <td>916</td>\n",
       "      <td>103</td>\n",
       "      <td>671.11</td>\n",
       "      <td>781</td>\n",
       "      <td>665.90</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.16</td>\n",
       "      <td>670.98</td>\n",
       "      <td>784</td>\n",
       "      <td>666.47</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144000</th>\n",
       "      <td>15</td>\n",
       "      <td>923</td>\n",
       "      <td>11</td>\n",
       "      <td>671.10</td>\n",
       "      <td>781</td>\n",
       "      <td>665.89</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.16</td>\n",
       "      <td>670.94</td>\n",
       "      <td>784</td>\n",
       "      <td>666.45</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145000</th>\n",
       "      <td>18</td>\n",
       "      <td>929</td>\n",
       "      <td>75</td>\n",
       "      <td>671.10</td>\n",
       "      <td>781</td>\n",
       "      <td>665.88</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.16</td>\n",
       "      <td>670.99</td>\n",
       "      <td>785</td>\n",
       "      <td>666.53</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146000</th>\n",
       "      <td>17</td>\n",
       "      <td>935</td>\n",
       "      <td>139</td>\n",
       "      <td>671.10</td>\n",
       "      <td>781</td>\n",
       "      <td>665.87</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.15</td>\n",
       "      <td>671.02</td>\n",
       "      <td>785</td>\n",
       "      <td>666.55</td>\n",
       "      <td>3.46</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147000</th>\n",
       "      <td>14</td>\n",
       "      <td>942</td>\n",
       "      <td>47</td>\n",
       "      <td>671.09</td>\n",
       "      <td>781</td>\n",
       "      <td>665.86</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.15</td>\n",
       "      <td>671.05</td>\n",
       "      <td>785</td>\n",
       "      <td>666.59</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148000</th>\n",
       "      <td>17</td>\n",
       "      <td>948</td>\n",
       "      <td>111</td>\n",
       "      <td>671.09</td>\n",
       "      <td>781</td>\n",
       "      <td>665.86</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.15</td>\n",
       "      <td>670.88</td>\n",
       "      <td>784</td>\n",
       "      <td>666.43</td>\n",
       "      <td>3.44</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149000</th>\n",
       "      <td>15</td>\n",
       "      <td>955</td>\n",
       "      <td>19</td>\n",
       "      <td>671.08</td>\n",
       "      <td>781</td>\n",
       "      <td>665.85</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.15</td>\n",
       "      <td>671.00</td>\n",
       "      <td>785</td>\n",
       "      <td>666.52</td>\n",
       "      <td>3.48</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150000</th>\n",
       "      <td>15</td>\n",
       "      <td>961</td>\n",
       "      <td>83</td>\n",
       "      <td>671.08</td>\n",
       "      <td>781</td>\n",
       "      <td>665.84</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.15</td>\n",
       "      <td>670.97</td>\n",
       "      <td>785</td>\n",
       "      <td>666.51</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151000</th>\n",
       "      <td>15</td>\n",
       "      <td>967</td>\n",
       "      <td>147</td>\n",
       "      <td>671.08</td>\n",
       "      <td>781</td>\n",
       "      <td>665.83</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.15</td>\n",
       "      <td>670.96</td>\n",
       "      <td>784</td>\n",
       "      <td>666.47</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152000</th>\n",
       "      <td>15</td>\n",
       "      <td>974</td>\n",
       "      <td>55</td>\n",
       "      <td>671.07</td>\n",
       "      <td>781</td>\n",
       "      <td>665.82</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.15</td>\n",
       "      <td>670.76</td>\n",
       "      <td>783</td>\n",
       "      <td>666.30</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153000</th>\n",
       "      <td>16</td>\n",
       "      <td>980</td>\n",
       "      <td>119</td>\n",
       "      <td>671.07</td>\n",
       "      <td>781</td>\n",
       "      <td>665.81</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.15</td>\n",
       "      <td>670.91</td>\n",
       "      <td>784</td>\n",
       "      <td>666.43</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154000</th>\n",
       "      <td>15</td>\n",
       "      <td>987</td>\n",
       "      <td>27</td>\n",
       "      <td>671.06</td>\n",
       "      <td>781</td>\n",
       "      <td>665.81</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.15</td>\n",
       "      <td>670.99</td>\n",
       "      <td>784</td>\n",
       "      <td>666.48</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155000</th>\n",
       "      <td>15</td>\n",
       "      <td>993</td>\n",
       "      <td>91</td>\n",
       "      <td>671.06</td>\n",
       "      <td>781</td>\n",
       "      <td>665.80</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.15</td>\n",
       "      <td>670.91</td>\n",
       "      <td>784</td>\n",
       "      <td>666.42</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156000</th>\n",
       "      <td>16</td>\n",
       "      <td>999</td>\n",
       "      <td>155</td>\n",
       "      <td>671.06</td>\n",
       "      <td>780</td>\n",
       "      <td>665.79</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.15</td>\n",
       "      <td>670.92</td>\n",
       "      <td>784</td>\n",
       "      <td>666.45</td>\n",
       "      <td>3.48</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       TRAIN:   TM                      VALID:   TM          \\\n",
       "       Time   Ep   Ct    LOSS  PPL     NLL    KL   REG    LOSS  PPL     NLL   \n",
       "1000     17    6   63  686.79  874  677.35  1.25  0.82  682.65  854  675.04   \n",
       "2000     15   12  127  683.51  853  674.87  1.44  0.72  679.73  827  671.75   \n",
       "3000     14   19   35  681.68  841  673.48  1.64  0.66  677.90  825  671.53   \n",
       "4000     14   25   99  680.34  834  672.61  1.78  0.60  676.46  816  670.43   \n",
       "5000     15   32    7  679.20  828  671.88  1.89  0.54  675.08  810  669.75   \n",
       "6000     15   38   71  678.29  823  671.27  1.98  0.50  674.61  806  669.16   \n",
       "7000     15   44  135  677.59  819  670.78  2.06  0.47  674.26  802  668.76   \n",
       "8000     13   51   43  677.03  815  670.37  2.13  0.45  674.28  802  668.69   \n",
       "9000     15   57  107  676.57  813  670.02  2.20  0.43  673.75  799  668.29   \n",
       "10000    14   64   15  676.11  810  669.73  2.27  0.41  672.57  797  668.09   \n",
       "11000    16   70   79  675.69  808  669.47  2.33  0.39  672.60  796  668.00   \n",
       "12000    14   76  143  675.33  806  669.25  2.38  0.37  672.58  797  668.07   \n",
       "13000    14   83   51  675.03  805  669.06  2.43  0.35  672.39  795  667.84   \n",
       "14000    14   89  115  674.76  803  668.89  2.46  0.34  672.38  795  667.88   \n",
       "15000    15   96   23  674.53  802  668.74  2.50  0.33  672.48  796  667.97   \n",
       "16000    16  102   87  674.32  801  668.61  2.53  0.32  672.49  796  667.91   \n",
       "17000    16  108  151  674.14  800  668.49  2.56  0.31  672.30  794  667.74   \n",
       "18000    14  115   59  673.97  799  668.38  2.58  0.30  672.25  794  667.70   \n",
       "19000    14  121  123  673.82  799  668.28  2.60  0.29  672.38  795  667.84   \n",
       "20000    16  128   31  673.69  798  668.19  2.62  0.29  672.26  794  667.69   \n",
       "21000    15  134   95  673.56  797  668.11  2.65  0.28  672.16  793  667.59   \n",
       "22000    14  141    3  673.45  797  668.03  2.67  0.28  672.24  794  667.71   \n",
       "23000    16  147   67  673.34  796  667.96  2.68  0.27  672.14  793  667.62   \n",
       "24000    13  153  131  673.25  795  667.89  2.70  0.27  672.34  795  667.78   \n",
       "25000    16  160   39  673.16  795  667.83  2.72  0.26  672.21  794  667.66   \n",
       "26000    14  166  103  673.08  794  667.77  2.73  0.26  672.08  792  667.45   \n",
       "27000    15  173   11  673.00  794  667.71  2.75  0.25  672.06  792  667.40   \n",
       "28000    17  179   75  672.93  794  667.66  2.76  0.25  672.14  793  667.55   \n",
       "29000    13  185  139  672.86  793  667.61  2.78  0.25  672.03  792  667.45   \n",
       "30000    16  192   47  672.80  793  667.56  2.79  0.24  672.14  793  667.54   \n",
       "...     ...  ...  ...     ...  ...     ...   ...   ...     ...  ...     ...   \n",
       "127000   16  814   15  671.18  782  666.06  3.23  0.16  670.96  785  666.52   \n",
       "128000   16  820   79  671.17  782  666.05  3.23  0.16  671.17  786  666.69   \n",
       "129000   16  826  143  671.17  782  666.04  3.23  0.16  671.07  785  666.61   \n",
       "130000   16  833   51  671.16  782  666.03  3.23  0.16  670.98  784  666.48   \n",
       "131000   15  839  115  671.16  782  666.02  3.23  0.16  671.05  785  666.60   \n",
       "132000   17  846   23  671.16  782  666.00  3.24  0.16  670.96  784  666.50   \n",
       "133000   16  852   87  671.15  782  665.99  3.24  0.16  670.90  784  666.47   \n",
       "134000   15  858  151  671.15  782  665.98  3.24  0.16  671.00  785  666.55   \n",
       "135000   16  865   59  671.14  781  665.97  3.24  0.16  671.08  785  666.62   \n",
       "136000   15  871  123  671.14  781  665.97  3.24  0.16  670.99  785  666.53   \n",
       "137000   19  878   31  671.13  781  665.96  3.25  0.16  670.89  784  666.43   \n",
       "138000   15  884   95  671.13  781  665.95  3.25  0.16  670.99  785  666.52   \n",
       "139000   15  891    3  671.12  781  665.94  3.25  0.16  670.98  785  666.51   \n",
       "140000   15  897   67  671.12  781  665.93  3.25  0.16  671.04  785  666.58   \n",
       "141000   15  903  131  671.12  781  665.92  3.25  0.16  671.14  785  666.63   \n",
       "142000   15  910   39  671.11  781  665.91  3.26  0.16  671.01  785  666.56   \n",
       "143000   16  916  103  671.11  781  665.90  3.26  0.16  670.98  784  666.47   \n",
       "144000   15  923   11  671.10  781  665.89  3.26  0.16  670.94  784  666.45   \n",
       "145000   18  929   75  671.10  781  665.88  3.26  0.16  670.99  785  666.53   \n",
       "146000   17  935  139  671.10  781  665.87  3.26  0.15  671.02  785  666.55   \n",
       "147000   14  942   47  671.09  781  665.86  3.27  0.15  671.05  785  666.59   \n",
       "148000   17  948  111  671.09  781  665.86  3.27  0.15  670.88  784  666.43   \n",
       "149000   15  955   19  671.08  781  665.85  3.27  0.15  671.00  785  666.52   \n",
       "150000   15  961   83  671.08  781  665.84  3.27  0.15  670.97  785  666.51   \n",
       "151000   15  967  147  671.08  781  665.83  3.27  0.15  670.96  784  666.47   \n",
       "152000   15  974   55  671.07  781  665.82  3.28  0.15  670.76  783  666.30   \n",
       "153000   16  980  119  671.07  781  665.81  3.28  0.15  670.91  784  666.43   \n",
       "154000   15  987   27  671.06  781  665.81  3.28  0.15  670.99  784  666.48   \n",
       "155000   15  993   91  671.06  781  665.80  3.28  0.15  670.91  784  666.42   \n",
       "156000   16  999  155  671.06  780  665.79  3.28  0.15  670.92  784  666.45   \n",
       "\n",
       "                    \n",
       "          KL   REG  \n",
       "1000    1.15  0.65  \n",
       "2000    1.83  0.61  \n",
       "3000    2.08  0.43  \n",
       "4000    2.18  0.38  \n",
       "5000    2.23  0.31  \n",
       "6000    2.39  0.31  \n",
       "7000    2.48  0.30  \n",
       "8000    2.61  0.30  \n",
       "9000    2.76  0.27  \n",
       "10000   2.80  0.17  \n",
       "11000   2.98  0.16  \n",
       "12000   2.88  0.16  \n",
       "13000   2.92  0.16  \n",
       "14000   2.87  0.16  \n",
       "15000   2.89  0.16  \n",
       "16000   2.95  0.16  \n",
       "17000   2.93  0.16  \n",
       "18000   2.92  0.16  \n",
       "19000   2.92  0.16  \n",
       "20000   2.96  0.16  \n",
       "21000   2.98  0.16  \n",
       "22000   2.95  0.16  \n",
       "23000   2.95  0.16  \n",
       "24000   2.99  0.16  \n",
       "25000   2.99  0.16  \n",
       "26000   3.07  0.16  \n",
       "27000   3.09  0.16  \n",
       "28000   3.03  0.16  \n",
       "29000   3.03  0.16  \n",
       "30000   3.06  0.15  \n",
       "...      ...   ...  \n",
       "127000  3.40  0.10  \n",
       "128000  3.43  0.10  \n",
       "129000  3.43  0.10  \n",
       "130000  3.46  0.10  \n",
       "131000  3.42  0.10  \n",
       "132000  3.43  0.10  \n",
       "133000  3.40  0.10  \n",
       "134000  3.42  0.10  \n",
       "135000  3.44  0.10  \n",
       "136000  3.44  0.10  \n",
       "137000  3.44  0.10  \n",
       "138000  3.44  0.10  \n",
       "139000  3.45  0.10  \n",
       "140000  3.44  0.10  \n",
       "141000  3.48  0.10  \n",
       "142000  3.44  0.10  \n",
       "143000  3.50  0.10  \n",
       "144000  3.49  0.10  \n",
       "145000  3.45  0.10  \n",
       "146000  3.46  0.10  \n",
       "147000  3.45  0.10  \n",
       "148000  3.44  0.10  \n",
       "149000  3.48  0.10  \n",
       "150000  3.47  0.10  \n",
       "151000  3.49  0.10  \n",
       "152000  3.47  0.10  \n",
       "153000  3.49  0.10  \n",
       "154000  3.51  0.10  \n",
       "155000  3.50  0.10  \n",
       "156000  3.48  0.10  \n",
       "\n",
       "[156 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x1080 with 13 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if len(log_df) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "while epoch < config.epochs:\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = get_feed_dict(batch)\n",
    "\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, ppls_batch, global_step_log = \\\n",
    "        sess.run([opt, loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, topic_ppls, tf.train.get_global_step()], feed_dict = feed_dict)\n",
    "\n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if global_step_log%config.log_period==0:\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev = get_loss(sess, dev_batches)\n",
    "            \n",
    "#             if loss_dev < loss_min:\n",
    "#                 loss_min = loss_dev\n",
    "#                 saver.save(sess, config.modelpath, global_step=global_step_log)\n",
    "\n",
    "            clear_output()\n",
    "    \n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_reg_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_reg_dev],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "            \n",
    "            # visualize topic\n",
    "            print_topic_sample()\n",
    "\n",
    "            time_start = time.time()\n",
    "            \n",
    "    epoch += 1\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "    \n",
    "display(log_df)\n",
    "print_topic_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_topic_embeddings = tf.concat([tree_states_topic_embeddings[topic_idx] for topic_idx in topic_idxs], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "debug_value([states_topic_embeddings[:, :6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_embeddings = tf.concat([tree_topic_embeddings[topic_idx] for topic_idx in topic_idxs], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_value([topic_embeddings[:, :6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_topics_bow, = debug_value([topic_bow], return_value=True)\n",
    "np.max(_topics_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_topics_bow, = debug_value([topic_bow], return_value=True)\n",
    "\n",
    "plt.figure(figsize=(12, 20))\n",
    "    \n",
    "_topic_bow = _topics_bow[0]\n",
    "plt.subplot(5,3,2)\n",
    "plt.ylim([0, np.max(_topics_bow)])\n",
    "plt.bar(bow_idxs, _topic_bow)\n",
    "\n",
    "for i in range(1, len(topic_idxs)):\n",
    "    _topic_bow = _topics_bow[i]\n",
    "    plt.subplot(5,3,i+3)\n",
    "    plt.ylim([0, np.max(_topics_bow)])\n",
    "#     plt.axis('off')\n",
    "    plt.bar(bow_idxs, _topic_bow)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.37399197 0.10826537 0.0361492  0.09407969 0.07117239 0.04244342\n",
      " 0.05811434 0.02009552 0.01129199 0.01132447 0.09071141 0.0239397\n",
      " 0.05842046]\n"
     ]
    }
   ],
   "source": [
    "_prob_topics = []\n",
    "for ct, batch in dev_batches:\n",
    "    feed_dict = get_feed_dict(batch)\n",
    "    _prob_topic, = sess.run([prob_topic], feed_dict = feed_dict)\n",
    "    _prob_topics.append(_prob_topic)\n",
    "    \n",
    "_prob_topics = np.concatenate(_prob_topics, 0)\n",
    "_prob_topic_mean = np.mean(_prob_topics, 0)\n",
    "\n",
    "print(_prob_topic_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_value([topic_dots])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_value([topic_losses_reg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_mask_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_topic_bow, = debug_value([topic_bow], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bow_idxs, _topic_bow[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bow_idxs, _topic_bow[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bow_idxs, _topic_bow[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bow_idxs, _topic_bow[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(_topic_bow, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bow_idxs, _topic_bow[-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bow_idxs, _topic_bow[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bow_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_value([prob_topic[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_value([tf.exp(-tf.divide(topic_losses_recon, n_bow))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_shape([bow, hidden_bow, latents_bow, prob_topic, bow_embeddings, topic_embeddings, topic_bow, prob_bow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_shape([topic_losses_recon, topic_loss_recon, n_bow, ppls, topic_embeddings_norm, tf.expand_dims(topic_angles_mean, -1), topic_angles_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_value([tf.reduce_sum(tf.square(topic_embeddings_norm), 1)], return_value=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_value([tf.reduce_sum(prob_topic, -1), tf.reduce_sum(topic_bow, -1), tf.reduce_sum(tf.exp(prob_bow), 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_bow = tf.exp(0.5 * logvars_bow)\n",
    "dist_bow = tfd.Normal(means_bow, sigma_bow)\n",
    "dist_std = tfd.Normal(0., 1.)\n",
    "topic_loss_kl_tmp = tf.reduce_mean(tf.reduce_sum(tfd.kl_divergence(dist_bow, dist_std), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_value([topic_loss_recon, topic_loss_kl, topic_loss_kl_tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_logvars, _means, _kl_losses, _latents, _output_logits = sess.run([logvars, means, kl_losses, latents, output_logits], feed_dict=feed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_logvars.shape, _means.shape, _kl_losses.shape, _latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_output_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_output_logits, _dec_target_idxs_do, _dec_mask_tokens_do, _recon_loss, _kl_losses, _ = sess.run([output_logits, dec_target_idxs_do, dec_mask_tokens_do, recon_loss, kl_losses, opt], feed_dict=feed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_max(output_logits, 2).eval(session=sess, feed_dict=feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_output_logits.shape, _dec_target_idxs_do.shape, _dec_mask_tokens_do.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_logits = np.exp(_output_logits) / np.sum(np.exp(_output_logits), 2)[:, :, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_idxs = _dec_target_idxs_do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_losses = np.array([[-np.log(_logits[i, j, _idxs[i, j]]) for j in range(_idxs.shape[1])] for i in range(_idxs.shape[0])]) * _dec_mask_tokens_do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(_losses)/np.sum(_dec_mask_tokens_do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_recon_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_kl_losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
