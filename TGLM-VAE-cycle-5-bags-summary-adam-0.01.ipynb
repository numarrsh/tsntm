{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pdb\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from six.moves import zip_longest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import distributions as tfd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from data_structure import get_batches, get_test_batches\n",
    "from components import tf_log, sample_latents, compute_kl_loss, dynamic_rnn, dynamic_bi_rnn\n",
    "from topic_model import TopicModel\n",
    "\n",
    "from topic_beam_search_decoder import BeamSearchDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data & set config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('gpu', '0', 'visible gpu')\n",
    "\n",
    "flags.DEFINE_string('mode', 'train', 'set train or eval')\n",
    "\n",
    "flags.DEFINE_string('data_path', 'data/bags/instances.pkl', 'path of data')\n",
    "flags.DEFINE_string('modeldir', 'model/tglm_vae_tmp0', 'directory of model')\n",
    "flags.DEFINE_string('modelname', 'bags', 'name of model')\n",
    "\n",
    "flags.DEFINE_integer('epochs', 50, 'epochs')\n",
    "flags.DEFINE_integer('batch_size', 64, 'number of sentences in each batch')\n",
    "flags.DEFINE_integer('log_period', 500, 'valid period')\n",
    "\n",
    "flags.DEFINE_string('opt', 'Adagrad', 'optimizer')\n",
    "flags.DEFINE_float('lr', 0.01, 'lr')\n",
    "flags.DEFINE_float('reg', 1., 'regularization term')\n",
    "flags.DEFINE_float('grad_clip', 5., 'grad_clip')\n",
    "\n",
    "flags.DEFINE_float('keep_prob', 0.8, 'dropout rate')\n",
    "flags.DEFINE_float('word_keep_prob', 0.75, 'word dropout rate')\n",
    "\n",
    "flags.DEFINE_bool('warmup', True, 'flg of warming up')\n",
    "flags.DEFINE_integer('epochs_cycle', 5, 'number of epochs within a cycle')\n",
    "flags.DEFINE_float('r_cycle', 0.5, 'proportion used to increase beta within a cycle')\n",
    "flags.DEFINE_integer('warmup_topic', 0, 'warmup period for KL of topic')\n",
    "\n",
    "flags.DEFINE_integer('beam_width', 2, 'beam_width')\n",
    "flags.DEFINE_float('length_penalty_weight', 0.0, 'length_penalty_weight')\n",
    "\n",
    "flags.DEFINE_integer('n_topic', 10, 'number of topic')\n",
    "flags.DEFINE_integer('dim_hidden_bow', 256, 'dim of hidden bow')\n",
    "flags.DEFINE_integer('dim_latent_bow', 32, 'dim of latent topic')\n",
    "flags.DEFINE_integer('dim_emb', 256, 'dim_emb')\n",
    "flags.DEFINE_integer('dim_hidden', 512, 'dim_hidden')\n",
    "flags.DEFINE_integer('dim_hidden_topic', 512, 'dim_hidden_topic')\n",
    "flags.DEFINE_integer('dim_latent', 32, 'dim_latent')\n",
    "flags.DEFINE_bool('bidirectional', True, 'flg of bidirectional encoding')\n",
    "\n",
    "# for evaluation\n",
    "flags.DEFINE_string('refdir', 'ref', 'refdir')\n",
    "flags.DEFINE_string('outdir', 'out', 'outdir')\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_bool('logtostderr', True, 'kernel')\n",
    "flags.DEFINE_bool('showprefixforinfo', False, '')\n",
    "flags.DEFINE_bool('verbosity', False, '')\n",
    "# flags.DEFINE_integer('stderrthreshold', 20, 'kernel')\n",
    "\n",
    "config = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('modelpath', os.path.join(config.modeldir, config.modelname), 'path of model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_train, instances_dev, instances_test, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.data_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_batches(instances_train, config.batch_size)\n",
    "dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "test_batches = get_test_batches(instances_test, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('PAD_IDX', word_to_idx[PAD], 'PAD_IDX')\n",
    "flags.DEFINE_integer('UNK_IDX', word_to_idx[UNK], 'UNK_IDX')\n",
    "flags.DEFINE_integer('BOS_IDX', word_to_idx[BOS], 'BOS_IDX')\n",
    "flags.DEFINE_integer('EOS_IDX', word_to_idx[EOS], 'EOS_IDX')\n",
    "\n",
    "flags.DEFINE_integer('n_vocab', len(word_to_idx), 'n_vocab')\n",
    "flags.DEFINE_integer('dim_bow', len(bow_idxs), 'dim_bow')\n",
    "\n",
    "maximum_iterations = max([max([instance.max_sent_l for instance in batch]) for ct, batch in dev_batches])\n",
    "flags.DEFINE_integer('maximum_iterations', maximum_iterations, 'maximum_iterations')\n",
    "\n",
    "flags.DEFINE_integer('cycle_steps', len(train_batches)*config.epochs_cycle, 'number of steps for each cycle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build language model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feed dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "t_variables = {}\n",
    "t_variables['bow'] = tf.placeholder(tf.float32, [None, config.dim_bow], name='bow')\n",
    "t_variables['input_token_idxs'] = tf.placeholder(tf.int32, [None, None], name='input_token_idxs')\n",
    "t_variables['dec_input_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_input_idxs')\n",
    "t_variables['dec_target_idxs'] = tf.placeholder(tf.int32, [None, None], name='dec_target_idxs')\n",
    "t_variables['batch_l'] = tf.placeholder(tf.int32, name='batch_l')\n",
    "t_variables['doc_l'] = tf.placeholder(tf.int32, [None], name='doc_l')\n",
    "t_variables['sent_l'] = tf.placeholder(tf.int32, [None], name='sent_l')\n",
    "t_variables['keep_prob'] = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feed_dict(batch, mode='train', assertion=False):\n",
    "    def token_dropout(sent_idxs):\n",
    "        sent_idxs_dropout = np.asarray(sent_idxs)\n",
    "        sent_idxs_dropout[np.random.rand(len(sent_idxs)) > config.word_keep_prob] = config.UNK_IDX\n",
    "        return list(sent_idxs_dropout)\n",
    "\n",
    "    bow = np.array([instance.bow for instance in batch]).astype(np.float32)\n",
    "    \n",
    "    doc_l = np.array([len(instance.token_idxs) for instance in batch])\n",
    "    \n",
    "    feed_input_token_idxs_list = [sent_idxs for instance in batch for sent_idxs in instance.token_idxs]\n",
    "    feed_dec_input_idxs_list = [[config.BOS_IDX] + token_dropout(sent_idxs) for sent_idxs in feed_input_token_idxs_list]\n",
    "    feed_dec_target_idxs_list = [sent_idxs + [config.EOS_IDX]  for sent_idxs in feed_input_token_idxs_list]\n",
    "        \n",
    "    sent_l = np.array([len(sent_idxs) for sent_idxs in feed_input_token_idxs_list], np.int32)\n",
    "    batch_l = len(sent_l)\n",
    "    \n",
    "    feed_input_token_idxs = pad_sequences(feed_input_token_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_input_idxs = pad_sequences(feed_dec_input_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    feed_dec_target_idxs = pad_sequences(feed_dec_target_idxs_list, padding='post', value=config.PAD_IDX, dtype=np.int32)\n",
    "    \n",
    "    if assertion:\n",
    "        index = 0\n",
    "        for instance in batch:\n",
    "            for line_idxs in instance.token_idxs:\n",
    "                assert feed_input_token_idxs_list[index] == line_idxs\n",
    "                index += 1\n",
    "        assert feed_input_token_idxs.shape[1] == np.max(sent_l)\n",
    "        assert feed_dec_input_idxs.shape[1] == np.max(sent_l) + 1\n",
    "        assert feed_dec_target_idxs.shape[1] == np.max(sent_l) + 1\n",
    "    \n",
    "    keep_prob = config.keep_prob if mode == 'train' else 1.0\n",
    "\n",
    "    feed_dict = {\n",
    "                t_variables['bow']: bow, \n",
    "                t_variables['batch_l']: batch_l, t_variables['doc_l']: doc_l, t_variables['sent_l']: sent_l, \n",
    "                t_variables['input_token_idxs']: feed_input_token_idxs, t_variables['dec_input_idxs']: feed_dec_input_idxs, t_variables['dec_target_idxs']: feed_dec_target_idxs, \n",
    "                t_variables['keep_prob']: keep_prob\n",
    "    }\n",
    "    return  feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     10,
     24,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def debug_shape(variables):\n",
    "    sample_batch = dev_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "\n",
    "def debug_value(variables, return_value=False):\n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "\n",
    "    if return_value: \n",
    "        return _variables\n",
    "    else:\n",
    "        for _variable, variable in zip(_variables, variables):\n",
    "            if hasattr(variable, 'name'):\n",
    "                print(variable.name, ':', _variable)\n",
    "            else:\n",
    "                print(_variable)\n",
    "                \n",
    "def check_shape(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable.shape)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()\n",
    "    \n",
    "def check_value(variables):\n",
    "    if 'sess' in globals(): raise\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sample_batch = test_batches[0][1]\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    _variables = sess.run(variables, feed_dict=feed_dict)\n",
    "    for _variable, variable in zip(_variables, variables):\n",
    "        if hasattr(variable, 'name'):\n",
    "            print(variable.name, ':', _variable)\n",
    "        else:\n",
    "            print(_variable.shape)\n",
    "            \n",
    "    sess.close()    \n",
    "    \n",
    "# sent_loss_kl_categ_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, tf_log(prob_topic_infer/prob_topic_sents)), 1))\n",
    "# debug_value([sent_loss_kl_categ, sent_loss_kl_categ_tmp])\n",
    "# sent_loss_kl_gauss_tmp = 0.5 * tf.reduce_sum(tf.exp(logvars_topic_infer-logvars_topic) + tf.square(means_topic - means_topic_infer) / tf.exp(logvars_topic) - 1 + (logvars_topic - logvars_topic_infer), -1)\n",
    "# sent_loss_kl_gmm_tmp = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss_tmp), -1))\n",
    "# debug_value([sent_loss_kl_gmm_tmp, sent_loss_kl_gmm])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encode bow\n",
    "with tf.variable_scope('topic/enc', reuse=False):\n",
    "    hidden_bow_ = tf.layers.Dense(units=config.dim_hidden_bow, activation=tf.nn.relu, name='hidden_bow')(t_variables['bow'])\n",
    "    hidden_bow = tf.layers.Dropout(t_variables['keep_prob'])(hidden_bow_)\n",
    "    means_bow = tf.layers.Dense(units=config.dim_latent_bow, name='mean_bow')(hidden_bow)\n",
    "    logvars_bow = tf.layers.Dense(units=config.dim_latent_bow, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_bow)\n",
    "    latents_bow = sample_latents(means_bow, logvars_bow) # sample latent vectors\n",
    "\n",
    "    prob_topic = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic')(latents_bow) # inference of topic probabilities\n",
    "\n",
    "# decode bow\n",
    "with tf.variable_scope('shared', reuse=False):\n",
    "    embeddings = tf.get_variable('emb', [config.n_vocab, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of vocab\n",
    "\n",
    "bow_embeddings = tf.nn.embedding_lookup(embeddings, bow_idxs) # embeddings of each bow features\n",
    "\n",
    "with tf.variable_scope('topic/dec', reuse=False):\n",
    "    topic_embeddings = tf.get_variable('topic_emb', [config.n_topic, config.dim_emb], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer()) # embeddings of topics\n",
    "\n",
    "    topic_bow = tf.nn.softmax(tf.matmul(topic_embeddings, bow_embeddings, transpose_b=True), 1) # bow vectors for each topic\n",
    "    logits_bow = tf_log(tf.matmul(prob_topic, topic_bow)) # predicted bow distribution\n",
    "\n",
    "    # prior of each gaussian distribution (computed for each topic)\n",
    "    hidden_topic = tf.layers.Dense(units=config.dim_hidden_topic, activation=tf.nn.relu, name='hidden_topic')(topic_bow)\n",
    "    means_topic = tf.layers.Dense(units=config.dim_latent, name='mean_topic')(hidden_topic)\n",
    "    logvars_topic = tf.layers.Dense(units=config.dim_latent, kernel_initializer=tf.constant_initializer(0), bias_initializer=tf.constant_initializer(0), name='logvar_topic')(hidden_topic)\n",
    "    sigma_topic = tf.exp(0.5 * logvars_topic)\n",
    "    gauss_topic = tfd.Normal(loc=means_topic, scale=sigma_topic)    \n",
    "    \n",
    "# define losses\n",
    "topic_losses_recon = -tf.reduce_sum(tf.multiply(t_variables['bow'], logits_bow), 1)\n",
    "topic_loss_recon = tf.reduce_mean(topic_losses_recon) # negative log likelihood of each words\n",
    "\n",
    "topic_loss_kl = compute_kl_loss(means_bow, logvars_bow) # KL divergence b/w latent dist & gaussian std\n",
    "\n",
    "topic_bow_norm = topic_bow / tf.norm(topic_bow, axis=1, keepdims=True)\n",
    "topic_dots = tf.clip_by_value(tf.matmul(topic_bow_norm, tf.transpose(topic_bow_norm)), -1., 1.)\n",
    "topic_loss_reg = tf.reduce_mean(tf.square(topic_dots - tf.eye(config.n_topic)))\n",
    "# topic_angles = tf.acos(topic_dots)\n",
    "# topic_angles_mean = tf.reduce_mean(topic_angles)\n",
    "# topic_angles_vars = tf.reduce_mean(tf.square(topic_angles - topic_angles_mean))\n",
    "# topic_loss_reg = tf.exp(topic_angles_vars - topic_angles_mean)\n",
    "\n",
    "# monitor\n",
    "n_bow = tf.reduce_sum(t_variables['bow'], 1)\n",
    "topic_ppls = tf.divide(topic_losses_recon, tf.maximum(1e-5, n_bow))\n",
    "topics_freq_bow_indices = tf.nn.top_k(topic_bow, 10, name='topic_freq_bow').indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "input_token_idxs = t_variables['input_token_idxs']\n",
    "batch_l = t_variables['batch_l']\n",
    "sent_l = t_variables['sent_l']\n",
    "max_sent_l = tf.reduce_max(sent_l)\n",
    "\n",
    "with tf.variable_scope('sent/enc', reuse=False):\n",
    "    # get word embedding\n",
    "    enc_input = tf.nn.embedding_lookup(embeddings, input_token_idxs)\n",
    "\n",
    "    # get sentence embedding\n",
    "    _, enc_state = dynamic_bi_rnn(enc_input, sent_l, config.dim_hidden, t_variables['keep_prob'])\n",
    "\n",
    "    # TODO House Holder flow\n",
    "    hidden_topic_infer =  tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='hidden_topic_infer')(enc_state)\n",
    "    prob_topic_infer = tf.layers.Dense(units=config.n_topic, activation=tf.nn.softmax, name='prob_topic_infer')(hidden_topic_infer)\n",
    "\n",
    "    w_mean_topic_infer = tf.get_variable('mean_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32)\n",
    "    b_mean_topic_infer = tf.get_variable('mean_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32)\n",
    "    means_topic_infer = tf.tensordot(enc_state, w_mean_topic_infer, axes=[[1], [1]]) + b_mean_topic_infer\n",
    "    \n",
    "    w_logvar_topic_infer = tf.get_variable('logvar_topic_infer/kernel', [config.n_topic, enc_state.shape[-1], config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    b_logvar_topic_infer = tf.get_variable('logvar_topic_infer/bias', [1, config.n_topic, config.dim_latent], dtype=tf.float32, initializer=tf.constant_initializer(0))\n",
    "    logvars_topic_infer = tf.tensordot(enc_state, w_logvar_topic_infer, axes=[[1], [1]]) + b_logvar_topic_infer\n",
    "    sigma_topic_infer = tf.exp(0.5 * logvars_topic_infer)\n",
    "    gauss_topic_infer = tfd.Normal(loc=means_topic_infer, scale=sigma_topic_infer)\n",
    "    \n",
    "    # latent vectors from each gaussian dist.\n",
    "    latents_topic_infer = sample_latents(means_topic_infer, logvars_topic_infer) \n",
    "    # latent vector from gaussian mixture\n",
    "    latents_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), latents_topic_infer, transpose_a=True)\n",
    "    \n",
    "    # for beam search\n",
    "    means_input = tf.matmul(tf.expand_dims(prob_topic_infer, -1), means_topic_infer, transpose_a=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for decoding\n",
    "dec_sent_l = tf.add(sent_l, 1)\n",
    "dec_input_idxs = t_variables['dec_input_idxs']\n",
    "dec_input = tf.nn.embedding_lookup(embeddings, dec_input_idxs)\n",
    "\n",
    "dec_latents_input = tf.tile(latents_input, [1, tf.shape(dec_input)[1], 1])\n",
    "dec_concat_input = tf.concat([dec_input, dec_latents_input], -1)\n",
    "\n",
    "# decode for training\n",
    "with tf.variable_scope('sent/dec/rnn', initializer=tf.contrib.layers.xavier_initializer(), dtype = tf.float32, reuse=False):\n",
    "    dec_cell = tf.contrib.rnn.GRUCell(config.dim_hidden)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob = t_variables['keep_prob'])\n",
    "\n",
    "    dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(latents_input, 1))\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_concat_input, sequence_length=dec_sent_l)\n",
    "\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        cell=dec_cell,\n",
    "        helper=helper,\n",
    "        initial_state=dec_initial_state)\n",
    "\n",
    "    dec_outputs, _, output_sent_l = tf.contrib.seq2seq.dynamic_decode(train_decoder)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(config.n_vocab, use_bias=False, name='out')\n",
    "    output_logits = output_layer(dec_outputs.rnn_output)\n",
    "    \n",
    "    output_token_idxs = tf.argmax(output_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tokens = tf.fill([batch_l], config.BOS_IDX)\n",
    "end_token = config.EOS_IDX\n",
    "\n",
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    infer_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(tf.squeeze(means_input, 1))\n",
    "    beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(infer_dec_initial_state, multiplier=config.beam_width)\n",
    "    beam_latents_input = tf.contrib.seq2seq.tile_batch(tf.squeeze(means_input, 1), multiplier=config.beam_width) # added\n",
    "    \n",
    "    beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=beam_latents_input)\n",
    "\n",
    "    beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    beam_output_token_idxs = beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    inter_means_input = tf.placeholder(tf.float32, [None, config.dim_latent])\n",
    "    \n",
    "    inter_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(inter_means_input)\n",
    "    inter_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(inter_dec_initial_state, multiplier=config.beam_width)\n",
    "    inter_beam_latents_input = tf.contrib.seq2seq.tile_batch(inter_means_input, multiplier=config.beam_width) # added\n",
    "    \n",
    "    inter_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=inter_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=inter_beam_latents_input)\n",
    "\n",
    "    inter_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        inter_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    inter_beam_output_token_idxs = inter_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    topic_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic)\n",
    "    topic_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(topic_dec_initial_state, multiplier=config.beam_width)\n",
    "    topic_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic, multiplier=config.beam_width) # added\n",
    "    \n",
    "    topic_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=topic_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width, \n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=topic_beam_latents_input)\n",
    "\n",
    "    topic_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        topic_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    topic_beam_output_token_idxs = topic_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('sent/dec/rnn', reuse=True):\n",
    "    means_topic_summary = tf.reduce_mean(means_topic_infer, 0)\n",
    "    \n",
    "    summary_dec_initial_state = tf.layers.Dense(units=config.dim_hidden, activation=tf.nn.relu, name='init_state')(means_topic_summary)\n",
    "    summary_beam_dec_initial_state = tf.contrib.seq2seq.tile_batch(summary_dec_initial_state, multiplier=config.beam_width)\n",
    "    summary_beam_latents_input = tf.contrib.seq2seq.tile_batch(means_topic_summary, multiplier=config.beam_width) # added\n",
    "    \n",
    "    summary_beam_decoder = BeamSearchDecoder(\n",
    "        cell=dec_cell,\n",
    "        embedding=embeddings,\n",
    "        start_tokens=start_tokens,\n",
    "        end_token=end_token,\n",
    "        initial_state=summary_beam_dec_initial_state,\n",
    "        beam_width=config.beam_width,\n",
    "        output_layer=output_layer,\n",
    "        length_penalty_weight=config.length_penalty_weight,\n",
    "        latents_input=summary_beam_latents_input)\n",
    "\n",
    "    summary_beam_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        summary_beam_decoder,\n",
    "        maximum_iterations = config.maximum_iterations)\n",
    "\n",
    "    summary_beam_output_token_idxs = summary_beam_dec_outputs.predicted_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language modeling cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target and mask\n",
    "dec_target_idxs = t_variables['dec_target_idxs']\n",
    "dec_mask_tokens = tf.sequence_mask(dec_sent_l, maxlen=max_sent_l+1, dtype=tf.float32)\n",
    "\n",
    "# nll for each token (averaged over batch & sentence)\n",
    "sent_loss_recon = tf.contrib.seq2seq.sequence_loss(output_logits, dec_target_idxs, dec_mask_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_l = t_variables['doc_l']\n",
    "mask_sents = tf.sequence_mask(doc_l)\n",
    "mask_sents_flatten = tf.reshape(mask_sents, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1]])\n",
    "\n",
    "prob_topic_tiled = tf.tile(tf.expand_dims(prob_topic, 1), [1, tf.shape(mask_sents)[1], 1])\n",
    "prob_topic_flatten = tf.reshape(prob_topic_tiled, [tf.shape(mask_sents)[0]*tf.shape(mask_sents)[1], config.n_topic])\n",
    "prob_topic_sents = tf.boolean_mask(prob_topic_flatten, mask_sents_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferred mixture probabilities (computed for each sentence)\n",
    "categ_topic_infer = tfd.Categorical(probs=prob_topic_infer)\n",
    "\n",
    "# prior of mixture probabilities (computed for each document, tiled for each sentence)\n",
    "categ_topic = tfd.Categorical(probs=prob_topic_sents)\n",
    "\n",
    "sent_loss_kl_categ = tf.reduce_mean(tfd.kl_divergence(categ_topic_infer, categ_topic))\n",
    "\n",
    "# inference of each gaussian gaussribution (computed for each sentence)\n",
    "\n",
    "sent_loss_kl_gauss = tf.reduce_sum(tfd.kl_divergence(gauss_topic_infer, gauss_topic), -1)\n",
    "sent_loss_kl_gmm = tf.reduce_mean(tf.reduce_sum(tf.multiply(prob_topic_infer, sent_loss_kl_gauss), -1))\n",
    "\n",
    "sent_loss_kl = sent_loss_kl_categ + sent_loss_kl_gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-isonuma/.pyenv/versions/anaconda2-5.3.0/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "tau = tf.cast(tf.divide(tf.mod(global_step, tf.constant(config.cycle_steps)), tf.constant(config.cycle_steps)), dtype=tf.float32)\n",
    "beta = tf.minimum(1., tau/config.r_cycle)\n",
    "\n",
    "sent_loss = sent_loss_recon + beta * sent_loss_kl\n",
    "\n",
    "topic_loss = topic_loss_recon + topic_loss_kl + config.reg * topic_loss_reg\n",
    "loss = topic_loss + sent_loss\n",
    "\n",
    "# define optimizer\n",
    "if config.opt == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(config.lr)\n",
    "elif config.opt == 'Adagrad':\n",
    "    optimizer = tf.train.AdagradOptimizer(config.lr)\n",
    "    \n",
    "grad_vars = optimizer.compute_gradients(loss)\n",
    "clipped_grad_vars = [(tf.clip_by_value(grad, -config.grad_clip, config.grad_clip), var) for grad, var in grad_vars]\n",
    "\n",
    "opt = optimizer.apply_gradients(clipped_grad_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxs_to_sents(token_idxs, config, idx_to_word):\n",
    "    sents = []\n",
    "    for sent_idxs in token_idxs:\n",
    "        tokens = []\n",
    "        for idx in sent_idxs:\n",
    "            if idx == config.EOS_IDX: break\n",
    "            tokens.append(idx_to_word[idx])\n",
    "        sent = ' '.join(tokens)\n",
    "        sents.append(sent)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(sess, batches):\n",
    "    losses = []\n",
    "    ppl_list = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, ppls_batch \\\n",
    "            = sess.run([loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, topic_ppls], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppl_list += list(ppls_batch)\n",
    "    loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, sent_loss_recon_mean, sent_loss_kl_mean = np.mean(losses, 0)\n",
    "    ppl_mean = np.exp(np.mean(ppl_list))\n",
    "    return loss_mean, topic_loss_recon_mean, topic_loss_kl_mean, topic_loss_reg_mean, sent_loss_recon_mean, sent_loss_kl_mean, ppl_mean\n",
    "\n",
    "def get_all_losses(sess, batches):\n",
    "    losses = []\n",
    "    for ct, batch in batches:\n",
    "        feed_dict = get_feed_dict(batch, mode='test')\n",
    "        loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch = \\\n",
    "        sess.run([loss, topic_loss_recon, topic_loss_kl, sent_loss_recon, sent_loss_kl], feed_dict = feed_dict)\n",
    "        losses += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "    print('LOSS %.2f | TM NLL: %.2f, KL: %.4f | LM NLL: %.2f, KL: %.4f' %  np.mean(losses, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_batch):\n",
    "    feed_dict = get_feed_dict(sample_batch)\n",
    "    pred_token_idxs = sess.run(output_token_idxs, feed_dict = feed_dict)\n",
    "    true_token_idxs = [sent_idxs for instance in sample_batch for sent_idxs in instance.token_idxs]\n",
    "    \n",
    "    assert len(pred_token_idxs) == len(true_token_idxs)\n",
    "    \n",
    "    pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "    true_sents = idxs_to_sents(true_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    for i, (true_sent, pred_sent) in enumerate(zip(true_sents, pred_sents)):        \n",
    "        print(i, 'TRUE: %s' % true_sent)\n",
    "        print(i, 'PRED: %s' % pred_sent)\n",
    "\n",
    "def print_topic_sample():\n",
    "    pred_topics_freq_bow_indices, pred_topic_token_idxs = sess.run([topics_freq_bow_indices, topic_beam_output_token_idxs], \n",
    "                                                                                                           feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "    pred_topic_sents = idxs_to_sents(pred_topic_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Topic Samples-----------')\n",
    "    for i, (topic_freq_bow_idxs, pred_topic_sent) in enumerate(zip(topics_freq_bow_idxs, pred_topic_sents)):\n",
    "        print(i, ' BOW:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' SENTENCE:', pred_topic_sent)\n",
    "        \n",
    "def print_summary(test_batch):\n",
    "    feed_dict = get_feed_dict(test_batch)\n",
    "    feed_dict[t_variables['batch_l']] = config.n_topic\n",
    "    feed_dict[t_variables['keep_prob']] = 1.\n",
    "    pred_topics_freq_bow_indices, pred_summary_token_idxs = sess.run([topics_freq_bow_indices, summary_beam_output_token_idxs], feed_dict=feed_dict)\n",
    "    pred_summary_sents = idxs_to_sents(pred_summary_token_idxs, config, idx_to_word)\n",
    "    \n",
    "    topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]\n",
    "    \n",
    "    print('-----------Output sentences for each topic-----------')\n",
    "    print('Item idx:', test_batch[0].item_idx)\n",
    "    for i, (topic_freq_bow_idxs, pred_summary_sent) in enumerate(zip(topics_freq_bow_idxs, pred_summary_sents)):\n",
    "        print(i, ' BOW:', ' '.join([idx_to_word[idx] for idx in topic_freq_bow_idxs]))\n",
    "        print(i, ' SENTENCE:', pred_summary_sent)\n",
    "        \n",
    "    print('-----------Summaries-----------')\n",
    "    for i, summary in enumerate(test_batch[0].summaries):\n",
    "        print('SUMMARY %i :'%i, '\\n', summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'sess' in globals(): sess.close()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "losses_train = []\n",
    "ppls_train = []\n",
    "loss_min = np.inf\n",
    "beta_eval = 1.\n",
    "epoch = 0\n",
    "train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "saver = tf.train.Saver(max_to_keep=10)\n",
    "\n",
    "log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                    list(zip(*[['','','','TRAIN:','TM','','','','LM','','VALID:','TM','','','','LM','', ''],\n",
    "                            ['Time','Ep','Ct','LOSS','PPL','NLL','KL','REG','NLL','KL','LOSS','PPL','NLL','KL','REG','NLL','KL', 'Beta']]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>TRAIN:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>LM</th>\n",
       "      <th></th>\n",
       "      <th>VALID:</th>\n",
       "      <th>TM</th>\n",
       "      <th colspan=\"3\" halign=\"left\"></th>\n",
       "      <th>LM</th>\n",
       "      <th colspan=\"2\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Ep</th>\n",
       "      <th>Ct</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>PPL</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>REG</th>\n",
       "      <th>NLL</th>\n",
       "      <th>KL</th>\n",
       "      <th>Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>137.42</td>\n",
       "      <td>1034</td>\n",
       "      <td>126.92</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.90</td>\n",
       "      <td>9.13</td>\n",
       "      <td>1.46</td>\n",
       "      <td>126.64</td>\n",
       "      <td>1035</td>\n",
       "      <td>116.21</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.90</td>\n",
       "      <td>9.13</td>\n",
       "      <td>1.42</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>128.30</td>\n",
       "      <td>776</td>\n",
       "      <td>118.94</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.84</td>\n",
       "      <td>8.28</td>\n",
       "      <td>1.38</td>\n",
       "      <td>113.58</td>\n",
       "      <td>550</td>\n",
       "      <td>105.81</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.78</td>\n",
       "      <td>6.77</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>124.32</td>\n",
       "      <td>660</td>\n",
       "      <td>115.97</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.80</td>\n",
       "      <td>7.31</td>\n",
       "      <td>1.19</td>\n",
       "      <td>112.11</td>\n",
       "      <td>530</td>\n",
       "      <td>105.19</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.73</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>1500</td>\n",
       "      <td>122.93</td>\n",
       "      <td>622</td>\n",
       "      <td>115.06</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.77</td>\n",
       "      <td>6.87</td>\n",
       "      <td>1.01</td>\n",
       "      <td>111.86</td>\n",
       "      <td>526</td>\n",
       "      <td>105.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.68</td>\n",
       "      <td>5.83</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>121.81</td>\n",
       "      <td>603</td>\n",
       "      <td>114.21</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.74</td>\n",
       "      <td>6.62</td>\n",
       "      <td>0.89</td>\n",
       "      <td>111.71</td>\n",
       "      <td>525</td>\n",
       "      <td>105.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.62</td>\n",
       "      <td>5.77</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>121.55</td>\n",
       "      <td>596</td>\n",
       "      <td>114.05</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.72</td>\n",
       "      <td>6.52</td>\n",
       "      <td>0.84</td>\n",
       "      <td>111.68</td>\n",
       "      <td>525</td>\n",
       "      <td>105.06</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.60</td>\n",
       "      <td>5.75</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2776</th>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>121.01</td>\n",
       "      <td>588</td>\n",
       "      <td>113.66</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.69</td>\n",
       "      <td>6.40</td>\n",
       "      <td>0.75</td>\n",
       "      <td>111.65</td>\n",
       "      <td>528</td>\n",
       "      <td>105.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.54</td>\n",
       "      <td>5.73</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3276</th>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>120.85</td>\n",
       "      <td>581</td>\n",
       "      <td>113.61</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.67</td>\n",
       "      <td>6.31</td>\n",
       "      <td>0.68</td>\n",
       "      <td>111.57</td>\n",
       "      <td>525</td>\n",
       "      <td>105.07</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.50</td>\n",
       "      <td>5.72</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3776</th>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>120.59</td>\n",
       "      <td>576</td>\n",
       "      <td>113.44</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.64</td>\n",
       "      <td>6.24</td>\n",
       "      <td>0.63</td>\n",
       "      <td>111.39</td>\n",
       "      <td>522</td>\n",
       "      <td>104.96</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.46</td>\n",
       "      <td>5.71</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4276</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>120.42</td>\n",
       "      <td>573</td>\n",
       "      <td>113.34</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.62</td>\n",
       "      <td>6.18</td>\n",
       "      <td>0.58</td>\n",
       "      <td>111.32</td>\n",
       "      <td>521</td>\n",
       "      <td>104.91</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.43</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4551</th>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>120.35</td>\n",
       "      <td>571</td>\n",
       "      <td>113.31</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.61</td>\n",
       "      <td>6.16</td>\n",
       "      <td>0.55</td>\n",
       "      <td>111.33</td>\n",
       "      <td>522</td>\n",
       "      <td>104.93</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.41</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5051</th>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>120.12</td>\n",
       "      <td>569</td>\n",
       "      <td>113.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.59</td>\n",
       "      <td>6.12</td>\n",
       "      <td>0.51</td>\n",
       "      <td>111.25</td>\n",
       "      <td>520</td>\n",
       "      <td>104.87</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.40</td>\n",
       "      <td>5.69</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5551</th>\n",
       "      <td>54</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>120.07</td>\n",
       "      <td>566</td>\n",
       "      <td>113.13</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.57</td>\n",
       "      <td>6.09</td>\n",
       "      <td>0.48</td>\n",
       "      <td>111.26</td>\n",
       "      <td>521</td>\n",
       "      <td>104.91</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.37</td>\n",
       "      <td>5.69</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6051</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>1500</td>\n",
       "      <td>119.95</td>\n",
       "      <td>564</td>\n",
       "      <td>113.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.55</td>\n",
       "      <td>6.06</td>\n",
       "      <td>0.45</td>\n",
       "      <td>111.20</td>\n",
       "      <td>520</td>\n",
       "      <td>104.89</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.35</td>\n",
       "      <td>5.68</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6551</th>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>119.91</td>\n",
       "      <td>563</td>\n",
       "      <td>113.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.53</td>\n",
       "      <td>6.04</td>\n",
       "      <td>0.43</td>\n",
       "      <td>111.11</td>\n",
       "      <td>519</td>\n",
       "      <td>104.83</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.31</td>\n",
       "      <td>5.68</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6826</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>119.85</td>\n",
       "      <td>562</td>\n",
       "      <td>113.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.53</td>\n",
       "      <td>6.03</td>\n",
       "      <td>0.41</td>\n",
       "      <td>111.08</td>\n",
       "      <td>519</td>\n",
       "      <td>104.81</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.30</td>\n",
       "      <td>5.68</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7326</th>\n",
       "      <td>54</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>119.73</td>\n",
       "      <td>560</td>\n",
       "      <td>112.91</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.51</td>\n",
       "      <td>6.01</td>\n",
       "      <td>0.39</td>\n",
       "      <td>111.08</td>\n",
       "      <td>520</td>\n",
       "      <td>104.83</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.28</td>\n",
       "      <td>5.68</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7826</th>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>119.65</td>\n",
       "      <td>559</td>\n",
       "      <td>112.86</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.49</td>\n",
       "      <td>5.99</td>\n",
       "      <td>0.37</td>\n",
       "      <td>111.04</td>\n",
       "      <td>517</td>\n",
       "      <td>104.79</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.27</td>\n",
       "      <td>5.67</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8326</th>\n",
       "      <td>54</td>\n",
       "      <td>3</td>\n",
       "      <td>1500</td>\n",
       "      <td>119.61</td>\n",
       "      <td>557</td>\n",
       "      <td>112.84</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.48</td>\n",
       "      <td>5.98</td>\n",
       "      <td>0.36</td>\n",
       "      <td>111.04</td>\n",
       "      <td>518</td>\n",
       "      <td>104.81</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.26</td>\n",
       "      <td>5.67</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8826</th>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>119.55</td>\n",
       "      <td>556</td>\n",
       "      <td>112.80</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.47</td>\n",
       "      <td>5.96</td>\n",
       "      <td>0.34</td>\n",
       "      <td>110.98</td>\n",
       "      <td>517</td>\n",
       "      <td>104.75</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.25</td>\n",
       "      <td>5.67</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9101</th>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>119.54</td>\n",
       "      <td>556</td>\n",
       "      <td>112.80</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.46</td>\n",
       "      <td>5.96</td>\n",
       "      <td>0.34</td>\n",
       "      <td>111.07</td>\n",
       "      <td>518</td>\n",
       "      <td>104.83</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.24</td>\n",
       "      <td>5.67</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9601</th>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "      <td>119.44</td>\n",
       "      <td>554</td>\n",
       "      <td>112.73</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.45</td>\n",
       "      <td>5.94</td>\n",
       "      <td>0.32</td>\n",
       "      <td>110.82</td>\n",
       "      <td>512</td>\n",
       "      <td>104.59</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.22</td>\n",
       "      <td>5.66</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10101</th>\n",
       "      <td>55</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>119.46</td>\n",
       "      <td>553</td>\n",
       "      <td>112.76</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.44</td>\n",
       "      <td>5.93</td>\n",
       "      <td>0.31</td>\n",
       "      <td>110.91</td>\n",
       "      <td>514</td>\n",
       "      <td>104.68</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.20</td>\n",
       "      <td>5.66</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10601</th>\n",
       "      <td>54</td>\n",
       "      <td>4</td>\n",
       "      <td>1500</td>\n",
       "      <td>119.37</td>\n",
       "      <td>552</td>\n",
       "      <td>112.68</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.43</td>\n",
       "      <td>5.92</td>\n",
       "      <td>0.30</td>\n",
       "      <td>110.90</td>\n",
       "      <td>513</td>\n",
       "      <td>104.67</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.19</td>\n",
       "      <td>5.66</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11101</th>\n",
       "      <td>55</td>\n",
       "      <td>4</td>\n",
       "      <td>2000</td>\n",
       "      <td>119.33</td>\n",
       "      <td>551</td>\n",
       "      <td>112.65</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.42</td>\n",
       "      <td>5.92</td>\n",
       "      <td>0.30</td>\n",
       "      <td>110.85</td>\n",
       "      <td>513</td>\n",
       "      <td>104.60</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.18</td>\n",
       "      <td>5.66</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11376</th>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>119.30</td>\n",
       "      <td>551</td>\n",
       "      <td>112.63</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.41</td>\n",
       "      <td>5.91</td>\n",
       "      <td>0.29</td>\n",
       "      <td>110.72</td>\n",
       "      <td>510</td>\n",
       "      <td>104.58</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.17</td>\n",
       "      <td>5.66</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11876</th>\n",
       "      <td>63</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>119.24</td>\n",
       "      <td>549</td>\n",
       "      <td>112.59</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.40</td>\n",
       "      <td>5.90</td>\n",
       "      <td>0.28</td>\n",
       "      <td>110.67</td>\n",
       "      <td>508</td>\n",
       "      <td>104.48</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.16</td>\n",
       "      <td>5.65</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12376</th>\n",
       "      <td>63</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>119.22</td>\n",
       "      <td>548</td>\n",
       "      <td>112.57</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.39</td>\n",
       "      <td>5.90</td>\n",
       "      <td>0.28</td>\n",
       "      <td>110.64</td>\n",
       "      <td>508</td>\n",
       "      <td>104.43</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.15</td>\n",
       "      <td>5.65</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12876</th>\n",
       "      <td>54</td>\n",
       "      <td>5</td>\n",
       "      <td>1500</td>\n",
       "      <td>119.17</td>\n",
       "      <td>547</td>\n",
       "      <td>112.53</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>5.89</td>\n",
       "      <td>0.27</td>\n",
       "      <td>110.76</td>\n",
       "      <td>511</td>\n",
       "      <td>104.52</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.15</td>\n",
       "      <td>5.65</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13376</th>\n",
       "      <td>54</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>119.13</td>\n",
       "      <td>546</td>\n",
       "      <td>112.50</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.37</td>\n",
       "      <td>5.88</td>\n",
       "      <td>0.27</td>\n",
       "      <td>110.68</td>\n",
       "      <td>507</td>\n",
       "      <td>104.42</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.15</td>\n",
       "      <td>5.65</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100101</th>\n",
       "      <td>31</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>116.72</td>\n",
       "      <td>481</td>\n",
       "      <td>110.26</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.30</td>\n",
       "      <td>0.21</td>\n",
       "      <td>108.59</td>\n",
       "      <td>453</td>\n",
       "      <td>102.50</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.74</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100601</th>\n",
       "      <td>54</td>\n",
       "      <td>44</td>\n",
       "      <td>500</td>\n",
       "      <td>116.72</td>\n",
       "      <td>481</td>\n",
       "      <td>110.25</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.30</td>\n",
       "      <td>0.21</td>\n",
       "      <td>108.67</td>\n",
       "      <td>457</td>\n",
       "      <td>102.61</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.74</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101101</th>\n",
       "      <td>60</td>\n",
       "      <td>44</td>\n",
       "      <td>1000</td>\n",
       "      <td>116.71</td>\n",
       "      <td>481</td>\n",
       "      <td>110.25</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.30</td>\n",
       "      <td>0.21</td>\n",
       "      <td>108.67</td>\n",
       "      <td>457</td>\n",
       "      <td>102.60</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.74</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101601</th>\n",
       "      <td>61</td>\n",
       "      <td>44</td>\n",
       "      <td>1500</td>\n",
       "      <td>116.71</td>\n",
       "      <td>481</td>\n",
       "      <td>110.24</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.29</td>\n",
       "      <td>0.21</td>\n",
       "      <td>108.58</td>\n",
       "      <td>453</td>\n",
       "      <td>102.51</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.74</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102101</th>\n",
       "      <td>60</td>\n",
       "      <td>44</td>\n",
       "      <td>2000</td>\n",
       "      <td>116.70</td>\n",
       "      <td>481</td>\n",
       "      <td>110.24</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.29</td>\n",
       "      <td>0.21</td>\n",
       "      <td>108.66</td>\n",
       "      <td>455</td>\n",
       "      <td>102.58</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.73</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102376</th>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>116.70</td>\n",
       "      <td>481</td>\n",
       "      <td>110.24</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.29</td>\n",
       "      <td>0.21</td>\n",
       "      <td>108.31</td>\n",
       "      <td>451</td>\n",
       "      <td>102.44</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.74</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102876</th>\n",
       "      <td>58</td>\n",
       "      <td>45</td>\n",
       "      <td>500</td>\n",
       "      <td>116.70</td>\n",
       "      <td>481</td>\n",
       "      <td>110.24</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.29</td>\n",
       "      <td>0.21</td>\n",
       "      <td>108.40</td>\n",
       "      <td>454</td>\n",
       "      <td>102.53</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.73</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103376</th>\n",
       "      <td>57</td>\n",
       "      <td>45</td>\n",
       "      <td>1000</td>\n",
       "      <td>116.69</td>\n",
       "      <td>481</td>\n",
       "      <td>110.23</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.29</td>\n",
       "      <td>0.21</td>\n",
       "      <td>108.37</td>\n",
       "      <td>452</td>\n",
       "      <td>102.49</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.73</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103876</th>\n",
       "      <td>58</td>\n",
       "      <td>45</td>\n",
       "      <td>1500</td>\n",
       "      <td>116.68</td>\n",
       "      <td>480</td>\n",
       "      <td>110.22</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>108.45</td>\n",
       "      <td>453</td>\n",
       "      <td>102.53</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.73</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104376</th>\n",
       "      <td>58</td>\n",
       "      <td>45</td>\n",
       "      <td>2000</td>\n",
       "      <td>116.68</td>\n",
       "      <td>480</td>\n",
       "      <td>110.22</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>108.53</td>\n",
       "      <td>456</td>\n",
       "      <td>102.63</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.71</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104651</th>\n",
       "      <td>33</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>116.68</td>\n",
       "      <td>480</td>\n",
       "      <td>110.22</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>108.41</td>\n",
       "      <td>452</td>\n",
       "      <td>102.47</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.72</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105151</th>\n",
       "      <td>62</td>\n",
       "      <td>46</td>\n",
       "      <td>500</td>\n",
       "      <td>116.67</td>\n",
       "      <td>480</td>\n",
       "      <td>110.22</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>108.47</td>\n",
       "      <td>452</td>\n",
       "      <td>102.50</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.73</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105651</th>\n",
       "      <td>61</td>\n",
       "      <td>46</td>\n",
       "      <td>1000</td>\n",
       "      <td>116.67</td>\n",
       "      <td>480</td>\n",
       "      <td>110.22</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>108.58</td>\n",
       "      <td>457</td>\n",
       "      <td>102.59</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.72</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106151</th>\n",
       "      <td>60</td>\n",
       "      <td>46</td>\n",
       "      <td>1500</td>\n",
       "      <td>116.66</td>\n",
       "      <td>480</td>\n",
       "      <td>110.22</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.27</td>\n",
       "      <td>0.21</td>\n",
       "      <td>108.45</td>\n",
       "      <td>452</td>\n",
       "      <td>102.44</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.72</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106651</th>\n",
       "      <td>60</td>\n",
       "      <td>46</td>\n",
       "      <td>2000</td>\n",
       "      <td>116.66</td>\n",
       "      <td>480</td>\n",
       "      <td>110.21</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.27</td>\n",
       "      <td>0.21</td>\n",
       "      <td>108.49</td>\n",
       "      <td>450</td>\n",
       "      <td>102.49</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.71</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106926</th>\n",
       "      <td>36</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>116.65</td>\n",
       "      <td>480</td>\n",
       "      <td>110.21</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.27</td>\n",
       "      <td>0.21</td>\n",
       "      <td>108.49</td>\n",
       "      <td>453</td>\n",
       "      <td>102.48</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.71</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107426</th>\n",
       "      <td>59</td>\n",
       "      <td>47</td>\n",
       "      <td>500</td>\n",
       "      <td>116.65</td>\n",
       "      <td>480</td>\n",
       "      <td>110.20</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.27</td>\n",
       "      <td>0.21</td>\n",
       "      <td>108.54</td>\n",
       "      <td>453</td>\n",
       "      <td>102.50</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.71</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107926</th>\n",
       "      <td>59</td>\n",
       "      <td>47</td>\n",
       "      <td>1000</td>\n",
       "      <td>116.65</td>\n",
       "      <td>480</td>\n",
       "      <td>110.20</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.27</td>\n",
       "      <td>0.21</td>\n",
       "      <td>108.54</td>\n",
       "      <td>453</td>\n",
       "      <td>102.48</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.71</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108426</th>\n",
       "      <td>62</td>\n",
       "      <td>47</td>\n",
       "      <td>1500</td>\n",
       "      <td>116.64</td>\n",
       "      <td>480</td>\n",
       "      <td>110.19</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.27</td>\n",
       "      <td>0.21</td>\n",
       "      <td>108.53</td>\n",
       "      <td>450</td>\n",
       "      <td>102.45</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4.71</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108926</th>\n",
       "      <td>61</td>\n",
       "      <td>47</td>\n",
       "      <td>2000</td>\n",
       "      <td>116.63</td>\n",
       "      <td>479</td>\n",
       "      <td>110.19</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.26</td>\n",
       "      <td>0.21</td>\n",
       "      <td>108.63</td>\n",
       "      <td>454</td>\n",
       "      <td>102.58</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.70</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109201</th>\n",
       "      <td>35</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>116.63</td>\n",
       "      <td>479</td>\n",
       "      <td>110.19</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.26</td>\n",
       "      <td>0.21</td>\n",
       "      <td>108.58</td>\n",
       "      <td>453</td>\n",
       "      <td>102.51</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.71</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109701</th>\n",
       "      <td>65</td>\n",
       "      <td>48</td>\n",
       "      <td>500</td>\n",
       "      <td>116.63</td>\n",
       "      <td>479</td>\n",
       "      <td>110.19</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.26</td>\n",
       "      <td>0.21</td>\n",
       "      <td>108.64</td>\n",
       "      <td>457</td>\n",
       "      <td>102.60</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.70</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110201</th>\n",
       "      <td>62</td>\n",
       "      <td>48</td>\n",
       "      <td>1000</td>\n",
       "      <td>116.63</td>\n",
       "      <td>479</td>\n",
       "      <td>110.18</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.26</td>\n",
       "      <td>0.21</td>\n",
       "      <td>108.57</td>\n",
       "      <td>453</td>\n",
       "      <td>102.53</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.70</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110701</th>\n",
       "      <td>63</td>\n",
       "      <td>48</td>\n",
       "      <td>1500</td>\n",
       "      <td>116.62</td>\n",
       "      <td>479</td>\n",
       "      <td>110.18</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.26</td>\n",
       "      <td>0.21</td>\n",
       "      <td>108.58</td>\n",
       "      <td>453</td>\n",
       "      <td>102.56</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.69</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111201</th>\n",
       "      <td>63</td>\n",
       "      <td>48</td>\n",
       "      <td>2000</td>\n",
       "      <td>116.62</td>\n",
       "      <td>479</td>\n",
       "      <td>110.18</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.25</td>\n",
       "      <td>0.21</td>\n",
       "      <td>108.63</td>\n",
       "      <td>455</td>\n",
       "      <td>102.58</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.69</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111476</th>\n",
       "      <td>35</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>116.61</td>\n",
       "      <td>479</td>\n",
       "      <td>110.17</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.25</td>\n",
       "      <td>0.21</td>\n",
       "      <td>108.56</td>\n",
       "      <td>453</td>\n",
       "      <td>102.50</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.70</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111976</th>\n",
       "      <td>63</td>\n",
       "      <td>49</td>\n",
       "      <td>500</td>\n",
       "      <td>116.61</td>\n",
       "      <td>479</td>\n",
       "      <td>110.17</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.25</td>\n",
       "      <td>0.21</td>\n",
       "      <td>108.51</td>\n",
       "      <td>452</td>\n",
       "      <td>102.45</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.69</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112476</th>\n",
       "      <td>63</td>\n",
       "      <td>49</td>\n",
       "      <td>1000</td>\n",
       "      <td>116.60</td>\n",
       "      <td>479</td>\n",
       "      <td>110.16</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.25</td>\n",
       "      <td>0.21</td>\n",
       "      <td>108.55</td>\n",
       "      <td>454</td>\n",
       "      <td>102.51</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.69</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112976</th>\n",
       "      <td>63</td>\n",
       "      <td>49</td>\n",
       "      <td>1500</td>\n",
       "      <td>116.60</td>\n",
       "      <td>479</td>\n",
       "      <td>110.16</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.07</td>\n",
       "      <td>5.25</td>\n",
       "      <td>0.21</td>\n",
       "      <td>108.54</td>\n",
       "      <td>453</td>\n",
       "      <td>102.50</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.68</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113476</th>\n",
       "      <td>65</td>\n",
       "      <td>49</td>\n",
       "      <td>2000</td>\n",
       "      <td>116.60</td>\n",
       "      <td>479</td>\n",
       "      <td>110.16</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.07</td>\n",
       "      <td>5.24</td>\n",
       "      <td>0.21</td>\n",
       "      <td>108.55</td>\n",
       "      <td>454</td>\n",
       "      <td>102.50</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.69</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows  18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       TRAIN:    TM                        LM        VALID:  \\\n",
       "       Time  Ep    Ct    LOSS   PPL     NLL    KL   REG   NLL    KL    LOSS   \n",
       "1        20   0     0  137.42  1034  126.92  0.47  0.90  9.13  1.46  126.64   \n",
       "501      70   0   500  128.30   776  118.94  0.19  0.84  8.28  1.38  113.58   \n",
       "1001     67   0  1000  124.32   660  115.97  0.15  0.80  7.31  1.19  112.11   \n",
       "1501     65   0  1500  122.93   622  115.06  0.13  0.77  6.87  1.01  111.86   \n",
       "2001     63   0  2000  121.81   603  114.21  0.12  0.74  6.62  0.89  111.71   \n",
       "2276     41   1     0  121.55   596  114.05  0.12  0.72  6.52  0.84  111.68   \n",
       "2776     65   1   500  121.01   588  113.66  0.12  0.69  6.40  0.75  111.65   \n",
       "3276     65   1  1000  120.85   581  113.61  0.13  0.67  6.31  0.68  111.57   \n",
       "3776     64   1  1500  120.59   576  113.44  0.13  0.64  6.24  0.63  111.39   \n",
       "4276     70   1  2000  120.42   573  113.34  0.13  0.62  6.18  0.58  111.32   \n",
       "4551     30   2     0  120.35   571  113.31  0.13  0.61  6.16  0.55  111.33   \n",
       "5051     69   2   500  120.12   569  113.14  0.14  0.59  6.12  0.51  111.25   \n",
       "5551     54   2  1000  120.07   566  113.13  0.14  0.57  6.09  0.48  111.26   \n",
       "6051     64   2  1500  119.95   564  113.05  0.15  0.55  6.06  0.45  111.20   \n",
       "6551     74   2  2000  119.91   563  113.05  0.15  0.53  6.04  0.43  111.11   \n",
       "6826     42   3     0  119.85   562  113.00  0.16  0.53  6.03  0.41  111.08   \n",
       "7326     54   3   500  119.73   560  112.91  0.16  0.51  6.01  0.39  111.08   \n",
       "7826     64   3  1000  119.65   559  112.86  0.17  0.49  5.99  0.37  111.04   \n",
       "8326     54   3  1500  119.61   557  112.84  0.18  0.48  5.98  0.36  111.04   \n",
       "8826     82   3  2000  119.55   556  112.80  0.18  0.47  5.96  0.34  110.98   \n",
       "9101     30   4     0  119.54   556  112.80  0.19  0.46  5.96  0.34  111.07   \n",
       "9601     64   4   500  119.44   554  112.73  0.20  0.45  5.94  0.32  110.82   \n",
       "10101    55   4  1000  119.46   553  112.76  0.20  0.44  5.93  0.31  110.91   \n",
       "10601    54   4  1500  119.37   552  112.68  0.21  0.43  5.92  0.30  110.90   \n",
       "11101    55   4  2000  119.33   551  112.65  0.22  0.42  5.92  0.30  110.85   \n",
       "11376    40   5     0  119.30   551  112.63  0.23  0.41  5.91  0.29  110.72   \n",
       "11876    63   5   500  119.24   549  112.59  0.24  0.40  5.90  0.28  110.67   \n",
       "12376    63   5  1000  119.22   548  112.57  0.25  0.39  5.90  0.28  110.64   \n",
       "12876    54   5  1500  119.17   547  112.53  0.26  0.38  5.89  0.27  110.76   \n",
       "13376    54   5  2000  119.13   546  112.50  0.27  0.37  5.88  0.27  110.68   \n",
       "...     ...  ..   ...     ...   ...     ...   ...   ...   ...   ...     ...   \n",
       "100101   31  44     0  116.72   481  110.26  0.94  0.08  5.30  0.21  108.59   \n",
       "100601   54  44   500  116.72   481  110.25  0.94  0.08  5.30  0.21  108.67   \n",
       "101101   60  44  1000  116.71   481  110.25  0.94  0.08  5.30  0.21  108.67   \n",
       "101601   61  44  1500  116.71   481  110.24  0.94  0.08  5.29  0.21  108.58   \n",
       "102101   60  44  2000  116.70   481  110.24  0.94  0.08  5.29  0.21  108.66   \n",
       "102376   45  45     0  116.70   481  110.24  0.94  0.08  5.29  0.21  108.31   \n",
       "102876   58  45   500  116.70   481  110.24  0.94  0.08  5.29  0.21  108.40   \n",
       "103376   57  45  1000  116.69   481  110.23  0.95  0.08  5.29  0.21  108.37   \n",
       "103876   58  45  1500  116.68   480  110.22  0.95  0.08  5.28  0.21  108.45   \n",
       "104376   58  45  2000  116.68   480  110.22  0.95  0.08  5.28  0.21  108.53   \n",
       "104651   33  46     0  116.68   480  110.22  0.95  0.08  5.28  0.21  108.41   \n",
       "105151   62  46   500  116.67   480  110.22  0.95  0.08  5.28  0.21  108.47   \n",
       "105651   61  46  1000  116.67   480  110.22  0.95  0.08  5.28  0.21  108.58   \n",
       "106151   60  46  1500  116.66   480  110.22  0.95  0.08  5.27  0.21  108.45   \n",
       "106651   60  46  2000  116.66   480  110.21  0.95  0.08  5.27  0.21  108.49   \n",
       "106926   36  47     0  116.65   480  110.21  0.95  0.08  5.27  0.21  108.49   \n",
       "107426   59  47   500  116.65   480  110.20  0.96  0.08  5.27  0.21  108.54   \n",
       "107926   59  47  1000  116.65   480  110.20  0.96  0.08  5.27  0.21  108.54   \n",
       "108426   62  47  1500  116.64   480  110.19  0.96  0.08  5.27  0.21  108.53   \n",
       "108926   61  47  2000  116.63   479  110.19  0.96  0.08  5.26  0.21  108.63   \n",
       "109201   35  48     0  116.63   479  110.19  0.96  0.08  5.26  0.21  108.58   \n",
       "109701   65  48   500  116.63   479  110.19  0.96  0.08  5.26  0.21  108.64   \n",
       "110201   62  48  1000  116.63   479  110.18  0.96  0.08  5.26  0.21  108.57   \n",
       "110701   63  48  1500  116.62   479  110.18  0.96  0.08  5.26  0.21  108.58   \n",
       "111201   63  48  2000  116.62   479  110.18  0.97  0.08  5.25  0.21  108.63   \n",
       "111476   35  49     0  116.61   479  110.17  0.97  0.08  5.25  0.21  108.56   \n",
       "111976   63  49   500  116.61   479  110.17  0.97  0.08  5.25  0.21  108.51   \n",
       "112476   63  49  1000  116.60   479  110.16  0.97  0.08  5.25  0.21  108.55   \n",
       "112976   63  49  1500  116.60   479  110.16  0.97  0.07  5.25  0.21  108.54   \n",
       "113476   65  49  2000  116.60   479  110.16  0.97  0.07  5.24  0.21  108.55   \n",
       "\n",
       "          TM                        LM               \n",
       "         PPL     NLL    KL   REG   NLL    KL   Beta  \n",
       "1       1035  116.21  0.40  0.90  9.13  1.42  0.000  \n",
       "501      550  105.81  0.11  0.78  6.77  1.31  0.088  \n",
       "1001     530  105.19  0.07  0.73  6.00  0.73  0.176  \n",
       "1501     526  105.09  0.09  0.68  5.83  0.61  0.264  \n",
       "2001     525  105.07  0.08  0.62  5.77  0.44  0.352  \n",
       "2276     525  105.06  0.11  0.60  5.75  0.39  0.400  \n",
       "2776     528  105.11  0.11  0.54  5.73  0.32  0.488  \n",
       "3276     525  105.07  0.12  0.50  5.72  0.28  0.576  \n",
       "3776     522  104.96  0.12  0.46  5.71  0.22  0.664  \n",
       "4276     521  104.91  0.14  0.43  5.70  0.18  0.752  \n",
       "4551     522  104.93  0.15  0.41  5.70  0.17  0.800  \n",
       "5051     520  104.87  0.16  0.40  5.69  0.14  0.888  \n",
       "5551     521  104.91  0.16  0.37  5.69  0.13  0.976  \n",
       "6051     520  104.89  0.16  0.35  5.68  0.11  1.000  \n",
       "6551     519  104.83  0.18  0.31  5.68  0.10  1.000  \n",
       "6826     519  104.81  0.18  0.30  5.68  0.11  1.000  \n",
       "7326     520  104.83  0.20  0.28  5.68  0.09  1.000  \n",
       "7826     517  104.79  0.20  0.27  5.67  0.10  1.000  \n",
       "8326     518  104.81  0.20  0.26  5.67  0.10  1.000  \n",
       "8826     517  104.75  0.22  0.25  5.67  0.10  1.000  \n",
       "9101     518  104.83  0.23  0.24  5.67  0.09  1.000  \n",
       "9601     512  104.59  0.26  0.22  5.66  0.09  1.000  \n",
       "10101    514  104.68  0.27  0.20  5.66  0.09  1.000  \n",
       "10601    513  104.67  0.28  0.19  5.66  0.11  1.000  \n",
       "11101    513  104.60  0.30  0.18  5.66  0.11  1.000  \n",
       "11376    510  104.58  0.31  0.17  5.66  0.12  0.000  \n",
       "11876    508  104.48  0.36  0.16  5.65  0.14  0.088  \n",
       "12376    508  104.43  0.38  0.15  5.65  0.15  0.176  \n",
       "12876    511  104.52  0.39  0.15  5.65  0.16  0.264  \n",
       "13376    507  104.42  0.40  0.15  5.65  0.18  0.352  \n",
       "...      ...     ...   ...   ...   ...   ...    ...  \n",
       "100101   453  102.50  1.10  0.02  4.74  0.23  1.000  \n",
       "100601   457  102.61  1.09  0.02  4.74  0.22  1.000  \n",
       "101101   457  102.60  1.09  0.02  4.74  0.22  1.000  \n",
       "101601   453  102.51  1.10  0.02  4.74  0.22  1.000  \n",
       "102101   455  102.58  1.10  0.02  4.73  0.23  1.000  \n",
       "102376   451  102.44  1.11  0.02  4.74  0.24  0.000  \n",
       "102876   454  102.53  1.10  0.02  4.73  0.23  0.088  \n",
       "103376   452  102.49  1.10  0.02  4.73  0.23  0.176  \n",
       "103876   453  102.53  1.11  0.02  4.73  0.24  0.264  \n",
       "104376   456  102.63  1.09  0.02  4.71  0.23  0.352  \n",
       "104651   452  102.47  1.10  0.02  4.72  0.24  0.400  \n",
       "105151   452  102.50  1.11  0.02  4.73  0.24  0.488  \n",
       "105651   457  102.59  1.11  0.02  4.72  0.25  0.576  \n",
       "106151   452  102.44  1.12  0.02  4.72  0.24  0.664  \n",
       "106651   450  102.49  1.10  0.01  4.71  0.23  0.752  \n",
       "106926   453  102.48  1.10  0.02  4.71  0.22  0.800  \n",
       "107426   453  102.50  1.11  0.01  4.71  0.23  0.888  \n",
       "107926   453  102.48  1.12  0.02  4.71  0.22  0.976  \n",
       "108426   450  102.45  1.11  0.02  4.71  0.25  1.000  \n",
       "108926   454  102.58  1.11  0.01  4.70  0.22  1.000  \n",
       "109201   453  102.51  1.11  0.01  4.71  0.24  1.000  \n",
       "109701   457  102.60  1.10  0.01  4.70  0.22  1.000  \n",
       "110201   453  102.53  1.11  0.01  4.70  0.22  1.000  \n",
       "110701   453  102.56  1.11  0.01  4.69  0.21  1.000  \n",
       "111201   455  102.58  1.11  0.01  4.69  0.23  1.000  \n",
       "111476   453  102.50  1.12  0.01  4.70  0.24  1.000  \n",
       "111976   452  102.45  1.12  0.01  4.69  0.23  1.000  \n",
       "112476   454  102.51  1.11  0.01  4.69  0.22  1.000  \n",
       "112976   453  102.50  1.12  0.01  4.68  0.22  1.000  \n",
       "113476   454  102.50  1.12  0.01  4.69  0.23  1.000  \n",
       "\n",
       "[250 rows x 18 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Output sentences for each topic-----------\n",
      "Item idx: B000VB7EFW\n",
      "0  BOW: ! love color ... perfect recommend perfectly cute buy ordered\n",
      "0  SENTENCE: it is a great bag\n",
      "1  BOW: plug rip vaio functionality squeeze floor searching sewn abuse local\n",
      "1  SENTENCE: it is a great bag\n",
      "2  BOW: cover color bottom keyboard mac pro hard top scratches apple\n",
      "2  SENTENCE: it is a great bag\n",
      "3  BOW: broke neoprene thin foam loose started chromebook month returned close\n",
      "3  SENTENCE: it is a great bag\n",
      "4  BOW: bought & ; price quality nice 'm time perfectly buy\n",
      "4  SENTENCE: it is a great bag\n",
      "5  BOW: inside made - back zipper big inch put padding 've\n",
      "5  SENTENCE: it is a great bag\n",
      "6  BOW: sleeve protection material zipper months tight snug inch fine bit\n",
      "6  SENTENCE: it is a great bag\n",
      "7  BOW: flap logic stitching asus toshiba acer zip inches usb memory\n",
      "7  SENTENCE: it is a great bag\n",
      "8  BOW: carry perfect nice ipad handle work size carrying charger extra\n",
      "8  SENTENCE: it is a great bag\n",
      "9  BOW: pockets room pocket strap shoulder small compartment straps plenty space\n",
      "9  SENTENCE: it is a great bag\n",
      "-----------Summaries-----------\n",
      "SUMMARY 0 : \n",
      " This is a very well made bag, nice construction, lots of pockets.\n",
      "the straps are very comfortable.\n",
      "and protects everything inside.\n",
      "It says\n",
      "it fits a 17inch notebook,\n",
      "however it did not.\n",
      "after using the pack for less than a month,\n",
      "it is ripping out already.\n",
      "SUMMARY 1 : \n",
      " This is a very well made bag, nice construction, lots of pockets.\n",
      "The quality is excellent\n",
      "and it is very durable.\n",
      "the straps are very comfortable.\n",
      "and protects everything inside.\n",
      "The color is a true red\n",
      "and it fits nicely.\n",
      "it is ripping out already.\n",
      "and it doesnt fit.\n",
      "It's just not the lightest backpack\n",
      "because some zipper teeth were not aligned.\n",
      "SUMMARY 2 : \n",
      " The quality is excellent\n",
      "and I love all the pockets and compartments.\n",
      "and it is very durable.\n",
      "and can't beleive the price\n",
      "and protects everything inside.\n",
      "The laptop\n",
      "doesn't fit in it.\n",
      "it is ripping out already.\n",
      "It's just not the lightest backpack\n",
      "0 TRUE: i researched other models\n",
      "0 PRED: i have this laptop a and , and\n",
      "1 TRUE: i thought i had settled on one , but then something just told me to buy this one\n",
      "1 PRED: i , it have to for the of\n",
      "2 TRUE: i do not regret the decision at all\n",
      "2 PRED: i is n't fit the\n",
      "3 TRUE: i really like this stand\n",
      "3 PRED: i have not it case it and\n",
      "4 TRUE: it is durable , i can leave my <unk> # ipad # cover on , and the ipad still fits comfortably on the stand\n",
      "4 PRED: i is a the and was have it laptop\n",
      "5 TRUE: i can also leave the ipad # plugged in while on the stand without having to turn it upside down\n",
      "5 PRED: i bought have the the bag and `` and the i the case\n",
      "6 TRUE: the pictures do not show it , but the stand does have a setting that places it on a small incline for typing information comfortably into the ipad\n",
      "6 PRED: i only is n't have the\n",
      "7 TRUE: yes , this product is heavy , but i see this as a positive because there are metal parts , improving the durability and stability of the stand\n",
      "7 PRED: i , the bag a a for and i have n't for a\n",
      "8 TRUE: i most certainly purchase this stand again , if i needed another one\n",
      "8 PRED: i bought this recommend it bag\n",
      "9 TRUE: great quality & product but\n",
      "9 PRED: i , for #\n",
      "10 TRUE: while the width was fine , it is not deep enough to cover the hp pavillion # series laptops\n",
      "10 PRED: i is case of a\n",
      "11 TRUE: this is the perfect sleeve for additional protection from dust , shock , etc . for your laptop\n",
      "11 PRED: i is a a and and the\n",
      "12 TRUE: i needed it so i could carry my laptop in my leather briefcase that has no padding and was not designed for laptops\n",
      "12 PRED: i have this for it was n't it laptop and the laptop\n",
      "13 TRUE: it would have been perfect if it had only been deeper\n",
      "13 PRED: i 's be a a\n",
      "14 TRUE: it was shallow by about # inches\n",
      "14 PRED: i is a and the the # ,\n",
      "15 TRUE: if anyone knows of a neoprene sleeve that will fit the <unk> # series # `` post a note\n",
      "15 PRED: i you had it the , , , i be the laptop\n",
      "16 TRUE: this product is pricey , but worth the extra money\n",
      "16 PRED: i is is a and and i it color of\n",
      "17 TRUE: this backpack is large , roomy and well made\n",
      "17 PRED: i is is great for and , has\n",
      "18 TRUE: my # yr . old son -lrb- who routinely <unk> his backpack -rrb- has finally met a product that he can not break or damage through over use\n",
      "18 PRED: i is . # # # is # is a to #\n",
      "19 TRUE: this item is well designed and very sturdy\n",
      "19 PRED: i is is the\n",
      "20 TRUE: i do n't know if it is really ready for afghanistan -lrb- as <unk> to , by another user -rrb- but it is ready for high <unk>\n",
      "20 PRED: i have this for it it was a good\n",
      "21 TRUE: the side pockets could be larger and sealed for increased all weather <unk> minor gripe - but overall i give it very high marks\n",
      "21 PRED: i case is is be a for the\n",
      "22 TRUE: i wanted a hard case to fit my nook color in while still keeping the ereader in it 's cover as well\n",
      "22 PRED: i have this # of and carry my # and\n",
      "23 TRUE: but i wanted a case that had either a front zipper pocket or front velcro pocket to fit the charger and usb cable in it as well at the same time while i travel\n",
      "23 PRED: i is have this # for the a to few\n",
      "24 TRUE: while this is not really a `` hard `` case , it is stiff and protects the ereader nicely\n",
      "24 PRED: i i the a a good great\n",
      "25 TRUE: i have zero complaints\n",
      "25 PRED: i have a a to the and and and and\n",
      "26 TRUE: i highly recommend this item\n",
      "26 PRED: i have recommend this case\n",
      "27 TRUE: until barnes and noble or someone else can come up with a travel case similar to this for the nook color , this is definitely the way to go\n",
      "27 PRED: i is is the , the\n",
      "28 TRUE: after looking all over the internet and in brick & mortar stores , i finally found a messenger bag that was perfect for me\n",
      "28 PRED: i the for the the price\n",
      "29 TRUE: it has two side pockets , two front pockets and a main compartment that zips and an inner pocket in the main compartment that also zips\n",
      "29 PRED: i is a of pockets , the of ,\n",
      "30 TRUE: it has large , but shallow pocket on the rear of the bag -lrb- great for carrying a newspaper or magazine .\n",
      "30 PRED: i 's a a pockets the the the\n",
      "31 TRUE: my one little gripe would be that it does n't have a small handle at the <unk> of the bag so you can pick it up without using the main strap\n",
      "31 PRED: i is is is , the the is is n't fit a little of for the laptop\n",
      "32 TRUE: not enough of an issue to dock it any stars though\n",
      "32 PRED: i a the the # , the\n",
      "33 TRUE: this was a perfect little case for my boyfriend 's laptop\n",
      "33 PRED: i is a little for than\n",
      "34 TRUE: it 's soft but still a durable material and the handles makes it easy to carry\n",
      "34 PRED: i is a and\n",
      "35 TRUE: it is really sleek so you can slide it in your bag or just hold it under your arm\n",
      "35 PRED: i is a a , i\n",
      "36 TRUE: i liked the white interior\n",
      "36 PRED: i have it bag and and , and\n",
      "37 TRUE: for the price you can not beat the value\n",
      "37 PRED: i the price , have to to it laptop\n",
      "38 TRUE: it is n't pretty but it works\n",
      "38 PRED: i is a a a\n",
      "39 TRUE: i work in <unk> so does n't matter , its so very dirty there\n",
      "39 PRED: i was it the and i n't have it but i i happy\n",
      "40 TRUE: but for most , i would say order a color other then <unk>\n",
      "40 PRED: i i the , i 'm n't it it it\n",
      "41 TRUE: this is a hand , strong useful ... but ugly\n",
      "41 PRED: i is a great for but ,\n",
      "42 TRUE: received the keyboard promptly , but was disappointed it was a <unk> keyboard\n",
      "42 PRED: i is bag is is the is a\n",
      "43 TRUE: the case is nice , keeps the tablet still a compact traveling size and does the job\n",
      "43 PRED: i case is a the and the case , the little\n",
      "44 TRUE: but , the <unk> did n't allow for speedy typing , and was an <unk> in using it\n",
      "44 PRED: i i is is the\n",
      "45 TRUE: i ended up returning it\n",
      "45 PRED: i is the the the\n",
      "46 TRUE: <unk> leather case for apple new macbook air # `` - book type -lrb- black\n",
      "46 PRED: i is is is my #\n",
      "47 TRUE: stylish protector for macbook air\n",
      "47 PRED: i , , my pro\n",
      "48 TRUE: it fits perfectly giving ample protection yet still shows the beauty of your mac\n",
      "48 PRED: i 's a # and #\n",
      "49 TRUE: highly recommended for macbook air owners\n",
      "49 PRED: i is it my pro\n",
      "50 TRUE: i have a caselogic case , so i needed more sleeves and ordered these\n",
      "50 PRED: i is a little bag for the i is it to\n",
      "51 TRUE: the sleeves work well , however , the <unk> holes are not at all the same locations as our case , of the same brand\n",
      "51 PRED: i only is the\n",
      "52 TRUE: i used a single punch and just made my own\n",
      "52 PRED: i is the lot of for i to a\n",
      "53 TRUE: the bag design is excellent and very convenience , however the quality and craftmanship is very poor\n",
      "53 PRED: i is is for a and and well\n",
      "54 TRUE: with just # days of use it started to break the front pocket\n",
      "54 PRED: i the a . , the ,\n",
      "55 TRUE: i contacted the vendor i twice agreed on send me a new one , but never receive it\n",
      "55 PRED: i have the the of have it\n",
      "56 TRUE: when the product first arrived it looked small , but when i measured it the dimensions were as specified\n",
      "56 PRED: i i case is the is is\n",
      "57 TRUE: it fits snugly on my laptop and looks nice\n",
      "57 PRED: i is my and the # and it the\n",
      "58 TRUE: looks high quality enough to last for a while\n",
      "58 PRED: i a , and to\n",
      "59 TRUE: this case is compact but at the same time holds all of the extra accessories that i need like a mouse , adaptor , pens and a few more things\n",
      "59 PRED: i is is great and the the price of\n",
      "60 TRUE: it is lightweight yet seems to be very durable\n",
      "60 PRED: i is a a , to be\n",
      "61 TRUE: the apple logo does n't shine as bright as the picture does\n",
      "61 PRED: i case is is n't fit the the\n",
      "62 TRUE: the little black circles on the bottom that hold the laptop off of the surface that it 's on are slowly coming out of their place but i do n't pick at them so they are fine for now\n",
      "62 PRED: i case thing is , the case is is the laptop\n"
     ]
    }
   ],
   "source": [
    "if len(log_df) == 0:\n",
    "    cmd_rm = 'rm -r %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_rm.split())\n",
    "\n",
    "    cmd_mk = 'mkdir %s' % config.modeldir\n",
    "    res = subprocess.call(cmd_mk.split())\n",
    "\n",
    "time_start = time.time()\n",
    "while epoch < config.epochs:\n",
    "    for ct, batch in train_batches:\n",
    "        feed_dict = get_feed_dict(batch)\n",
    "\n",
    "        _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch, sent_loss_kl_categ_batch, sent_loss_kl_gmm_batch, ppls_batch = \\\n",
    "        sess.run([opt, loss, topic_loss_recon, topic_loss_kl, topic_loss_reg, sent_loss_recon, sent_loss_kl, sent_loss_kl_categ, sent_loss_kl_gmm, topic_ppls], feed_dict = feed_dict)\n",
    "   \n",
    "        if sent_loss_kl_batch == np.inf:\n",
    "            print('Nan occured')\n",
    "            ckpt = tf.train.get_checkpoint_state(config.modeldir)\n",
    "            model_checkpoint_path = ckpt.all_model_checkpoint_paths[-1]\n",
    "            saver.restore(sess, model_checkpoint_path)            \n",
    "            break\n",
    "            \n",
    "        losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, sent_loss_recon_batch, sent_loss_kl_batch]]\n",
    "        ppls_train += list(ppls_batch)\n",
    "\n",
    "        if ct%config.log_period==0:\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train, sent_loss_recon_train, sent_loss_kl_train = np.mean(losses_train, 0)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, sent_loss_recon_dev, sent_loss_kl_dev, ppl_dev = get_loss(sess, dev_batches)\n",
    "            global_step_log, beta_eval = sess.run([tf.train.get_global_step(), beta])\n",
    "            \n",
    "            if loss_dev < loss_min:\n",
    "                loss_min = loss_dev\n",
    "                saver.save(sess, config.modelpath, global_step=global_step_log)\n",
    "\n",
    "            clear_output()\n",
    "    \n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_reg_train, '%.2f'%sent_loss_recon_train, '%.2f'%sent_loss_kl_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_reg_dev, '%.2f'%sent_loss_recon_dev, '%.2f'%sent_loss_kl_dev,  '%.3f'%beta_eval],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            display(log_df)\n",
    "\n",
    "            print_summary(test_batches[1][1])\n",
    "            print_sample(batch)\n",
    "            \n",
    "            time_start = time.time()\n",
    "            \n",
    "    epoch += 1\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirm variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "_prob_topic, _prob_topic_sents, _prob_topic_infer, _means_topic_infer = debug_value([prob_topic, prob_topic_sents, prob_topic_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.07953417, 0.03046988, 0.08800525, 0.08928147, 0.21003477,\n",
       "        0.12841846, 0.08533092, 0.05192988, 0.10679288, 0.13020231],\n",
       "       dtype=float32),\n",
       " array([0.03983815, 0.03048858, 0.05354526, 0.09184394, 0.1779047 ,\n",
       "        0.14580864, 0.08479333, 0.0659744 , 0.11976699, 0.19003609],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_i = 4\n",
    "_prob_topic_sents[batch_i], _prob_topic_infer[batch_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.2152851 , -0.7954707 , -0.13175818,  0.5613478 ],\n",
       "       [-0.17747657, -0.768313  , -0.11121741,  0.5308447 ],\n",
       "       [-0.18701425, -0.7767259 , -0.11736107,  0.542892  ],\n",
       "       [-0.17468357, -0.7817751 , -0.11434852,  0.53876525],\n",
       "       [-0.18690863, -0.7800032 , -0.11768776,  0.5405479 ],\n",
       "       [-0.17249909, -0.7702862 , -0.10337244,  0.53890795],\n",
       "       [-0.184178  , -0.78831995, -0.11584231,  0.53070223],\n",
       "       [-0.16777574, -0.76620877, -0.10552885,  0.5318073 ],\n",
       "       [-0.16172384, -0.77200407, -0.11794638,  0.5309891 ],\n",
       "       [-0.16318968, -0.77438337, -0.10628442,  0.5325533 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_means_topic_infer[0][:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_means_topic, b_means_topic = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"topic/dec/mean_topic\")\n",
    "\n",
    "pred_topic_embeddings, pred_topic_bow, pred_means_topic, pred_logvars_topic, pred_token_idxs, _w_means_topic, _b_means_topic, _w_mean_topic_infer = \\\n",
    "                                sess.run([topic_embeddings, topic_bow, means_topic, logvars_topic, topic_beam_output_token_idxs, w_means_topic, b_means_topic, w_mean_topic_infer], \n",
    "                                         feed_dict={t_variables['batch_l']: config.n_topic, t_variables['keep_prob']: 1.,})\n",
    "\n",
    "pred_sents = idxs_to_sents(pred_token_idxs, config, idx_to_word)\n",
    "\n",
    "pred_topics_freq_bow_indices = np.argsort(pred_topic_bow, 1)[:, ::-1][:, :10]\n",
    "pred_topics_freq_bow_idxs = bow_idxs[pred_topics_freq_bow_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', 'love', 'color', '...', 'recommend', 'perfect', 'perfectly', 'cute', 'ordered', 'buy']\n",
      "['plug', 'vaio', 'rip', 'resistant', 'searching', 'squeeze', 'classy', 'functionality', 'local', 'abuse']\n",
      "['cover', 'color', 'keyboard', 'bottom', 'mac', 'pro', 'top', 'hard', 'apple', 'scratches']\n",
      "['broke', 'neoprene', 'thin', 'loose', 'foam', 'chromebook', 'started', 'month', 'close', 'returned']\n",
      "[';', 'bought', '&', 'price', 'quality', 'nice', \"'m\", 'time', 'buy', 'perfectly']\n",
      "['inside', 'made', '-', 'back', 'zipper', 'big', 'put', 'inch', \"'ve\", 'padding']\n",
      "['sleeve', 'protection', 'material', 'zipper', 'months', 'tight', 'snug', 'fine', 'inch', ';']\n",
      "['flap', 'asus', 'logic', 'stitching', 'toshiba', 'acer', 'zip', 'inches', 'usb', 'memory']\n",
      "['carry', 'perfect', 'nice', 'ipad', 'work', 'handle', 'size', 'charger', 'carrying', 'extra']\n",
      "['pockets', 'room', 'pocket', 'strap', 'shoulder', 'small', 'compartment', 'straps', 'plenty', 'items']\n"
     ]
    }
   ],
   "source": [
    "for idxs in pred_topics_freq_bow_idxs:\n",
    "    print([idx_to_word[idx] for idx in idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07030167,  0.34280053,  0.5620051 , -0.36249042, -0.37130642,\n",
       "        -0.35529038, -0.52851105,  0.2897714 , -0.09859769,  0.02688825],\n",
       "       [ 0.00454117, -0.14383823, -0.4358602 , -0.3000859 ,  0.36200735,\n",
       "         0.6090798 ,  0.04143322,  0.25105503,  0.39387962, -0.2521794 ],\n",
       "       [ 0.51439863, -0.5203031 ,  0.26585394,  0.18821827,  0.04780925,\n",
       "        -0.2502492 , -0.31570742, -0.09478665, -0.15000348,  0.186701  ],\n",
       "       [ 0.5495507 , -0.37986752,  0.37048703, -0.2601502 , -0.09304344,\n",
       "         0.15158871, -0.40401134,  0.15019475,  0.20674033,  0.04148453],\n",
       "       [ 0.5675219 ,  0.3590145 ,  0.06957109, -0.36036927, -0.4935817 ,\n",
       "        -0.5302313 , -0.23160999, -0.39796302, -0.527676  ,  0.41196147],\n",
       "       [-0.03068164,  0.42594528, -0.0811015 ,  0.53738195, -0.53529024,\n",
       "        -0.43130898,  0.26999566,  0.04198602, -0.488762  ,  0.5043671 ],\n",
       "       [ 0.46481928,  0.03554478,  0.32813412, -0.23864001, -0.30504122,\n",
       "        -0.17199972,  0.2143851 , -0.01828023, -0.31219226,  0.40960118],\n",
       "       [-0.34314156, -0.58172077, -0.1488965 ,  0.35319465,  0.08103113,\n",
       "         0.2948137 ,  0.34469905, -0.1270156 ,  0.2501823 , -0.12113991],\n",
       "       [-0.2431757 , -0.10950624, -0.21328864,  0.45708016, -0.51859564,\n",
       "        -0.3251262 ,  0.49893737, -0.42454964, -0.2941471 ,  0.49275428],\n",
       "       [ 0.07914881,  0.30881578,  0.20016417,  0.47784787, -0.3170194 ,\n",
       "        -0.3231789 ,  0.3750319 , -0.35375553,  0.17877898,  0.25367454]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_topic_embeddings[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.70199955e-01, 3.15083051e-03, 5.96725731e-04, ...,\n",
       "        5.76106868e-06, 7.55719593e-05, 2.34827962e-06],\n",
       "       [3.09307943e-05, 8.20664536e-06, 1.75214736e-04, ...,\n",
       "        1.94706372e-04, 1.93003834e-05, 3.84568446e-03],\n",
       "       [2.76639767e-04, 1.80042302e-03, 3.83189268e-04, ...,\n",
       "        1.13867945e-05, 3.29279865e-05, 5.01591921e-06],\n",
       "       ...,\n",
       "       [2.29592001e-06, 5.66102390e-05, 4.39598400e-04, ...,\n",
       "        1.38007279e-04, 2.17028172e-03, 4.33204434e-04],\n",
       "       [7.07316940e-05, 4.76617366e-04, 1.21361249e-04, ...,\n",
       "        1.57259579e-04, 9.18528438e-03, 2.12362811e-05],\n",
       "       [6.60414298e-06, 6.34977114e-05, 7.13269255e-05, ...,\n",
       "        3.65038170e-03, 1.73785468e-03, 5.41338581e-04]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_topic_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03424398,  0.0096985 , -0.06413093, ...,  0.03797114,\n",
       "        -0.01338133, -0.01297894],\n",
       "       [-0.03279095,  0.05930585,  0.03451761, ..., -0.02780341,\n",
       "        -0.10020281,  0.03512232],\n",
       "       [ 0.03666572,  0.08548579, -0.00936586, ..., -0.06135559,\n",
       "        -0.09365493,  0.00985266],\n",
       "       ...,\n",
       "       [-0.04459329, -0.00699388, -0.007137  , ...,  0.10423539,\n",
       "        -0.00303482,  0.028828  ],\n",
       "       [-0.02674356, -0.01834767,  0.04752089, ...,  0.05400897,\n",
       "        -0.02181414, -0.06615067],\n",
       "       [ 0.07697956,  0.05168388,  0.07922779, ...,  0.09811964,\n",
       "        -0.01671151, -0.01895013]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_w_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01163579, -0.31689972, -0.06394058,  0.19231673, -0.1380821 ,\n",
       "        0.24288706,  0.2933419 , -0.03508246,  0.08912257, -0.26633072,\n",
       "       -0.37472242, -0.06411445, -0.38825554,  0.02678456, -0.05968409,\n",
       "       -0.20403576, -0.02953517,  0.0643271 , -0.09376531, -0.00238578,\n",
       "       -0.14848402,  0.39407536, -0.13403031,  0.24029757, -0.17921838,\n",
       "       -0.01551062, -0.1424737 ,  0.23313083,  0.19225577, -0.30093712,\n",
       "       -0.11009891, -0.01441393], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.20777631, -0.7880086 , -0.12567243,  0.56142694, -0.327168  ,\n",
       "         0.60216874,  0.7404998 , -0.23602808,  0.289317  , -0.58346194,\n",
       "        -1.0768209 , -0.08863895, -1.237956  ,  0.13280813, -0.09187369,\n",
       "        -0.33609003, -0.1595888 ,  0.15013331, -0.20481265, -0.01800749,\n",
       "        -0.4365478 ,  1.2110704 , -0.45148912,  0.688657  , -0.3711225 ,\n",
       "        -0.06612568, -0.353558  ,  0.5758501 ,  0.44075334, -0.8059681 ,\n",
       "        -0.25110504, -0.09649536],\n",
       "       [-0.17156062, -0.7611089 , -0.11639434,  0.5273537 , -0.30693853,\n",
       "         0.57691735,  0.6997352 , -0.2109155 ,  0.2612798 , -0.53453314,\n",
       "        -1.0336132 , -0.07709154, -1.1726186 ,  0.10545208, -0.086261  ,\n",
       "        -0.33172426, -0.13553052,  0.16665927, -0.16889113, -0.00922497,\n",
       "        -0.39735556,  1.1698847 , -0.4418127 ,  0.66840285, -0.36015117,\n",
       "        -0.07463156, -0.35274646,  0.5593424 ,  0.4279335 , -0.77467144,\n",
       "        -0.2585973 , -0.07788712],\n",
       "       [-0.18116352, -0.76906085, -0.11618015,  0.54016614, -0.30886608,\n",
       "         0.58061796,  0.7110631 , -0.21863566,  0.2756563 , -0.5483674 ,\n",
       "        -1.042819  , -0.08032898, -1.1976655 ,  0.11106279, -0.09263518,\n",
       "        -0.3256148 , -0.14587039,  0.16146478, -0.17594256, -0.01516185,\n",
       "        -0.4009857 ,  1.184826  , -0.4488337 ,  0.6808503 , -0.36321628,\n",
       "        -0.0657055 , -0.3564292 ,  0.55981535,  0.4391356 , -0.78756887,\n",
       "        -0.26009065, -0.07761481],\n",
       "       [-0.17223716, -0.7641872 , -0.10902344,  0.5295489 , -0.3101349 ,\n",
       "         0.58262825,  0.7030324 , -0.20922263,  0.26135206, -0.5343384 ,\n",
       "        -1.0397003 , -0.07880907, -1.1803055 ,  0.10449354, -0.09067526,\n",
       "        -0.33286577, -0.13909023,  0.16363412, -0.17383838, -0.00889234,\n",
       "        -0.39900973,  1.1760454 , -0.4477577 ,  0.67139083, -0.3558131 ,\n",
       "        -0.07089961, -0.35286334,  0.564123  ,  0.43263447, -0.7792728 ,\n",
       "        -0.2582533 , -0.07642874],\n",
       "       [-0.18314986, -0.77246535, -0.11323328,  0.53559995, -0.3151626 ,\n",
       "         0.5806635 ,  0.71624696, -0.2172509 ,  0.26199827, -0.5410957 ,\n",
       "        -1.0558634 , -0.09208576, -1.1906624 ,  0.10476287, -0.08386711,\n",
       "        -0.3340008 , -0.13705933,  0.16447757, -0.17630483, -0.01617595,\n",
       "        -0.40710297,  1.1964004 , -0.45371485,  0.67906195, -0.37513727,\n",
       "        -0.07902989, -0.36655372,  0.56602573,  0.43544337, -0.7875031 ,\n",
       "        -0.26561463, -0.07428841],\n",
       "       [-0.16493736, -0.7612814 , -0.10607868,  0.52777034, -0.30767655,\n",
       "         0.5769212 ,  0.70950615, -0.20655309,  0.25625113, -0.53292704,\n",
       "        -1.0418172 , -0.08122528, -1.174434  ,  0.10078888, -0.08739989,\n",
       "        -0.3334903 , -0.1376653 ,  0.16603276, -0.17426115, -0.0054038 ,\n",
       "        -0.39906618,  1.1766497 , -0.44518247,  0.66664654, -0.35174906,\n",
       "        -0.07237518, -0.34932092,  0.5583883 ,  0.43167478, -0.7860851 ,\n",
       "        -0.2567122 , -0.07700241],\n",
       "       [-0.18358143, -0.77745885, -0.1121548 ,  0.5274612 , -0.30909353,\n",
       "         0.57604575,  0.7017652 , -0.21661042,  0.26888725, -0.54302716,\n",
       "        -1.0401008 , -0.08026591, -1.1799233 ,  0.10071075, -0.07259351,\n",
       "        -0.31669846, -0.13741466,  0.16112551, -0.17919788, -0.01075562,\n",
       "        -0.40416917,  1.1794456 , -0.4481578 ,  0.6820454 , -0.3550316 ,\n",
       "        -0.06003695, -0.36365402,  0.5642846 ,  0.4392792 , -0.77334285,\n",
       "        -0.2636246 , -0.08096559],\n",
       "       [-0.17045112, -0.7609225 , -0.10780662,  0.5278978 , -0.30998504,\n",
       "         0.5776644 ,  0.6987504 , -0.20589164,  0.25916138, -0.53692544,\n",
       "        -1.0339388 , -0.07803081, -1.1715336 ,  0.10370842, -0.08552373,\n",
       "        -0.3334723 , -0.13705689,  0.17214689, -0.17283586, -0.00675977,\n",
       "        -0.3942558 ,  1.1668969 , -0.44222054,  0.6641817 , -0.35790133,\n",
       "        -0.07587828, -0.35021347,  0.5619865 ,  0.43145832, -0.7743662 ,\n",
       "        -0.25755996, -0.07530789],\n",
       "       [-0.15490769, -0.7623619 , -0.11353825,  0.52425504, -0.31013793,\n",
       "         0.57725734,  0.7081411 , -0.21078971,  0.25337327, -0.52638113,\n",
       "        -1.0378671 , -0.08193106, -1.1759174 ,  0.09942528, -0.08295649,\n",
       "        -0.32994837, -0.13102923,  0.17406422, -0.1747709 , -0.00957677,\n",
       "        -0.3950378 ,  1.1739106 , -0.44814256,  0.6740878 , -0.3611704 ,\n",
       "        -0.08370163, -0.35439903,  0.5677383 ,  0.43178046, -0.7866634 ,\n",
       "        -0.26003695, -0.07382159],\n",
       "       [-0.15804821, -0.7637867 , -0.10911925,  0.52307314, -0.31162286,\n",
       "         0.580521  ,  0.70060873, -0.2009563 ,  0.25356466, -0.52299815,\n",
       "        -1.0320975 , -0.07597532, -1.1695514 ,  0.09872495, -0.08956133,\n",
       "        -0.33458215, -0.13008742,  0.1673021 , -0.16934258, -0.00797342,\n",
       "        -0.3905491 ,  1.1786226 , -0.44479927,  0.6731226 , -0.3577113 ,\n",
       "        -0.08079331, -0.3562538 ,  0.56167465,  0.42412546, -0.780318  ,\n",
       "        -0.25429034, -0.06878549]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.49821369e-02, -1.74026974e-02, -1.21583277e-03,\n",
       "         6.53449260e-03, -1.54133858e-02, -2.18948219e-02,\n",
       "        -1.90638546e-02,  1.32693239e-02,  1.11634033e-02,\n",
       "        -1.93855334e-02],\n",
       "       [-7.75512168e-03, -8.57588742e-03,  1.91637203e-02,\n",
       "         6.88495440e-03,  5.73747957e-06, -1.56788658e-02,\n",
       "         2.55801971e-03,  6.88684639e-03,  2.42787339e-02,\n",
       "         1.79753304e-02],\n",
       "       [ 4.94250655e-03, -1.07696308e-02, -1.53253425e-03,\n",
       "         1.99725777e-02, -1.92840025e-02, -1.49392604e-03,\n",
       "         1.78743172e-02,  7.22448155e-03,  1.98036060e-02,\n",
       "         1.24130938e-02],\n",
       "       [ 1.22360634e-02,  1.82047524e-02,  2.89237418e-04,\n",
       "        -4.22067009e-03, -2.82733538e-03, -4.09162091e-03,\n",
       "        -1.15149086e-02,  2.29803356e-03, -7.73391640e-03,\n",
       "        -1.84050482e-02],\n",
       "       [ 1.64312497e-02,  7.79688545e-03, -2.61567952e-03,\n",
       "        -1.22013697e-02,  9.48593393e-03,  3.44272843e-03,\n",
       "        -1.87890939e-02, -1.10813207e-03,  1.73099767e-02,\n",
       "        -1.08600163e-03],\n",
       "       [ 8.24670482e-04, -1.68375839e-02,  1.33197475e-02,\n",
       "        -1.90603454e-02, -3.29794036e-03, -6.07779250e-03,\n",
       "        -4.29327879e-03,  1.13869095e-02, -1.65915564e-02,\n",
       "        -1.82876624e-02],\n",
       "       [ 2.66914116e-03, -1.40774366e-03,  1.05562396e-02,\n",
       "        -6.40052417e-03, -1.33985803e-02,  2.63693966e-02,\n",
       "        -1.50953289e-02,  1.43189095e-02,  1.98177285e-02,\n",
       "         2.04899218e-02],\n",
       "       [ 1.63925197e-02,  2.55462341e-02, -5.34921745e-03,\n",
       "        -9.95522831e-03,  5.63760754e-03, -2.10041385e-02,\n",
       "        -4.30993875e-03, -2.28823107e-02, -8.41235742e-05,\n",
       "        -1.58724338e-02],\n",
       "       [-1.43980673e-02,  1.57965180e-02, -5.14870393e-04,\n",
       "         1.01220617e-02, -9.93878767e-03,  3.89452279e-03,\n",
       "        -8.36424716e-03,  9.08575300e-03,  2.20653620e-02,\n",
       "         2.78446507e-02],\n",
       "       [-1.68324802e-02,  2.84798979e-03,  9.01852548e-03,\n",
       "        -7.80768646e-03,  1.93685777e-02,  8.68271664e-03,\n",
       "         1.69014428e-02, -1.36431521e-02, -2.89252843e-03,\n",
       "         7.08283670e-03]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_w_mean_topic_infer[:, :10, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01163579, -0.31689972, -0.06394058,  0.19231673, -0.1380821 ,\n",
       "        0.24288706,  0.2933419 , -0.03508246,  0.08912257, -0.26633072,\n",
       "       -0.37472242, -0.06411445, -0.38825554,  0.02678456, -0.05968409,\n",
       "       -0.20403576, -0.02953517,  0.0643271 , -0.09376531, -0.00238578,\n",
       "       -0.14848402,  0.39407536, -0.13403031,  0.24029757, -0.17921838,\n",
       "       -0.01551062, -0.1424737 ,  0.23313083,  0.19225577, -0.30093712,\n",
       "       -0.11009891, -0.01441393], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_b_means_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enc_state_infer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-ee7c3cd147b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_enc_state_infer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_means_topic_infer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdebug_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menc_state_infer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans_topic_infer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'enc_state_infer' is not defined"
     ]
    }
   ],
   "source": [
    "_enc_state_infer, _means_topic_infer = debug_value([enc_state_infer, means_topic_infer], return_value=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_enc_state_infer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_means_topic_infer[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
